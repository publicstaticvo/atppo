[2023-01-16 01:39:47,821.821 dlc26te6b6pxn0nk-master-0:40 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 01:39:48,664.664 dlc26te6b6pxn0nk-master-0:108 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:39:48,664.664 dlc26te6b6pxn0nk-master-0:105 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:39:48,700.700 dlc26te6b6pxn0nk-master-0:107 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:39:48,709.709 dlc26te6b6pxn0nk-master-0:106 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:39:50,299.299 dlc26te6b6pxn0nk-master-0:107 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 01:39:50,300.300 dlc26te6b6pxn0nk-master-0:106 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 01:39:51,270.270 dlc26te6b6pxn0nk-master-0:108 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 01:39:51,270.270 dlc26te6b6pxn0nk-master-0:105 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:105:105 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:107:107 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:106:106 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:108:108 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5940), 0.4922501336183859, 0.8191933240611962, tensor(1.8672)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5940), 0.4922501336183859, 0.8191933240611962, tensor(1.8672)]
[tensor(-0.5546), 0.5163014430785676, 0.8379694019471489, tensor(2.0269)]
[tensor(-0.5365), 0.5339390700160342, 0.8532684283727399, tensor(2.1332)]
[tensor(-0.5365), 0.5339390700160342, 0.8546592489568846, tensor(2.1332)]
[2023-01-16 01:52:13,363.363 dlc26te6b6pxn0nk-master-0:185 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 01:52:14,022.022 dlc26te6b6pxn0nk-master-0:251 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:14,022.022 dlc26te6b6pxn0nk-master-0:252 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:14,024.024 dlc26te6b6pxn0nk-master-0:250 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:14,051.051 dlc26te6b6pxn0nk-master-0:253 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:15,100.100 dlc26te6b6pxn0nk-master-0:253 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 01:52:16,080.080 dlc26te6b6pxn0nk-master-0:252 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 01:52:16,085.085 dlc26te6b6pxn0nk-master-0:251 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 01:52:16,095.095 dlc26te6b6pxn0nk-master-0:250 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:250:250 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:253:253 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:252:252 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:251:251 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5815), 0.5024051309460181, 0.8442280945757997, tensor(1.9306)]
[tensor(-0.5199), 0.5403527525387494, 0.8630041724617524, tensor(2.1818)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5199), 0.5403527525387494, 0.8630041724617524, tensor(2.1818)]
[tensor(-0.5185), 0.5403527525387494, 0.8630041724617524, tensor(2.1818)]
[tensor(-0.5185), 0.5403527525387494, 0.8630041724617524, tensor(2.1818)]
[2023-01-16 02:03:29,855.855 dlc26te6b6pxn0nk-master-0:329 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 02:03:30,499.499 dlc26te6b6pxn0nk-master-0:395 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:03:30,499.499 dlc26te6b6pxn0nk-master-0:396 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:03:30,587.587 dlc26te6b6pxn0nk-master-0:397 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:03:30,593.593 dlc26te6b6pxn0nk-master-0:394 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:03:32,457.457 dlc26te6b6pxn0nk-master-0:396 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 02:03:32,458.458 dlc26te6b6pxn0nk-master-0:395 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 02:03:33,009.009 dlc26te6b6pxn0nk-master-0:397 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 02:03:33,014.014 dlc26te6b6pxn0nk-master-0:394 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:394:394 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:396:396 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:395:395 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:397:397 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7212), 0.44575093532870125, 0.7712100139082059, tensor(1.5076)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5877), 0.5077498663816141, 0.8324061196105702, tensor(1.9510)]
[tensor(-0.5548), 0.5312667022982362, 0.8324061196105702, tensor(2.1016)]
[tensor(-0.5280), 0.5398182789951897, 0.8546592489568846, tensor(2.1711)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5280), 0.5398182789951897, 0.8546592489568846, tensor(2.1711)]
[tensor(-0.5280), 0.5398182789951897, 0.8546592489568846, tensor(2.1711)]
[tensor(-0.5280), 0.5398182789951897, 0.8546592489568846, tensor(2.1711)]
[tensor(-0.5280), 0.5398182789951897, 0.8546592489568846, tensor(2.1711)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5280), 0.5398182789951897, 0.8546592489568846, tensor(2.1711)]
early stopping at 9
[2023-01-16 02:23:55,791.791 dlc26te6b6pxn0nk-master-0:487 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 02:23:56,550.550 dlc26te6b6pxn0nk-master-0:555 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:23:56,586.586 dlc26te6b6pxn0nk-master-0:553 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:23:56,662.662 dlc26te6b6pxn0nk-master-0:552 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:23:56,734.734 dlc26te6b6pxn0nk-master-0:554 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:23:57,914.914 dlc26te6b6pxn0nk-master-0:553 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 02:23:58,004.004 dlc26te6b6pxn0nk-master-0:554 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 02:23:58,447.447 dlc26te6b6pxn0nk-master-0:555 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 02:23:58,451.451 dlc26te6b6pxn0nk-master-0:552 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:552:552 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:555:555 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:553:553 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:554:554 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7355), 0.4494922501336184, 0.6439499304589708, tensor(1.5120)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7136), 0.4494922501336184, 0.7760778859527121, tensor(1.5120)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6690), 0.4537680384820951, 0.7760778859527121, tensor(1.5999)]
[tensor(-0.6291), 0.47140566541956175, 0.7969401947148818, tensor(1.7279)]
[tensor(-0.6080), 0.48583645109567075, 0.7976356050069541, tensor(1.8212)]
[tensor(-0.6053), 0.4911811865312667, 0.8025034770514604, tensor(1.8506)]
[tensor(-0.5899), 0.5136290753607696, 0.8031988873435327, tensor(1.9782)]
[tensor(-0.5845), 0.5136290753607696, 0.8171070931849791, tensor(1.9782)]
[tensor(-0.5835), 0.5136290753607696, 0.8351877607788595, tensor(1.9846)]
[tensor(-0.5835), 0.5136290753607696, 0.8351877607788595, tensor(1.9846)]
[tensor(-0.5756), 0.518439337252806, 0.8351877607788595, tensor(2.0166)]
[tensor(-0.5740), 0.5243185462319615, 0.8351877607788595, tensor(2.0476)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5740), 0.5243185462319615, 0.8351877607788595, tensor(2.0476)]
[tensor(-0.5623), 0.5285943345804383, 0.8358831710709318, tensor(2.0806)]
[tensor(-0.5623), 0.5285943345804383, 0.8400556328233658, tensor(2.0806)]
[tensor(-0.5478), 0.5285943345804383, 0.8400556328233658, tensor(2.0952)]
[tensor(-0.5478), 0.5334045964724746, 0.8400556328233658, tensor(2.1031)]
[tensor(-0.5478), 0.5334045964724746, 0.8400556328233658, tensor(2.1031)]
[tensor(-0.5478), 0.5334045964724746, 0.8400556328233658, tensor(2.1031)]
[tensor(-0.5478), 0.5334045964724746, 0.8414464534075105, tensor(2.1031)]
[tensor(-0.5478), 0.5334045964724746, 0.8414464534075105, tensor(2.1031)]
[tensor(-0.5478), 0.5334045964724746, 0.8414464534075105, tensor(2.1031)]
[tensor(-0.5478), 0.5334045964724746, 0.8414464534075105, tensor(2.1031)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.5478), 0.5334045964724746, 0.8414464534075105, tensor(2.1031)]
[tensor(-0.5478), 0.5334045964724746, 0.8414464534075105, tensor(2.1031)]
early stopping at 25
[2023-01-16 03:19:54,826.826 dlc26te6b6pxn0nk-master-0:697 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 03:19:55,475.475 dlc26te6b6pxn0nk-master-0:764 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:55,481.481 dlc26te6b6pxn0nk-master-0:765 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:55,482.482 dlc26te6b6pxn0nk-master-0:762 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:55,482.482 dlc26te6b6pxn0nk-master-0:763 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:57,406.406 dlc26te6b6pxn0nk-master-0:764 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 03:19:57,511.511 dlc26te6b6pxn0nk-master-0:765 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 03:19:57,513.513 dlc26te6b6pxn0nk-master-0:763 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 03:19:57,518.518 dlc26te6b6pxn0nk-master-0:762 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:762:762 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:765:765 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:763:763 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:764:764 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.6990), 0.4478888295029396, 0.7976356050069541, tensor(1.5405)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5482), 0.5291288081239979, 0.8497913769123783, tensor(2.0974)]
[tensor(-0.5482), 0.5291288081239979, 0.8497913769123783, tensor(2.0974)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5482), 0.5291288081239979, 0.8546592489568846, tensor(2.0974)]
[tensor(-0.5482), 0.5291288081239979, 0.8546592489568846, tensor(2.0974)]
[2023-01-16 03:31:31,264.264 dlc26te6b6pxn0nk-master-0:842 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 03:31:31,952.952 dlc26te6b6pxn0nk-master-0:909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:31:31,954.954 dlc26te6b6pxn0nk-master-0:910 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:31:31,954.954 dlc26te6b6pxn0nk-master-0:907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:31:31,981.981 dlc26te6b6pxn0nk-master-0:908 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:31:33,102.102 dlc26te6b6pxn0nk-master-0:908 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 03:31:34,075.075 dlc26te6b6pxn0nk-master-0:910 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 03:31:34,095.095 dlc26te6b6pxn0nk-master-0:909 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 03:31:34,105.105 dlc26te6b6pxn0nk-master-0:907 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:907:907 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:910:910 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:909:909 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:908:908 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.5783), 0.5253874933190807, 0.8122392211404729, tensor(2.0487)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5783), 0.5253874933190807, 0.8365785813630042, tensor(2.0487)]
[tensor(-0.5255), 0.5334045964724746, 0.849095966620306, tensor(2.1415)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5161), 0.5510422234099412, 0.849095966620306, tensor(2.2391)]
[tensor(-0.5161), 0.5510422234099412, 0.8595271210013908, tensor(2.2391)]
[2023-01-16 03:43:22,785.785 dlc26te6b6pxn0nk-master-0:987 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 03:43:23,451.451 dlc26te6b6pxn0nk-master-0:1052 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:43:23,459.459 dlc26te6b6pxn0nk-master-0:1055 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:43:23,459.459 dlc26te6b6pxn0nk-master-0:1053 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:43:23,459.459 dlc26te6b6pxn0nk-master-0:1054 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:43:24,548.548 dlc26te6b6pxn0nk-master-0:1053 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 03:43:25,516.516 dlc26te6b6pxn0nk-master-0:1055 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 03:43:25,532.532 dlc26te6b6pxn0nk-master-0:1054 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 03:43:25,539.539 dlc26te6b6pxn0nk-master-0:1052 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:1052:1052 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:1054:1054 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1055:1055 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1053:1053 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7129), 0.4462854088722608, 0.7677329624478443, tensor(1.5185)]
[tensor(-0.7040), 0.44735435595938, 0.7726008344923505, tensor(1.5328)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6494), 0.46766435061464456, 0.7927677329624478, tensor(1.6889)]
[tensor(-0.6191), 0.4831640833778728, 0.7927677329624478, tensor(1.7967)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.6191), 0.4831640833778728, 0.8073713490959666, tensor(1.7967)]
[tensor(-0.6172), 0.4917156600748263, 0.8261474269819193, tensor(1.8414)]
[tensor(-0.5882), 0.5045430251202565, 0.8261474269819193, tensor(1.9345)]
[tensor(-0.5701), 0.5120256547300909, 0.8261474269819193, tensor(1.9900)]
[tensor(-0.5701), 0.515766969535008, 0.8351877607788595, tensor(2.0006)]
[tensor(-0.5638), 0.515766969535008, 0.8351877607788595, tensor(2.0006)]
[tensor(-0.5638), 0.515766969535008, 0.8372739916550765, tensor(2.0006)]
[tensor(-0.5526), 0.5318011758417959, 0.8372739916550765, tensor(2.1064)]
[tensor(-0.5526), 0.5318011758417959, 0.8372739916550765, tensor(2.1064)]
[tensor(-0.5526), 0.5318011758417959, 0.8372739916550765, tensor(2.1064)]
[tensor(-0.5526), 0.5318011758417959, 0.8372739916550765, tensor(2.1064)]
[tensor(-0.5526), 0.5318011758417959, 0.8372739916550765, tensor(2.1064)]
[tensor(-0.5526), 0.5318011758417959, 0.8372739916550765, tensor(2.1064)]
early stopping at 17
[2023-01-16 04:23:15,927.927 dlc26te6b6pxn0nk-master-0:1173 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 04:23:16,610.610 dlc26te6b6pxn0nk-master-0:1241 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:23:16,610.610 dlc26te6b6pxn0nk-master-0:1240 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:23:16,692.692 dlc26te6b6pxn0nk-master-0:1239 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:23:16,697.697 dlc26te6b6pxn0nk-master-0:1238 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:23:17,765.765 dlc26te6b6pxn0nk-master-0:1239 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 04:23:18,568.568 dlc26te6b6pxn0nk-master-0:1240 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 04:23:18,597.597 dlc26te6b6pxn0nk-master-0:1241 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 04:23:18,603.603 dlc26te6b6pxn0nk-master-0:1238 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:1238:1238 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:1240:1240 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1239:1239 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1241:1241 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.7145), 0.45911277391769106, 0.7545201668984701, tensor(1.5810)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6478), 0.467129877071085, 0.7955493741307371, tensor(1.6879)]
[tensor(-0.6478), 0.467129877071085, 0.7955493741307371, tensor(1.6879)]
[tensor(-0.6211), 0.4879743452699091, 0.7955493741307371, tensor(1.8188)]
[tensor(-0.5796), 0.5018706574024586, 0.8108484005563282, tensor(1.9297)]
[tensor(-0.5796), 0.5104222340994121, 0.8108484005563282, tensor(1.9636)]
[tensor(-0.5691), 0.5163014430785676, 0.8108484005563282, tensor(2.0124)]
[tensor(-0.5691), 0.5163014430785676, 0.8268428372739917, tensor(2.0124)]
[tensor(-0.5691), 0.5163014430785676, 0.8268428372739917, tensor(2.0124)]
[tensor(-0.5691), 0.5163014430785676, 0.8268428372739917, tensor(2.0124)]
[tensor(-0.5688), 0.5221806520577231, 0.8351877607788595, tensor(2.0421)]
[tensor(-0.5688), 0.5221806520577231, 0.8351877607788595, tensor(2.0421)]
[tensor(-0.5688), 0.5221806520577231, 0.8351877607788595, tensor(2.0421)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5645), 0.5307322287546766, 0.8351877607788595, tensor(2.0892)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5645), 0.5307322287546766, 0.8351877607788595, tensor(2.0892)]
[tensor(-0.5645), 0.5307322287546766, 0.8351877607788595, tensor(2.0892)]
[tensor(-0.5645), 0.5307322287546766, 0.8351877607788595, tensor(2.0892)]
[tensor(-0.5645), 0.5307322287546766, 0.8351877607788595, tensor(2.0892)]
[tensor(-0.5645), 0.5307322287546766, 0.8351877607788595, tensor(2.0892)]
early stopping at 19
[2023-01-16 05:05:39,215.215 dlc26te6b6pxn0nk-master-0:1364 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 05:05:39,867.867 dlc26te6b6pxn0nk-master-0:1432 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:05:39,867.867 dlc26te6b6pxn0nk-master-0:1429 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:05:39,927.927 dlc26te6b6pxn0nk-master-0:1430 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:05:40,011.011 dlc26te6b6pxn0nk-master-0:1431 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:05:41,541.541 dlc26te6b6pxn0nk-master-0:1430 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 05:05:41,559.559 dlc26te6b6pxn0nk-master-0:1431 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 05:05:42,058.058 dlc26te6b6pxn0nk-master-0:1432 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 05:05:42,067.067 dlc26te6b6pxn0nk-master-0:1429 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:1429:1429 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:1430:1430 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1432:1432 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1431:1431 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Mon Jan 16 05:06:44 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5729), 0.501336183858899, 0.849095966620306, tensor(1.9338)]
[Mon Jan 16 05:08:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 05:08:59 2023] [cudaHostAllocator] allocates 3.42 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Mon Jan 16 05:09:38 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5389), 0.5243185462319615, 0.849095966620306, tensor(2.0827)]
[Mon Jan 16 05:11:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 05:12:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5240), 0.5307322287546766, 0.8581363004172462, tensor(2.1297)]
[Mon Jan 16 05:14:27 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Mon Jan 16 05:15:18 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5088), 0.5350080171031534, 0.8671766342141863, tensor(2.1662)]
[Mon Jan 16 05:16:36 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 05:17:27 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5088), 0.5350080171031534, 0.868567454798331, tensor(2.1662)]
[2023-01-16 05:19:06,722.722 dlc26te6b6pxn0nk-master-0:1511 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 05:19:07,396.396 dlc26te6b6pxn0nk-master-0:1576 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:19:07,397.397 dlc26te6b6pxn0nk-master-0:1577 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:19:07,477.477 dlc26te6b6pxn0nk-master-0:1579 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:19:07,480.480 dlc26te6b6pxn0nk-master-0:1578 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:19:08,882.882 dlc26te6b6pxn0nk-master-0:1578 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 05:19:08,885.885 dlc26te6b6pxn0nk-master-0:1579 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 05:19:09,380.380 dlc26te6b6pxn0nk-master-0:1577 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 05:19:09,388.388 dlc26te6b6pxn0nk-master-0:1576 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:1576:1576 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:1577:1577 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1578:1578 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1579:1579 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Mon Jan 16 05:20:21 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5476), 0.5163014430785676, 0.8553546592489569, tensor(2.0339)]
[Mon Jan 16 05:22:13 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 05:22:30 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5405), 0.5163014430785676, 0.8553546592489569, tensor(2.0339)]
[Mon Jan 16 05:25:43 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5289), 0.5307322287546766, 0.8553546592489569, tensor(2.1248)]
[Mon Jan 16 05:27:18 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5289), 0.5307322287546766, 0.8616133518776078, tensor(2.1248)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[Mon Jan 16 05:29:09 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5086), 0.5521111704970604, 0.8616133518776078, tensor(2.2520)]
[2023-01-16 05:31:34,229.229 dlc26te6b6pxn0nk-master-0:1657 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 05:31:34,949.949 dlc26te6b6pxn0nk-master-0:1724 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:31:34,970.970 dlc26te6b6pxn0nk-master-0:1723 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:31:35,060.060 dlc26te6b6pxn0nk-master-0:1722 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:31:35,149.149 dlc26te6b6pxn0nk-master-0:1725 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:31:36,319.319 dlc26te6b6pxn0nk-master-0:1723 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 05:31:36,421.421 dlc26te6b6pxn0nk-master-0:1725 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 05:31:36,841.841 dlc26te6b6pxn0nk-master-0:1724 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 05:31:36,842.842 dlc26te6b6pxn0nk-master-0:1722 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:1722:1722 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:1725:1725 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1724:1724 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1723:1723 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Mon Jan 16 05:32:38 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6419), 0.46873329770176375, 0.7920723226703755, tensor(1.7018)]
[Mon Jan 16 05:34:51 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 05:35:17 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5684), 0.51309460181721, 0.8442280945757997, tensor(1.9970)]
[Mon Jan 16 05:37:07 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 05:38:23 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5684), 0.515766969535008, 0.8442280945757997, tensor(2.0065)]
[Mon Jan 16 05:40:04 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 05:40:55 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5332), 0.532870122928915, 0.8581363004172462, tensor(2.1312)]
[Mon Jan 16 05:42:13 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 05:43:05 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5303), 0.5392838054516301, 0.8581363004172462, tensor(2.1661)]
[Mon Jan 16 05:45:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 05:46:01 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 05:46:05 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5303), 0.5392838054516301, 0.8581363004172462, tensor(2.1661)]
[Mon Jan 16 05:47:48 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5303), 0.5392838054516301, 0.8581363004172462, tensor(2.1661)]
[Mon Jan 16 05:49:39 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 05:50:27 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 05:50:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5303), 0.5392838054516301, 0.8581363004172462, tensor(2.1661)]
[Mon Jan 16 05:52:23 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 05:52:37 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 05:53:33 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5303), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 05:55:01 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 05:55:09 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5303), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 05:58:19 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 05:58:27 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5299), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 05:59:44 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 06:01:09 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 06:01:11 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5299), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 06:02:48 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 06:03:39 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5299), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 06:05:05 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5230), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 06:07:46 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Mon Jan 16 06:08:57 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5230), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 06:11:11 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 06:11:17 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5230), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 06:13:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 06:14:18 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5230), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 06:16:25 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5230), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
[Mon Jan 16 06:18:29 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 06:18:43 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 06:19:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5230), 0.5392838054516301, 0.8609179415855355, tensor(2.1661)]
early stopping at 19
[2023-01-16 06:20:29,707.707 dlc26te6b6pxn0nk-master-0:1857 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 06:20:30,446.446 dlc26te6b6pxn0nk-master-0:1923 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:20:30,488.488 dlc26te6b6pxn0nk-master-0:1922 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:20:30,525.525 dlc26te6b6pxn0nk-master-0:1925 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:20:30,525.525 dlc26te6b6pxn0nk-master-0:1924 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:20:32,481.481 dlc26te6b6pxn0nk-master-0:1924 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 06:20:32,483.483 dlc26te6b6pxn0nk-master-0:1925 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 06:20:32,812.812 dlc26te6b6pxn0nk-master-0:1923 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 06:20:32,817.817 dlc26te6b6pxn0nk-master-0:1922 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:1922:1922 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:1924:1924 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1925:1925 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:1923:1923 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Mon Jan 16 06:21:36 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7250), 0.4462854088722608, 0.7246175243393602, tensor(1.5064)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Mon Jan 16 06:23:27 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[Mon Jan 16 06:23:44 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7250), 0.4462854088722608, 0.7246175243393602, tensor(1.5064)]
[Mon Jan 16 06:26:54 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7250), 0.4462854088722608, 0.7246175243393602, tensor(1.5064)]
[Mon Jan 16 06:28:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7250), 0.4462854088722608, 0.7246175243393602, tensor(1.5064)]
[Mon Jan 16 06:30:21 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7250), 0.4462854088722608, 0.7246175243393602, tensor(1.5064)]
[Mon Jan 16 06:33:19 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 06:33:58 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.7250), 0.4462854088722608, 0.7246175243393602, tensor(1.5064)]
[Mon Jan 16 06:35:45 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.7250), 0.4462854088722608, 0.7246175243393602, tensor(1.5064)]
[Mon Jan 16 06:37:24 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7226), 0.4462854088722608, 0.7246175243393602, tensor(1.5088)]
[Mon Jan 16 06:40:11 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 06:40:59 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6664), 0.46178514163548906, 0.7753824756606398, tensor(1.6426)]
[Mon Jan 16 06:42:03 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 06:42:34 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6664), 0.46178514163548906, 0.7892906815020863, tensor(1.6426)]
[Mon Jan 16 06:45:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6346), 0.4778193479422769, 0.7892906815020863, tensor(1.7545)]
[Mon Jan 16 06:48:06 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6240), 0.49599144842330306, 0.7892906815020863, tensor(1.8560)]
[Mon Jan 16 06:49:38 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 06:50:12 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6177), 0.5029396044895778, 0.7892906815020863, tensor(1.8970)]
[Mon Jan 16 06:51:27 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.6147), 0.5029396044895778, 0.7892906815020863, tensor(1.8970)]
[Mon Jan 16 06:54:01 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 06:54:52 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6147), 0.5029396044895778, 0.7892906815020863, tensor(1.8970)]
[Mon Jan 16 06:57:00 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6147), 0.5029396044895778, 0.7892906815020863, tensor(1.8970)]
[Mon Jan 16 06:58:51 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6147), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:00:54 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 07:01:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6147), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:03:36 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.6132), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:06:39 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6069), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:08:53 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6069), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:10:24 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 07:10:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 07:10:54 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.6069), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:12:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 07:12:53 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.6063), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:14:43 2023] [cudaHostAllocator] allocates 1.71 GiB
[Mon Jan 16 07:15:18 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6037), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:17:29 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 07:18:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5980), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:19:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 07:20:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5980), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:22:34 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5980), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:24:54 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5980), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:26:38 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5980), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
[Mon Jan 16 07:29:46 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5980), 0.5029396044895778, 0.8018080667593881, tensor(1.8970)]
early stopping at 30
[2023-01-16 07:31:06,317.317 dlc26te6b6pxn0nk-master-0:2090 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 07:31:06,970.970 dlc26te6b6pxn0nk-master-0:2157 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:31:06,970.970 dlc26te6b6pxn0nk-master-0:2156 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:31:07,052.052 dlc26te6b6pxn0nk-master-0:2158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:31:07,053.053 dlc26te6b6pxn0nk-master-0:2155 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:31:08,055.055 dlc26te6b6pxn0nk-master-0:2158 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 07:31:08,923.923 dlc26te6b6pxn0nk-master-0:2157 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 07:31:08,924.924 dlc26te6b6pxn0nk-master-0:2156 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 07:31:08,932.932 dlc26te6b6pxn0nk-master-0:2155 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:2155:2155 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:2158:2158 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2157:2157 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2156:2156 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Mon Jan 16 07:32:10 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 07:32:52 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6810), 0.4623196151790486, 0.7816411682892906, tensor(1.6306)]
[Mon Jan 16 07:34:21 2023] [cudaHostAllocator] allocates 3.42 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6054), 0.5018706574024586, 0.8122392211404729, tensor(1.9040)]
[Mon Jan 16 07:37:22 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5741), 0.5034740780331374, 0.8122392211404729, tensor(1.9433)]
[Mon Jan 16 07:39:35 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5741), 0.5034740780331374, 0.8129346314325452, tensor(1.9433)]
[Mon Jan 16 07:41:36 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 07:42:27 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 07:42:37 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5664), 0.5232495991448424, 0.8358831710709318, tensor(2.0498)]
[2023-01-16 07:44:16,910.910 dlc26te6b6pxn0nk-master-0:2237 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 07:44:17,555.555 dlc26te6b6pxn0nk-master-0:2305 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:44:17,555.555 dlc26te6b6pxn0nk-master-0:2302 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:44:17,639.639 dlc26te6b6pxn0nk-master-0:2304 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:44:17,640.640 dlc26te6b6pxn0nk-master-0:2303 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:44:18,938.938 dlc26te6b6pxn0nk-master-0:2304 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 07:44:18,941.941 dlc26te6b6pxn0nk-master-0:2303 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 07:44:19,521.521 dlc26te6b6pxn0nk-master-0:2305 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 07:44:19,528.528 dlc26te6b6pxn0nk-master-0:2302 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:2302:2302 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:2305:2305 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2303:2303 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2304:2304 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[Mon Jan 16 07:45:24 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6143), 0.48583645109567075, 0.8372739916550765, tensor(1.8149)]
[Mon Jan 16 07:47:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 07:47:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5525), 0.5195082843399251, 0.8407510431154381, tensor(2.0451)]
[Mon Jan 16 07:50:41 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5407), 0.5195082843399251, 0.8463143254520167, tensor(2.0451)]
[Mon Jan 16 07:52:14 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 07:53:03 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5407), 0.5195082843399251, 0.847009735744089, tensor(2.0451)]
[Mon Jan 16 07:54:06 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 07:54:55 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5407), 0.5195082843399251, 0.8553546592489569, tensor(2.0451)]
[2023-01-16 07:56:29,419.419 dlc26te6b6pxn0nk-master-0:2382 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 07:56:30,071.071 dlc26te6b6pxn0nk-master-0:2448 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:56:30,075.075 dlc26te6b6pxn0nk-master-0:2449 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:56:30,075.075 dlc26te6b6pxn0nk-master-0:2447 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:56:30,083.083 dlc26te6b6pxn0nk-master-0:2450 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:56:31,134.134 dlc26te6b6pxn0nk-master-0:2450 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 07:56:32,119.119 dlc26te6b6pxn0nk-master-0:2449 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 07:56:32,128.128 dlc26te6b6pxn0nk-master-0:2448 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 07:56:32,135.135 dlc26te6b6pxn0nk-master-0:2447 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:2447:2447 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:2450:2450 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2448:2448 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2449:2449 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Mon Jan 16 07:57:34 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 07:58:16 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7736), 0.4462854088722608, 0.6481223922114048, tensor(1.4578)]
[Mon Jan 16 07:59:44 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.6244), 0.48690539818278994, 0.760778859527121, tensor(1.8101)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Mon Jan 16 08:02:48 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.6244), 0.48690539818278994, 0.8011126564673157, tensor(1.8101)]
[Mon Jan 16 08:04:57 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5567), 0.5200427578834848, 0.8400556328233658, tensor(2.0436)]
[Mon Jan 16 08:06:54 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 08:07:43 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 08:07:52 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5567), 0.5200427578834848, 0.8400556328233658, tensor(2.0436)]
[Mon Jan 16 08:10:33 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 08:10:37 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5567), 0.5200427578834848, 0.8400556328233658, tensor(2.0436)]
[Mon Jan 16 08:12:26 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5467), 0.5291288081239979, 0.8400556328233658, tensor(2.0989)]
[Mon Jan 16 08:14:14 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 08:15:13 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 08:15:22 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5467), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:17:18 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 08:18:14 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5462), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:19:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 08:19:45 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5462), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:23:01 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 08:23:08 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5462), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:25:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 08:25:47 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5459), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:27:28 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5368), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:29:28 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 08:30:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5368), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:32:19 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 08:33:20 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5368), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:35:36 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5368), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:37:30 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5368), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
[Mon Jan 16 08:39:55 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5368), 0.5334045964724746, 0.8400556328233658, tensor(2.1103)]
early stopping at 18
[2023-01-16 08:42:38,633.633 dlc26te6b6pxn0nk-master-0:2578 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 08:42:39,356.356 dlc26te6b6pxn0nk-master-0:2646 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:42:39,419.419 dlc26te6b6pxn0nk-master-0:2645 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:42:39,454.454 dlc26te6b6pxn0nk-master-0:2644 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:42:39,454.454 dlc26te6b6pxn0nk-master-0:2643 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:42:40,389.389 dlc26te6b6pxn0nk-master-0:2644 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 08:42:40,704.704 dlc26te6b6pxn0nk-master-0:2646 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 08:42:40,748.748 dlc26te6b6pxn0nk-master-0:2645 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 08:42:40,753.753 dlc26te6b6pxn0nk-master-0:2643 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:2643:2643 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:2644:2644 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2646:2646 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2645:2645 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Mon Jan 16 08:43:44 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7425), 0.4462854088722608, 0.7114047287899861, tensor(1.4889)]
[Mon Jan 16 08:45:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 08:45:53 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6192), 0.48102618920363444, 0.8289290681502086, tensor(1.7860)]
[Mon Jan 16 08:48:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5940), 0.498663816141101, 0.8289290681502086, tensor(1.8994)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[Mon Jan 16 08:50:38 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 08:51:27 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5678), 0.5120256547300909, 0.8303198887343533, tensor(1.9923)]
[Mon Jan 16 08:52:32 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[Mon Jan 16 08:53:23 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5561), 0.521111704970604, 0.8303198887343533, tensor(2.0494)]
[Mon Jan 16 08:56:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 08:56:11 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5503), 0.5280598610368786, 0.8303198887343533, tensor(2.0900)]
[Mon Jan 16 08:57:58 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5503), 0.5285943345804383, 0.8324061196105702, tensor(2.0900)]
[Mon Jan 16 08:59:38 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 09:00:23 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5503), 0.5285943345804383, 0.8477051460361613, tensor(2.0900)]
[Mon Jan 16 09:02:46 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5469), 0.5285943345804383, 0.8477051460361613, tensor(2.0900)]
[Mon Jan 16 09:05:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 09:05:24 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5469), 0.5285943345804383, 0.8477051460361613, tensor(2.0900)]
[Mon Jan 16 09:08:27 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5469), 0.530197755211117, 0.8477051460361613, tensor(2.0990)]
[Mon Jan 16 09:09:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 09:11:04 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 09:11:05 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5469), 0.530197755211117, 0.8477051460361613, tensor(2.0990)]
[Mon Jan 16 09:12:37 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 09:13:17 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5469), 0.530197755211117, 0.8477051460361613, tensor(2.0990)]
[Mon Jan 16 09:14:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 09:15:43 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5469), 0.530197755211117, 0.8477051460361613, tensor(2.0990)]
[Mon Jan 16 09:17:13 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 09:18:05 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5469), 0.530197755211117, 0.8477051460361613, tensor(2.0990)]
[Mon Jan 16 09:20:12 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5469), 0.530197755211117, 0.8477051460361613, tensor(2.0990)]
early stopping at 16
[2023-01-16 09:21:38,597.597 dlc26te6b6pxn0nk-master-0:2761 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 09:21:39,366.366 dlc26te6b6pxn0nk-master-0:2829 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:21:39,372.372 dlc26te6b6pxn0nk-master-0:2827 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:21:39,519.519 dlc26te6b6pxn0nk-master-0:2828 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:21:39,519.519 dlc26te6b6pxn0nk-master-0:2826 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:21:40,516.516 dlc26te6b6pxn0nk-master-0:2828 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 09:21:41,432.432 dlc26te6b6pxn0nk-master-0:2827 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 09:21:41,445.445 dlc26te6b6pxn0nk-master-0:2829 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 09:21:41,448.448 dlc26te6b6pxn0nk-master-0:2826 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:2826:2826 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:2828:2828 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2827:2827 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2829:2829 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5469), 0.521111704970604, 0.8289290681502086, tensor(2.0587)]
[tensor(-0.5469), 0.5248530197755211, 0.8692628650904033, tensor(2.0774)]
[tensor(-0.5223), 0.5248530197755211, 0.8692628650904033, tensor(2.0860)]
[tensor(-0.5223), 0.5248530197755211, 0.8692628650904033, tensor(2.0860)]
[tensor(-0.5223), 0.5280598610368786, 0.8692628650904033, tensor(2.0975)]
[2023-01-16 09:33:04,104.104 dlc26te6b6pxn0nk-master-0:2905 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 09:33:04,864.864 dlc26te6b6pxn0nk-master-0:2971 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:33:04,864.864 dlc26te6b6pxn0nk-master-0:2973 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:33:04,871.871 dlc26te6b6pxn0nk-master-0:2970 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:33:04,872.872 dlc26te6b6pxn0nk-master-0:2972 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:33:05,925.925 dlc26te6b6pxn0nk-master-0:2972 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 09:33:05,940.940 dlc26te6b6pxn0nk-master-0:2973 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 09:33:05,941.941 dlc26te6b6pxn0nk-master-0:2971 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 09:33:05,945.945 dlc26te6b6pxn0nk-master-0:2970 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:2970:2970 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:2973:2973 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2971:2971 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:2972:2972 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5315), 0.5376803848209514, 0.8428372739916551, tensor(2.1569)]
[tensor(-0.5315), 0.5376803848209514, 0.8636995827538247, tensor(2.1569)]
[tensor(-0.5106), 0.5376803848209514, 0.8657858136300417, tensor(2.1671)]
[tensor(-0.5106), 0.5376803848209514, 0.8713490959666204, tensor(2.1671)]
[tensor(-0.5106), 0.5424906467129877, 0.8713490959666204, tensor(2.2002)]
[2023-01-16 09:44:29,655.655 dlc26te6b6pxn0nk-master-0:3050 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 09:44:30,320.320 dlc26te6b6pxn0nk-master-0:3116 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:44:30,321.321 dlc26te6b6pxn0nk-master-0:3118 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:44:30,492.492 dlc26te6b6pxn0nk-master-0:3115 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:44:30,578.578 dlc26te6b6pxn0nk-master-0:3117 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:44:31,668.668 dlc26te6b6pxn0nk-master-0:3116 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 09:44:31,838.838 dlc26te6b6pxn0nk-master-0:3117 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 09:44:32,200.200 dlc26te6b6pxn0nk-master-0:3118 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 09:44:32,209.209 dlc26te6b6pxn0nk-master-0:3115 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:3115:3115 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:3117:3117 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3118:3118 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3116:3116 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5656), 0.5227151256012827, 0.8539638386648123, tensor(2.0479)]
[tensor(-0.5305), 0.5462319615179049, 0.8539638386648123, tensor(2.2006)]
[tensor(-0.5262), 0.5462319615179049, 0.8650904033379694, tensor(2.2006)]
[tensor(-0.5073), 0.5483698556921432, 0.8650904033379694, tensor(2.2345)]
[tensor(-0.5073), 0.5483698556921432, 0.8650904033379694, tensor(2.2345)]
[tensor(-0.5073), 0.5483698556921432, 0.8657858136300417, tensor(2.2345)]
[tensor(-0.5073), 0.5537145911277391, 0.8657858136300417, tensor(2.2427)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5073), 0.5537145911277391, 0.8699582753824756, tensor(2.2427)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5073), 0.5537145911277391, 0.8706536856745479, tensor(2.2427)]
[tensor(-0.5073), 0.5537145911277391, 0.8706536856745479, tensor(2.2427)]
[tensor(-0.5073), 0.5537145911277391, 0.8706536856745479, tensor(2.2427)]
[tensor(-0.5073), 0.5537145911277391, 0.8706536856745479, tensor(2.2427)]
[tensor(-0.5073), 0.5537145911277391, 0.8706536856745479, tensor(2.2427)]
[tensor(-0.5073), 0.5537145911277391, 0.8706536856745479, tensor(2.2427)]
early stopping at 14
[2023-01-16 10:15:48,283.283 dlc26te6b6pxn0nk-master-0:3224 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 10:15:48,989.989 dlc26te6b6pxn0nk-master-0:3290 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:15:49,097.097 dlc26te6b6pxn0nk-master-0:3291 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:15:49,106.106 dlc26te6b6pxn0nk-master-0:3292 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:15:49,106.106 dlc26te6b6pxn0nk-master-0:3289 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:15:50,035.035 dlc26te6b6pxn0nk-master-0:3292 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 10:15:50,352.352 dlc26te6b6pxn0nk-master-0:3290 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 10:15:50,403.403 dlc26te6b6pxn0nk-master-0:3291 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 10:15:50,410.410 dlc26te6b6pxn0nk-master-0:3289 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:3289:3289 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:3292:3292 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3291:3291 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3290:3290 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5502), 0.5291288081239979, 0.8595271210013908, tensor(2.0954)]
[tensor(-0.5494), 0.5344735435595938, 0.8630041724617524, tensor(2.1229)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5138), 0.5462319615179049, 0.8657858136300417, tensor(2.2173)]
[tensor(-0.5138), 0.5531801175841796, 0.8657858136300417, tensor(2.2517)]
[tensor(-0.5138), 0.5595938001068947, 0.8657858136300417, tensor(2.2807)]
[tensor(-0.5138), 0.5595938001068947, 0.8657858136300417, tensor(2.2807)]
[tensor(-0.5138), 0.5595938001068947, 0.8755215577190543, tensor(2.2807)]
[tensor(-0.5138), 0.5595938001068947, 0.8755215577190543, tensor(2.2807)]
[tensor(-0.5138), 0.5595938001068947, 0.8755215577190543, tensor(2.2807)]
[tensor(-0.5138), 0.5595938001068947, 0.8755215577190543, tensor(2.2807)]
[tensor(-0.5138), 0.5595938001068947, 0.8755215577190543, tensor(2.2807)]
[tensor(-0.5138), 0.5595938001068947, 0.8755215577190543, tensor(2.2807)]
early stopping at 12
[2023-01-16 10:43:43,653.653 dlc26te6b6pxn0nk-master-0:3392 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 10:43:44,342.342 dlc26te6b6pxn0nk-master-0:3460 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:43:44,376.376 dlc26te6b6pxn0nk-master-0:3458 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:43:44,444.444 dlc26te6b6pxn0nk-master-0:3457 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:43:44,532.532 dlc26te6b6pxn0nk-master-0:3459 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:43:45,717.717 dlc26te6b6pxn0nk-master-0:3458 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 10:43:45,811.811 dlc26te6b6pxn0nk-master-0:3459 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 10:43:46,203.203 dlc26te6b6pxn0nk-master-0:3460 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 10:43:46,206.206 dlc26te6b6pxn0nk-master-0:3457 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:3457:3457 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:3458:3458 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3459:3459 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3460:3460 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5741), 0.5056119722073757, 0.8317107093184979, tensor(1.9540)]
[tensor(-0.5295), 0.5366114377338321, 0.8539638386648123, tensor(2.1536)]
[tensor(-0.5230), 0.5392838054516301, 0.8678720445062587, tensor(2.1734)]
[tensor(-0.5230), 0.5392838054516301, 0.8678720445062587, tensor(2.1734)]
[tensor(-0.5230), 0.5392838054516301, 0.8678720445062587, tensor(2.1734)]
[2023-01-16 10:55:23,182.182 dlc26te6b6pxn0nk-master-0:3537 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 10:55:23,947.947 dlc26te6b6pxn0nk-master-0:3602 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:55:23,952.952 dlc26te6b6pxn0nk-master-0:3605 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:55:23,952.952 dlc26te6b6pxn0nk-master-0:3604 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:55:23,953.953 dlc26te6b6pxn0nk-master-0:3603 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:55:25,071.071 dlc26te6b6pxn0nk-master-0:3603 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 10:55:25,075.075 dlc26te6b6pxn0nk-master-0:3605 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 10:55:25,077.077 dlc26te6b6pxn0nk-master-0:3604 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 10:55:25,080.080 dlc26te6b6pxn0nk-master-0:3602 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:3602:3602 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:3605:3605 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3604:3604 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3603:3603 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5183), 0.5467664350614645, 0.8692628650904033, tensor(2.2155)]
[tensor(-0.5183), 0.5467664350614645, 0.8692628650904033, tensor(2.2155)]
[tensor(-0.5142), 0.5467664350614645, 0.874826147426982, tensor(2.2170)]
[tensor(-0.5142), 0.5467664350614645, 0.8755215577190543, tensor(2.2170)]
[tensor(-0.5142), 0.5467664350614645, 0.8755215577190543, tensor(2.2170)]
[2023-01-16 11:06:46,757.757 dlc26te6b6pxn0nk-master-0:3681 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 11:06:47,513.513 dlc26te6b6pxn0nk-master-0:3748 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:06:47,513.513 dlc26te6b6pxn0nk-master-0:3747 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:06:47,595.595 dlc26te6b6pxn0nk-master-0:3749 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:06:47,595.595 dlc26te6b6pxn0nk-master-0:3746 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:06:48,677.677 dlc26te6b6pxn0nk-master-0:3748 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 11:06:48,679.679 dlc26te6b6pxn0nk-master-0:3749 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 11:06:48,683.683 dlc26te6b6pxn0nk-master-0:3747 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 11:06:48,685.685 dlc26te6b6pxn0nk-master-0:3746 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:3746:3746 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:3749:3749 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3748:3748 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3747:3747 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5562), 0.5168359166221272, 0.8456189151599444, tensor(2.0280)]
[tensor(-0.5562), 0.5221806520577231, 0.8456189151599444, tensor(2.0494)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5481), 0.5275253874933191, 0.8643949930458971, tensor(2.0896)]
[tensor(-0.5211), 0.5419561731694281, 0.8643949930458971, tensor(2.1887)]
[tensor(-0.5211), 0.5419561731694281, 0.8643949930458971, tensor(2.1887)]
[tensor(-0.5211), 0.5419561731694281, 0.8643949930458971, tensor(2.1887)]
[tensor(-0.5211), 0.5456974879743453, 0.8643949930458971, tensor(2.1898)]
[tensor(-0.5211), 0.5456974879743453, 0.8643949930458971, tensor(2.1898)]
[tensor(-0.5211), 0.5456974879743453, 0.8643949930458971, tensor(2.1898)]
[tensor(-0.5211), 0.5456974879743453, 0.8643949930458971, tensor(2.1898)]
[tensor(-0.5211), 0.5456974879743453, 0.8643949930458971, tensor(2.1898)]
[tensor(-0.5211), 0.5456974879743453, 0.8643949930458971, tensor(2.1898)]
early stopping at 12
[2023-01-16 11:34:18,009.009 dlc26te6b6pxn0nk-master-0:3849 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 11:34:18,658.658 dlc26te6b6pxn0nk-master-0:3914 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:18,661.661 dlc26te6b6pxn0nk-master-0:3916 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:18,746.746 dlc26te6b6pxn0nk-master-0:3915 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:18,750.750 dlc26te6b6pxn0nk-master-0:3917 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:20,164.164 dlc26te6b6pxn0nk-master-0:3915 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 11:34:20,166.166 dlc26te6b6pxn0nk-master-0:3917 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 11:34:20,627.627 dlc26te6b6pxn0nk-master-0:3916 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 11:34:20,631.631 dlc26te6b6pxn0nk-master-0:3914 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:3914:3914 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:3916:3916 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3917:3917 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:3915:3915 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5704), 0.5173703901656868, 0.8477051460361613, tensor(2.0165)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5687), 0.5200427578834848, 0.8539638386648123, tensor(2.0315)]
[tensor(-0.5212), 0.5376803848209514, 0.8567454798331016, tensor(2.1672)]
[tensor(-0.5212), 0.538214858364511, 0.8609179415855355, tensor(2.1672)]
[tensor(-0.5212), 0.5483698556921432, 0.8630041724617524, tensor(2.2185)]
[tensor(-0.5212), 0.5483698556921432, 0.8630041724617524, tensor(2.2185)]
[tensor(-0.5179), 0.5483698556921432, 0.8650904033379694, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.866481223922114, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.8671766342141863, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.8671766342141863, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.8671766342141863, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.8713490959666204, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.8713490959666204, tensor(2.2186)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5179), 0.5483698556921432, 0.8713490959666204, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.8713490959666204, tensor(2.2186)]
[tensor(-0.5179), 0.5483698556921432, 0.8713490959666204, tensor(2.2186)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5179), 0.5483698556921432, 0.8713490959666204, tensor(2.2186)]
early stopping at 17
[2023-01-16 12:12:20,938.938 dlc26te6b6pxn0nk-master-0:4033 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 12:12:21,605.605 dlc26te6b6pxn0nk-master-0:4100 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:12:21,605.605 dlc26te6b6pxn0nk-master-0:4099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:12:21,617.617 dlc26te6b6pxn0nk-master-0:4101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:12:21,623.623 dlc26te6b6pxn0nk-master-0:4098 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:12:23,622.622 dlc26te6b6pxn0nk-master-0:4099 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 12:12:23,653.653 dlc26te6b6pxn0nk-master-0:4100 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 12:12:23,669.669 dlc26te6b6pxn0nk-master-0:4101 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 12:12:23,673.673 dlc26te6b6pxn0nk-master-0:4098 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:4098:4098 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:4101:4101 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4099:4099 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4100:4100 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5567), 0.5077498663816141, 0.8428372739916551, tensor(1.9820)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5379), 0.532870122928915, 0.8484005563282336, tensor(2.1264)]
[tensor(-0.5379), 0.532870122928915, 0.8581363004172462, tensor(2.1264)]
[tensor(-0.5234), 0.538214858364511, 0.8588317107093185, tensor(2.1676)]
[tensor(-0.5234), 0.538214858364511, 0.8588317107093185, tensor(2.1676)]
[2023-01-16 12:24:29,409.409 dlc26te6b6pxn0nk-master-0:4178 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 12:24:30,058.058 dlc26te6b6pxn0nk-master-0:4246 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:24:30,058.058 dlc26te6b6pxn0nk-master-0:4245 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:24:30,058.058 dlc26te6b6pxn0nk-master-0:4244 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:24:30,060.060 dlc26te6b6pxn0nk-master-0:4243 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:24:32,087.087 dlc26te6b6pxn0nk-master-0:4244 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 12:24:32,100.100 dlc26te6b6pxn0nk-master-0:4246 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 12:24:32,102.102 dlc26te6b6pxn0nk-master-0:4245 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 12:24:32,109.109 dlc26te6b6pxn0nk-master-0:4243 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:4243:4243 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:4245:4245 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4246:4246 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4244:4244 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7451), 0.4462854088722608, 0.7155771905424201, tensor(1.4863)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6858), 0.4596472474612507, 0.7552155771905424, tensor(1.6124)]
[tensor(-0.5879), 0.5018706574024586, 0.8115438108484005, tensor(1.9214)]
[tensor(-0.5707), 0.5114911811865313, 0.8317107093184979, tensor(1.9868)]
[tensor(-0.5559), 0.5205772314270444, 0.8372739916550765, tensor(2.0470)]
[2023-01-16 12:35:51,935.935 dlc26te6b6pxn0nk-master-0:4322 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 12:35:52,583.583 dlc26te6b6pxn0nk-master-0:4390 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:35:52,587.587 dlc26te6b6pxn0nk-master-0:4387 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:35:52,587.587 dlc26te6b6pxn0nk-master-0:4389 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:35:52,589.589 dlc26te6b6pxn0nk-master-0:4388 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:35:53,641.641 dlc26te6b6pxn0nk-master-0:4389 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 12:35:54,592.592 dlc26te6b6pxn0nk-master-0:4388 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 12:35:54,630.630 dlc26te6b6pxn0nk-master-0:4390 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 12:35:54,633.633 dlc26te6b6pxn0nk-master-0:4387 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:4387:4387 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:4388:4388 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4390:4390 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4389:4389 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.8626), 0.29021913415285944, 0.605702364394993, tensor(0.5885)]
[tensor(-0.6392), 0.4559059326563335, 0.8087621696801113, tensor(1.6403)]
[tensor(-0.5689), 0.515766969535008, 0.8296244784422809, tensor(2.0099)]
[tensor(-0.5555), 0.5318011758417959, 0.8317107093184979, tensor(2.1035)]
[tensor(-0.5555), 0.5318011758417959, 0.8372739916550765, tensor(2.1035)]
[tensor(-0.5555), 0.5318011758417959, 0.8400556328233658, tensor(2.1035)]
[tensor(-0.5468), 0.5398182789951897, 0.8421418636995828, tensor(2.1523)]
[tensor(-0.5463), 0.5398182789951897, 0.8421418636995828, tensor(2.1523)]
[tensor(-0.5460), 0.5398182789951897, 0.8463143254520167, tensor(2.1523)]
[tensor(-0.5460), 0.5398182789951897, 0.8463143254520167, tensor(2.1523)]
[tensor(-0.5460), 0.5398182789951897, 0.8463143254520167, tensor(2.1523)]
[tensor(-0.5426), 0.5403527525387494, 0.8463143254520167, tensor(2.1591)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5426), 0.5403527525387494, 0.8463143254520167, tensor(2.1591)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5426), 0.5403527525387494, 0.8463143254520167, tensor(2.1591)]
[tensor(-0.5426), 0.5403527525387494, 0.8463143254520167, tensor(2.1591)]
[tensor(-0.5426), 0.5403527525387494, 0.8463143254520167, tensor(2.1591)]
[tensor(-0.5426), 0.5403527525387494, 0.8463143254520167, tensor(2.1591)]
early stopping at 17
[2023-01-16 13:13:38,029.029 dlc26te6b6pxn0nk-master-0:4506 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 13:13:38,766.766 dlc26te6b6pxn0nk-master-0:4561 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:13:38,767.767 dlc26te6b6pxn0nk-master-0:4562 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:13:38,940.940 dlc26te6b6pxn0nk-master-0:4563 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:13:38,940.940 dlc26te6b6pxn0nk-master-0:4560 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:13:40,328.328 dlc26te6b6pxn0nk-master-0:4563 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 13:13:41,182.182 dlc26te6b6pxn0nk-master-0:4561 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 13:13:41,183.183 dlc26te6b6pxn0nk-master-0:4562 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 13:13:41,188.188 dlc26te6b6pxn0nk-master-0:4560 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:4560:4560 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:4563:4563 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4562:4562 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4561:4561 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5703), 0.5029396044895778, 0.8421418636995828, tensor(1.9444)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5451), 0.5376803848209514, 0.8477051460361613, tensor(2.1433)]
[tensor(-0.5317), 0.5376803848209514, 0.8477051460361613, tensor(2.1434)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5317), 0.5376803848209514, 0.8588317107093185, tensor(2.1434)]
[tensor(-0.5246), 0.5376803848209514, 0.8588317107093185, tensor(2.1434)]
[tensor(-0.5246), 0.5376803848209514, 0.8588317107093185, tensor(2.1434)]
[tensor(-0.5246), 0.5376803848209514, 0.8588317107093185, tensor(2.1434)]
[tensor(-0.5246), 0.5376803848209514, 0.8588317107093185, tensor(2.1434)]
[tensor(-0.5246), 0.5376803848209514, 0.8588317107093185, tensor(2.1434)]
[tensor(-0.5246), 0.5376803848209514, 0.8588317107093185, tensor(2.1434)]
early stopping at 10
[2023-01-16 13:39:01,320.320 dlc26te6b6pxn0nk-master-0:4660 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 13:39:01,973.973 dlc26te6b6pxn0nk-master-0:4715 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:39:01,977.977 dlc26te6b6pxn0nk-master-0:4717 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:39:02,064.064 dlc26te6b6pxn0nk-master-0:4714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:39:02,071.071 dlc26te6b6pxn0nk-master-0:4716 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:39:03,913.913 dlc26te6b6pxn0nk-master-0:4717 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 13:39:03,914.914 dlc26te6b6pxn0nk-master-0:4715 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 13:39:04,431.431 dlc26te6b6pxn0nk-master-0:4716 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 13:39:04,437.437 dlc26te6b6pxn0nk-master-0:4714 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:4714:4714 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:4715:4715 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4717:4717 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4716:4716 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6084), 0.5002672367717798, 0.7225312934631433, tensor(1.8929)]
[tensor(-0.5620), 0.5253874933190807, 0.8205841446453408, tensor(2.0649)]
[tensor(-0.5620), 0.5253874933190807, 0.8623087621696801, tensor(2.0649)]
[tensor(-0.5259), 0.5307322287546766, 0.8630041724617524, tensor(2.1277)]
[tensor(-0.5259), 0.5307322287546766, 0.8630041724617524, tensor(2.1277)]
[2023-01-16 13:50:26,682.682 dlc26te6b6pxn0nk-master-0:4793 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 13:50:27,332.332 dlc26te6b6pxn0nk-master-0:4849 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:50:27,333.333 dlc26te6b6pxn0nk-master-0:4848 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:50:27,419.419 dlc26te6b6pxn0nk-master-0:4850 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:50:27,425.425 dlc26te6b6pxn0nk-master-0:4847 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:50:28,587.587 dlc26te6b6pxn0nk-master-0:4850 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 13:50:29,302.302 dlc26te6b6pxn0nk-master-0:4848 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 13:50:29,304.304 dlc26te6b6pxn0nk-master-0:4849 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 13:50:29,307.307 dlc26te6b6pxn0nk-master-0:4847 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:4847:4847 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:4848:4848 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4850:4850 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4849:4849 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.6682), 0.4890432923570283, 0.7656467315716272, tensor(1.7771)]
[tensor(-0.6283), 0.49438802779262425, 0.7656467315716272, tensor(1.8436)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6029), 0.5008017103153394, 0.786509040333797, tensor(1.9012)]
[tensor(-0.5536), 0.5189738107963656, 0.849095966620306, tensor(2.0412)]
[tensor(-0.5522), 0.5269909139497595, 0.849095966620306, tensor(2.0827)]
[2023-01-16 14:01:49,081.081 dlc26te6b6pxn0nk-master-0:4926 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 14:01:49,729.729 dlc26te6b6pxn0nk-master-0:4983 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:01:49,779.779 dlc26te6b6pxn0nk-master-0:4980 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:01:49,812.812 dlc26te6b6pxn0nk-master-0:4982 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:01:49,822.822 dlc26te6b6pxn0nk-master-0:4981 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:01:50,812.812 dlc26te6b6pxn0nk-master-0:4982 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 14:01:50,820.820 dlc26te6b6pxn0nk-master-0:4981 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 14:01:51,604.604 dlc26te6b6pxn0nk-master-0:4983 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 14:01:51,612.612 dlc26te6b6pxn0nk-master-0:4980 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:4980:4980 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:4982:4982 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4981:4981 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:4983:4983 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.6649), 0.46018172100481025, 0.7809457579972183, tensor(1.6360)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6471), 0.46018172100481025, 0.8094575799721836, tensor(1.6360)]
[tensor(-0.5908), 0.5045430251202565, 0.8317107093184979, tensor(1.9319)]
[tensor(-0.5485), 0.5259219668626403, 0.8379694019471489, tensor(2.0812)]
[tensor(-0.5485), 0.5259219668626403, 0.8484005563282336, tensor(2.0812)]
[tensor(-0.5485), 0.5259219668626403, 0.8484005563282336, tensor(2.0812)]
[tensor(-0.5424), 0.5334045964724746, 0.8484005563282336, tensor(2.1246)]
[tensor(-0.5408), 0.5334045964724746, 0.8484005563282336, tensor(2.1246)]
[tensor(-0.5408), 0.5334045964724746, 0.8484005563282336, tensor(2.1246)]
[tensor(-0.5408), 0.5334045964724746, 0.8484005563282336, tensor(2.1246)]
[tensor(-0.5408), 0.5334045964724746, 0.8484005563282336, tensor(2.1246)]
[tensor(-0.5408), 0.5334045964724746, 0.8484005563282336, tensor(2.1246)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5373), 0.5366114377338321, 0.8484005563282336, tensor(2.1458)]
[tensor(-0.5373), 0.5366114377338321, 0.8484005563282336, tensor(2.1458)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5373), 0.5366114377338321, 0.8484005563282336, tensor(2.1458)]
[tensor(-0.5373), 0.5366114377338321, 0.8504867872044506, tensor(2.1458)]
[tensor(-0.5373), 0.5366114377338321, 0.8504867872044506, tensor(2.1458)]
[tensor(-0.5373), 0.5366114377338321, 0.8504867872044506, tensor(2.1458)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5373), 0.5366114377338321, 0.8504867872044506, tensor(2.1458)]
[tensor(-0.5373), 0.5366114377338321, 0.8504867872044506, tensor(2.1458)]
[tensor(-0.5373), 0.5366114377338321, 0.8504867872044506, tensor(2.1458)]
early stopping at 21
[2023-01-16 14:48:37,627.627 dlc26te6b6pxn0nk-master-0:5112 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 14:48:38,275.275 dlc26te6b6pxn0nk-master-0:5167 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:48:38,284.284 dlc26te6b6pxn0nk-master-0:5169 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:48:38,360.360 dlc26te6b6pxn0nk-master-0:5166 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:48:38,362.362 dlc26te6b6pxn0nk-master-0:5168 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:48:40,209.209 dlc26te6b6pxn0nk-master-0:5167 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 14:48:40,210.210 dlc26te6b6pxn0nk-master-0:5169 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 14:48:40,752.752 dlc26te6b6pxn0nk-master-0:5168 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 14:48:40,757.757 dlc26te6b6pxn0nk-master-0:5166 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:5166:5166 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:5169:5169 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5168:5168 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5167:5167 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.7203), 0.39176910742918225, 0.7684283727399166, tensor(1.2385)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.6252), 0.48476750400855156, 0.8004172461752433, tensor(1.7986)]
[tensor(-0.6252), 0.48476750400855156, 0.808066759388039, tensor(1.7986)]
[tensor(-0.5615), 0.530197755211117, 0.8344923504867872, tensor(2.0895)]
[tensor(-0.5515), 0.535542490646713, 0.8344923504867872, tensor(2.1262)]
[tensor(-0.5515), 0.535542490646713, 0.8344923504867872, tensor(2.1262)]
[tensor(-0.5515), 0.535542490646713, 0.8344923504867872, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8344923504867872, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8344923504867872, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8344923504867872, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8365785813630042, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8435326842837274, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8435326842837274, tensor(2.1262)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5508), 0.535542490646713, 0.8435326842837274, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8435326842837274, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8435326842837274, tensor(2.1262)]
[tensor(-0.5508), 0.535542490646713, 0.8435326842837274, tensor(2.1262)]
early stopping at 17
[2023-01-16 15:26:44,473.473 dlc26te6b6pxn0nk-master-0:5285 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 15:26:45,147.147 dlc26te6b6pxn0nk-master-0:5339 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:26:45,147.147 dlc26te6b6pxn0nk-master-0:5342 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:26:45,231.231 dlc26te6b6pxn0nk-master-0:5340 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:26:45,234.234 dlc26te6b6pxn0nk-master-0:5341 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:26:46,456.456 dlc26te6b6pxn0nk-master-0:5341 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 15:26:46,457.457 dlc26te6b6pxn0nk-master-0:5340 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 15:26:47,105.105 dlc26te6b6pxn0nk-master-0:5342 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 15:26:47,111.111 dlc26te6b6pxn0nk-master-0:5339 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:5339:5339 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:5340:5340 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5341:5341 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5342:5342 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Mon Jan 16 15:27:49 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[Mon Jan 16 15:30:07 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6553), 0.4462854088722608, 0.7885952712100139, tensor(1.5761)]
[Mon Jan 16 15:33:19 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5996), 0.4917156600748263, 0.8414464534075105, tensor(1.8589)]
[Mon Jan 16 15:34:59 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 15:35:52 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5922), 0.4965259219668626, 0.8414464534075105, tensor(1.8904)]
[Mon Jan 16 15:37:10 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 15:38:06 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5461), 0.530197755211117, 0.8414464534075105, tensor(2.1049)]
[2023-01-16 15:39:44,049.049 dlc26te6b6pxn0nk-master-0:5421 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 15:39:44,697.697 dlc26te6b6pxn0nk-master-0:5477 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:39:44,700.700 dlc26te6b6pxn0nk-master-0:5476 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:39:44,787.787 dlc26te6b6pxn0nk-master-0:5478 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:39:44,789.789 dlc26te6b6pxn0nk-master-0:5475 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:39:45,912.912 dlc26te6b6pxn0nk-master-0:5478 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 15:39:46,653.653 dlc26te6b6pxn0nk-master-0:5477 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 15:39:46,654.654 dlc26te6b6pxn0nk-master-0:5476 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 15:39:46,662.662 dlc26te6b6pxn0nk-master-0:5475 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:5475:5475 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:5476:5476 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5478:5478 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5477:5477 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Mon Jan 16 15:40:49 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5572), 0.5195082843399251, 0.8616133518776078, tensor(2.0404)]
[Mon Jan 16 15:42:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 15:42:59 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5270), 0.532870122928915, 0.866481223922114, tensor(2.1373)]
[Mon Jan 16 15:46:06 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5200), 0.532870122928915, 0.866481223922114, tensor(2.1373)]
[Mon Jan 16 15:47:40 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5168), 0.5414216996258685, 0.866481223922114, tensor(2.1903)]
[Mon Jan 16 15:49:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5069), 0.5414216996258685, 0.866481223922114, tensor(2.1903)]
[2023-01-16 15:52:00,577.577 dlc26te6b6pxn0nk-master-0:5555 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 15:52:01,319.319 dlc26te6b6pxn0nk-master-0:5609 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:52:01,320.320 dlc26te6b6pxn0nk-master-0:5612 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:52:01,357.357 dlc26te6b6pxn0nk-master-0:5610 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:52:01,357.357 dlc26te6b6pxn0nk-master-0:5611 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:52:02,388.388 dlc26te6b6pxn0nk-master-0:5610 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 15:52:02,390.390 dlc26te6b6pxn0nk-master-0:5611 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 15:52:03,319.319 dlc26te6b6pxn0nk-master-0:5612 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 15:52:03,324.324 dlc26te6b6pxn0nk-master-0:5609 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:5609:5609 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:5611:5611 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5610:5610 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5612:5612 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Mon Jan 16 15:53:05 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5622), 0.5082843399251737, 0.8428372739916551, tensor(1.9792)]
[Mon Jan 16 15:55:01 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 15:55:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 15:55:58 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5372), 0.535542490646713, 0.8428372739916551, tensor(2.1405)]
[Mon Jan 16 15:57:46 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 15:58:39 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5372), 0.535542490646713, 0.8428372739916551, tensor(2.1405)]
[Mon Jan 16 16:00:44 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5232), 0.535542490646713, 0.8504867872044506, tensor(2.1405)]
[Mon Jan 16 16:02:40 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 16:03:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5232), 0.535542490646713, 0.8546592489568846, tensor(2.1405)]
[Mon Jan 16 16:06:22 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 16:06:26 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5232), 0.535542490646713, 0.8560500695410292, tensor(2.1405)]
[Mon Jan 16 16:08:24 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5232), 0.535542490646713, 0.8560500695410292, tensor(2.1509)]
[Mon Jan 16 16:10:12 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 16:11:07 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 16:11:22 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5232), 0.535542490646713, 0.8560500695410292, tensor(2.1509)]
[Mon Jan 16 16:13:15 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 16:14:11 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5232), 0.535542490646713, 0.8560500695410292, tensor(2.1509)]
[Mon Jan 16 16:15:40 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 16:15:44 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5232), 0.535542490646713, 0.8560500695410292, tensor(2.1509)]
[Mon Jan 16 16:18:48 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 16:18:57 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5232), 0.535542490646713, 0.8560500695410292, tensor(2.1509)]
[Mon Jan 16 16:20:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 16:21:47 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5232), 0.535542490646713, 0.8560500695410292, tensor(2.1509)]
early stopping at 12
[2023-01-16 16:23:00,213.213 dlc26te6b6pxn0nk-master-0:5718 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 16:23:00,907.907 dlc26te6b6pxn0nk-master-0:5772 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:23:00,910.910 dlc26te6b6pxn0nk-master-0:5774 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:23:00,999.999 dlc26te6b6pxn0nk-master-0:5773 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:23:01,006.006 dlc26te6b6pxn0nk-master-0:5775 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:23:01,859.859 dlc26te6b6pxn0nk-master-0:5774 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 16:23:02,380.380 dlc26te6b6pxn0nk-master-0:5773 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 16:23:02,381.381 dlc26te6b6pxn0nk-master-0:5775 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 16:23:02,385.385 dlc26te6b6pxn0nk-master-0:5772 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:5772:5772 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:5773:5773 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5775:5775 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5774:5774 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Mon Jan 16 16:24:04 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Mon Jan 16 16:25:57 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 16:26:14 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Mon Jan 16 16:29:23 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7745), 0.4462854088722608, 0.7538247566063978, tensor(1.4569)]
[Mon Jan 16 16:30:58 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6660), 0.4623196151790486, 0.7538247566063978, tensor(1.6456)]
[Mon Jan 16 16:32:48 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6236), 0.4719401389631213, 0.7635605006954103, tensor(1.7361)]
[Mon Jan 16 16:36:06 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 16:36:45 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.6174), 0.4778193479422769, 0.7635605006954103, tensor(1.7717)]
[Mon Jan 16 16:38:35 2023] [cudaHostAllocator] allocates 1.71 GiB
[Mon Jan 16 16:38:58 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6098), 0.49545697487974344, 0.7698191933240612, tensor(1.8675)]
[Mon Jan 16 16:40:22 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.6098), 0.49545697487974344, 0.7788595271210014, tensor(1.8675)]
[Mon Jan 16 16:43:12 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.4965259219668626, 0.8018080667593881, tensor(1.8965)]
[Mon Jan 16 16:45:29 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.4965259219668626, 0.8018080667593881, tensor(1.8965)]
[Mon Jan 16 16:48:26 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.49706039551042225, 0.8066759388038943, tensor(1.8965)]
[Mon Jan 16 16:50:58 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.5002672367717798, 0.8066759388038943, tensor(1.9078)]
[Mon Jan 16 16:52:31 2023] [cudaHostAllocator] allocates 3.42 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Mon Jan 16 16:53:07 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5862), 0.5008017103153394, 0.8066759388038943, tensor(1.9177)]
[Mon Jan 16 16:54:22 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5862), 0.5008017103153394, 0.808066759388039, tensor(1.9177)]
[Mon Jan 16 16:56:58 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 16:57:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5862), 0.5008017103153394, 0.808066759388039, tensor(1.9177)]
[Mon Jan 16 16:59:58 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.5008017103153394, 0.808066759388039, tensor(1.9177)]
[Mon Jan 16 17:01:52 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.504008551576697, 0.8129346314325452, tensor(1.9316)]
[Mon Jan 16 17:04:30 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 17:04:30 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5862), 0.504008551576697, 0.8129346314325452, tensor(1.9316)]
[Mon Jan 16 17:06:33 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5862), 0.504008551576697, 0.8129346314325452, tensor(1.9316)]
[Mon Jan 16 17:09:39 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.504008551576697, 0.8129346314325452, tensor(1.9316)]
[Mon Jan 16 17:11:47 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 17:11:56 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.5045430251202565, 0.8129346314325452, tensor(1.9351)]
[Mon Jan 16 17:13:34 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[Mon Jan 16 17:13:59 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 17:14:04 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.51309460181721, 0.8129346314325452, tensor(1.9636)]
[Mon Jan 16 17:15:53 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5862), 0.51309460181721, 0.8129346314325452, tensor(1.9636)]
[Mon Jan 16 17:17:42 2023] [cudaHostAllocator] allocates 1.71 GiB
[Mon Jan 16 17:18:16 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5862), 0.51309460181721, 0.8129346314325452, tensor(1.9636)]
[Mon Jan 16 17:20:27 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.51309460181721, 0.8129346314325452, tensor(1.9636)]
[Mon Jan 16 17:23:25 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.51309460181721, 0.8129346314325452, tensor(1.9636)]
[Mon Jan 16 17:25:38 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5862), 0.51309460181721, 0.8129346314325452, tensor(1.9636)]
early stopping at 27
[2023-01-16 17:27:18,334.334 dlc26te6b6pxn0nk-master-0:5931 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 17:27:19,062.062 dlc26te6b6pxn0nk-master-0:5986 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:27:19,074.074 dlc26te6b6pxn0nk-master-0:5987 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:27:19,154.154 dlc26te6b6pxn0nk-master-0:5985 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:27:19,154.154 dlc26te6b6pxn0nk-master-0:5988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:27:20,124.124 dlc26te6b6pxn0nk-master-0:5988 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 17:27:20,252.252 dlc26te6b6pxn0nk-master-0:5987 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 17:27:20,254.254 dlc26te6b6pxn0nk-master-0:5986 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 17:27:20,255.255 dlc26te6b6pxn0nk-master-0:5985 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:5985:5985 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:5986:5986 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5988:5988 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:5987:5987 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Mon Jan 16 17:28:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 17:29:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6423), 0.46766435061464456, 0.7892906815020863, tensor(1.6960)]
[Mon Jan 16 17:30:37 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5451), 0.5173703901656868, 0.849095966620306, tensor(2.0418)]
[Mon Jan 16 17:33:37 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5375), 0.5323356493853554, 0.849095966620306, tensor(2.1242)]
[Mon Jan 16 17:35:46 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5375), 0.5323356493853554, 0.8588317107093185, tensor(2.1242)]
[Mon Jan 16 17:37:50 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Mon Jan 16 17:38:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 17:38:51 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5375), 0.5323356493853554, 0.8588317107093185, tensor(2.1242)]
[2023-01-16 17:40:26,838.838 dlc26te6b6pxn0nk-master-0:6066 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 17:40:27,571.571 dlc26te6b6pxn0nk-master-0:6122 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:40:27,573.573 dlc26te6b6pxn0nk-master-0:6121 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:40:27,747.747 dlc26te6b6pxn0nk-master-0:6123 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:40:27,747.747 dlc26te6b6pxn0nk-master-0:6120 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:40:29,962.962 dlc26te6b6pxn0nk-master-0:6121 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 17:40:29,978.978 dlc26te6b6pxn0nk-master-0:6122 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 17:40:30,103.103 dlc26te6b6pxn0nk-master-0:6123 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 17:40:30,109.109 dlc26te6b6pxn0nk-master-0:6120 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:6120:6120 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:6121:6121 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6123:6123 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6122:6122 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Mon Jan 16 17:41:43 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6610), 0.4746125066809193, 0.7496522948539638, tensor(1.7121)]
[Mon Jan 16 17:43:37 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 17:43:54 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5416), 0.5275253874933191, 0.8344923504867872, tensor(2.0960)]
[Mon Jan 16 17:47:03 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5269), 0.5360769641902726, 0.8428372739916551, tensor(2.1535)]
[Mon Jan 16 17:48:36 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 17:49:25 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5269), 0.5360769641902726, 0.8525730180806675, tensor(2.1535)]
[Mon Jan 16 17:50:28 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 17:51:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 17:51:29 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5245), 0.5360769641902726, 0.8609179415855355, tensor(2.1535)]
[2023-01-16 17:52:57,344.344 dlc26te6b6pxn0nk-master-0:6201 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 17:52:57,990.990 dlc26te6b6pxn0nk-master-0:6255 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:52:57,990.990 dlc26te6b6pxn0nk-master-0:6257 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:52:57,990.990 dlc26te6b6pxn0nk-master-0:6256 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:52:58,001.001 dlc26te6b6pxn0nk-master-0:6258 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:52:59,060.060 dlc26te6b6pxn0nk-master-0:6257 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 17:52:59,064.064 dlc26te6b6pxn0nk-master-0:6258 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 17:52:59,068.068 dlc26te6b6pxn0nk-master-0:6256 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 17:52:59,077.077 dlc26te6b6pxn0nk-master-0:6255 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:6255:6255 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:6258:6258 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6256:6256 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6257:6257 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Mon Jan 16 17:54:01 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Mon Jan 16 17:54:47 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7013), 0.4206306787814003, 0.7670375521557719, tensor(1.4019)]
[Mon Jan 16 17:56:16 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5652), 0.5125601282736505, 0.827538247566064, tensor(1.9976)]
[Mon Jan 16 17:59:17 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5619), 0.5125601282736505, 0.8324061196105702, tensor(1.9982)]
[Mon Jan 16 18:01:26 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5424), 0.5323356493853554, 0.8504867872044506, tensor(2.1193)]
[Mon Jan 16 18:03:24 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 18:04:14 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:04:24 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5363), 0.5414216996258685, 0.8504867872044506, tensor(2.1708)]
[Mon Jan 16 18:07:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:07:12 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5363), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
[Mon Jan 16 18:09:16 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5363), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
[Mon Jan 16 18:10:53 2023] [cudaHostAllocator] allocates 3.42 GiB
[Mon Jan 16 18:11:57 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5320), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
[Mon Jan 16 18:14:01 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 18:14:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5320), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
[Mon Jan 16 18:16:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:16:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5320), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
[Mon Jan 16 18:19:19 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5320), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
[Mon Jan 16 18:21:53 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:21:55 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5320), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
[Mon Jan 16 18:23:33 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5320), 0.5414216996258685, 0.8560500695410292, tensor(2.1708)]
early stopping at 13
[2023-01-16 18:25:30,836.836 dlc26te6b6pxn0nk-master-0:6366 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 18:25:31,488.488 dlc26te6b6pxn0nk-master-0:6420 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:25:31,489.489 dlc26te6b6pxn0nk-master-0:6422 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:25:31,580.580 dlc26te6b6pxn0nk-master-0:6423 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:25:31,583.583 dlc26te6b6pxn0nk-master-0:6421 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:25:33,054.054 dlc26te6b6pxn0nk-master-0:6423 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 18:25:33,058.058 dlc26te6b6pxn0nk-master-0:6421 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 18:25:33,463.463 dlc26te6b6pxn0nk-master-0:6422 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 18:25:33,468.468 dlc26te6b6pxn0nk-master-0:6420 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:6420:6420 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:6422:6422 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6423:6423 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6421:6421 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[Mon Jan 16 18:26:35 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Mon Jan 16 18:28:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:28:46 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6813), 0.46178514163548906, 0.7600834492350487, tensor(1.6277)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Mon Jan 16 18:31:29 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5974), 0.5034740780331374, 0.7920723226703755, tensor(1.9200)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Mon Jan 16 18:33:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 18:34:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5767), 0.5168359166221272, 0.8317107093184979, tensor(2.0075)]
[Mon Jan 16 18:35:24 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 18:36:16 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:36:25 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5594), 0.5227151256012827, 0.8317107093184979, tensor(2.0542)]
[Mon Jan 16 18:39:03 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:39:07 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5534), 0.5275253874933191, 0.8400556328233658, tensor(2.0842)]
[Mon Jan 16 18:40:56 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5469), 0.5376803848209514, 0.8421418636995828, tensor(2.1415)]
[Mon Jan 16 18:42:38 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 18:43:24 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5469), 0.5376803848209514, 0.8421418636995828, tensor(2.1415)]
[Mon Jan 16 18:45:13 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:45:27 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5465), 0.5376803848209514, 0.8497913769123783, tensor(2.1415)]
[Mon Jan 16 18:47:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:47:48 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5465), 0.5376803848209514, 0.8497913769123783, tensor(2.1415)]
[Mon Jan 16 18:50:46 2023] [cudaHostAllocator] allocates 1.71 GiB
[Mon Jan 16 18:50:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5440), 0.5376803848209514, 0.8497913769123783, tensor(2.1444)]
[Mon Jan 16 18:53:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[Mon Jan 16 18:53:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5398), 0.5376803848209514, 0.8497913769123783, tensor(2.1444)]
[Mon Jan 16 18:54:55 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 18:55:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5398), 0.5376803848209514, 0.8497913769123783, tensor(2.1444)]
[Mon Jan 16 18:56:51 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5339), 0.5376803848209514, 0.8497913769123783, tensor(2.1518)]
[Mon Jan 16 18:59:32 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5339), 0.5376803848209514, 0.8497913769123783, tensor(2.1518)]
[Mon Jan 16 19:02:33 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5339), 0.5376803848209514, 0.8497913769123783, tensor(2.1518)]
[Mon Jan 16 19:04:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5339), 0.5376803848209514, 0.8497913769123783, tensor(2.1518)]
[Mon Jan 16 19:07:10 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5339), 0.5376803848209514, 0.8497913769123783, tensor(2.1518)]
[Mon Jan 16 19:09:18 2023] [cudaHostAllocator] allocates 340.32 MiB
[Mon Jan 16 19:10:12 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5339), 0.5376803848209514, 0.8497913769123783, tensor(2.1518)]
early stopping at 19
[2023-01-16 19:11:15,170.170 dlc26te6b6pxn0nk-master-0:6550 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 19:11:15,824.824 dlc26te6b6pxn0nk-master-0:6605 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:11:15,834.834 dlc26te6b6pxn0nk-master-0:6606 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:11:15,987.987 dlc26te6b6pxn0nk-master-0:6604 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:11:16,283.283 dlc26te6b6pxn0nk-master-0:6607 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:11:17,071.071 dlc26te6b6pxn0nk-master-0:6605 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 19:11:17,417.417 dlc26te6b6pxn0nk-master-0:6607 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 19:11:17,727.727 dlc26te6b6pxn0nk-master-0:6606 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 19:11:17,730.730 dlc26te6b6pxn0nk-master-0:6604 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:6604:6604 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:6607:6607 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6606:6606 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6605:6605 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.6146), 0.4991982896846606, 0.7726008344923505, tensor(1.8814)]
[tensor(-0.5866), 0.521111704970604, 0.8386648122392212, tensor(2.0189)]
[tensor(-0.5399), 0.5334045964724746, 0.8609179415855355, tensor(2.1271)]
[tensor(-0.5195), 0.5419561731694281, 0.8609179415855355, tensor(2.1902)]
[tensor(-0.5195), 0.5419561731694281, 0.8609179415855355, tensor(2.1902)]
[2023-01-16 19:22:34,595.595 dlc26te6b6pxn0nk-master-0:6683 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 19:22:35,237.237 dlc26te6b6pxn0nk-master-0:6738 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:22:35,237.237 dlc26te6b6pxn0nk-master-0:6739 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:22:35,414.414 dlc26te6b6pxn0nk-master-0:6740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:22:35,497.497 dlc26te6b6pxn0nk-master-0:6737 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:22:37,120.120 dlc26te6b6pxn0nk-master-0:6739 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 19:22:37,344.344 dlc26te6b6pxn0nk-master-0:6740 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 19:22:37,347.347 dlc26te6b6pxn0nk-master-0:6738 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 19:22:37,356.356 dlc26te6b6pxn0nk-master-0:6737 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:6737:6737 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:6738:6738 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6740:6740 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6739:6739 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5241), 0.5403527525387494, 0.8595271210013908, tensor(2.1777)]
[tensor(-0.5228), 0.5462319615179049, 0.868567454798331, tensor(2.2083)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5161), 0.5462319615179049, 0.868567454798331, tensor(2.2083)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5161), 0.5462319615179049, 0.8699582753824756, tensor(2.2083)]
[tensor(-0.5161), 0.5462319615179049, 0.8699582753824756, tensor(2.2111)]
[2023-01-16 19:33:51,048.048 dlc26te6b6pxn0nk-master-0:6816 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 19:33:51,700.700 dlc26te6b6pxn0nk-master-0:6871 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:33:51,755.755 dlc26te6b6pxn0nk-master-0:6873 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:33:51,762.762 dlc26te6b6pxn0nk-master-0:6872 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:33:51,838.838 dlc26te6b6pxn0nk-master-0:6870 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:33:53,585.585 dlc26te6b6pxn0nk-master-0:6871 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 19:33:53,642.642 dlc26te6b6pxn0nk-master-0:6873 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 19:33:54,113.113 dlc26te6b6pxn0nk-master-0:6872 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 19:33:54,117.117 dlc26te6b6pxn0nk-master-0:6870 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:6870:6870 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:6871:6871 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6873:6873 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:6872:6872 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5313), 0.5339390700160342, 0.8595271210013908, tensor(2.1384)]
[tensor(-0.5215), 0.5440940673436665, 0.8595271210013908, tensor(2.1990)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5215), 0.5440940673436665, 0.8595271210013908, tensor(2.1990)]
[tensor(-0.5090), 0.5456974879743453, 0.8595271210013908, tensor(2.2195)]
[tensor(-0.5090), 0.5456974879743453, 0.8630041724617524, tensor(2.2195)]
[tensor(-0.5090), 0.5456974879743453, 0.8630041724617524, tensor(2.2195)]
[tensor(-0.5090), 0.5456974879743453, 0.8636995827538247, tensor(2.2195)]
[tensor(-0.5090), 0.5462319615179049, 0.8636995827538247, tensor(2.2195)]
[tensor(-0.5090), 0.5462319615179049, 0.8636995827538247, tensor(2.2195)]
[tensor(-0.5090), 0.5462319615179049, 0.866481223922114, tensor(2.2195)]
[tensor(-0.5090), 0.5462319615179049, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5462319615179049, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5462319615179049, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5478353821485836, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5478353821485836, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5478353821485836, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5478353821485836, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5478353821485836, 0.8720445062586927, tensor(2.2195)]
[tensor(-0.5090), 0.5478353821485836, 0.8720445062586927, tensor(2.2195)]
early stopping at 19
[2023-01-16 20:18:20,259.259 dlc26te6b6pxn0nk-master-0:6999 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 20:18:20,991.991 dlc26te6b6pxn0nk-master-0:7055 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:18:20,997.997 dlc26te6b6pxn0nk-master-0:7054 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:18:21,168.168 dlc26te6b6pxn0nk-master-0:7056 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:18:21,168.168 dlc26te6b6pxn0nk-master-0:7053 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:18:22,293.293 dlc26te6b6pxn0nk-master-0:7056 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 20:18:22,453.453 dlc26te6b6pxn0nk-master-0:7054 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 20:18:22,514.514 dlc26te6b6pxn0nk-master-0:7055 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 20:18:22,519.519 dlc26te6b6pxn0nk-master-0:7053 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:7053:7053 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:7054:7054 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7056:7056 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7055:7055 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5391), 0.5339390700160342, 0.8636995827538247, tensor(2.1306)]
[tensor(-0.5257), 0.5371459112773918, 0.8636995827538247, tensor(2.1600)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5075), 0.5376803848209514, 0.8636995827538247, tensor(2.1809)]
[tensor(-0.5075), 0.5376803848209514, 0.8636995827538247, tensor(2.1809)]
[tensor(-0.5075), 0.5456974879743453, 0.866481223922114, tensor(2.2117)]
[tensor(-0.5075), 0.5456974879743453, 0.866481223922114, tensor(2.2117)]
[tensor(-0.5075), 0.5456974879743453, 0.866481223922114, tensor(2.2117)]
[tensor(-0.5075), 0.5456974879743453, 0.866481223922114, tensor(2.2117)]
[tensor(-0.5075), 0.5456974879743453, 0.866481223922114, tensor(2.2117)]
[tensor(-0.5075), 0.5456974879743453, 0.866481223922114, tensor(2.2117)]
early stopping at 10
[2023-01-16 20:40:36,359.359 dlc26te6b6pxn0nk-master-0:7148 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 20:40:37,015.015 dlc26te6b6pxn0nk-master-0:7204 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:40:37,018.018 dlc26te6b6pxn0nk-master-0:7203 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:40:37,189.189 dlc26te6b6pxn0nk-master-0:7205 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:40:37,266.266 dlc26te6b6pxn0nk-master-0:7202 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:40:38,902.902 dlc26te6b6pxn0nk-master-0:7204 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 20:40:39,110.110 dlc26te6b6pxn0nk-master-0:7205 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 20:40:39,367.367 dlc26te6b6pxn0nk-master-0:7203 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 20:40:39,368.368 dlc26te6b6pxn0nk-master-0:7202 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:7202:7202 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:7204:7204 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7203:7203 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7205:7205 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5263), 0.5350080171031534, 0.8602225312934632, tensor(2.1488)]
[tensor(-0.5162), 0.5435595938001069, 0.8643949930458971, tensor(2.2016)]
[tensor(-0.5162), 0.5435595938001069, 0.8657858136300417, tensor(2.2016)]
[tensor(-0.5159), 0.5435595938001069, 0.8671766342141863, tensor(2.2016)]
[tensor(-0.5159), 0.5435595938001069, 0.8713490959666204, tensor(2.2016)]
[2023-01-16 20:52:00,740.740 dlc26te6b6pxn0nk-master-0:7281 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 20:52:01,460.460 dlc26te6b6pxn0nk-master-0:7337 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:52:01,485.485 dlc26te6b6pxn0nk-master-0:7336 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:52:01,572.572 dlc26te6b6pxn0nk-master-0:7335 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:52:01,663.663 dlc26te6b6pxn0nk-master-0:7338 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:52:02,832.832 dlc26te6b6pxn0nk-master-0:7336 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 20:52:02,943.943 dlc26te6b6pxn0nk-master-0:7338 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 20:52:03,354.354 dlc26te6b6pxn0nk-master-0:7337 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 20:52:03,357.357 dlc26te6b6pxn0nk-master-0:7335 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:7335:7335 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:7338:7338 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7336:7336 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7337:7337 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5227), 0.5387493319080705, 0.8643949930458971, tensor(2.1710)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5227), 0.5435595938001069, 0.8643949930458971, tensor(2.1949)]
[tensor(-0.5227), 0.5435595938001069, 0.8643949930458971, tensor(2.1949)]
[tensor(-0.5227), 0.5435595938001069, 0.8706536856745479, tensor(2.1949)]
[tensor(-0.5125), 0.5478353821485836, 0.872739916550765, tensor(2.2267)]
[2023-01-16 21:03:52,150.150 dlc26te6b6pxn0nk-master-0:7415 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 21:03:52,803.803 dlc26te6b6pxn0nk-master-0:7472 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:03:52,840.840 dlc26te6b6pxn0nk-master-0:7469 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:03:52,889.889 dlc26te6b6pxn0nk-master-0:7471 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:03:52,896.896 dlc26te6b6pxn0nk-master-0:7470 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:03:54,185.185 dlc26te6b6pxn0nk-master-0:7470 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 21:03:54,289.289 dlc26te6b6pxn0nk-master-0:7471 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 21:03:54,704.704 dlc26te6b6pxn0nk-master-0:7472 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 21:03:54,714.714 dlc26te6b6pxn0nk-master-0:7469 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:7469:7469 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:7472:7472 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7471:7471 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7470:7470 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5693), 0.5146980224478889, 0.8414464534075105, tensor(2.0042)]
[tensor(-0.5482), 0.5195082843399251, 0.8477051460361613, tensor(2.0494)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5296), 0.5296632816675575, 0.8678720445062587, tensor(2.1187)]
[tensor(-0.5117), 0.5462319615179049, 0.8678720445062587, tensor(2.2194)]
[tensor(-0.5117), 0.5462319615179049, 0.8678720445062587, tensor(2.2194)]
[tensor(-0.5117), 0.5462319615179049, 0.8678720445062587, tensor(2.2194)]
[tensor(-0.5117), 0.5462319615179049, 0.8678720445062587, tensor(2.2194)]
[tensor(-0.5117), 0.5462319615179049, 0.8678720445062587, tensor(2.2194)]
[tensor(-0.5117), 0.5462319615179049, 0.8678720445062587, tensor(2.2194)]
early stopping at 9
[2023-01-16 21:24:01,112.112 dlc26te6b6pxn0nk-master-0:7561 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 21:24:01,828.828 dlc26te6b6pxn0nk-master-0:7616 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:24:01,852.852 dlc26te6b6pxn0nk-master-0:7617 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:24:01,941.941 dlc26te6b6pxn0nk-master-0:7618 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:24:02,031.031 dlc26te6b6pxn0nk-master-0:7615 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:24:03,713.713 dlc26te6b6pxn0nk-master-0:7616 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 21:24:03,812.812 dlc26te6b6pxn0nk-master-0:7618 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 21:24:04,198.198 dlc26te6b6pxn0nk-master-0:7617 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 21:24:04,202.202 dlc26te6b6pxn0nk-master-0:7615 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:7615:7615 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:7618:7618 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7616:7616 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7617:7617 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5792), 0.5077498663816141, 0.8504867872044506, tensor(1.9596)]
[tensor(-0.5685), 0.5173703901656868, 0.8588317107093185, tensor(2.0183)]
[tensor(-0.5233), 0.5408872260823089, 0.8588317107093185, tensor(2.1811)]
[tensor(-0.5177), 0.5521111704970604, 0.8636995827538247, tensor(2.2428)]
[tensor(-0.5177), 0.5521111704970604, 0.8650904033379694, tensor(2.2428)]
[tensor(-0.5177), 0.5521111704970604, 0.8720445062586927, tensor(2.2428)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5177), 0.5547835382148584, 0.8720445062586927, tensor(2.2540)]
[tensor(-0.5177), 0.5547835382148584, 0.8720445062586927, tensor(2.2540)]
[tensor(-0.5177), 0.5547835382148584, 0.8720445062586927, tensor(2.2540)]
[tensor(-0.5177), 0.5547835382148584, 0.8720445062586927, tensor(2.2540)]
[tensor(-0.5177), 0.5547835382148584, 0.8720445062586927, tensor(2.2540)]
[tensor(-0.5177), 0.5547835382148584, 0.8720445062586927, tensor(2.2540)]
early stopping at 12
[2023-01-16 21:51:37,495.495 dlc26te6b6pxn0nk-master-0:7719 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 21:51:38,135.135 dlc26te6b6pxn0nk-master-0:7773 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:51:38,135.135 dlc26te6b6pxn0nk-master-0:7776 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:51:38,220.220 dlc26te6b6pxn0nk-master-0:7774 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:51:38,230.230 dlc26te6b6pxn0nk-master-0:7775 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:51:39,083.083 dlc26te6b6pxn0nk-master-0:7776 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 21:51:39,614.614 dlc26te6b6pxn0nk-master-0:7775 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 21:51:39,618.618 dlc26te6b6pxn0nk-master-0:7774 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 21:51:39,620.620 dlc26te6b6pxn0nk-master-0:7773 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:7773:7773 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:7776:7776 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7774:7774 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7775:7775 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7762), 0.4462854088722608, 0.6481223922114048, tensor(1.4552)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7387), 0.4462854088722608, 0.6495132127955494, tensor(1.4552)]
[tensor(-0.6357), 0.46392303580972744, 0.7920723226703755, tensor(1.6839)]
[tensor(-0.5862), 0.5034740780331374, 0.8212795549374131, tensor(1.9311)]
[tensor(-0.5843), 0.5088188134687333, 0.8212795549374131, tensor(1.9598)]
[2023-01-16 22:03:17,914.914 dlc26te6b6pxn0nk-master-0:7853 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 22:03:18,555.555 dlc26te6b6pxn0nk-master-0:7908 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:03:18,560.560 dlc26te6b6pxn0nk-master-0:7907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:03:18,641.641 dlc26te6b6pxn0nk-master-0:7910 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:03:18,652.652 dlc26te6b6pxn0nk-master-0:7909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:03:20,022.022 dlc26te6b6pxn0nk-master-0:7910 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 22:03:20,024.024 dlc26te6b6pxn0nk-master-0:7909 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 22:03:20,465.465 dlc26te6b6pxn0nk-master-0:7908 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 22:03:20,474.474 dlc26te6b6pxn0nk-master-0:7907 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:7907:7907 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:7908:7908 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7910:7910 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:7909:7909 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7486), 0.4462854088722608, 0.6481223922114048, tensor(1.4829)]
[tensor(-0.6721), 0.48690539818278994, 0.6995827538247567, tensor(1.7625)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5989), 0.49599144842330306, 0.8184979137691237, tensor(1.8810)]
[tensor(-0.5793), 0.5120256547300909, 0.8261474269819193, tensor(1.9809)]
[tensor(-0.5649), 0.5195082843399251, 0.8289290681502086, tensor(2.0326)]
[2023-01-16 22:15:52,418.418 dlc26te6b6pxn0nk-master-0:7987 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 22:15:53,055.055 dlc26te6b6pxn0nk-master-0:8042 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:15:53,055.055 dlc26te6b6pxn0nk-master-0:8044 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:15:53,056.056 dlc26te6b6pxn0nk-master-0:8041 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:15:53,058.058 dlc26te6b6pxn0nk-master-0:8043 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:15:54,117.117 dlc26te6b6pxn0nk-master-0:8042 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 22:15:55,103.103 dlc26te6b6pxn0nk-master-0:8044 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 22:15:55,111.111 dlc26te6b6pxn0nk-master-0:8043 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 22:15:55,119.119 dlc26te6b6pxn0nk-master-0:8041 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:8041:8041 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:8044:8044 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8043:8043 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8042:8042 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6229), 0.4938535542490647, 0.8045897079276774, tensor(1.8463)]
[tensor(-0.5826), 0.4965259219668626, 0.825452016689847, tensor(1.9000)]
[tensor(-0.5672), 0.5173703901656868, 0.8351877607788595, tensor(2.0197)]
[tensor(-0.5392), 0.5334045964724746, 0.8351877607788595, tensor(2.1278)]
[tensor(-0.5392), 0.5334045964724746, 0.8365785813630042, tensor(2.1278)]
[tensor(-0.5392), 0.5334045964724746, 0.8365785813630042, tensor(2.1278)]
[tensor(-0.5362), 0.55264564404062, 0.8365785813630042, tensor(2.2270)]
[tensor(-0.5362), 0.55264564404062, 0.8365785813630042, tensor(2.2270)]
[tensor(-0.5362), 0.55264564404062, 0.8407510431154381, tensor(2.2270)]
[tensor(-0.5362), 0.55264564404062, 0.844923504867872, tensor(2.2270)]
[tensor(-0.5362), 0.55264564404062, 0.844923504867872, tensor(2.2270)]
[tensor(-0.5362), 0.55264564404062, 0.844923504867872, tensor(2.2270)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5362), 0.55264564404062, 0.844923504867872, tensor(2.2270)]
[tensor(-0.5362), 0.55264564404062, 0.844923504867872, tensor(2.2270)]
[tensor(-0.5362), 0.55264564404062, 0.844923504867872, tensor(2.2270)]
early stopping at 15
[2023-01-16 22:49:26,071.071 dlc26te6b6pxn0nk-master-0:8154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 22:49:26,793.793 dlc26te6b6pxn0nk-master-0:8210 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:49:26,813.813 dlc26te6b6pxn0nk-master-0:8209 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:49:26,874.874 dlc26te6b6pxn0nk-master-0:8211 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:49:27,036.036 dlc26te6b6pxn0nk-master-0:8208 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:49:28,696.696 dlc26te6b6pxn0nk-master-0:8210 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 22:49:28,742.742 dlc26te6b6pxn0nk-master-0:8211 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 22:49:29,170.170 dlc26te6b6pxn0nk-master-0:8209 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 22:49:29,173.173 dlc26te6b6pxn0nk-master-0:8208 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:8208:8208 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:8211:8211 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8209:8209 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8210:8210 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5505), 0.5253874933190807, 0.8546592489568846, tensor(2.0765)]
[tensor(-0.5411), 0.5307322287546766, 0.8546592489568846, tensor(2.1125)]
[tensor(-0.5203), 0.5392838054516301, 0.8546592489568846, tensor(2.1761)]
[tensor(-0.5203), 0.5414216996258685, 0.8546592489568846, tensor(2.1833)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5203), 0.5467664350614645, 0.8602225312934632, tensor(2.2090)]
[tensor(-0.5203), 0.5467664350614645, 0.8602225312934632, tensor(2.2090)]
[tensor(-0.5203), 0.5467664350614645, 0.8602225312934632, tensor(2.2090)]
[tensor(-0.5203), 0.5467664350614645, 0.8602225312934632, tensor(2.2090)]
[tensor(-0.5203), 0.5467664350614645, 0.8602225312934632, tensor(2.2090)]
[tensor(-0.5203), 0.5467664350614645, 0.8602225312934632, tensor(2.2090)]
early stopping at 10
[2023-01-16 23:14:41,193.193 dlc26te6b6pxn0nk-master-0:8307 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 23:14:41,834.834 dlc26te6b6pxn0nk-master-0:8361 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:14:41,834.834 dlc26te6b6pxn0nk-master-0:8363 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:14:41,923.923 dlc26te6b6pxn0nk-master-0:8362 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:14:41,927.927 dlc26te6b6pxn0nk-master-0:8364 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:14:43,077.077 dlc26te6b6pxn0nk-master-0:8364 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 23:14:43,078.078 dlc26te6b6pxn0nk-master-0:8362 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 23:14:43,777.777 dlc26te6b6pxn0nk-master-0:8363 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 23:14:43,778.778 dlc26te6b6pxn0nk-master-0:8361 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:8361:8361 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:8363:8363 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8364:8364 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8362:8362 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5444), 0.5173703901656868, 0.8553546592489569, tensor(2.0425)]
[tensor(-0.5244), 0.5323356493853554, 0.8588317107093185, tensor(2.1372)]
[tensor(-0.5244), 0.5344735435595938, 0.8588317107093185, tensor(2.1436)]
[tensor(-0.5048), 0.5419561731694281, 0.8616133518776078, tensor(2.2050)]
[tensor(-0.5048), 0.5419561731694281, 0.8706536856745479, tensor(2.2050)]
[2023-01-16 23:26:04,613.613 dlc26te6b6pxn0nk-master-0:8440 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 23:26:05,261.261 dlc26te6b6pxn0nk-master-0:8495 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:26:05,261.261 dlc26te6b6pxn0nk-master-0:8496 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:26:05,430.430 dlc26te6b6pxn0nk-master-0:8494 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:26:05,515.515 dlc26te6b6pxn0nk-master-0:8497 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:26:06,610.610 dlc26te6b6pxn0nk-master-0:8496 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 23:26:06,766.766 dlc26te6b6pxn0nk-master-0:8497 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 23:26:07,138.138 dlc26te6b6pxn0nk-master-0:8495 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 23:26:07,143.143 dlc26te6b6pxn0nk-master-0:8494 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:8494:8494 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:8495:8495 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8496:8496 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8497:8497 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5608), 0.5237840726884019, 0.8581363004172462, tensor(2.0581)]
[tensor(-0.5608), 0.5430251202565473, 0.8581363004172462, tensor(2.1441)]
[tensor(-0.5221), 0.5430251202565473, 0.8616133518776078, tensor(2.1930)]
[tensor(-0.5221), 0.5430251202565473, 0.8650904033379694, tensor(2.1930)]
[tensor(-0.5132), 0.5462319615179049, 0.8650904033379694, tensor(2.2180)]
[2023-01-16 23:38:48,170.170 dlc26te6b6pxn0nk-master-0:8576 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 23:38:48,896.896 dlc26te6b6pxn0nk-master-0:8631 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:38:49,050.050 dlc26te6b6pxn0nk-master-0:8632 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:38:49,092.092 dlc26te6b6pxn0nk-master-0:8630 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:38:49,172.172 dlc26te6b6pxn0nk-master-0:8633 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:38:50,385.385 dlc26te6b6pxn0nk-master-0:8632 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 23:38:50,458.458 dlc26te6b6pxn0nk-master-0:8633 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 23:38:50,770.770 dlc26te6b6pxn0nk-master-0:8631 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 23:38:50,771.771 dlc26te6b6pxn0nk-master-0:8630 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:8630:8630 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:8631:8631 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8632:8632 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8633:8633 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5407), 0.5344735435595938, 0.8574408901251739, tensor(2.1316)]
[tensor(-0.5407), 0.5360769641902726, 0.8574408901251739, tensor(2.1316)]
[tensor(-0.5248), 0.5403527525387494, 0.8650904033379694, tensor(2.1769)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5096), 0.5563869588455371, 0.8650904033379694, tensor(2.2724)]
[tensor(-0.5096), 0.5563869588455371, 0.8734353268428373, tensor(2.2724)]
[tensor(-0.5096), 0.5563869588455371, 0.8734353268428373, tensor(2.2724)]
[tensor(-0.5096), 0.5563869588455371, 0.8734353268428373, tensor(2.2724)]
[tensor(-0.5096), 0.5563869588455371, 0.8734353268428373, tensor(2.2724)]
[tensor(-0.5096), 0.5563869588455371, 0.8734353268428373, tensor(2.2724)]
[tensor(-0.5096), 0.5563869588455371, 0.8734353268428373, tensor(2.2724)]
early stopping at 10
[2023-01-17 00:01:13,261.261 dlc26te6b6pxn0nk-master-0:8725 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:01:13,916.916 dlc26te6b6pxn0nk-master-0:8779 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:01:13,916.916 dlc26te6b6pxn0nk-master-0:8781 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:01:13,999.999 dlc26te6b6pxn0nk-master-0:8780 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:01:14,007.007 dlc26te6b6pxn0nk-master-0:8782 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:01:15,150.150 dlc26te6b6pxn0nk-master-0:8782 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:01:15,151.151 dlc26te6b6pxn0nk-master-0:8780 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:01:15,861.861 dlc26te6b6pxn0nk-master-0:8781 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:01:15,868.868 dlc26te6b6pxn0nk-master-0:8779 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:8779:8779 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:8782:8782 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8780:8780 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8781:8781 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5864), 0.498663816141101, 0.8351877607788595, tensor(1.9069)]
[tensor(-0.5534), 0.5296632816675575, 0.8532684283727399, tensor(2.0949)]
[tensor(-0.5409), 0.5296632816675575, 0.8532684283727399, tensor(2.0949)]
[tensor(-0.5408), 0.5366114377338321, 0.8595271210013908, tensor(2.1422)]
[tensor(-0.5263), 0.5408872260823089, 0.8595271210013908, tensor(2.1781)]
[tensor(-0.5263), 0.5408872260823089, 0.8595271210013908, tensor(2.1781)]
[tensor(-0.5122), 0.5537145911277391, 0.8595271210013908, tensor(2.2564)]
[tensor(-0.5122), 0.5537145911277391, 0.8595271210013908, tensor(2.2564)]
[tensor(-0.5122), 0.5537145911277391, 0.8630041724617524, tensor(2.2564)]
[tensor(-0.5122), 0.5537145911277391, 0.8630041724617524, tensor(2.2564)]
[tensor(-0.5122), 0.5537145911277391, 0.8630041724617524, tensor(2.2564)]
[tensor(-0.5122), 0.5537145911277391, 0.8630041724617524, tensor(2.2564)]
[tensor(-0.5122), 0.5537145911277391, 0.8630041724617524, tensor(2.2564)]
[tensor(-0.5122), 0.5537145911277391, 0.8630041724617524, tensor(2.2564)]
early stopping at 14
[2023-01-17 00:32:41,787.787 dlc26te6b6pxn0nk-master-0:8888 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:32:42,441.441 dlc26te6b6pxn0nk-master-0:8945 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:32:42,443.443 dlc26te6b6pxn0nk-master-0:8944 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:32:42,445.445 dlc26te6b6pxn0nk-master-0:8943 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:32:42,446.446 dlc26te6b6pxn0nk-master-0:8942 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:32:43,481.481 dlc26te6b6pxn0nk-master-0:8945 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:32:43,482.482 dlc26te6b6pxn0nk-master-0:8944 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:32:44,444.444 dlc26te6b6pxn0nk-master-0:8943 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:32:44,445.445 dlc26te6b6pxn0nk-master-0:8942 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:8942:8942 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:8943:8943 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8945:8945 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:8944:8944 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 00:33:46 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7395), 0.44468198824158206, 0.6912378303198887, tensor(1.4839)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 00:35:59 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6011), 0.48957776590058794, 0.8164116828929068, tensor(1.8468)]
[Tue Jan 17 00:38:47 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6011), 0.48957776590058794, 0.8164116828929068, tensor(1.8468)]
[Tue Jan 17 00:40:51 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5881), 0.5018706574024586, 0.8344923504867872, tensor(1.9212)]
[Tue Jan 17 00:42:49 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 00:43:42 2023] [cudaHostAllocator] allocates 3.42 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5881), 0.5018706574024586, 0.8344923504867872, tensor(1.9212)]
[2023-01-17 00:45:27,343.343 dlc26te6b6pxn0nk-master-0:9024 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:45:27,993.993 dlc26te6b6pxn0nk-master-0:9078 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:45:28,013.013 dlc26te6b6pxn0nk-master-0:9081 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:45:28,078.078 dlc26te6b6pxn0nk-master-0:9079 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:45:28,093.093 dlc26te6b6pxn0nk-master-0:9080 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:45:28,983.983 dlc26te6b6pxn0nk-master-0:9081 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:45:29,501.501 dlc26te6b6pxn0nk-master-0:9079 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:45:29,503.503 dlc26te6b6pxn0nk-master-0:9080 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:45:29,511.511 dlc26te6b6pxn0nk-master-0:9078 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:9078:9078 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:9079:9079 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9081:9081 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9080:9080 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 00:46:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5519), 0.5136290753607696, 0.8477051460361613, tensor(2.0162)]
[Tue Jan 17 00:48:23 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 00:48:40 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5515), 0.5136290753607696, 0.8477051460361613, tensor(2.0162)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Tue Jan 17 00:51:46 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5515), 0.5136290753607696, 0.8789986091794159, tensor(2.0162)]
[Tue Jan 17 00:53:20 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5158), 0.5419561731694281, 0.8789986091794159, tensor(2.1940)]
[Tue Jan 17 00:55:11 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5004), 0.5494388027792624, 0.8789986091794159, tensor(2.2467)]
[2023-01-17 00:57:35,798.798 dlc26te6b6pxn0nk-master-0:9158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:57:36,478.478 dlc26te6b6pxn0nk-master-0:9213 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:57:36,491.491 dlc26te6b6pxn0nk-master-0:9214 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:57:36,635.635 dlc26te6b6pxn0nk-master-0:9212 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:57:36,741.741 dlc26te6b6pxn0nk-master-0:9215 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:57:37,842.842 dlc26te6b6pxn0nk-master-0:9214 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:57:37,978.978 dlc26te6b6pxn0nk-master-0:9215 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:57:38,349.349 dlc26te6b6pxn0nk-master-0:9213 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:57:38,359.359 dlc26te6b6pxn0nk-master-0:9212 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:9212:9212 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:9213:9213 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9215:9215 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9214:9214 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 00:58:43 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Tue Jan 17 01:00:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:01:02 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7742), 0.4462854088722608, 0.6481223922114048, tensor(1.4572)]
[Tue Jan 17 01:04:14 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.7286), 0.4462854088722608, 0.6481223922114048, tensor(1.4572)]
[Tue Jan 17 01:05:55 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 01:06:46 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6588), 0.4462854088722608, 0.8018080667593881, tensor(1.5165)]
[Tue Jan 17 01:08:06 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 01:09:00 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6231), 0.481560662747194, 0.8178025034770514, tensor(1.7847)]
[Tue Jan 17 01:11:10 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:11:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:12:16 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5886), 0.4831640833778728, 0.8212795549374131, tensor(1.8272)]
[Tue Jan 17 01:14:10 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5754), 0.5120256547300909, 0.8268428372739917, tensor(1.9848)]
[Tue Jan 17 01:15:55 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 01:16:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5754), 0.5227151256012827, 0.8268428372739917, tensor(2.0359)]
[Tue Jan 17 01:18:49 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 01:19:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5724), 0.5227151256012827, 0.8268428372739917, tensor(2.0359)]
[Tue Jan 17 01:21:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:21:18 2023] [cudaHostAllocator] allocates 1.71 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5705), 0.5227151256012827, 0.8268428372739917, tensor(2.0359)]
[Tue Jan 17 01:24:26 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5705), 0.5227151256012827, 0.8268428372739917, tensor(2.0359)]
[Tue Jan 17 01:27:05 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5705), 0.5227151256012827, 0.8331015299026425, tensor(2.0359)]
[Tue Jan 17 01:28:46 2023] [cudaHostAllocator] allocates 3.42 GiB
[Tue Jan 17 01:29:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5705), 0.5227151256012827, 0.8331015299026425, tensor(2.0359)]
[Tue Jan 17 01:30:57 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5705), 0.5227151256012827, 0.8331015299026425, tensor(2.0359)]
[Tue Jan 17 01:33:40 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5598), 0.5253874933190807, 0.8331015299026425, tensor(2.0672)]
[Tue Jan 17 01:36:44 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:36:50 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5544), 0.5253874933190807, 0.8331015299026425, tensor(2.0672)]
[Tue Jan 17 01:39:07 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 01:39:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5544), 0.5318011758417959, 0.8331015299026425, tensor(2.0985)]
[Tue Jan 17 01:41:17 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5544), 0.5318011758417959, 0.8400556328233658, tensor(2.0985)]
[Tue Jan 17 01:43:58 2023] [cudaHostAllocator] allocates 3.42 GiB
[Tue Jan 17 01:44:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:44:58 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5544), 0.5318011758417959, 0.8400556328233658, tensor(2.0985)]
[Tue Jan 17 01:47:11 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5544), 0.5318011758417959, 0.8400556328233658, tensor(2.0985)]
[Tue Jan 17 01:49:22 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:49:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5544), 0.5318011758417959, 0.8421418636995828, tensor(2.0985)]
[Tue Jan 17 01:51:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:51:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:51:46 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5544), 0.5318011758417959, 0.8421418636995828, tensor(2.0985)]
[Tue Jan 17 01:53:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:54:22 2023] [cudaHostAllocator] allocates 170.16 MiB
[tensor(-0.5544), 0.5318011758417959, 0.8421418636995828, tensor(2.0985)]
[Tue Jan 17 01:56:12 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5544), 0.5318011758417959, 0.8421418636995828, tensor(2.0985)]
[Tue Jan 17 01:58:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 01:58:55 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 01:59:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5544), 0.5318011758417959, 0.8421418636995828, tensor(2.0985)]
[Tue Jan 17 02:01:13 2023] [cudaHostAllocator] allocates 3.42 GiB
[Tue Jan 17 02:02:06 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5544), 0.5318011758417959, 0.8421418636995828, tensor(2.0985)]
early stopping at 26
[2023-01-17 02:03:36,189.189 dlc26te6b6pxn0nk-master-0:9373 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 02:03:36,896.896 dlc26te6b6pxn0nk-master-0:9428 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:03:36,928.928 dlc26te6b6pxn0nk-master-0:9429 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:03:37,006.006 dlc26te6b6pxn0nk-master-0:9430 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:03:37,093.093 dlc26te6b6pxn0nk-master-0:9427 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:03:38,796.796 dlc26te6b6pxn0nk-master-0:9428 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 02:03:38,884.884 dlc26te6b6pxn0nk-master-0:9430 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 02:03:39,268.268 dlc26te6b6pxn0nk-master-0:9429 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 02:03:39,268.268 dlc26te6b6pxn0nk-master-0:9427 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:9427:9427 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:9430:9430 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9428:9428 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9429:9429 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 02:04:43 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6149), 0.47621592731159806, 0.8115438108484005, tensor(1.7662)]
[Tue Jan 17 02:06:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:06:52 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5894), 0.49812934259754144, 0.827538247566064, tensor(1.9012)]
[Tue Jan 17 02:09:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5763), 0.49812934259754144, 0.8358831710709318, tensor(1.9117)]
[Tue Jan 17 02:11:41 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5560), 0.5253874933190807, 0.8358831710709318, tensor(2.0709)]
[Tue Jan 17 02:13:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5560), 0.5253874933190807, 0.844923504867872, tensor(2.0709)]
[Tue Jan 17 02:17:12 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5443), 0.5312667022982362, 0.844923504867872, tensor(2.1120)]
[Tue Jan 17 02:18:59 2023] [cudaHostAllocator] allocates 1.71 GiB
[Tue Jan 17 02:19:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5443), 0.5344735435595938, 0.844923504867872, tensor(2.1258)]
[Tue Jan 17 02:20:42 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5443), 0.5344735435595938, 0.844923504867872, tensor(2.1258)]
[Tue Jan 17 02:23:27 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5443), 0.5344735435595938, 0.844923504867872, tensor(2.1258)]
[Tue Jan 17 02:25:44 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5443), 0.5344735435595938, 0.844923504867872, tensor(2.1258)]
[Tue Jan 17 02:28:41 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5443), 0.5344735435595938, 0.844923504867872, tensor(2.1258)]
[Tue Jan 17 02:31:21 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1591)]
[Tue Jan 17 02:32:57 2023] [cudaHostAllocator] allocates 3.42 GiB
[Tue Jan 17 02:33:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1591)]
[Tue Jan 17 02:34:54 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1591)]
[Tue Jan 17 02:37:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 02:38:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:38:23 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1591)]
[Tue Jan 17 02:40:25 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:40:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1591)]
[Tue Jan 17 02:42:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1608)]
[Tue Jan 17 02:44:24 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:45:04 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1608)]
[Tue Jan 17 02:47:07 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1608)]
[Tue Jan 17 02:50:10 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5443), 0.5414216996258685, 0.8484005563282336, tensor(2.1608)]
[Tue Jan 17 02:52:17 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:52:25 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5336), 0.5478353821485836, 0.8484005563282336, tensor(2.2055)]
[Tue Jan 17 02:54:02 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:54:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:54:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5336), 0.5478353821485836, 0.8484005563282336, tensor(2.2055)]
[Tue Jan 17 02:56:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 02:56:24 2023] [cudaHostAllocator] allocates 1.71 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5336), 0.5478353821485836, 0.8484005563282336, tensor(2.2055)]
[Tue Jan 17 02:58:13 2023] [cudaHostAllocator] allocates 1.71 GiB
[Tue Jan 17 02:58:47 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5336), 0.5478353821485836, 0.8484005563282336, tensor(2.2055)]
[Tue Jan 17 03:00:57 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 03:01:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5336), 0.5478353821485836, 0.8484005563282336, tensor(2.2055)]
[Tue Jan 17 03:03:55 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5336), 0.5478353821485836, 0.8484005563282336, tensor(2.2055)]
early stopping at 26
[2023-01-17 03:05:21,340.340 dlc26te6b6pxn0nk-master-0:9582 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 03:05:21,993.993 dlc26te6b6pxn0nk-master-0:9637 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:05:21,994.994 dlc26te6b6pxn0nk-master-0:9638 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:05:22,081.081 dlc26te6b6pxn0nk-master-0:9636 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:05:22,084.084 dlc26te6b6pxn0nk-master-0:9639 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:05:23,860.860 dlc26te6b6pxn0nk-master-0:9639 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 03:05:24,654.654 dlc26te6b6pxn0nk-master-0:9638 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 03:05:24,660.660 dlc26te6b6pxn0nk-master-0:9637 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 03:05:24,663.663 dlc26te6b6pxn0nk-master-0:9636 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:9636:9636 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:9639:9639 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9637:9637 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9638:9638 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 03:06:31 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 03:07:18 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5443), 0.5179048637092464, 0.8393602225312935, tensor(2.0452)]
[Tue Jan 17 03:08:46 2023] [cudaHostAllocator] allocates 3.42 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5248), 0.5414216996258685, 0.8588317107093185, tensor(2.1823)]
[Tue Jan 17 03:11:48 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5248), 0.5414216996258685, 0.8588317107093185, tensor(2.1823)]
[Tue Jan 17 03:13:57 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5206), 0.5414216996258685, 0.8650904033379694, tensor(2.1865)]
[Tue Jan 17 03:15:56 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 03:16:46 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 03:16:56 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5052), 0.5494388027792624, 0.8650904033379694, tensor(2.2420)]
[2023-01-17 03:18:32,842.842 dlc26te6b6pxn0nk-master-0:9717 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 03:18:33,552.552 dlc26te6b6pxn0nk-master-0:9773 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:18:33,555.555 dlc26te6b6pxn0nk-master-0:9772 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:18:33,669.669 dlc26te6b6pxn0nk-master-0:9771 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:18:33,673.673 dlc26te6b6pxn0nk-master-0:9774 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:18:34,610.610 dlc26te6b6pxn0nk-master-0:9774 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 03:18:35,485.485 dlc26te6b6pxn0nk-master-0:9773 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 03:18:35,485.485 dlc26te6b6pxn0nk-master-0:9772 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 03:18:35,488.488 dlc26te6b6pxn0nk-master-0:9771 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:9771:9771 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:9774:9774 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9773:9773 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9772:9772 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Tue Jan 17 03:19:39 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5784), 0.5146980224478889, 0.8497913769123783, tensor(1.9951)]
[Tue Jan 17 03:21:31 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 03:21:48 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5373), 0.535542490646713, 0.8699582753824756, tensor(2.1404)]
[Tue Jan 17 03:24:59 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5205), 0.5462319615179049, 0.8699582753824756, tensor(2.2106)]
[Tue Jan 17 03:26:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 03:27:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5205), 0.5462319615179049, 0.8699582753824756, tensor(2.2106)]
[Tue Jan 17 03:28:24 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 03:29:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5067), 0.5467664350614645, 0.8699582753824756, tensor(2.2271)]
[2023-01-17 03:30:50,371.371 dlc26te6b6pxn0nk-master-0:9852 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 03:30:51,106.106 dlc26te6b6pxn0nk-master-0:9907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:30:51,108.108 dlc26te6b6pxn0nk-master-0:9909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:30:51,240.240 dlc26te6b6pxn0nk-master-0:9906 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:30:51,240.240 dlc26te6b6pxn0nk-master-0:9908 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:30:52,473.473 dlc26te6b6pxn0nk-master-0:9909 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 03:30:52,474.474 dlc26te6b6pxn0nk-master-0:9907 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 03:30:53,211.211 dlc26te6b6pxn0nk-master-0:9908 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 03:30:53,219.219 dlc26te6b6pxn0nk-master-0:9906 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:9906:9906 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:9909:9909 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9907:9907 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:9908:9908 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 03:31:55 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 03:32:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5722), 0.49706039551042225, 0.8532684283727399, tensor(1.9131)]
[Tue Jan 17 03:34:10 2023] [cudaHostAllocator] allocates 3.42 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5299), 0.5344735435595938, 0.8553546592489569, tensor(2.1424)]
[Tue Jan 17 03:37:11 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5253), 0.5344735435595938, 0.8595271210013908, tensor(2.1424)]
[Tue Jan 17 03:39:21 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5253), 0.5424906467129877, 0.8616133518776078, tensor(2.1846)]
[Tue Jan 17 03:41:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 03:42:13 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 03:42:22 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5253), 0.5424906467129877, 0.8616133518776078, tensor(2.1846)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Tue Jan 17 03:45:05 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 03:45:09 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5253), 0.5424906467129877, 0.8616133518776078, tensor(2.1846)]
[Tue Jan 17 03:47:14 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5253), 0.5424906467129877, 0.8616133518776078, tensor(2.1846)]
[Tue Jan 17 03:49:06 2023] [cudaHostAllocator] allocates 3.42 GiB
[Tue Jan 17 03:50:13 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5253), 0.5424906467129877, 0.8616133518776078, tensor(2.1846)]
[Tue Jan 17 03:52:27 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5253), 0.5424906467129877, 0.8616133518776078, tensor(2.1846)]
early stopping at 9
[2023-01-17 03:54:32,596.596 dlc26te6b6pxn0nk-master-0:10003 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 03:54:33,300.300 dlc26te6b6pxn0nk-master-0:10059 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:54:33,330.330 dlc26te6b6pxn0nk-master-0:10058 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:54:33,408.408 dlc26te6b6pxn0nk-master-0:10057 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:54:33,498.498 dlc26te6b6pxn0nk-master-0:10060 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:54:34,677.677 dlc26te6b6pxn0nk-master-0:10058 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 03:54:34,778.778 dlc26te6b6pxn0nk-master-0:10060 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 03:54:35,193.193 dlc26te6b6pxn0nk-master-0:10059 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 03:54:35,201.201 dlc26te6b6pxn0nk-master-0:10057 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:10057:10057 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:10059:10059 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10058:10058 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10060:10060 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Tue Jan 17 03:55:39 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5291), 0.5339390700160342, 0.8532684283727399, tensor(2.1406)]
[Tue Jan 17 03:57:30 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Tue Jan 17 03:57:48 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5179), 0.5489043292357029, 0.8588317107093185, tensor(2.2266)]
[Tue Jan 17 04:00:59 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5179), 0.5489043292357029, 0.8588317107093185, tensor(2.2266)]
[Tue Jan 17 04:02:30 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 04:03:19 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5044), 0.5515766969535008, 0.8588317107093185, tensor(2.2535)]
[Tue Jan 17 04:04:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 04:05:12 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 04:05:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5044), 0.5515766969535008, 0.8588317107093185, tensor(2.2535)]
[Tue Jan 17 04:07:59 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 04:08:03 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5044), 0.5515766969535008, 0.866481223922114, tensor(2.2535)]
[Tue Jan 17 04:09:53 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5044), 0.5515766969535008, 0.866481223922114, tensor(2.2535)]
[Tue Jan 17 04:11:36 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 04:12:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5044), 0.5515766969535008, 0.866481223922114, tensor(2.2535)]
[Tue Jan 17 04:14:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 04:14:22 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5044), 0.5515766969535008, 0.866481223922114, tensor(2.2535)]
[Tue Jan 17 04:16:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 04:16:44 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5044), 0.5515766969535008, 0.866481223922114, tensor(2.2535)]
[Tue Jan 17 04:19:38 2023] [cudaHostAllocator] allocates 1.71 GiB
[Tue Jan 17 04:19:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5044), 0.5515766969535008, 0.866481223922114, tensor(2.2535)]
early stopping at 11
[2023-01-17 04:21:00,976.976 dlc26te6b6pxn0nk-master-0:10159 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 04:21:01,686.686 dlc26te6b6pxn0nk-master-0:10214 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:21:01,712.712 dlc26te6b6pxn0nk-master-0:10215 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:21:01,800.800 dlc26te6b6pxn0nk-master-0:10216 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:21:01,892.892 dlc26te6b6pxn0nk-master-0:10213 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:21:03,582.582 dlc26te6b6pxn0nk-master-0:10214 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 04:21:03,672.672 dlc26te6b6pxn0nk-master-0:10216 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 04:21:04,059.059 dlc26te6b6pxn0nk-master-0:10215 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 04:21:04,067.067 dlc26te6b6pxn0nk-master-0:10213 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:10213:10213 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:10216:10216 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10215:10215 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10214:10214 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5482), 0.5125601282736505, 0.8407510431154381, tensor(2.0146)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5482), 0.5259219668626403, 0.8407510431154381, tensor(2.0657)]
[tensor(-0.5185), 0.5344735435595938, 0.8602225312934632, tensor(2.1539)]
[tensor(-0.5185), 0.5344735435595938, 0.8602225312934632, tensor(2.1539)]
[tensor(-0.5185), 0.5451630144307856, 0.8602225312934632, tensor(2.1895)]
[2023-01-17 04:32:25,351.351 dlc26te6b6pxn0nk-master-0:10292 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 04:32:26,005.005 dlc26te6b6pxn0nk-master-0:10346 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:32:26,009.009 dlc26te6b6pxn0nk-master-0:10348 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:32:26,089.089 dlc26te6b6pxn0nk-master-0:10349 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:32:26,105.105 dlc26te6b6pxn0nk-master-0:10347 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:32:27,345.345 dlc26te6b6pxn0nk-master-0:10349 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 04:32:27,579.579 dlc26te6b6pxn0nk-master-0:10347 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 04:32:27,970.970 dlc26te6b6pxn0nk-master-0:10348 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 04:32:27,976.976 dlc26te6b6pxn0nk-master-0:10346 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:10346:10346 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:10347:10347 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10349:10349 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10348:10348 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5227), 0.5376803848209514, 0.8623087621696801, tensor(2.1657)]
[tensor(-0.5227), 0.538214858364511, 0.8678720445062587, tensor(2.1683)]
[tensor(-0.5089), 0.5547835382148584, 0.8678720445062587, tensor(2.2650)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5089), 0.5547835382148584, 0.8678720445062587, tensor(2.2650)]
[tensor(-0.5089), 0.5547835382148584, 0.8678720445062587, tensor(2.2650)]
[2023-01-17 04:43:58,784.784 dlc26te6b6pxn0nk-master-0:10425 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 04:43:59,494.494 dlc26te6b6pxn0nk-master-0:10481 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:43:59,518.518 dlc26te6b6pxn0nk-master-0:10480 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:43:59,601.601 dlc26te6b6pxn0nk-master-0:10479 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:43:59,692.692 dlc26te6b6pxn0nk-master-0:10482 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 04:44:00,857.857 dlc26te6b6pxn0nk-master-0:10480 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 04:44:00,976.976 dlc26te6b6pxn0nk-master-0:10482 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 04:44:01,382.382 dlc26te6b6pxn0nk-master-0:10481 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 04:44:01,384.384 dlc26te6b6pxn0nk-master-0:10479 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:10479:10479 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:10481:10481 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10482:10482 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10480:10480 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5469), 0.5189738107963656, 0.8539638386648123, tensor(2.0479)]
[tensor(-0.5319), 0.5387493319080705, 0.8602225312934632, tensor(2.1619)]
[tensor(-0.5267), 0.5387493319080705, 0.8602225312934632, tensor(2.1643)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5057), 0.5521111704970604, 0.8623087621696801, tensor(2.2548)]
[tensor(-0.5057), 0.5521111704970604, 0.8623087621696801, tensor(2.2548)]
[tensor(-0.5057), 0.5521111704970604, 0.8623087621696801, tensor(2.2548)]
[tensor(-0.5057), 0.5521111704970604, 0.8623087621696801, tensor(2.2548)]
[tensor(-0.5057), 0.5521111704970604, 0.8623087621696801, tensor(2.2548)]
[tensor(-0.5057), 0.5521111704970604, 0.8623087621696801, tensor(2.2548)]
early stopping at 9
[2023-01-17 05:04:32,777.777 dlc26te6b6pxn0nk-master-0:10572 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 05:04:33,494.494 dlc26te6b6pxn0nk-master-0:10627 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:04:33,521.521 dlc26te6b6pxn0nk-master-0:10628 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:04:33,609.609 dlc26te6b6pxn0nk-master-0:10626 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:04:33,700.700 dlc26te6b6pxn0nk-master-0:10629 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:04:34,883.883 dlc26te6b6pxn0nk-master-0:10628 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 05:04:35,003.003 dlc26te6b6pxn0nk-master-0:10629 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 05:04:35,377.377 dlc26te6b6pxn0nk-master-0:10627 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 05:04:35,379.379 dlc26te6b6pxn0nk-master-0:10626 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:10626:10626 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:10627:10627 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10628:10628 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10629:10629 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5319), 0.5334045964724746, 0.8546592489568846, tensor(2.1351)]
[tensor(-0.5184), 0.5360769641902726, 0.8643949930458971, tensor(2.1620)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5149), 0.5387493319080705, 0.8643949930458971, tensor(2.1788)]
[tensor(-0.5069), 0.5494388027792624, 0.868567454798331, tensor(2.2402)]
[tensor(-0.5069), 0.5542490646712988, 0.868567454798331, tensor(2.2549)]
[tensor(-0.5069), 0.5542490646712988, 0.868567454798331, tensor(2.2549)]
[tensor(-0.5069), 0.5542490646712988, 0.868567454798331, tensor(2.2549)]
[tensor(-0.5069), 0.5542490646712988, 0.868567454798331, tensor(2.2549)]
[tensor(-0.5069), 0.5542490646712988, 0.868567454798331, tensor(2.2549)]
[tensor(-0.5069), 0.5542490646712988, 0.868567454798331, tensor(2.2549)]
early stopping at 10
[2023-01-17 05:26:57,777.777 dlc26te6b6pxn0nk-master-0:10722 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 05:26:58,417.417 dlc26te6b6pxn0nk-master-0:10779 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:26:58,417.417 dlc26te6b6pxn0nk-master-0:10776 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:26:58,504.504 dlc26te6b6pxn0nk-master-0:10777 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:26:58,508.508 dlc26te6b6pxn0nk-master-0:10778 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:26:59,918.918 dlc26te6b6pxn0nk-master-0:10777 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 05:26:59,920.920 dlc26te6b6pxn0nk-master-0:10778 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 05:27:00,367.367 dlc26te6b6pxn0nk-master-0:10779 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 05:27:00,372.372 dlc26te6b6pxn0nk-master-0:10776 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:10776:10776 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:10779:10779 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10777:10777 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10778:10778 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5211), 0.5494388027792624, 0.866481223922114, tensor(2.2261)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5211), 0.5494388027792624, 0.866481223922114, tensor(2.2261)]
[tensor(-0.5211), 0.5494388027792624, 0.866481223922114, tensor(2.2261)]
[tensor(-0.5208), 0.5494388027792624, 0.866481223922114, tensor(2.2261)]
[tensor(-0.5208), 0.5494388027792624, 0.8706536856745479, tensor(2.2261)]
[2023-01-17 05:38:35,204.204 dlc26te6b6pxn0nk-master-0:10855 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 05:38:35,848.848 dlc26te6b6pxn0nk-master-0:10911 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:38:35,849.849 dlc26te6b6pxn0nk-master-0:10909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:38:35,932.932 dlc26te6b6pxn0nk-master-0:10912 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:38:35,933.933 dlc26te6b6pxn0nk-master-0:10910 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:38:36,794.794 dlc26te6b6pxn0nk-master-0:10911 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 05:38:36,944.944 dlc26te6b6pxn0nk-master-0:10910 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 05:38:36,945.945 dlc26te6b6pxn0nk-master-0:10912 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 05:38:36,954.954 dlc26te6b6pxn0nk-master-0:10909 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:10909:10909 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:10910:10910 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10911:10911 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:10912:10912 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.5665), 0.501336183858899, 0.8324061196105702, tensor(1.9402)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5150), 0.5515766969535008, 0.8623087621696801, tensor(2.2429)]
[tensor(-0.5150), 0.5515766969535008, 0.8699582753824756, tensor(2.2429)]
[tensor(-0.5150), 0.5515766969535008, 0.8713490959666204, tensor(2.2429)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5150), 0.5515766969535008, 0.8762169680111266, tensor(2.2429)]
[2023-01-17 05:49:59,607.607 dlc26te6b6pxn0nk-master-0:10988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 05:50:00,250.250 dlc26te6b6pxn0nk-master-0:11042 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:50:00,251.251 dlc26te6b6pxn0nk-master-0:11043 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:50:00,251.251 dlc26te6b6pxn0nk-master-0:11045 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:50:00,253.253 dlc26te6b6pxn0nk-master-0:11044 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:50:01,335.335 dlc26te6b6pxn0nk-master-0:11045 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 05:50:02,321.321 dlc26te6b6pxn0nk-master-0:11043 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 05:50:02,328.328 dlc26te6b6pxn0nk-master-0:11044 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 05:50:02,338.338 dlc26te6b6pxn0nk-master-0:11042 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:11042:11042 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:11043:11043 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11044:11044 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11045:11045 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5414), 0.5339390700160342, 0.849095966620306, tensor(2.1283)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5297), 0.5403527525387494, 0.8567454798331016, tensor(2.1721)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5273), 0.5403527525387494, 0.8706536856745479, tensor(2.1721)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8720445062586927, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8720445062586927, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8720445062586927, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8720445062586927, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8720445062586927, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8720445062586927, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8720445062586927, tensor(2.2584)]
early stopping at 13
[2023-01-17 06:19:22,029.029 dlc26te6b6pxn0nk-master-0:11149 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 06:19:22,761.761 dlc26te6b6pxn0nk-master-0:11204 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:19:22,765.765 dlc26te6b6pxn0nk-master-0:11205 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:19:22,848.848 dlc26te6b6pxn0nk-master-0:11206 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:19:22,848.848 dlc26te6b6pxn0nk-master-0:11203 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:19:23,787.787 dlc26te6b6pxn0nk-master-0:11206 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 06:19:23,936.936 dlc26te6b6pxn0nk-master-0:11204 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 06:19:23,939.939 dlc26te6b6pxn0nk-master-0:11205 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 06:19:23,947.947 dlc26te6b6pxn0nk-master-0:11203 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:11203:11203 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:11206:11206 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11204:11204 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11205:11205 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5468), 0.5269909139497595, 0.8511821974965229, tensor(2.0882)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5246), 0.5419561731694281, 0.8602225312934632, tensor(2.1852)]
[tensor(-0.5056), 0.5419561731694281, 0.8630041724617524, tensor(2.1852)]
[tensor(-0.5056), 0.5419561731694281, 0.8706536856745479, tensor(2.1931)]
[tensor(-0.5032), 0.5510422234099412, 0.8706536856745479, tensor(2.2520)]
[tensor(-0.5032), 0.5510422234099412, 0.8720445062586927, tensor(2.2520)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8720445062586927, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8741307371349096, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8741307371349096, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8741307371349096, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8741307371349096, tensor(2.2714)]
[tensor(-0.5025), 0.5547835382148584, 0.8741307371349096, tensor(2.2714)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5025), 0.5547835382148584, 0.8741307371349096, tensor(2.2714)]
early stopping at 20
[2023-01-17 07:03:43,272.272 dlc26te6b6pxn0nk-master-0:11331 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 07:03:43,914.914 dlc26te6b6pxn0nk-master-0:11385 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:03:43,914.914 dlc26te6b6pxn0nk-master-0:11388 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:03:44,000.000 dlc26te6b6pxn0nk-master-0:11386 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:03:44,002.002 dlc26te6b6pxn0nk-master-0:11387 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:03:44,842.842 dlc26te6b6pxn0nk-master-0:11388 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 07:03:45,024.024 dlc26te6b6pxn0nk-master-0:11387 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 07:03:45,027.027 dlc26te6b6pxn0nk-master-0:11386 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 07:03:45,034.034 dlc26te6b6pxn0nk-master-0:11385 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:11385:11385 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:11387:11387 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11388:11388 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11386:11386 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5837), 0.5146980224478889, 0.7983310152990264, tensor(1.9898)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5186), 0.5483698556921432, 0.8609179415855355, tensor(2.2232)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5186), 0.5483698556921432, 0.8623087621696801, tensor(2.2232)]
[tensor(-0.5171), 0.5483698556921432, 0.8623087621696801, tensor(2.2232)]
[tensor(-0.5171), 0.5483698556921432, 0.8623087621696801, tensor(2.2232)]
[2023-01-17 07:15:14,685.685 dlc26te6b6pxn0nk-master-0:11464 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 07:15:15,342.342 dlc26te6b6pxn0nk-master-0:11520 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:15:15,342.342 dlc26te6b6pxn0nk-master-0:11519 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:15:15,511.511 dlc26te6b6pxn0nk-master-0:11521 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:15:15,596.596 dlc26te6b6pxn0nk-master-0:11518 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:15:17,215.215 dlc26te6b6pxn0nk-master-0:11520 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 07:15:17,400.400 dlc26te6b6pxn0nk-master-0:11521 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 07:15:17,689.689 dlc26te6b6pxn0nk-master-0:11519 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 07:15:17,696.696 dlc26te6b6pxn0nk-master-0:11518 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:11518:11518 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:11519:11519 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11521:11521 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11520:11520 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5860), 0.4836985569214324, 0.8393602225312935, tensor(1.8324)]
[tensor(-0.5318), 0.5344735435595938, 0.8595271210013908, tensor(2.1406)]
[tensor(-0.5318), 0.5344735435595938, 0.8595271210013908, tensor(2.1406)]
[tensor(-0.5277), 0.5344735435595938, 0.8609179415855355, tensor(2.1406)]
[tensor(-0.5208), 0.5360769641902726, 0.8609179415855355, tensor(2.1595)]
[2023-01-17 07:26:43,170.170 dlc26te6b6pxn0nk-master-0:11597 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 07:26:43,875.875 dlc26te6b6pxn0nk-master-0:11653 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:26:43,909.909 dlc26te6b6pxn0nk-master-0:11652 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:26:43,980.980 dlc26te6b6pxn0nk-master-0:11651 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:26:44,071.071 dlc26te6b6pxn0nk-master-0:11654 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:26:45,248.248 dlc26te6b6pxn0nk-master-0:11652 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 07:26:45,355.355 dlc26te6b6pxn0nk-master-0:11654 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 07:26:45,752.752 dlc26te6b6pxn0nk-master-0:11653 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 07:26:45,758.758 dlc26te6b6pxn0nk-master-0:11651 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:11651:11651 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:11653:11653 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11652:11652 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11654:11654 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7623), 0.4462854088722608, 0.7447844228094576, tensor(1.4691)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6810), 0.4462854088722608, 0.7677329624478443, tensor(1.5023)]
[tensor(-0.6626), 0.45430251202565475, 0.8011126564673157, tensor(1.6089)]
[tensor(-0.6223), 0.4778193479422769, 0.8094575799721836, tensor(1.7668)]
[tensor(-0.6223), 0.4778193479422769, 0.8094575799721836, tensor(1.7668)]
[tensor(-0.6223), 0.4778193479422769, 0.8178025034770514, tensor(1.7668)]
[tensor(-0.6094), 0.4778193479422769, 0.8178025034770514, tensor(1.7668)]
[tensor(-0.6056), 0.47835382148583644, 0.8240611961057024, tensor(1.7862)]
[tensor(-0.5878), 0.501336183858899, 0.8296244784422809, tensor(1.9189)]
[tensor(-0.5878), 0.501336183858899, 0.8296244784422809, tensor(1.9189)]
[tensor(-0.5878), 0.501336183858899, 0.8296244784422809, tensor(1.9189)]
[tensor(-0.5649), 0.5152324959914484, 0.8296244784422809, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8296244784422809, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8296244784422809, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8386648122392212, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8386648122392212, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8386648122392212, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8386648122392212, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8386648122392212, tensor(2.0112)]
[tensor(-0.5649), 0.5152324959914484, 0.8386648122392212, tensor(2.0112)]
early stopping at 20
[2023-01-17 08:11:29,615.615 dlc26te6b6pxn0nk-master-0:11781 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 08:11:30,256.256 dlc26te6b6pxn0nk-master-0:11835 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:11:30,259.259 dlc26te6b6pxn0nk-master-0:11837 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:11:30,259.259 dlc26te6b6pxn0nk-master-0:11836 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:11:30,259.259 dlc26te6b6pxn0nk-master-0:11838 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:11:31,315.315 dlc26te6b6pxn0nk-master-0:11837 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 08:11:32,308.308 dlc26te6b6pxn0nk-master-0:11838 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 08:11:32,312.312 dlc26te6b6pxn0nk-master-0:11836 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 08:11:32,317.317 dlc26te6b6pxn0nk-master-0:11835 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:11835:11835 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:11838:11838 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11836:11836 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:11837:11837 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5938), 0.4997327632282202, 0.8205841446453408, tensor(1.9048)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5571), 0.5237840726884019, 0.8477051460361613, tensor(2.0618)]
[tensor(-0.5339), 0.5494388027792624, 0.8477051460361613, tensor(2.2133)]
[tensor(-0.5339), 0.5494388027792624, 0.8525730180806675, tensor(2.2133)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5339), 0.5494388027792624, 0.8525730180806675, tensor(2.2133)]
[tensor(-0.5339), 0.5494388027792624, 0.8560500695410292, tensor(2.2133)]
[tensor(-0.5294), 0.5494388027792624, 0.8560500695410292, tensor(2.2133)]
[tensor(-0.5238), 0.5494388027792624, 0.8560500695410292, tensor(2.2133)]
[tensor(-0.5207), 0.5494388027792624, 0.8560500695410292, tensor(2.2133)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2133)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2133)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2181)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2181)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2181)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2181)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2181)]
[tensor(-0.5207), 0.5494388027792624, 0.8595271210013908, tensor(2.2181)]
early stopping at 17
[2023-01-17 08:49:04,422.422 dlc26te6b6pxn0nk-master-0:11953 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 08:49:05,161.161 dlc26te6b6pxn0nk-master-0:12008 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:49:05,161.161 dlc26te6b6pxn0nk-master-0:12009 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:49:05,245.245 dlc26te6b6pxn0nk-master-0:12007 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:49:05,245.245 dlc26te6b6pxn0nk-master-0:12010 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:49:07,334.334 dlc26te6b6pxn0nk-master-0:12010 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 08:49:07,344.344 dlc26te6b6pxn0nk-master-0:12009 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 08:49:07,351.351 dlc26te6b6pxn0nk-master-0:12008 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 08:49:07,357.357 dlc26te6b6pxn0nk-master-0:12007 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:12007:12007 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:12009:12009 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12010:12010 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12008:12008 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5752), 0.5056119722073757, 0.8400556328233658, tensor(1.9529)]
[tensor(-0.5228), 0.5269909139497595, 0.8546592489568846, tensor(2.1121)]
[tensor(-0.5100), 0.5334045964724746, 0.8650904033379694, tensor(2.1571)]
[tensor(-0.5100), 0.5334045964724746, 0.8650904033379694, tensor(2.1571)]
[tensor(-0.5100), 0.5334045964724746, 0.8650904033379694, tensor(2.1571)]
[2023-01-17 09:00:57,842.842 dlc26te6b6pxn0nk-master-0:12087 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 09:00:58,484.484 dlc26te6b6pxn0nk-master-0:12142 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:00:58,490.490 dlc26te6b6pxn0nk-master-0:12143 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:00:58,708.708 dlc26te6b6pxn0nk-master-0:12144 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:00:58,730.730 dlc26te6b6pxn0nk-master-0:12141 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:01:00,363.363 dlc26te6b6pxn0nk-master-0:12142 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 09:01:00,575.575 dlc26te6b6pxn0nk-master-0:12144 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 09:01:00,837.837 dlc26te6b6pxn0nk-master-0:12143 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 09:01:00,840.840 dlc26te6b6pxn0nk-master-0:12141 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:12141:12141 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:12142:12142 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12144:12144 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12143:12143 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5430), 0.5216461785141635, 0.8463143254520167, tensor(2.0653)]
[tensor(-0.5429), 0.5387493319080705, 0.8463143254520167, tensor(2.1508)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5110), 0.5387493319080705, 0.8762169680111266, tensor(2.1560)]
[tensor(-0.5110), 0.5387493319080705, 0.8762169680111266, tensor(2.1718)]
[tensor(-0.5105), 0.5595938001068947, 0.8762169680111266, tensor(2.2875)]
[2023-01-17 09:13:45,311.311 dlc26te6b6pxn0nk-master-0:12222 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 09:13:46,018.018 dlc26te6b6pxn0nk-master-0:12277 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:13:46,048.048 dlc26te6b6pxn0nk-master-0:12278 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:13:46,129.129 dlc26te6b6pxn0nk-master-0:12276 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:13:46,219.219 dlc26te6b6pxn0nk-master-0:12279 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:13:47,403.403 dlc26te6b6pxn0nk-master-0:12278 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 09:13:47,495.495 dlc26te6b6pxn0nk-master-0:12279 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 09:13:47,895.895 dlc26te6b6pxn0nk-master-0:12277 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 09:13:47,905.905 dlc26te6b6pxn0nk-master-0:12276 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:12276:12276 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:12278:12278 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12277:12277 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12279:12279 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5636), 0.5269909139497595, 0.8226703755215578, tensor(2.0714)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5545), 0.5269909139497595, 0.8574408901251739, tensor(2.0714)]
[tensor(-0.5221), 0.5403527525387494, 0.8581363004172462, tensor(2.1797)]
[tensor(-0.5130), 0.5494388027792624, 0.8734353268428373, tensor(2.2342)]
[tensor(-0.5130), 0.5494388027792624, 0.8734353268428373, tensor(2.2342)]
[tensor(-0.5130), 0.5494388027792624, 0.8734353268428373, tensor(2.2342)]
[tensor(-0.5067), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5067), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5067), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5067), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5067), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5065), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5065), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5065), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5065), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5065), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
[tensor(-0.5065), 0.5574559059326564, 0.8734353268428373, tensor(2.2806)]
early stopping at 17
[2023-01-17 09:51:59,354.354 dlc26te6b6pxn0nk-master-0:12395 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 09:51:59,995.995 dlc26te6b6pxn0nk-master-0:12451 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:51:59,997.997 dlc26te6b6pxn0nk-master-0:12452 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:52:00,083.083 dlc26te6b6pxn0nk-master-0:12449 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:52:00,093.093 dlc26te6b6pxn0nk-master-0:12450 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 09:52:01,934.934 dlc26te6b6pxn0nk-master-0:12452 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 09:52:01,935.935 dlc26te6b6pxn0nk-master-0:12451 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 09:52:02,293.293 dlc26te6b6pxn0nk-master-0:12450 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 09:52:02,297.297 dlc26te6b6pxn0nk-master-0:12449 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:12449:12449 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:12450:12450 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12451:12451 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12452:12452 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5683), 0.5088188134687333, 0.8414464534075105, tensor(1.9758)]
[tensor(-0.5427), 0.5435595938001069, 0.8463143254520167, tensor(2.1751)]
[tensor(-0.5381), 0.5435595938001069, 0.8463143254520167, tensor(2.1751)]
[tensor(-0.5381), 0.5435595938001069, 0.8518776077885952, tensor(2.1751)]
[tensor(-0.5350), 0.5462319615179049, 0.8518776077885952, tensor(2.1961)]
[tensor(-0.5350), 0.5462319615179049, 0.8560500695410292, tensor(2.1961)]
[tensor(-0.5203), 0.5547835382148584, 0.8560500695410292, tensor(2.2537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5203), 0.5547835382148584, 0.8560500695410292, tensor(2.2537)]
[tensor(-0.5203), 0.5547835382148584, 0.8602225312934632, tensor(2.2537)]
[tensor(-0.5203), 0.5547835382148584, 0.8602225312934632, tensor(2.2537)]
[tensor(-0.5203), 0.5547835382148584, 0.8602225312934632, tensor(2.2537)]
[tensor(-0.5203), 0.5547835382148584, 0.8602225312934632, tensor(2.2537)]
[tensor(-0.5203), 0.5547835382148584, 0.8602225312934632, tensor(2.2537)]
[tensor(-0.5203), 0.5547835382148584, 0.8602225312934632, tensor(2.2537)]
early stopping at 14
[2023-01-17 10:23:00,768.768 dlc26te6b6pxn0nk-master-0:12558 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 10:23:01,512.512 dlc26te6b6pxn0nk-master-0:12613 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:23:01,513.513 dlc26te6b6pxn0nk-master-0:12614 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:23:01,598.598 dlc26te6b6pxn0nk-master-0:12612 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:23:01,598.598 dlc26te6b6pxn0nk-master-0:12615 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:23:02,549.549 dlc26te6b6pxn0nk-master-0:12615 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 10:23:02,906.906 dlc26te6b6pxn0nk-master-0:12613 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 10:23:02,909.909 dlc26te6b6pxn0nk-master-0:12614 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 10:23:02,912.912 dlc26te6b6pxn0nk-master-0:12612 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:12612:12612 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:12614:12614 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12615:12615 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12613:12613 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 10:24:04 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6284), 0.44681988241582044, 0.8087621696801113, tensor(1.6057)]
[Tue Jan 17 10:25:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 10:26:24 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 10:26:50 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5380), 0.5205772314270444, 0.8643949930458971, tensor(2.0649)]
[Tue Jan 17 10:28:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 10:30:01 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5380), 0.5205772314270444, 0.8643949930458971, tensor(2.0649)]
[Tue Jan 17 10:31:41 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 10:32:31 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5072), 0.5440940673436665, 0.8643949930458971, tensor(2.2132)]
[Tue Jan 17 10:33:49 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 10:34:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5072), 0.5440940673436665, 0.8643949930458971, tensor(2.2132)]
[2023-01-17 10:36:18,210.210 dlc26te6b6pxn0nk-master-0:12694 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 10:36:18,859.859 dlc26te6b6pxn0nk-master-0:12748 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:36:18,859.859 dlc26te6b6pxn0nk-master-0:12751 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:36:18,949.949 dlc26te6b6pxn0nk-master-0:12750 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:36:18,949.949 dlc26te6b6pxn0nk-master-0:12749 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:36:19,798.798 dlc26te6b6pxn0nk-master-0:12751 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 10:36:20,160.160 dlc26te6b6pxn0nk-master-0:12749 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 10:36:20,161.161 dlc26te6b6pxn0nk-master-0:12750 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 10:36:20,171.171 dlc26te6b6pxn0nk-master-0:12748 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:12748:12748 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:12749:12749 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12751:12751 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12750:12750 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 10:37:22 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5400), 0.521111704970604, 0.8421418636995828, tensor(2.0656)]
[Tue Jan 17 10:39:12 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 10:39:29 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5400), 0.521111704970604, 0.8497913769123783, tensor(2.0656)]
[Tue Jan 17 10:42:36 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5164), 0.5403527525387494, 0.8595271210013908, tensor(2.1854)]
[Tue Jan 17 10:44:11 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5164), 0.5542490646712988, 0.8636995827538247, tensor(2.2547)]
[Tue Jan 17 10:46:02 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5061), 0.5542490646712988, 0.8636995827538247, tensor(2.2547)]
[2023-01-17 10:48:27,627.627 dlc26te6b6pxn0nk-master-0:12828 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 10:48:28,271.271 dlc26te6b6pxn0nk-master-0:12883 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:48:28,271.271 dlc26te6b6pxn0nk-master-0:12882 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:48:28,360.360 dlc26te6b6pxn0nk-master-0:12884 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:48:28,364.364 dlc26te6b6pxn0nk-master-0:12885 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:48:29,462.462 dlc26te6b6pxn0nk-master-0:12885 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 10:48:29,463.463 dlc26te6b6pxn0nk-master-0:12884 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 10:48:30,192.192 dlc26te6b6pxn0nk-master-0:12883 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 10:48:30,199.199 dlc26te6b6pxn0nk-master-0:12882 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:12882:12882 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:12885:12885 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12883:12883 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:12884:12884 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 10:49:32 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.6215), 0.4746125066809193, 0.8261474269819193, tensor(1.7515)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 10:51:25 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 10:51:52 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5535), 0.5141635489043292, 0.8553546592489569, tensor(2.0174)]
[Tue Jan 17 10:54:40 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5417), 0.5141635489043292, 0.8574408901251739, tensor(2.0174)]
[Tue Jan 17 10:56:45 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5392), 0.5195082843399251, 0.8574408901251739, tensor(2.0584)]
[Tue Jan 17 10:58:42 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5392), 0.5387493319080705, 0.8574408901251739, tensor(2.1489)]
[Tue Jan 17 11:02:22 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5392), 0.5387493319080705, 0.8574408901251739, tensor(2.1489)]
[Tue Jan 17 11:04:16 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5372), 0.5387493319080705, 0.8609179415855355, tensor(2.1489)]
[Tue Jan 17 11:06:01 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 11:07:03 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5372), 0.5387493319080705, 0.8609179415855355, tensor(2.1489)]
[Tue Jan 17 11:08:44 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 11:08:58 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 11:09:50 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5372), 0.5387493319080705, 0.8609179415855355, tensor(2.1489)]
[Tue Jan 17 11:11:19 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 11:11:23 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5372), 0.5387493319080705, 0.8609179415855355, tensor(2.1489)]
[Tue Jan 17 11:14:30 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 11:14:38 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5372), 0.5387493319080705, 0.8609179415855355, tensor(2.1489)]
[Tue Jan 17 11:17:09 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5372), 0.5387493319080705, 0.8609179415855355, tensor(2.1489)]
early stopping at 12
[2023-01-17 11:18:24,005.005 dlc26te6b6pxn0nk-master-0:12989 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 11:18:24,658.658 dlc26te6b6pxn0nk-master-0:13046 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:18:24,662.662 dlc26te6b6pxn0nk-master-0:13044 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:18:24,743.743 dlc26te6b6pxn0nk-master-0:13043 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:18:24,747.747 dlc26te6b6pxn0nk-master-0:13045 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:18:25,904.904 dlc26te6b6pxn0nk-master-0:13045 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 11:18:26,594.594 dlc26te6b6pxn0nk-master-0:13046 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 11:18:26,597.597 dlc26te6b6pxn0nk-master-0:13044 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 11:18:26,602.602 dlc26te6b6pxn0nk-master-0:13043 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:13043:13043 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:13045:13045 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13044:13044 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13046:13046 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 11:19:28 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5517), 0.5136290753607696, 0.8421418636995828, tensor(2.0164)]
[Tue Jan 17 11:21:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 11:21:37 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5384), 0.5280598610368786, 0.8442280945757997, tensor(2.1019)]
[Tue Jan 17 11:24:45 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5324), 0.5280598610368786, 0.8560500695410292, tensor(2.1079)]
[Tue Jan 17 11:26:20 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5232), 0.5419561731694281, 0.8560500695410292, tensor(2.1866)]
[Tue Jan 17 11:28:12 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 11:29:04 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5169), 0.5419561731694281, 0.8560500695410292, tensor(2.1866)]
[Tue Jan 17 11:31:14 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 11:31:53 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5169), 0.5419561731694281, 0.8560500695410292, tensor(2.1866)]
[Tue Jan 17 11:33:41 2023] [cudaHostAllocator] allocates 1.71 GiB
[Tue Jan 17 11:34:02 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5071), 0.5521111704970604, 0.8560500695410292, tensor(2.2534)]
[Tue Jan 17 11:35:23 2023] [cudaHostAllocator] allocates 3.42 GiB
[Tue Jan 17 11:36:08 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5071), 0.5521111704970604, 0.8560500695410292, tensor(2.2534)]
[Tue Jan 17 11:38:15 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5071), 0.5521111704970604, 0.8595271210013908, tensor(2.2534)]
[Tue Jan 17 11:40:37 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5071), 0.5521111704970604, 0.8630041724617524, tensor(2.2534)]
[Tue Jan 17 11:43:09 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Tue Jan 17 11:43:36 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5071), 0.5521111704970604, 0.8636995827538247, tensor(2.2534)]
[Tue Jan 17 11:46:10 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5071), 0.5521111704970604, 0.8671766342141863, tensor(2.2534)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[Tue Jan 17 11:47:44 2023] [cudaHostAllocator] allocates 3.42 GiB
[Tue Jan 17 11:48:19 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5071), 0.5521111704970604, 0.8671766342141863, tensor(2.2534)]
[Tue Jan 17 11:49:34 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5071), 0.5521111704970604, 0.8671766342141863, tensor(2.2534)]
[Tue Jan 17 11:52:09 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5071), 0.5521111704970604, 0.8671766342141863, tensor(2.2534)]
[Tue Jan 17 11:55:22 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5071), 0.5521111704970604, 0.8671766342141863, tensor(2.2534)]
[Tue Jan 17 11:57:12 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5071), 0.5521111704970604, 0.8671766342141863, tensor(2.2534)]
early stopping at 17
[2023-01-17 11:59:06,986.986 dlc26te6b6pxn0nk-master-0:13166 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 11:59:07,689.689 dlc26te6b6pxn0nk-master-0:13221 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:59:07,717.717 dlc26te6b6pxn0nk-master-0:13222 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:59:07,805.805 dlc26te6b6pxn0nk-master-0:13223 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:59:07,893.893 dlc26te6b6pxn0nk-master-0:13220 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 11:59:09,590.590 dlc26te6b6pxn0nk-master-0:13221 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 11:59:09,678.678 dlc26te6b6pxn0nk-master-0:13223 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 11:59:10,051.051 dlc26te6b6pxn0nk-master-0:13222 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 11:59:10,059.059 dlc26te6b6pxn0nk-master-0:13220 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:13220:13220 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:13223:13223 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13222:13222 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13221:13221 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 12:00:12 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 12:00:55 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.6656), 0.47140566541956175, 0.7830319888734353, tensor(1.6914)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 12:02:24 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5773), 0.51309460181721, 0.8553546592489569, tensor(1.9882)]
[Tue Jan 17 12:05:25 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5773), 0.51309460181721, 0.8553546592489569, tensor(1.9882)]
[Tue Jan 17 12:07:33 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5102), 0.5456974879743453, 0.8643949930458971, tensor(2.2183)]
[Tue Jan 17 12:09:35 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 12:10:24 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 12:10:34 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5102), 0.5456974879743453, 0.8643949930458971, tensor(2.2183)]
[2023-01-17 12:12:20,557.557 dlc26te6b6pxn0nk-master-0:13302 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 12:12:21,267.267 dlc26te6b6pxn0nk-master-0:13358 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:12:21,267.267 dlc26te6b6pxn0nk-master-0:13357 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:12:21,374.374 dlc26te6b6pxn0nk-master-0:13359 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:12:21,374.374 dlc26te6b6pxn0nk-master-0:13356 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:12:23,229.229 dlc26te6b6pxn0nk-master-0:13357 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 12:12:23,232.232 dlc26te6b6pxn0nk-master-0:13358 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 12:12:23,316.316 dlc26te6b6pxn0nk-master-0:13359 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 12:12:23,323.323 dlc26te6b6pxn0nk-master-0:13356 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:13356:13356 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:13358:13358 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13359:13359 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13357:13357 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Tue Jan 17 12:13:25 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5398), 0.5366114377338321, 0.8581363004172462, tensor(2.1433)]
[Tue Jan 17 12:15:16 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 12:15:34 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5345), 0.5366114377338321, 0.8609179415855355, tensor(2.1433)]
[Tue Jan 17 12:18:45 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5242), 0.5366114377338321, 0.8692628650904033, tensor(2.1433)]
[Tue Jan 17 12:20:17 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 12:21:06 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5193), 0.5371459112773918, 0.8692628650904033, tensor(2.1664)]
[Tue Jan 17 12:22:09 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 12:22:59 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5193), 0.5408872260823089, 0.8692628650904033, tensor(2.1766)]
[2023-01-17 12:24:34,066.066 dlc26te6b6pxn0nk-master-0:13436 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 12:24:34,720.720 dlc26te6b6pxn0nk-master-0:13492 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:24:34,720.720 dlc26te6b6pxn0nk-master-0:13491 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:24:34,886.886 dlc26te6b6pxn0nk-master-0:13490 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:24:34,974.974 dlc26te6b6pxn0nk-master-0:13493 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:24:36,062.062 dlc26te6b6pxn0nk-master-0:13491 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 12:24:36,215.215 dlc26te6b6pxn0nk-master-0:13493 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 12:24:36,596.596 dlc26te6b6pxn0nk-master-0:13492 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 12:24:36,596.596 dlc26te6b6pxn0nk-master-0:13490 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:13490:13490 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:13492:13492 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13491:13491 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13493:13493 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[Tue Jan 17 12:25:44 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 12:26:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5454), 0.5088188134687333, 0.844923504867872, tensor(1.9987)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Tue Jan 17 12:27:56 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5151), 0.549973276322822, 0.8616133518776078, tensor(2.2348)]
[Tue Jan 17 12:30:57 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5151), 0.549973276322822, 0.8616133518776078, tensor(2.2348)]
[Tue Jan 17 12:33:10 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5136), 0.549973276322822, 0.8636995827538247, tensor(2.2348)]
[Tue Jan 17 12:35:11 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 12:36:02 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 12:36:12 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5136), 0.549973276322822, 0.8636995827538247, tensor(2.2348)]
[Tue Jan 17 12:38:59 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5129), 0.549973276322822, 0.8636995827538247, tensor(2.2348)]
[Tue Jan 17 12:41:00 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5082), 0.549973276322822, 0.868567454798331, tensor(2.2348)]
[Tue Jan 17 12:42:37 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5082), 0.549973276322822, 0.868567454798331, tensor(2.2348)]
[Tue Jan 17 12:45:38 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5082), 0.549973276322822, 0.868567454798331, tensor(2.2348)]
[Tue Jan 17 12:48:02 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 12:48:05 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5082), 0.549973276322822, 0.868567454798331, tensor(2.2348)]
[Tue Jan 17 12:51:19 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 12:51:22 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5082), 0.549973276322822, 0.868567454798331, tensor(2.2348)]
[Tue Jan 17 12:52:38 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 12:53:55 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 12:53:56 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5082), 0.549973276322822, 0.868567454798331, tensor(2.2348)]
early stopping at 12
[2023-01-17 12:55:07,656.656 dlc26te6b6pxn0nk-master-0:13598 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 12:55:08,413.413 dlc26te6b6pxn0nk-master-0:13653 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:55:08,413.413 dlc26te6b6pxn0nk-master-0:13654 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:55:08,484.484 dlc26te6b6pxn0nk-master-0:13652 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:55:08,484.484 dlc26te6b6pxn0nk-master-0:13655 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:55:09,799.799 dlc26te6b6pxn0nk-master-0:13654 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 12:55:09,800.800 dlc26te6b6pxn0nk-master-0:13653 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 12:55:10,437.437 dlc26te6b6pxn0nk-master-0:13655 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 12:55:10,444.444 dlc26te6b6pxn0nk-master-0:13652 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:13652:13652 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:13654:13654 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13653:13653 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13655:13655 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[Tue Jan 17 12:56:12 2023] [cudaHostAllocator] allocates 340.32 MiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5495), 0.5114911811865313, 0.8442280945757997, tensor(2.0079)]
[Tue Jan 17 12:58:04 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 12:58:21 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5218), 0.5494388027792624, 0.8553546592489569, tensor(2.2254)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Tue Jan 17 13:01:30 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5150), 0.5494388027792624, 0.8650904033379694, tensor(2.2254)]
[Tue Jan 17 13:03:02 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 13:03:51 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5150), 0.5494388027792624, 0.8650904033379694, tensor(2.2254)]
[Tue Jan 17 13:04:54 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 13:05:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5150), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
[Tue Jan 17 13:08:29 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 13:08:33 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5150), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
[Tue Jan 17 13:10:21 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5136), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
[Tue Jan 17 13:12:04 2023] [cudaHostAllocator] allocates 340.32 MiB
[Tue Jan 17 13:12:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5136), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
[Tue Jan 17 13:14:54 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5136), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
[Tue Jan 17 13:17:14 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 13:17:16 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5136), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
[Tue Jan 17 13:20:10 2023] [cudaHostAllocator] allocates 1.71 GiB
[tensor(-0.5136), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
[Tue Jan 17 13:22:44 2023] [cudaHostAllocator] allocates 1.95 GiB
[Tue Jan 17 13:22:45 2023] [cudaHostAllocator] allocates 340.32 MiB
[tensor(-0.5136), 0.5579903794762159, 0.8650904033379694, tensor(2.2645)]
early stopping at 12
[2023-01-17 13:23:53,954.954 dlc26te6b6pxn0nk-master-0:13757 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 13:23:54,662.662 dlc26te6b6pxn0nk-master-0:13812 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:23:54,690.690 dlc26te6b6pxn0nk-master-0:13813 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:23:54,778.778 dlc26te6b6pxn0nk-master-0:13811 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:23:54,865.865 dlc26te6b6pxn0nk-master-0:13814 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:23:56,032.032 dlc26te6b6pxn0nk-master-0:13813 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 13:23:56,144.144 dlc26te6b6pxn0nk-master-0:13814 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 13:23:56,533.533 dlc26te6b6pxn0nk-master-0:13812 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 13:23:56,533.533 dlc26te6b6pxn0nk-master-0:13811 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:13811:13811 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:13812:13812 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13813:13813 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13814:13814 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5463), 0.5253874933190807, 0.8602225312934632, tensor(2.0806)]
[tensor(-0.5463), 0.5253874933190807, 0.8630041724617524, tensor(2.0806)]
[tensor(-0.5143), 0.5350080171031534, 0.8630041724617524, tensor(2.1607)]
[tensor(-0.5143), 0.5387493319080705, 0.8630041724617524, tensor(2.1728)]
[tensor(-0.5143), 0.5387493319080705, 0.8636995827538247, tensor(2.1728)]
[2023-01-17 13:35:24,369.369 dlc26te6b6pxn0nk-master-0:13891 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 13:35:25,112.112 dlc26te6b6pxn0nk-master-0:13947 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:35:25,114.114 dlc26te6b6pxn0nk-master-0:13946 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:35:25,195.195 dlc26te6b6pxn0nk-master-0:13948 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:35:25,196.196 dlc26te6b6pxn0nk-master-0:13945 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:35:26,724.724 dlc26te6b6pxn0nk-master-0:13947 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 13:35:26,728.728 dlc26te6b6pxn0nk-master-0:13946 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 13:35:27,462.462 dlc26te6b6pxn0nk-master-0:13948 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 13:35:27,468.468 dlc26te6b6pxn0nk-master-0:13945 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:13945:13945 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:13948:13948 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13946:13946 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:13947:13947 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5310), 0.5248530197755211, 0.8511821974965229, tensor(2.0933)]
[tensor(-0.5212), 0.5435595938001069, 0.8595271210013908, tensor(2.1966)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5020), 0.5590593265633351, 0.8671766342141863, tensor(2.2933)]
[tensor(-0.5020), 0.5590593265633351, 0.8671766342141863, tensor(2.2933)]
[tensor(-0.5020), 0.5590593265633351, 0.8671766342141863, tensor(2.2933)]
[2023-01-17 13:46:43,751.751 dlc26te6b6pxn0nk-master-0:14023 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 13:46:44,414.414 dlc26te6b6pxn0nk-master-0:14078 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:46:44,414.414 dlc26te6b6pxn0nk-master-0:14079 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:46:44,499.499 dlc26te6b6pxn0nk-master-0:14080 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:46:44,508.508 dlc26te6b6pxn0nk-master-0:14077 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:46:46,364.364 dlc26te6b6pxn0nk-master-0:14078 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 13:46:46,365.365 dlc26te6b6pxn0nk-master-0:14079 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 13:46:46,875.875 dlc26te6b6pxn0nk-master-0:14080 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 13:46:46,879.879 dlc26te6b6pxn0nk-master-0:14077 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:14077:14077 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:14078:14078 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14080:14080 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14079:14079 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5522), 0.5248530197755211, 0.8518776077885952, tensor(2.0721)]
[tensor(-0.5304), 0.5323356493853554, 0.8602225312934632, tensor(2.1312)]
[tensor(-0.5203), 0.5392838054516301, 0.8706536856745479, tensor(2.1762)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
[tensor(-0.5075), 0.5531801175841796, 0.8706536856745479, tensor(2.2584)]
early stopping at 9
[2023-01-17 14:06:52,803.803 dlc26te6b6pxn0nk-master-0:14170 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 14:06:53,445.445 dlc26te6b6pxn0nk-master-0:14225 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:06:53,445.445 dlc26te6b6pxn0nk-master-0:14227 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:06:53,446.446 dlc26te6b6pxn0nk-master-0:14226 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:06:53,446.446 dlc26te6b6pxn0nk-master-0:14224 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:06:54,502.502 dlc26te6b6pxn0nk-master-0:14225 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 14:06:54,504.504 dlc26te6b6pxn0nk-master-0:14227 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 14:06:54,506.506 dlc26te6b6pxn0nk-master-0:14226 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 14:06:54,514.514 dlc26te6b6pxn0nk-master-0:14224 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:14224:14224 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:14227:14227 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14226:14226 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14225:14225 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5321), 0.5275253874933191, 0.8525730180806675, tensor(2.1055)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5302), 0.5403527525387494, 0.8525730180806675, tensor(2.1716)]
[tensor(-0.5176), 0.5462319615179049, 0.8630041724617524, tensor(2.2136)]
[tensor(-0.5176), 0.5489043292357029, 0.8630041724617524, tensor(2.2159)]
[tensor(-0.5176), 0.5489043292357029, 0.8630041724617524, tensor(2.2159)]
[tensor(-0.5176), 0.5489043292357029, 0.8630041724617524, tensor(2.2159)]
[tensor(-0.5176), 0.5515766969535008, 0.8630041724617524, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8630041724617524, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8630041724617524, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8630041724617524, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8630041724617524, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8630041724617524, tensor(2.2380)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5176), 0.5515766969535008, 0.8630041724617524, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8671766342141863, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8671766342141863, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8671766342141863, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8671766342141863, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8671766342141863, tensor(2.2380)]
[tensor(-0.5176), 0.5515766969535008, 0.8671766342141863, tensor(2.2380)]
early stopping at 19
[2023-01-17 14:50:11,168.168 dlc26te6b6pxn0nk-master-0:14351 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 14:50:11,811.811 dlc26te6b6pxn0nk-master-0:14406 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:50:11,837.837 dlc26te6b6pxn0nk-master-0:14408 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:50:11,900.900 dlc26te6b6pxn0nk-master-0:14405 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:50:11,906.906 dlc26te6b6pxn0nk-master-0:14407 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:50:13,751.751 dlc26te6b6pxn0nk-master-0:14406 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 14:50:13,773.773 dlc26te6b6pxn0nk-master-0:14408 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 14:50:14,289.289 dlc26te6b6pxn0nk-master-0:14407 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 14:50:14,294.294 dlc26te6b6pxn0nk-master-0:14405 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:14405:14405 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:14406:14406 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14408:14408 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14407:14407 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5118), 0.5515766969535008, 0.8581363004172462, tensor(2.2461)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5078), 0.5515766969535008, 0.8657858136300417, tensor(2.2461)]
[tensor(-0.5078), 0.5515766969535008, 0.8699582753824756, tensor(2.2461)]
[tensor(-0.5078), 0.5515766969535008, 0.8699582753824756, tensor(2.2461)]
[tensor(-0.5050), 0.5515766969535008, 0.8699582753824756, tensor(2.2461)]
[2023-01-17 15:01:45,536.536 dlc26te6b6pxn0nk-master-0:14484 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 15:01:46,185.185 dlc26te6b6pxn0nk-master-0:14540 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:01:46,185.185 dlc26te6b6pxn0nk-master-0:14539 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:01:46,186.186 dlc26te6b6pxn0nk-master-0:14538 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:01:46,186.186 dlc26te6b6pxn0nk-master-0:14541 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:01:48,247.247 dlc26te6b6pxn0nk-master-0:14539 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 15:01:48,258.258 dlc26te6b6pxn0nk-master-0:14540 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 15:01:48,260.260 dlc26te6b6pxn0nk-master-0:14541 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 15:01:48,266.266 dlc26te6b6pxn0nk-master-0:14538 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:14538:14538 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:14540:14540 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14539:14539 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14541:14541 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5362), 0.532870122928915, 0.8630041724617524, tensor(2.1281)]
[tensor(-0.5143), 0.5414216996258685, 0.8734353268428373, tensor(2.1928)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5143), 0.5414216996258685, 0.8755215577190543, tensor(2.1928)]
[tensor(-0.5143), 0.5414216996258685, 0.8755215577190543, tensor(2.1928)]
[tensor(-0.5131), 0.5430251202565473, 0.8755215577190543, tensor(2.2021)]
[2023-01-17 15:13:09,960.960 dlc26te6b6pxn0nk-master-0:14617 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 15:13:10,609.609 dlc26te6b6pxn0nk-master-0:14672 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:13:10,616.616 dlc26te6b6pxn0nk-master-0:14674 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:13:10,692.692 dlc26te6b6pxn0nk-master-0:14671 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:13:10,697.697 dlc26te6b6pxn0nk-master-0:14673 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:13:12,546.546 dlc26te6b6pxn0nk-master-0:14672 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 15:13:12,548.548 dlc26te6b6pxn0nk-master-0:14674 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 15:13:12,847.847 dlc26te6b6pxn0nk-master-0:14673 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 15:13:12,851.851 dlc26te6b6pxn0nk-master-0:14671 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:14671:14671 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:14673:14673 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14674:14674 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14672:14672 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5511), 0.5259219668626403, 0.8372739916550765, tensor(2.0785)]
[tensor(-0.5234), 0.5446285408872261, 0.8650904033379694, tensor(2.1997)]
[tensor(-0.5208), 0.5446285408872261, 0.8650904033379694, tensor(2.2023)]
[tensor(-0.5092), 0.5505077498663816, 0.866481223922114, tensor(2.2433)]
[tensor(-0.5092), 0.5505077498663816, 0.866481223922114, tensor(2.2433)]
[tensor(-0.5092), 0.5505077498663816, 0.866481223922114, tensor(2.2433)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5092), 0.5505077498663816, 0.866481223922114, tensor(2.2433)]
[tensor(-0.5092), 0.5505077498663816, 0.866481223922114, tensor(2.2433)]
[tensor(-0.5092), 0.5505077498663816, 0.866481223922114, tensor(2.2433)]
early stopping at 9
[2023-01-17 15:33:23,796.796 dlc26te6b6pxn0nk-master-0:14764 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 15:33:24,499.499 dlc26te6b6pxn0nk-master-0:14820 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:33:24,526.526 dlc26te6b6pxn0nk-master-0:14819 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:33:24,615.615 dlc26te6b6pxn0nk-master-0:14818 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:33:24,706.706 dlc26te6b6pxn0nk-master-0:14821 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:33:25,874.874 dlc26te6b6pxn0nk-master-0:14819 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 15:33:25,984.984 dlc26te6b6pxn0nk-master-0:14821 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 15:33:26,380.380 dlc26te6b6pxn0nk-master-0:14820 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 15:33:26,389.389 dlc26te6b6pxn0nk-master-0:14818 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
fusion layers 4
fusion layers 4
fusion layers 4
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:14818:14818 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:14820:14820 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14821:14821 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14819:14819 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5663), 0.518439337252806, 0.8532684283727399, tensor(2.0259)]
[tensor(-0.5573), 0.5195082843399251, 0.8574408901251739, tensor(2.0403)]
[tensor(-0.5194), 0.5408872260823089, 0.8574408901251739, tensor(2.1850)]
[tensor(-0.5165), 0.5521111704970604, 0.8636995827538247, tensor(2.2441)]
[tensor(-0.5165), 0.5521111704970604, 0.8636995827538247, tensor(2.2441)]
[tensor(-0.5165), 0.5521111704970604, 0.8657858136300417, tensor(2.2441)]
[tensor(-0.5124), 0.5547835382148584, 0.8657858136300417, tensor(2.2615)]
[tensor(-0.5124), 0.5547835382148584, 0.8657858136300417, tensor(2.2615)]
[tensor(-0.5124), 0.5547835382148584, 0.8657858136300417, tensor(2.2615)]
[tensor(-0.5124), 0.5547835382148584, 0.8692628650904033, tensor(2.2615)]
[tensor(-0.5124), 0.5547835382148584, 0.8692628650904033, tensor(2.2615)]
[tensor(-0.5124), 0.5547835382148584, 0.8692628650904033, tensor(2.2615)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5124), 0.5547835382148584, 0.8692628650904033, tensor(2.2615)]
[tensor(-0.5124), 0.5547835382148584, 0.8692628650904033, tensor(2.2615)]
[tensor(-0.5124), 0.5547835382148584, 0.8692628650904033, tensor(2.2615)]
early stopping at 15
[2023-01-17 16:07:52,642.642 dlc26te6b6pxn0nk-master-0:14931 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 16:07:53,299.299 dlc26te6b6pxn0nk-master-0:14987 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:07:53,299.299 dlc26te6b6pxn0nk-master-0:14986 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:07:53,299.299 dlc26te6b6pxn0nk-master-0:14985 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:07:53,300.300 dlc26te6b6pxn0nk-master-0:14988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:07:55,327.327 dlc26te6b6pxn0nk-master-0:14987 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 16:07:55,369.369 dlc26te6b6pxn0nk-master-0:14988 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 16:07:55,372.372 dlc26te6b6pxn0nk-master-0:14986 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 16:07:55,376.376 dlc26te6b6pxn0nk-master-0:14985 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:14985:14985 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:14986:14986 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14987:14987 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:14988:14988 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5331), 0.5275253874933191, 0.8532684283727399, tensor(2.1045)]
[tensor(-0.5331), 0.5275253874933191, 0.8650904033379694, tensor(2.1045)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5331), 0.5339390700160342, 0.8650904033379694, tensor(2.1361)]
[tensor(-0.5219), 0.5339390700160342, 0.8650904033379694, tensor(2.1478)]
[tensor(-0.5158), 0.5456974879743453, 0.8650904033379694, tensor(2.2127)]
[2023-01-17 16:18:46,019.019 dlc26te6b6pxn0nk-master-0:15063 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 16:18:46,739.739 dlc26te6b6pxn0nk-master-0:15119 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:18:46,765.765 dlc26te6b6pxn0nk-master-0:15117 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:18:46,851.851 dlc26te6b6pxn0nk-master-0:15120 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:18:46,944.944 dlc26te6b6pxn0nk-master-0:15118 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:18:48,237.237 dlc26te6b6pxn0nk-master-0:15118 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 16:18:48,615.615 dlc26te6b6pxn0nk-master-0:15119 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 16:18:48,715.715 dlc26te6b6pxn0nk-master-0:15120 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 16:18:48,718.718 dlc26te6b6pxn0nk-master-0:15117 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:15117:15117 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:15118:15118 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15119:15119 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15120:15120 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5306), 0.5360769641902726, 0.8560500695410292, tensor(2.1498)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5185), 0.5435595938001069, 0.8643949930458971, tensor(2.1993)]
[tensor(-0.5185), 0.5435595938001069, 0.8643949930458971, tensor(2.1993)]
[tensor(-0.5185), 0.5435595938001069, 0.8643949930458971, tensor(2.1993)]
[tensor(-0.5185), 0.5435595938001069, 0.8657858136300417, tensor(2.1993)]
[2023-01-17 16:30:13,478.478 dlc26te6b6pxn0nk-master-0:15197 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 16:30:14,119.119 dlc26te6b6pxn0nk-master-0:15253 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:30:14,119.119 dlc26te6b6pxn0nk-master-0:15252 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:30:14,119.119 dlc26te6b6pxn0nk-master-0:15251 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:30:14,119.119 dlc26te6b6pxn0nk-master-0:15254 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:30:15,171.171 dlc26te6b6pxn0nk-master-0:15252 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 16:30:16,161.161 dlc26te6b6pxn0nk-master-0:15254 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 16:30:16,168.168 dlc26te6b6pxn0nk-master-0:15253 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 16:30:16,174.174 dlc26te6b6pxn0nk-master-0:15251 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:15251:15251 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:15253:15253 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15252:15252 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15254:15254 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5286), 0.5366114377338321, 0.8623087621696801, tensor(2.1544)]
[tensor(-0.5286), 0.5366114377338321, 0.8623087621696801, tensor(2.1544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5274), 0.5376803848209514, 0.8623087621696801, tensor(2.1610)]
[tensor(-0.5170), 0.5489043292357029, 0.8623087621696801, tensor(2.2276)]
[tensor(-0.5170), 0.5489043292357029, 0.8623087621696801, tensor(2.2276)]
[tensor(-0.5170), 0.5489043292357029, 0.8623087621696801, tensor(2.2276)]
[tensor(-0.5170), 0.5489043292357029, 0.8623087621696801, tensor(2.2276)]
[tensor(-0.5170), 0.5489043292357029, 0.8623087621696801, tensor(2.2276)]
[tensor(-0.5170), 0.5489043292357029, 0.8623087621696801, tensor(2.2276)]
early stopping at 9
[2023-01-17 16:49:15,246.246 dlc26te6b6pxn0nk-master-0:15341 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 16:49:15,893.893 dlc26te6b6pxn0nk-master-0:15398 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:49:15,896.896 dlc26te6b6pxn0nk-master-0:15395 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:49:15,976.976 dlc26te6b6pxn0nk-master-0:15397 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:49:15,980.980 dlc26te6b6pxn0nk-master-0:15396 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 16:49:16,832.832 dlc26te6b6pxn0nk-master-0:15398 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 16:49:17,191.191 dlc26te6b6pxn0nk-master-0:15397 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 16:49:17,192.192 dlc26te6b6pxn0nk-master-0:15396 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 16:49:17,199.199 dlc26te6b6pxn0nk-master-0:15395 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:15395:15395 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:15396:15396 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15398:15398 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15397:15397 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7376), 0.4462854088722608, 0.7246175243393602, tensor(1.4938)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6170), 0.4927846071619455, 0.7837273991655076, tensor(1.8469)]
[tensor(-0.5986), 0.49438802779262425, 0.7962447844228094, tensor(1.8733)]
[tensor(-0.5918), 0.49438802779262425, 0.8289290681502086, tensor(1.8748)]
[tensor(-0.5731), 0.49545697487974344, 0.8289290681502086, tensor(1.9042)]
[tensor(-0.5530), 0.5163014430785676, 0.8303198887343533, tensor(2.0285)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5414), 0.5227151256012827, 0.8303198887343533, tensor(2.0722)]
[tensor(-0.5414), 0.5227151256012827, 0.8303198887343533, tensor(2.0722)]
[tensor(-0.5414), 0.5291288081239979, 0.8351877607788595, tensor(2.0988)]
[tensor(-0.5414), 0.5291288081239979, 0.8351877607788595, tensor(2.0988)]
[tensor(-0.5414), 0.5291288081239979, 0.8351877607788595, tensor(2.0988)]
[tensor(-0.5414), 0.5334045964724746, 0.8351877607788595, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8351877607788595, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8351877607788595, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8351877607788595, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8351877607788595, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8442280945757997, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8442280945757997, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8442280945757997, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8442280945757997, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8442280945757997, tensor(2.1040)]
[tensor(-0.5414), 0.5334045964724746, 0.8442280945757997, tensor(2.1040)]
early stopping at 22
[2023-01-17 17:35:29,579.579 dlc26te6b6pxn0nk-master-0:15527 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 17:35:30,232.232 dlc26te6b6pxn0nk-master-0:15581 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:35:30,314.314 dlc26te6b6pxn0nk-master-0:15584 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:35:30,319.319 dlc26te6b6pxn0nk-master-0:15583 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:35:30,330.330 dlc26te6b6pxn0nk-master-0:15582 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:35:31,256.256 dlc26te6b6pxn0nk-master-0:15582 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 17:35:31,525.525 dlc26te6b6pxn0nk-master-0:15584 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 17:35:31,526.526 dlc26te6b6pxn0nk-master-0:15583 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 17:35:31,527.527 dlc26te6b6pxn0nk-master-0:15581 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:15581:15581 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:15582:15582 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15584:15584 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15583:15583 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.5188), 0.5419561731694281, 0.8574408901251739, tensor(2.1910)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5188), 0.5435595938001069, 0.8678720445062587, tensor(2.1967)]
[tensor(-0.5044), 0.5446285408872261, 0.8720445062586927, tensor(2.2187)]
[tensor(-0.5044), 0.5494388027792624, 0.8720445062586927, tensor(2.2419)]
[tensor(-0.5044), 0.5494388027792624, 0.8720445062586927, tensor(2.2419)]
[2023-01-17 17:46:23,925.925 dlc26te6b6pxn0nk-master-0:15659 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 17:46:24,573.573 dlc26te6b6pxn0nk-master-0:15713 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:46:24,573.573 dlc26te6b6pxn0nk-master-0:15715 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:46:24,659.659 dlc26te6b6pxn0nk-master-0:15714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:46:24,660.660 dlc26te6b6pxn0nk-master-0:15716 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:46:25,507.507 dlc26te6b6pxn0nk-master-0:15715 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 17:46:25,887.887 dlc26te6b6pxn0nk-master-0:15716 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 17:46:25,891.891 dlc26te6b6pxn0nk-master-0:15714 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 17:46:25,900.900 dlc26te6b6pxn0nk-master-0:15713 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:15713:15713 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:15716:15716 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15715:15715 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15714:15714 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5254), 0.5446285408872261, 0.8574408901251739, tensor(2.1977)]
[tensor(-0.5144), 0.5515766969535008, 0.8741307371349096, tensor(2.2435)]
[tensor(-0.5144), 0.5515766969535008, 0.8741307371349096, tensor(2.2435)]
[tensor(-0.5128), 0.5515766969535008, 0.8741307371349096, tensor(2.2435)]
[tensor(-0.5128), 0.5515766969535008, 0.8741307371349096, tensor(2.2435)]
[2023-01-17 17:57:18,280.280 dlc26te6b6pxn0nk-master-0:15792 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 17:57:18,930.930 dlc26te6b6pxn0nk-master-0:15847 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:18,930.930 dlc26te6b6pxn0nk-master-0:15849 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:19,012.012 dlc26te6b6pxn0nk-master-0:15848 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:19,019.019 dlc26te6b6pxn0nk-master-0:15846 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:20,857.857 dlc26te6b6pxn0nk-master-0:15847 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 17:57:20,858.858 dlc26te6b6pxn0nk-master-0:15849 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 17:57:21,397.397 dlc26te6b6pxn0nk-master-0:15848 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 17:57:21,402.402 dlc26te6b6pxn0nk-master-0:15846 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:15846:15846 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:15849:15849 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15847:15847 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15848:15848 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.5421), 0.5179048637092464, 0.8497913769123783, tensor(2.0475)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5421), 0.5408872260823089, 0.8504867872044506, tensor(2.1597)]
[tensor(-0.5258), 0.5515766969535008, 0.8602225312934632, tensor(2.2321)]
[tensor(-0.5168), 0.5515766969535008, 0.8602225312934632, tensor(2.2330)]
[tensor(-0.5168), 0.5515766969535008, 0.8602225312934632, tensor(2.2330)]
[tensor(-0.5168), 0.5515766969535008, 0.8602225312934632, tensor(2.2330)]
[tensor(-0.5168), 0.5515766969535008, 0.8602225312934632, tensor(2.2330)]
[tensor(-0.5168), 0.5515766969535008, 0.8602225312934632, tensor(2.2330)]
[tensor(-0.5168), 0.5515766969535008, 0.8602225312934632, tensor(2.2330)]
early stopping at 9
[2023-01-17 18:16:37,105.105 dlc26te6b6pxn0nk-master-0:15936 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 18:16:37,750.750 dlc26te6b6pxn0nk-master-0:15992 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:16:37,751.751 dlc26te6b6pxn0nk-master-0:15991 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:16:37,927.927 dlc26te6b6pxn0nk-master-0:15990 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:16:38,009.009 dlc26te6b6pxn0nk-master-0:15993 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:16:39,096.096 dlc26te6b6pxn0nk-master-0:15991 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 18:16:39,252.252 dlc26te6b6pxn0nk-master-0:15993 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 18:16:39,613.613 dlc26te6b6pxn0nk-master-0:15992 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 18:16:39,622.622 dlc26te6b6pxn0nk-master-0:15990 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:15990:15990 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:15992:15992 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15991:15991 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:15993:15993 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5499), 0.5285943345804383, 0.8463143254520167, tensor(2.0930)]
[tensor(-0.5494), 0.5312667022982362, 0.8497913769123783, tensor(2.1070)]
[tensor(-0.5494), 0.5312667022982362, 0.8497913769123783, tensor(2.1070)]
[tensor(-0.5424), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5366), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5366), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5362), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5362), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5362), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5362), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5317), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5317), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5317), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5317), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5317), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
[tensor(-0.5317), 0.5392838054516301, 0.8553546592489569, tensor(2.1540)]
early stopping at 16
[2023-01-17 18:50:41,746.746 dlc26te6b6pxn0nk-master-0:16103 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 18:50:42,399.399 dlc26te6b6pxn0nk-master-0:16157 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:50:42,399.399 dlc26te6b6pxn0nk-master-0:16159 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:50:42,399.399 dlc26te6b6pxn0nk-master-0:16160 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:50:42,399.399 dlc26te6b6pxn0nk-master-0:16158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 18:50:43,454.454 dlc26te6b6pxn0nk-master-0:16158 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 18:50:44,436.436 dlc26te6b6pxn0nk-master-0:16160 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 18:50:44,447.447 dlc26te6b6pxn0nk-master-0:16159 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 18:50:44,457.457 dlc26te6b6pxn0nk-master-0:16157 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:16157:16157 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:16159:16159 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16158:16158 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16160:16160 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5192), 0.5376803848209514, 0.8595271210013908, tensor(2.1692)]
[Tue Jan 17 18:53:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5192), 0.5376803848209514, 0.8595271210013908, tensor(2.1692)]
[Tue Jan 17 18:56:23 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5188), 0.5376803848209514, 0.8595271210013908, tensor(2.1692)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5120), 0.5456974879743453, 0.8623087621696801, tensor(2.2165)]
[tensor(-0.5120), 0.5467664350614645, 0.8623087621696801, tensor(2.2165)]
[2023-01-17 19:01:42,177.177 dlc26te6b6pxn0nk-master-0:16236 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 19:01:42,886.886 dlc26te6b6pxn0nk-master-0:16292 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:01:42,919.919 dlc26te6b6pxn0nk-master-0:16291 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:01:42,991.991 dlc26te6b6pxn0nk-master-0:16293 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:01:43,081.081 dlc26te6b6pxn0nk-master-0:16290 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:01:44,752.752 dlc26te6b6pxn0nk-master-0:16292 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 19:01:44,851.851 dlc26te6b6pxn0nk-master-0:16293 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 19:01:45,259.259 dlc26te6b6pxn0nk-master-0:16291 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 19:01:45,259.259 dlc26te6b6pxn0nk-master-0:16290 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:16290:16290 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:16291:16291 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16292:16292 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16293:16293 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5407), 0.5179048637092464, 0.8567454798331016, tensor(2.0488)]
[tensor(-0.5342), 0.5350080171031534, 0.8567454798331016, tensor(2.1409)]
[tensor(-0.5155), 0.5371459112773918, 0.8567454798331016, tensor(2.1702)]
[tensor(-0.5155), 0.5371459112773918, 0.8567454798331016, tensor(2.1702)]
[tensor(-0.5141), 0.5547835382148584, 0.8567454798331016, tensor(2.2598)]
[2023-01-17 19:12:18,522.522 dlc26te6b6pxn0nk-master-0:16368 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 19:12:19,262.262 dlc26te6b6pxn0nk-master-0:16424 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:12:19,283.283 dlc26te6b6pxn0nk-master-0:16423 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:12:19,342.342 dlc26te6b6pxn0nk-master-0:16422 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:12:19,398.398 dlc26te6b6pxn0nk-master-0:16425 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:12:20,273.273 dlc26te6b6pxn0nk-master-0:16425 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 19:12:20,640.640 dlc26te6b6pxn0nk-master-0:16424 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 19:12:20,656.656 dlc26te6b6pxn0nk-master-0:16423 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 19:12:20,660.660 dlc26te6b6pxn0nk-master-0:16422 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:16422:16422 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:16423:16423 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16424:16424 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16425:16425 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5413), 0.5312667022982362, 0.8567454798331016, tensor(2.1150)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Tue Jan 17 19:15:10 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5355), 0.5323356493853554, 0.8616133518776078, tensor(2.1262)]
[Tue Jan 17 19:18:00 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5337), 0.5323356493853554, 0.8616133518776078, tensor(2.1262)]
[tensor(-0.5194), 0.5435595938001069, 0.8616133518776078, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8616133518776078, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8616133518776078, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8616133518776078, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8643949930458971, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8643949930458971, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8643949930458971, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8643949930458971, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8643949930458971, tensor(2.1984)]
[tensor(-0.5194), 0.5435595938001069, 0.8643949930458971, tensor(2.1984)]
early stopping at 13
[2023-01-17 19:40:07,797.797 dlc26te6b6pxn0nk-master-0:16526 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 19:40:08,453.453 dlc26te6b6pxn0nk-master-0:16580 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:40:08,453.453 dlc26te6b6pxn0nk-master-0:16581 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:40:08,455.455 dlc26te6b6pxn0nk-master-0:16582 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:40:08,463.463 dlc26te6b6pxn0nk-master-0:16583 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:40:09,495.495 dlc26te6b6pxn0nk-master-0:16583 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 19:40:10,479.479 dlc26te6b6pxn0nk-master-0:16582 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 19:40:10,486.486 dlc26te6b6pxn0nk-master-0:16581 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 19:40:10,493.493 dlc26te6b6pxn0nk-master-0:16580 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:16580:16580 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:16581:16581 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16583:16583 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16582:16582 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5373), 0.5323356493853554, 0.8574408901251739, tensor(2.1244)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5344), 0.5398182789951897, 0.8581363004172462, tensor(2.1647)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5344), 0.5398182789951897, 0.8581363004172462, tensor(2.1647)]
[tensor(-0.5230), 0.5398182789951897, 0.8595271210013908, tensor(2.1647)]
[tensor(-0.5230), 0.5408872260823089, 0.8595271210013908, tensor(2.1772)]
[tensor(-0.5178), 0.5408872260823089, 0.8630041724617524, tensor(2.1839)]
[tensor(-0.5178), 0.5408872260823089, 0.8630041724617524, tensor(2.1839)]
[tensor(-0.5178), 0.5521111704970604, 0.8630041724617524, tensor(2.2407)]
[tensor(-0.5178), 0.5521111704970604, 0.8636995827538247, tensor(2.2407)]
[tensor(-0.5178), 0.5521111704970604, 0.8636995827538247, tensor(2.2407)]
[tensor(-0.5178), 0.5521111704970604, 0.8636995827538247, tensor(2.2407)]
[tensor(-0.5178), 0.5521111704970604, 0.8636995827538247, tensor(2.2407)]
[tensor(-0.5178), 0.5521111704970604, 0.8636995827538247, tensor(2.2407)]
[tensor(-0.5178), 0.5521111704970604, 0.8636995827538247, tensor(2.2407)]
early stopping at 14
[2023-01-17 20:09:12,114.114 dlc26te6b6pxn0nk-master-0:16685 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 20:09:12,771.771 dlc26te6b6pxn0nk-master-0:16742 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:09:12,771.771 dlc26te6b6pxn0nk-master-0:16739 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:09:12,771.771 dlc26te6b6pxn0nk-master-0:16741 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:09:12,772.772 dlc26te6b6pxn0nk-master-0:16740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:09:14,813.813 dlc26te6b6pxn0nk-master-0:16741 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 20:09:14,834.834 dlc26te6b6pxn0nk-master-0:16742 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 20:09:14,838.838 dlc26te6b6pxn0nk-master-0:16740 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 20:09:14,844.844 dlc26te6b6pxn0nk-master-0:16739 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:16739:16739 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:16740:16740 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16742:16742 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16741:16741 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5291), 0.5440940673436665, 0.8372739916550765, tensor(2.1914)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5203), 0.5440940673436665, 0.8546592489568846, tensor(2.1914)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5168), 0.5440940673436665, 0.8581363004172462, tensor(2.1914)]
[tensor(-0.5099), 0.5483698556921432, 0.8581363004172462, tensor(2.2319)]
[Tue Jan 17 20:17:50 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5099), 0.5483698556921432, 0.8636995827538247, tensor(2.2319)]
[2023-01-17 20:19:58,556.556 dlc26te6b6pxn0nk-master-0:16817 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 20:19:59,214.214 dlc26te6b6pxn0nk-master-0:16872 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:19:59,214.214 dlc26te6b6pxn0nk-master-0:16873 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:19:59,369.369 dlc26te6b6pxn0nk-master-0:16874 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:19:59,492.492 dlc26te6b6pxn0nk-master-0:16871 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:20:01,145.145 dlc26te6b6pxn0nk-master-0:16872 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 20:20:01,238.238 dlc26te6b6pxn0nk-master-0:16874 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 20:20:01,393.393 dlc26te6b6pxn0nk-master-0:16873 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 20:20:01,398.398 dlc26te6b6pxn0nk-master-0:16871 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:16871:16871 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:16873:16873 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16872:16872 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:16874:16874 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5843), 0.4997327632282202, 0.8414464534075105, tensor(1.9143)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5245), 0.538214858364511, 0.8595271210013908, tensor(2.1666)]
[tensor(-0.5118), 0.5430251202565473, 0.8595271210013908, tensor(2.2034)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5118), 0.5430251202565473, 0.8595271210013908, tensor(2.2034)]
[tensor(-0.5118), 0.5430251202565473, 0.8595271210013908, tensor(2.2034)]
[2023-01-17 20:30:43,980.980 dlc26te6b6pxn0nk-master-0:16949 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 20:30:44,631.631 dlc26te6b6pxn0nk-master-0:17003 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:30:44,641.641 dlc26te6b6pxn0nk-master-0:17006 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:30:44,719.719 dlc26te6b6pxn0nk-master-0:17004 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:30:44,719.719 dlc26te6b6pxn0nk-master-0:17005 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:30:45,565.565 dlc26te6b6pxn0nk-master-0:17006 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 20:30:46,074.074 dlc26te6b6pxn0nk-master-0:17004 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 20:30:46,075.075 dlc26te6b6pxn0nk-master-0:17005 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 20:30:46,081.081 dlc26te6b6pxn0nk-master-0:17003 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17003:17003 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17004:17004 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17005:17005 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17006:17006 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5394), 0.5323356493853554, 0.8539638386648123, tensor(2.1223)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5116), 0.5451630144307856, 0.8623087621696801, tensor(2.2142)]
[tensor(-0.5116), 0.5451630144307856, 0.8623087621696801, tensor(2.2142)]
[tensor(-0.5116), 0.5451630144307856, 0.8623087621696801, tensor(2.2142)]
[tensor(-0.5116), 0.5451630144307856, 0.8623087621696801, tensor(2.2142)]
[Tue Jan 17 20:42:53 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5116), 0.5451630144307856, 0.8623087621696801, tensor(2.2142)]
[tensor(-0.5116), 0.5451630144307856, 0.8623087621696801, tensor(2.2142)]
early stopping at 7
[2023-01-17 20:46:09,556.556 dlc26te6b6pxn0nk-master-0:17089 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 20:46:10,287.287 dlc26te6b6pxn0nk-master-0:17145 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:46:10,294.294 dlc26te6b6pxn0nk-master-0:17144 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:46:10,373.373 dlc26te6b6pxn0nk-master-0:17143 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:46:10,373.373 dlc26te6b6pxn0nk-master-0:17146 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 20:46:11,323.323 dlc26te6b6pxn0nk-master-0:17146 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 20:46:11,424.424 dlc26te6b6pxn0nk-master-0:17144 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 20:46:11,426.426 dlc26te6b6pxn0nk-master-0:17145 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 20:46:11,433.433 dlc26te6b6pxn0nk-master-0:17143 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17143:17143 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17146:17146 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17144:17144 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17145:17145 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5183), 0.5424906467129877, 0.849095966620306, tensor(2.1941)]
[tensor(-0.5183), 0.5462319615179049, 0.8518776077885952, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8518776077885952, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8539638386648123, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8546592489568846, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8546592489568846, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8546592489568846, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8546592489568846, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8546592489568846, tensor(2.2106)]
[tensor(-0.5183), 0.5462319615179049, 0.8546592489568846, tensor(2.2106)]
early stopping at 10
[2023-01-17 21:07:07,461.461 dlc26te6b6pxn0nk-master-0:17232 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 21:07:08,112.112 dlc26te6b6pxn0nk-master-0:17287 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:07:08,114.114 dlc26te6b6pxn0nk-master-0:17288 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:07:08,278.278 dlc26te6b6pxn0nk-master-0:17286 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:07:08,362.362 dlc26te6b6pxn0nk-master-0:17289 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:07:09,470.470 dlc26te6b6pxn0nk-master-0:17288 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 21:07:09,610.610 dlc26te6b6pxn0nk-master-0:17289 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 21:07:09,983.983 dlc26te6b6pxn0nk-master-0:17287 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 21:07:09,985.985 dlc26te6b6pxn0nk-master-0:17286 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17286:17286 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17287:17287 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17288:17288 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17289:17289 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5435), 0.5221806520577231, 0.8518776077885952, tensor(2.0674)]
[tensor(-0.5214), 0.5339390700160342, 0.8671766342141863, tensor(2.1483)]
[tensor(-0.5183), 0.538214858364511, 0.8671766342141863, tensor(2.1728)]
[tensor(-0.5183), 0.538214858364511, 0.8671766342141863, tensor(2.1728)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5172), 0.5478353821485836, 0.8671766342141863, tensor(2.2220)]
[2023-01-17 21:18:30,925.925 dlc26te6b6pxn0nk-master-0:17365 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 21:18:31,638.638 dlc26te6b6pxn0nk-master-0:17420 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:18:31,662.662 dlc26te6b6pxn0nk-master-0:17421 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:18:31,751.751 dlc26te6b6pxn0nk-master-0:17419 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:18:31,841.841 dlc26te6b6pxn0nk-master-0:17422 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:18:33,009.009 dlc26te6b6pxn0nk-master-0:17421 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 21:18:33,129.129 dlc26te6b6pxn0nk-master-0:17422 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 21:18:33,527.527 dlc26te6b6pxn0nk-master-0:17420 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 21:18:33,531.531 dlc26te6b6pxn0nk-master-0:17419 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17419:17419 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17422:17422 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17421:17421 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17420:17420 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5248), 0.5344735435595938, 0.8636995827538247, tensor(2.1475)]
[tensor(-0.5248), 0.5408872260823089, 0.8636995827538247, tensor(2.1750)]
[tensor(-0.5158), 0.5408872260823089, 0.8671766342141863, tensor(2.1750)]
[tensor(-0.5158), 0.5408872260823089, 0.8671766342141863, tensor(2.1750)]
[tensor(-0.5158), 0.5408872260823089, 0.8671766342141863, tensor(2.1767)]
[2023-01-17 21:29:18,350.350 dlc26te6b6pxn0nk-master-0:17497 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 21:29:19,001.001 dlc26te6b6pxn0nk-master-0:17553 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:29:19,005.005 dlc26te6b6pxn0nk-master-0:17552 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:29:19,085.085 dlc26te6b6pxn0nk-master-0:17554 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:29:19,087.087 dlc26te6b6pxn0nk-master-0:17551 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:29:20,233.233 dlc26te6b6pxn0nk-master-0:17554 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 21:29:20,916.916 dlc26te6b6pxn0nk-master-0:17553 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 21:29:20,930.930 dlc26te6b6pxn0nk-master-0:17552 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 21:29:20,930.930 dlc26te6b6pxn0nk-master-0:17551 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17551:17551 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17554:17554 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17553:17553 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17552:17552 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5448), 0.530197755211117, 0.8497913769123783, tensor(2.1062)]
[tensor(-0.5299), 0.5376803848209514, 0.8630041724617524, tensor(2.1585)]
[tensor(-0.5261), 0.5467664350614645, 0.8636995827538247, tensor(2.2078)]
[tensor(-0.5200), 0.5478353821485836, 0.8636995827538247, tensor(2.2192)]
[tensor(-0.5183), 0.5478353821485836, 0.8636995827538247, tensor(2.2192)]
[tensor(-0.5183), 0.5478353821485836, 0.8636995827538247, tensor(2.2192)]
[tensor(-0.5183), 0.5478353821485836, 0.8636995827538247, tensor(2.2192)]
[tensor(-0.5183), 0.5478353821485836, 0.8636995827538247, tensor(2.2192)]
[tensor(-0.5183), 0.5478353821485836, 0.8636995827538247, tensor(2.2192)]
[tensor(-0.5183), 0.5478353821485836, 0.8636995827538247, tensor(2.2192)]
early stopping at 10
[2023-01-17 21:50:32,315.315 dlc26te6b6pxn0nk-master-0:17645 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 21:50:33,054.054 dlc26te6b6pxn0nk-master-0:17701 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:50:33,056.056 dlc26te6b6pxn0nk-master-0:17700 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:50:33,137.137 dlc26te6b6pxn0nk-master-0:17702 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:50:33,137.137 dlc26te6b6pxn0nk-master-0:17699 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:50:34,468.468 dlc26te6b6pxn0nk-master-0:17701 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 21:50:34,470.470 dlc26te6b6pxn0nk-master-0:17700 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 21:50:35,085.085 dlc26te6b6pxn0nk-master-0:17702 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 21:50:35,092.092 dlc26te6b6pxn0nk-master-0:17699 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17699:17699 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17702:17702 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17701:17701 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17700:17700 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5369), 0.5360769641902726, 0.8504867872044506, tensor(2.1435)]
[tensor(-0.5369), 0.5360769641902726, 0.8602225312934632, tensor(2.1435)]
[tensor(-0.5224), 0.5360769641902726, 0.8602225312934632, tensor(2.1435)]
[tensor(-0.5224), 0.5360769641902726, 0.8602225312934632, tensor(2.1435)]
[tensor(-0.5224), 0.5360769641902726, 0.8650904033379694, tensor(2.1435)]
[tensor(-0.5224), 0.5360769641902726, 0.8650904033379694, tensor(2.1435)]
[tensor(-0.5224), 0.5360769641902726, 0.8650904033379694, tensor(2.1435)]
[tensor(-0.5224), 0.5360769641902726, 0.8650904033379694, tensor(2.1435)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5224), 0.5360769641902726, 0.8650904033379694, tensor(2.1435)]
[tensor(-0.5224), 0.5360769641902726, 0.8650904033379694, tensor(2.1435)]
early stopping at 10
[2023-01-17 22:11:50,325.325 dlc26te6b6pxn0nk-master-0:17793 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 22:11:50,980.980 dlc26te6b6pxn0nk-master-0:17849 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:11:51,028.028 dlc26te6b6pxn0nk-master-0:17848 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:11:51,049.049 dlc26te6b6pxn0nk-master-0:17850 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:11:51,104.104 dlc26te6b6pxn0nk-master-0:17847 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:11:52,551.551 dlc26te6b6pxn0nk-master-0:17848 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 22:11:52,849.849 dlc26te6b6pxn0nk-master-0:17849 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 22:11:53,158.158 dlc26te6b6pxn0nk-master-0:17850 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 22:11:53,164.164 dlc26te6b6pxn0nk-master-0:17847 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17847:17847 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17850:17850 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17849:17849 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17848:17848 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5320), 0.5318011758417959, 0.847009735744089, tensor(2.1270)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5223), 0.5483698556921432, 0.8504867872044506, tensor(2.2195)]
[tensor(-0.5223), 0.5483698556921432, 0.8504867872044506, tensor(2.2195)]
[tensor(-0.5181), 0.5483698556921432, 0.8706536856745479, tensor(2.2195)]
[tensor(-0.5139), 0.5483698556921432, 0.8706536856745479, tensor(2.2195)]
[2023-01-17 22:22:42,732.732 dlc26te6b6pxn0nk-master-0:17925 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 22:22:43,443.443 dlc26te6b6pxn0nk-master-0:17980 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:22:43,471.471 dlc26te6b6pxn0nk-master-0:17981 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:22:43,556.556 dlc26te6b6pxn0nk-master-0:17982 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:22:43,648.648 dlc26te6b6pxn0nk-master-0:17979 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:22:45,309.309 dlc26te6b6pxn0nk-master-0:17980 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 22:22:45,414.414 dlc26te6b6pxn0nk-master-0:17982 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 22:22:45,816.816 dlc26te6b6pxn0nk-master-0:17981 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 22:22:45,818.818 dlc26te6b6pxn0nk-master-0:17979 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:17979:17979 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:17980:17980 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17982:17982 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:17981:17981 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5424), 0.5318011758417959, 0.8282336578581363, tensor(2.1166)]
[tensor(-0.5174), 0.5515766969535008, 0.8671766342141863, tensor(2.2404)]
[tensor(-0.5107), 0.5515766969535008, 0.868567454798331, tensor(2.2404)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5107), 0.5515766969535008, 0.868567454798331, tensor(2.2404)]
[tensor(-0.5107), 0.5515766969535008, 0.868567454798331, tensor(2.2404)]
[2023-01-17 22:33:34,141.141 dlc26te6b6pxn0nk-master-0:18057 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 22:33:34,796.796 dlc26te6b6pxn0nk-master-0:18111 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:33:34,796.796 dlc26te6b6pxn0nk-master-0:18112 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:33:34,796.796 dlc26te6b6pxn0nk-master-0:18114 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:33:34,807.807 dlc26te6b6pxn0nk-master-0:18113 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:33:35,855.855 dlc26te6b6pxn0nk-master-0:18112 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 22:33:35,857.857 dlc26te6b6pxn0nk-master-0:18114 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 22:33:35,861.861 dlc26te6b6pxn0nk-master-0:18113 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 22:33:35,869.869 dlc26te6b6pxn0nk-master-0:18111 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:18111:18111 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:18112:18112 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18114:18114 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18113:18113 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.5224), 0.5467664350614645, 0.8595271210013908, tensor(2.2115)]
[tensor(-0.5224), 0.5467664350614645, 0.8595271210013908, tensor(2.2115)]
[tensor(-0.5146), 0.5510422234099412, 0.8671766342141863, tensor(2.2406)]
[tensor(-0.5114), 0.5521111704970604, 0.8671766342141863, tensor(2.2492)]
[tensor(-0.5114), 0.5521111704970604, 0.8671766342141863, tensor(2.2492)]
[tensor(-0.5114), 0.5521111704970604, 0.8671766342141863, tensor(2.2492)]
[tensor(-0.5114), 0.5521111704970604, 0.8671766342141863, tensor(2.2492)]
[tensor(-0.5114), 0.5521111704970604, 0.8671766342141863, tensor(2.2492)]
[tensor(-0.5114), 0.5521111704970604, 0.8671766342141863, tensor(2.2492)]
early stopping at 9
[2023-01-17 22:52:42,041.041 dlc26te6b6pxn0nk-master-0:18201 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 22:52:42,703.703 dlc26te6b6pxn0nk-master-0:18256 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:52:42,711.711 dlc26te6b6pxn0nk-master-0:18258 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:52:42,779.779 dlc26te6b6pxn0nk-master-0:18255 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:52:42,786.786 dlc26te6b6pxn0nk-master-0:18257 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:52:44,640.640 dlc26te6b6pxn0nk-master-0:18256 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 22:52:44,642.642 dlc26te6b6pxn0nk-master-0:18258 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 22:52:45,184.184 dlc26te6b6pxn0nk-master-0:18257 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 22:52:45,187.187 dlc26te6b6pxn0nk-master-0:18255 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:18255:18255 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:18258:18258 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18257:18257 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18256:18256 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5367), 0.5307322287546766, 0.8574408901251739, tensor(2.1170)]
[tensor(-0.5311), 0.5392838054516301, 0.868567454798331, tensor(2.1653)]
[tensor(-0.5188), 0.5398182789951897, 0.868567454798331, tensor(2.1803)]
[tensor(-0.5188), 0.5414216996258685, 0.868567454798331, tensor(2.1830)]
[tensor(-0.5188), 0.5414216996258685, 0.868567454798331, tensor(2.1830)]
[tensor(-0.5188), 0.5414216996258685, 0.868567454798331, tensor(2.1830)]
[tensor(-0.5188), 0.5414216996258685, 0.868567454798331, tensor(2.1830)]
[tensor(-0.5188), 0.5414216996258685, 0.868567454798331, tensor(2.1830)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5188), 0.5414216996258685, 0.868567454798331, tensor(2.1830)]
early stopping at 9
[2023-01-17 23:12:06,915.915 dlc26te6b6pxn0nk-master-0:18346 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 23:12:07,651.651 dlc26te6b6pxn0nk-master-0:18401 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:12:07,651.651 dlc26te6b6pxn0nk-master-0:18402 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:12:07,819.819 dlc26te6b6pxn0nk-master-0:18403 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:12:07,819.819 dlc26te6b6pxn0nk-master-0:18400 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:12:09,136.136 dlc26te6b6pxn0nk-master-0:18401 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 23:12:09,141.141 dlc26te6b6pxn0nk-master-0:18402 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 23:12:10,125.125 dlc26te6b6pxn0nk-master-0:18403 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 23:12:10,127.127 dlc26te6b6pxn0nk-master-0:18400 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:18400:18400 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:18403:18403 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18401:18401 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18402:18402 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5367), 0.5435595938001069, 0.8358831710709318, tensor(2.1811)]
[tensor(-0.5219), 0.5435595938001069, 0.8532684283727399, tensor(2.1811)]
[tensor(-0.5219), 0.5435595938001069, 0.8643949930458971, tensor(2.1811)]
[tensor(-0.5219), 0.5435595938001069, 0.8643949930458971, tensor(2.1811)]
[tensor(-0.5124), 0.5435595938001069, 0.8643949930458971, tensor(2.1811)]
[2023-01-17 23:23:21,351.351 dlc26te6b6pxn0nk-master-0:18479 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 23:23:22,058.058 dlc26te6b6pxn0nk-master-0:18534 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:23:22,085.085 dlc26te6b6pxn0nk-master-0:18535 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:23:22,167.167 dlc26te6b6pxn0nk-master-0:18533 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:23:22,255.255 dlc26te6b6pxn0nk-master-0:18536 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:23:23,814.814 dlc26te6b6pxn0nk-master-0:18535 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 23:23:23,815.815 dlc26te6b6pxn0nk-master-0:18536 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 23:23:23,940.940 dlc26te6b6pxn0nk-master-0:18534 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 23:23:23,942.942 dlc26te6b6pxn0nk-master-0:18533 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:18533:18533 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:18534:18534 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18536:18536 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18535:18535 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5279), 0.5227151256012827, 0.8602225312934632, tensor(2.0856)]
[tensor(-0.5279), 0.5253874933190807, 0.8602225312934632, tensor(2.0856)]
[tensor(-0.5098), 0.5478353821485836, 0.8602225312934632, tensor(2.2294)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5098), 0.5478353821485836, 0.866481223922114, tensor(2.2294)]
[tensor(-0.5098), 0.5478353821485836, 0.866481223922114, tensor(2.2294)]
[2023-01-17 23:34:33,892.892 dlc26te6b6pxn0nk-master-0:18611 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 23:34:34,542.542 dlc26te6b6pxn0nk-master-0:18668 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:34:34,551.551 dlc26te6b6pxn0nk-master-0:18667 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:34:34,626.626 dlc26te6b6pxn0nk-master-0:18666 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:34:34,633.633 dlc26te6b6pxn0nk-master-0:18665 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:34:36,024.024 dlc26te6b6pxn0nk-master-0:18666 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 23:34:36,504.504 dlc26te6b6pxn0nk-master-0:18668 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 23:34:36,505.505 dlc26te6b6pxn0nk-master-0:18667 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 23:34:36,509.509 dlc26te6b6pxn0nk-master-0:18665 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:18665:18665 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:18668:18668 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18667:18667 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18666:18666 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5518), 0.5136290753607696, 0.8435326842837274, tensor(2.0164)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5394), 0.532870122928915, 0.8574408901251739, tensor(2.1250)]
[tensor(-0.5394), 0.532870122928915, 0.8609179415855355, tensor(2.1250)]
[tensor(-0.5169), 0.5483698556921432, 0.8609179415855355, tensor(2.2249)]
[tensor(-0.5169), 0.5483698556921432, 0.8609179415855355, tensor(2.2249)]
[tensor(-0.5169), 0.5483698556921432, 0.8609179415855355, tensor(2.2249)]
[tensor(-0.5169), 0.5483698556921432, 0.8609179415855355, tensor(2.2249)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5169), 0.5483698556921432, 0.8609179415855355, tensor(2.2249)]
[tensor(-0.5169), 0.5483698556921432, 0.8609179415855355, tensor(2.2249)]
early stopping at 9
[2023-01-17 23:53:46,726.726 dlc26te6b6pxn0nk-master-0:18756 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 23:53:47,374.374 dlc26te6b6pxn0nk-master-0:18811 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:53:47,377.377 dlc26te6b6pxn0nk-master-0:18812 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:53:47,548.548 dlc26te6b6pxn0nk-master-0:18813 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:53:47,625.625 dlc26te6b6pxn0nk-master-0:18810 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 23:53:49,281.281 dlc26te6b6pxn0nk-master-0:18811 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 23:53:49,428.428 dlc26te6b6pxn0nk-master-0:18813 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 23:53:49,738.738 dlc26te6b6pxn0nk-master-0:18812 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 23:53:49,747.747 dlc26te6b6pxn0nk-master-0:18810 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:18810:18810 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:18811:18811 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18813:18813 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18812:18812 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5932), 0.5179048637092464, 0.8296244784422809, tensor(1.9964)]
[tensor(-0.5762), 0.5179048637092464, 0.8463143254520167, tensor(1.9964)]
[tensor(-0.5253), 0.5424906467129877, 0.8539638386648123, tensor(2.1872)]
[tensor(-0.5253), 0.5424906467129877, 0.8539638386648123, tensor(2.1872)]
[tensor(-0.5253), 0.5424906467129877, 0.8602225312934632, tensor(2.1872)]
[tensor(-0.5253), 0.5440940673436665, 0.8602225312934632, tensor(2.1885)]
[tensor(-0.5253), 0.5440940673436665, 0.8602225312934632, tensor(2.1885)]
[tensor(-0.5253), 0.5440940673436665, 0.8602225312934632, tensor(2.1885)]
[tensor(-0.5253), 0.5440940673436665, 0.8602225312934632, tensor(2.1885)]
[tensor(-0.5253), 0.5440940673436665, 0.8602225312934632, tensor(2.1885)]
[tensor(-0.5253), 0.5440940673436665, 0.8602225312934632, tensor(2.1885)]
early stopping at 11
[2023-01-18 00:16:56,801.801 dlc26te6b6pxn0nk-master-0:18907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 00:16:57,512.512 dlc26te6b6pxn0nk-master-0:18963 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:16:57,540.540 dlc26te6b6pxn0nk-master-0:18962 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:16:57,627.627 dlc26te6b6pxn0nk-master-0:18961 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:16:57,718.718 dlc26te6b6pxn0nk-master-0:18964 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:16:58,885.885 dlc26te6b6pxn0nk-master-0:18962 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 00:16:58,997.997 dlc26te6b6pxn0nk-master-0:18964 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 00:16:59,382.382 dlc26te6b6pxn0nk-master-0:18963 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 00:16:59,389.389 dlc26te6b6pxn0nk-master-0:18961 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:18961:18961 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:18963:18963 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18964:18964 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:18962:18962 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5192), 0.5435595938001069, 0.8511821974965229, tensor(2.1986)]
[tensor(-0.5192), 0.5435595938001069, 0.8553546592489569, tensor(2.1986)]
[tensor(-0.5192), 0.5435595938001069, 0.8609179415855355, tensor(2.1986)]
[tensor(-0.5192), 0.5435595938001069, 0.8657858136300417, tensor(2.1986)]
[tensor(-0.5066), 0.5494388027792624, 0.8657858136300417, tensor(2.2406)]
[2023-01-18 00:28:15,231.231 dlc26te6b6pxn0nk-master-0:19039 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 00:28:15,971.971 dlc26te6b6pxn0nk-master-0:19094 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:28:15,976.976 dlc26te6b6pxn0nk-master-0:19095 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:28:16,053.053 dlc26te6b6pxn0nk-master-0:19093 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:28:16,054.054 dlc26te6b6pxn0nk-master-0:19096 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:28:17,019.019 dlc26te6b6pxn0nk-master-0:19096 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 00:28:17,139.139 dlc26te6b6pxn0nk-master-0:19094 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 00:28:17,140.140 dlc26te6b6pxn0nk-master-0:19095 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 00:28:17,142.142 dlc26te6b6pxn0nk-master-0:19093 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:19093:19093 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:19096:19096 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19094:19094 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19095:19095 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5386), 0.5280598610368786, 0.866481223922114, tensor(2.1017)]
[tensor(-0.5129), 0.5467664350614645, 0.866481223922114, tensor(2.2209)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5080), 0.5467664350614645, 0.866481223922114, tensor(2.2209)]
[tensor(-0.5080), 0.5467664350614645, 0.866481223922114, tensor(2.2209)]
[tensor(-0.5080), 0.5542490646712988, 0.866481223922114, tensor(2.2604)]
[2023-01-18 00:38:59,582.582 dlc26te6b6pxn0nk-master-0:19172 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 00:39:00,225.225 dlc26te6b6pxn0nk-master-0:19227 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:39:00,226.226 dlc26te6b6pxn0nk-master-0:19226 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:39:00,312.312 dlc26te6b6pxn0nk-master-0:19228 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:39:00,318.318 dlc26te6b6pxn0nk-master-0:19229 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:39:01,385.385 dlc26te6b6pxn0nk-master-0:19229 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 00:39:01,386.386 dlc26te6b6pxn0nk-master-0:19228 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 00:39:02,162.162 dlc26te6b6pxn0nk-master-0:19227 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 00:39:02,172.172 dlc26te6b6pxn0nk-master-0:19226 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:19226:19226 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:19228:19228 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19229:19229 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19227:19227 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5377), 0.5221806520577231, 0.8324061196105702, tensor(2.0732)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5377), 0.5334045964724746, 0.8560500695410292, tensor(2.1204)]
[tensor(-0.5238), 0.5387493319080705, 0.8581363004172462, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8581363004172462, tensor(2.1700)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5144), 0.5387493319080705, 0.8595271210013908, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8595271210013908, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8595271210013908, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8630041724617524, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8630041724617524, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8630041724617524, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8630041724617524, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8630041724617524, tensor(2.1700)]
[tensor(-0.5144), 0.5387493319080705, 0.8630041724617524, tensor(2.1700)]
early stopping at 13
[2023-01-18 01:06:27,905.905 dlc26te6b6pxn0nk-master-0:19329 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 01:06:28,564.564 dlc26te6b6pxn0nk-master-0:19386 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:06:28,569.569 dlc26te6b6pxn0nk-master-0:19384 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:06:28,730.730 dlc26te6b6pxn0nk-master-0:19385 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:06:28,810.810 dlc26te6b6pxn0nk-master-0:19383 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:06:30,443.443 dlc26te6b6pxn0nk-master-0:19384 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 01:06:30,599.599 dlc26te6b6pxn0nk-master-0:19385 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 01:06:30,925.925 dlc26te6b6pxn0nk-master-0:19386 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 01:06:30,932.932 dlc26te6b6pxn0nk-master-0:19383 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:19383:19383 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:19385:19385 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19384:19384 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19386:19386 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5237), 0.5419561731694281, 0.8650904033379694, tensor(2.1860)]
[tensor(-0.5237), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
[tensor(-0.5094), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
[tensor(-0.5094), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
[tensor(-0.5094), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
[tensor(-0.5094), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
[tensor(-0.5094), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
[tensor(-0.5094), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
[tensor(-0.5094), 0.5537145911277391, 0.8650904033379694, tensor(2.2421)]
early stopping at 9
[2023-01-18 01:25:44,794.794 dlc26te6b6pxn0nk-master-0:19473 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 01:25:45,441.441 dlc26te6b6pxn0nk-master-0:19527 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:25:45,441.441 dlc26te6b6pxn0nk-master-0:19530 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:25:45,525.525 dlc26te6b6pxn0nk-master-0:19528 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:25:45,530.530 dlc26te6b6pxn0nk-master-0:19529 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:25:46,399.399 dlc26te6b6pxn0nk-master-0:19530 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 01:25:46,903.903 dlc26te6b6pxn0nk-master-0:19529 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 01:25:46,904.904 dlc26te6b6pxn0nk-master-0:19528 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 01:25:46,904.904 dlc26te6b6pxn0nk-master-0:19527 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:19527:19527 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:19530:19530 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19528:19528 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19529:19529 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5314), 0.5360769641902726, 0.8539638386648123, tensor(2.1490)]
[tensor(-0.5235), 0.5360769641902726, 0.8616133518776078, tensor(2.1490)]
[Wed Jan 18 01:31:24 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5170), 0.5467664350614645, 0.8616133518776078, tensor(2.2169)]
[tensor(-0.5170), 0.5467664350614645, 0.8643949930458971, tensor(2.2169)]
[tensor(-0.5170), 0.5494388027792624, 0.8643949930458971, tensor(2.2291)]
[2023-01-18 01:36:27,163.163 dlc26te6b6pxn0nk-master-0:19605 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 01:36:27,807.807 dlc26te6b6pxn0nk-master-0:19660 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:36:27,807.807 dlc26te6b6pxn0nk-master-0:19661 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:36:27,985.985 dlc26te6b6pxn0nk-master-0:19659 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:36:28,065.065 dlc26te6b6pxn0nk-master-0:19662 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:36:29,156.156 dlc26te6b6pxn0nk-master-0:19661 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 01:36:29,298.298 dlc26te6b6pxn0nk-master-0:19662 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 01:36:29,685.685 dlc26te6b6pxn0nk-master-0:19660 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 01:36:29,690.690 dlc26te6b6pxn0nk-master-0:19659 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:19659:19659 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:19660:19660 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19661:19661 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19662:19662 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5311), 0.5424906467129877, 0.8602225312934632, tensor(2.1813)]
[tensor(-0.5186), 0.549973276322822, 0.8602225312934632, tensor(2.2313)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5122), 0.549973276322822, 0.8671766342141863, tensor(2.2313)]
[tensor(-0.5122), 0.549973276322822, 0.8671766342141863, tensor(2.2313)]
[tensor(-0.5122), 0.549973276322822, 0.8671766342141863, tensor(2.2313)]
[2023-01-18 01:47:16,575.575 dlc26te6b6pxn0nk-master-0:19737 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 01:47:17,221.221 dlc26te6b6pxn0nk-master-0:19792 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:47:17,221.221 dlc26te6b6pxn0nk-master-0:19793 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:47:17,401.401 dlc26te6b6pxn0nk-master-0:19794 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:47:17,485.485 dlc26te6b6pxn0nk-master-0:19791 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 01:47:19,120.120 dlc26te6b6pxn0nk-master-0:19793 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 01:47:19,268.268 dlc26te6b6pxn0nk-master-0:19794 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 01:47:19,567.567 dlc26te6b6pxn0nk-master-0:19792 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 01:47:19,570.570 dlc26te6b6pxn0nk-master-0:19791 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:19791:19791 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:19793:19793 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19792:19792 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19794:19794 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5385), 0.5350080171031534, 0.8574408901251739, tensor(2.1365)]
[tensor(-0.5264), 0.5350080171031534, 0.8574408901251739, tensor(2.1365)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Wed Jan 18 01:52:59 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5264), 0.5350080171031534, 0.8588317107093185, tensor(2.1365)]
[tensor(-0.5218), 0.5398182789951897, 0.8588317107093185, tensor(2.1773)]
[tensor(-0.5165), 0.5489043292357029, 0.8588317107093185, tensor(2.2280)]
[tensor(-0.5165), 0.5489043292357029, 0.8602225312934632, tensor(2.2280)]
[tensor(-0.5165), 0.5489043292357029, 0.8602225312934632, tensor(2.2280)]
[tensor(-0.5165), 0.5489043292357029, 0.8602225312934632, tensor(2.2280)]
[tensor(-0.5165), 0.5489043292357029, 0.8602225312934632, tensor(2.2280)]
[tensor(-0.5165), 0.5489043292357029, 0.8602225312934632, tensor(2.2280)]
[tensor(-0.5165), 0.5489043292357029, 0.8602225312934632, tensor(2.2280)]
early stopping at 11
[2023-01-18 02:10:48,738.738 dlc26te6b6pxn0nk-master-0:19888 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 02:10:49,383.383 dlc26te6b6pxn0nk-master-0:19944 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:10:49,384.384 dlc26te6b6pxn0nk-master-0:19943 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:10:49,555.555 dlc26te6b6pxn0nk-master-0:19945 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:10:49,639.639 dlc26te6b6pxn0nk-master-0:19942 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:10:51,261.261 dlc26te6b6pxn0nk-master-0:19944 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 02:10:51,418.418 dlc26te6b6pxn0nk-master-0:19945 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 02:10:51,729.729 dlc26te6b6pxn0nk-master-0:19943 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 02:10:51,730.730 dlc26te6b6pxn0nk-master-0:19942 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:19942:19942 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:19943:19943 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19945:19945 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:19944:19944 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5345), 0.5392838054516301, 0.868567454798331, tensor(2.1619)]
[tensor(-0.5345), 0.5392838054516301, 0.868567454798331, tensor(2.1619)]
[tensor(-0.5329), 0.5392838054516301, 0.868567454798331, tensor(2.1619)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5126), 0.549973276322822, 0.868567454798331, tensor(2.2373)]
[tensor(-0.5126), 0.549973276322822, 0.868567454798331, tensor(2.2373)]
[tensor(-0.5126), 0.549973276322822, 0.868567454798331, tensor(2.2373)]
[tensor(-0.5126), 0.549973276322822, 0.868567454798331, tensor(2.2373)]
[tensor(-0.5126), 0.549973276322822, 0.868567454798331, tensor(2.2373)]
[tensor(-0.5126), 0.549973276322822, 0.868567454798331, tensor(2.2373)]
early stopping at 9
[2023-01-18 02:29:40,589.589 dlc26te6b6pxn0nk-master-0:20032 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 02:29:41,300.300 dlc26te6b6pxn0nk-master-0:20087 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:29:41,326.326 dlc26te6b6pxn0nk-master-0:20088 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:29:41,412.412 dlc26te6b6pxn0nk-master-0:20089 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:29:41,506.506 dlc26te6b6pxn0nk-master-0:20086 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:29:43,166.166 dlc26te6b6pxn0nk-master-0:20087 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 02:29:43,277.277 dlc26te6b6pxn0nk-master-0:20089 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 02:29:43,666.666 dlc26te6b6pxn0nk-master-0:20088 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 02:29:43,667.667 dlc26te6b6pxn0nk-master-0:20086 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:20086:20086 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:20088:20088 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20087:20087 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20089:20089 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5345), 0.5253874933190807, 0.8560500695410292, tensor(2.0925)]
[tensor(-0.5206), 0.5350080171031534, 0.8574408901251739, tensor(2.1545)]
[tensor(-0.5153), 0.5419561731694281, 0.8588317107093185, tensor(2.1945)]
[tensor(-0.5153), 0.5419561731694281, 0.8595271210013908, tensor(2.1945)]
[tensor(-0.5153), 0.5419561731694281, 0.8602225312934632, tensor(2.1945)]
[2023-01-18 02:40:20,032.032 dlc26te6b6pxn0nk-master-0:20164 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 02:40:20,693.693 dlc26te6b6pxn0nk-master-0:20218 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:40:20,709.709 dlc26te6b6pxn0nk-master-0:20219 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:40:20,777.777 dlc26te6b6pxn0nk-master-0:20220 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:40:20,781.781 dlc26te6b6pxn0nk-master-0:20221 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:40:22,133.133 dlc26te6b6pxn0nk-master-0:20221 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 02:40:22,135.135 dlc26te6b6pxn0nk-master-0:20220 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 02:40:22,648.648 dlc26te6b6pxn0nk-master-0:20219 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 02:40:22,654.654 dlc26te6b6pxn0nk-master-0:20218 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:20218:20218 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:20221:20221 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20220:20220 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20219:20219 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5399), 0.5173703901656868, 0.8393602225312935, tensor(2.0469)]
[tensor(-0.5399), 0.5173703901656868, 0.8484005563282336, tensor(2.0469)]
[tensor(-0.5285), 0.5408872260823089, 0.8616133518776078, tensor(2.1760)]
[tensor(-0.5285), 0.5408872260823089, 0.8616133518776078, tensor(2.1760)]
[tensor(-0.5246), 0.5446285408872261, 0.8616133518776078, tensor(2.1986)]
[2023-01-18 02:50:57,429.429 dlc26te6b6pxn0nk-master-0:20296 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 02:50:58,159.159 dlc26te6b6pxn0nk-master-0:20352 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:50:58,163.163 dlc26te6b6pxn0nk-master-0:20350 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:50:58,238.238 dlc26te6b6pxn0nk-master-0:20351 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:50:58,238.238 dlc26te6b6pxn0nk-master-0:20353 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:51:00,188.188 dlc26te6b6pxn0nk-master-0:20353 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 02:51:00,189.189 dlc26te6b6pxn0nk-master-0:20351 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 02:51:00,511.511 dlc26te6b6pxn0nk-master-0:20352 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 02:51:00,517.517 dlc26te6b6pxn0nk-master-0:20350 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:20350:20350 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:20353:20353 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20351:20351 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20352:20352 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5262), 0.5478353821485836, 0.8574408901251739, tensor(2.2129)]
[tensor(-0.5149), 0.5478353821485836, 0.8574408901251739, tensor(2.2129)]
[tensor(-0.5149), 0.5478353821485836, 0.8650904033379694, tensor(2.2129)]
[tensor(-0.5149), 0.5569214323890967, 0.8650904033379694, tensor(2.2697)]
[Wed Jan 18 02:59:34 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5149), 0.5569214323890967, 0.8650904033379694, tensor(2.2697)]
[tensor(-0.5149), 0.5569214323890967, 0.8650904033379694, tensor(2.2697)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5149), 0.5569214323890967, 0.8650904033379694, tensor(2.2697)]
[tensor(-0.5149), 0.5569214323890967, 0.8650904033379694, tensor(2.2697)]
[tensor(-0.5149), 0.5569214323890967, 0.8650904033379694, tensor(2.2697)]
early stopping at 9
[2023-01-18 03:09:54,213.213 dlc26te6b6pxn0nk-master-0:20439 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 03:09:54,974.974 dlc26te6b6pxn0nk-master-0:20496 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:09:54,978.978 dlc26te6b6pxn0nk-master-0:20493 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:09:54,999.999 dlc26te6b6pxn0nk-master-0:20494 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:09:55,000.000 dlc26te6b6pxn0nk-master-0:20495 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:09:56,148.148 dlc26te6b6pxn0nk-master-0:20494 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 03:09:56,149.149 dlc26te6b6pxn0nk-master-0:20495 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 03:09:56,916.916 dlc26te6b6pxn0nk-master-0:20496 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 03:09:56,921.921 dlc26te6b6pxn0nk-master-0:20493 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:20493:20493 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:20494:20494 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20496:20496 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20495:20495 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5826), 0.5093532870122929, 0.8511821974965229, tensor(1.9641)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5198), 0.5430251202565473, 0.8616133518776078, tensor(2.1953)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5198), 0.5430251202565473, 0.8616133518776078, tensor(2.1953)]
[tensor(-0.5198), 0.5430251202565473, 0.8616133518776078, tensor(2.1953)]
[tensor(-0.5198), 0.5430251202565473, 0.8616133518776078, tensor(2.1953)]
[tensor(-0.5198), 0.5430251202565473, 0.8616133518776078, tensor(2.1953)]
[tensor(-0.5198), 0.5430251202565473, 0.8616133518776078, tensor(2.1953)]
early stopping at 7
[2023-01-18 03:24:42,897.897 dlc26te6b6pxn0nk-master-0:20577 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 03:24:43,607.607 dlc26te6b6pxn0nk-master-0:20633 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:24:43,635.635 dlc26te6b6pxn0nk-master-0:20632 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:24:43,723.723 dlc26te6b6pxn0nk-master-0:20634 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:24:43,814.814 dlc26te6b6pxn0nk-master-0:20631 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:24:45,482.482 dlc26te6b6pxn0nk-master-0:20633 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 03:24:45,602.602 dlc26te6b6pxn0nk-master-0:20634 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 03:24:46,008.008 dlc26te6b6pxn0nk-master-0:20632 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 03:24:46,010.010 dlc26te6b6pxn0nk-master-0:20631 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:20631:20631 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:20632:20632 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20634:20634 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20633:20633 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6329), 0.46178514163548906, 0.847009735744089, tensor(1.6760)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5936), 0.48850881881346875, 0.847009735744089, tensor(1.8490)]
[tensor(-0.5243), 0.5366114377338321, 0.8602225312934632, tensor(2.1588)]
[tensor(-0.5180), 0.5440940673436665, 0.8678720445062587, tensor(2.2025)]
[tensor(-0.5180), 0.5440940673436665, 0.8678720445062587, tensor(2.2025)]
[2023-01-18 03:35:33,349.349 dlc26te6b6pxn0nk-master-0:20709 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 03:35:33,988.988 dlc26te6b6pxn0nk-master-0:20766 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:35:33,988.988 dlc26te6b6pxn0nk-master-0:20764 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:35:34,074.074 dlc26te6b6pxn0nk-master-0:20763 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:35:34,087.087 dlc26te6b6pxn0nk-master-0:20765 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:35:35,445.445 dlc26te6b6pxn0nk-master-0:20765 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 03:35:35,952.952 dlc26te6b6pxn0nk-master-0:20764 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 03:35:35,953.953 dlc26te6b6pxn0nk-master-0:20766 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 03:35:35,962.962 dlc26te6b6pxn0nk-master-0:20763 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:20763:20763 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:20766:20766 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20765:20765 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20764:20764 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7215), 0.4462854088722608, 0.7121001390820584, tensor(1.5100)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5659), 0.5205772314270444, 0.8421418636995828, tensor(2.0370)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5442), 0.538214858364511, 0.8532684283727399, tensor(2.1469)]
[tensor(-0.5442), 0.538214858364511, 0.8532684283727399, tensor(2.1469)]
[tensor(-0.5418), 0.5430251202565473, 0.8532684283727399, tensor(2.1733)]
[2023-01-18 03:46:26,715.715 dlc26te6b6pxn0nk-master-0:20841 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 03:46:27,350.350 dlc26te6b6pxn0nk-master-0:20895 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:46:27,350.350 dlc26te6b6pxn0nk-master-0:20896 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:46:27,355.355 dlc26te6b6pxn0nk-master-0:20898 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:46:27,355.355 dlc26te6b6pxn0nk-master-0:20897 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 03:46:28,432.432 dlc26te6b6pxn0nk-master-0:20896 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 03:46:29,423.423 dlc26te6b6pxn0nk-master-0:20897 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 03:46:29,426.426 dlc26te6b6pxn0nk-master-0:20898 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 03:46:29,434.434 dlc26te6b6pxn0nk-master-0:20895 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:20895:20895 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:20896:20896 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20897:20897 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:20898:20898 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5421), 0.5280598610368786, 0.8553546592489569, tensor(2.0982)]
[tensor(-0.5421), 0.5280598610368786, 0.8602225312934632, tensor(2.0982)]
[tensor(-0.5311), 0.5387493319080705, 0.8602225312934632, tensor(2.1627)]
[tensor(-0.5211), 0.5387493319080705, 0.8602225312934632, tensor(2.1673)]
[tensor(-0.5211), 0.5387493319080705, 0.8602225312934632, tensor(2.1673)]
[tensor(-0.5211), 0.5387493319080705, 0.8602225312934632, tensor(2.1673)]
[tensor(-0.5211), 0.5387493319080705, 0.8602225312934632, tensor(2.1673)]
[tensor(-0.5211), 0.5387493319080705, 0.8602225312934632, tensor(2.1673)]
[tensor(-0.5211), 0.5387493319080705, 0.8602225312934632, tensor(2.1673)]
early stopping at 9
[2023-01-18 04:05:44,547.547 dlc26te6b6pxn0nk-master-0:20986 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 04:05:45,202.202 dlc26te6b6pxn0nk-master-0:21042 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:05:45,203.203 dlc26te6b6pxn0nk-master-0:21041 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:05:45,366.366 dlc26te6b6pxn0nk-master-0:21040 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:05:45,367.367 dlc26te6b6pxn0nk-master-0:21043 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:05:46,306.306 dlc26te6b6pxn0nk-master-0:21043 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 04:05:47,140.140 dlc26te6b6pxn0nk-master-0:21042 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 04:05:47,144.144 dlc26te6b6pxn0nk-master-0:21041 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 04:05:47,145.145 dlc26te6b6pxn0nk-master-0:21040 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:21040:21040 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:21043:21043 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21042:21042 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21041:21041 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5334), 0.5285943345804383, 0.8456189151599444, tensor(2.1096)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5334), 0.5430251202565473, 0.8567454798331016, tensor(2.1816)]
[tensor(-0.5197), 0.5489043292357029, 0.8609179415855355, tensor(2.2248)]
[tensor(-0.5197), 0.5489043292357029, 0.8657858136300417, tensor(2.2248)]
[tensor(-0.5197), 0.5489043292357029, 0.868567454798331, tensor(2.2248)]
[tensor(-0.5197), 0.5489043292357029, 0.868567454798331, tensor(2.2248)]
[tensor(-0.5197), 0.5489043292357029, 0.868567454798331, tensor(2.2248)]
[tensor(-0.5197), 0.5489043292357029, 0.868567454798331, tensor(2.2248)]
[tensor(-0.5197), 0.5489043292357029, 0.868567454798331, tensor(2.2248)]
[tensor(-0.5197), 0.5489043292357029, 0.868567454798331, tensor(2.2248)]
early stopping at 10
[2023-01-18 04:26:56,566.566 dlc26te6b6pxn0nk-master-0:21134 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 04:26:57,224.224 dlc26te6b6pxn0nk-master-0:21190 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:26:57,226.226 dlc26te6b6pxn0nk-master-0:21191 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:26:57,227.227 dlc26te6b6pxn0nk-master-0:21192 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:26:57,244.244 dlc26te6b6pxn0nk-master-0:21189 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:26:59,198.198 dlc26te6b6pxn0nk-master-0:21190 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 04:26:59,273.273 dlc26te6b6pxn0nk-master-0:21192 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 04:26:59,277.277 dlc26te6b6pxn0nk-master-0:21191 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 04:26:59,287.287 dlc26te6b6pxn0nk-master-0:21189 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:21189:21189 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:21192:21192 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21190:21190 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21191:21191 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5678), 0.5269909139497595, 0.8504867872044506, tensor(2.0672)]
[tensor(-0.5153), 0.5478353821485836, 0.8650904033379694, tensor(2.2239)]
[tensor(-0.5089), 0.5478353821485836, 0.8713490959666204, tensor(2.2239)]
[tensor(-0.5089), 0.5478353821485836, 0.8713490959666204, tensor(2.2239)]
[tensor(-0.5089), 0.5478353821485836, 0.8713490959666204, tensor(2.2239)]
[2023-01-18 04:37:45,979.979 dlc26te6b6pxn0nk-master-0:21266 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 04:37:46,662.662 dlc26te6b6pxn0nk-master-0:21320 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:37:46,663.663 dlc26te6b6pxn0nk-master-0:21322 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:37:46,744.744 dlc26te6b6pxn0nk-master-0:21323 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:37:46,753.753 dlc26te6b6pxn0nk-master-0:21321 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:37:47,591.591 dlc26te6b6pxn0nk-master-0:21322 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 04:37:48,144.144 dlc26te6b6pxn0nk-master-0:21323 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 04:37:48,147.147 dlc26te6b6pxn0nk-master-0:21321 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 04:37:48,149.149 dlc26te6b6pxn0nk-master-0:21320 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:21320:21320 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:21322:21322 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21321:21321 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21323:21323 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5604), 0.5109567076429716, 0.8358831710709318, tensor(1.9944)]
[tensor(-0.5206), 0.5387493319080705, 0.8657858136300417, tensor(2.1732)]
[tensor(-0.5139), 0.547300908605024, 0.8657858136300417, tensor(2.2226)]
[tensor(-0.5139), 0.547300908605024, 0.8671766342141863, tensor(2.2226)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5139), 0.547300908605024, 0.8671766342141863, tensor(2.2226)]
[2023-01-18 04:48:32,360.360 dlc26te6b6pxn0nk-master-0:21398 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 04:48:33,008.008 dlc26te6b6pxn0nk-master-0:21455 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:48:33,008.008 dlc26te6b6pxn0nk-master-0:21452 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:48:33,099.099 dlc26te6b6pxn0nk-master-0:21453 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:48:33,103.103 dlc26te6b6pxn0nk-master-0:21454 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:48:33,949.949 dlc26te6b6pxn0nk-master-0:21455 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 04:48:34,458.458 dlc26te6b6pxn0nk-master-0:21453 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 04:48:34,459.459 dlc26te6b6pxn0nk-master-0:21454 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 04:48:34,465.465 dlc26te6b6pxn0nk-master-0:21452 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:21452:21452 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:21455:21455 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21454:21454 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21453:21453 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5333), 0.5318011758417959, 0.8525730180806675, tensor(2.1257)]
[tensor(-0.5257), 0.5414216996258685, 0.8636995827538247, tensor(2.1814)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5201), 0.5440940673436665, 0.8657858136300417, tensor(2.2004)]
[tensor(-0.5201), 0.5451630144307856, 0.8657858136300417, tensor(2.2056)]
[tensor(-0.5194), 0.5451630144307856, 0.8657858136300417, tensor(2.2056)]
[tensor(-0.5194), 0.5451630144307856, 0.8657858136300417, tensor(2.2056)]
[tensor(-0.5194), 0.5451630144307856, 0.8657858136300417, tensor(2.2056)]
[tensor(-0.5194), 0.5451630144307856, 0.8657858136300417, tensor(2.2056)]
[tensor(-0.5194), 0.5451630144307856, 0.8657858136300417, tensor(2.2056)]
[tensor(-0.5194), 0.5451630144307856, 0.8657858136300417, tensor(2.2056)]
early stopping at 10
[2023-01-18 05:10:14,322.322 dlc26te6b6pxn0nk-master-0:21547 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:10:15,033.033 dlc26te6b6pxn0nk-master-0:21602 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:10:15,033.033 dlc26te6b6pxn0nk-master-0:21603 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:10:15,229.229 dlc26te6b6pxn0nk-master-0:21604 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:10:15,254.254 dlc26te6b6pxn0nk-master-0:21601 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:10:16,984.984 dlc26te6b6pxn0nk-master-0:21603 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:10:16,986.986 dlc26te6b6pxn0nk-master-0:21602 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:10:17,634.634 dlc26te6b6pxn0nk-master-0:21604 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:10:17,641.641 dlc26te6b6pxn0nk-master-0:21601 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc26te6b6pxn0nk-master-0:21601:21601 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc26te6b6pxn0nk-master-0:21602:21602 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21603:21603 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc26te6b6pxn0nk-master-0:21604:21604 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5236), 0.555318011758418, 0.8623087621696801, tensor(2.2530)]
[tensor(-0.5202), 0.555318011758418, 0.866481223922114, tensor(2.2530)]
[tensor(-0.5084), 0.555318011758418, 0.866481223922114, tensor(2.2530)]
[tensor(-0.5084), 0.555318011758418, 0.866481223922114, tensor(2.2530)]
[tensor(-0.5084), 0.555318011758418, 0.866481223922114, tensor(2.2530)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5084), 0.555318011758418, 0.866481223922114, tensor(2.2530)]
[tensor(-0.5084), 0.555318011758418, 0.866481223922114, tensor(2.2530)]
[tensor(-0.5084), 0.555318011758418, 0.866481223922114, tensor(2.2530)]
early stopping at 8
[2023-01-18 05:27:35,059.059 dlc26te6b6pxn0nk-master-0:21688 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:27:35,702.702 dlc26te6b6pxn0nk-master-0:21742 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:35,702.702 dlc26te6b6pxn0nk-master-0:21745 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:35,791.791 dlc26te6b6pxn0nk-master-0:21744 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:35,797.797 dlc26te6b6pxn0nk-master-0:21743 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:37,183.183 dlc26te6b6pxn0nk-master-0:21743 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:27:37,186.186 dlc26te6b6pxn0nk-master-0:21744 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:27:37,633.633 dlc26te6b6pxn0nk-master-0:21745 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:27:37,640.640 dlc26te6b6pxn0nk-master-0:21742 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "

OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "

OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 21742
Killing subprocess 21743
Killing subprocess 21744
Killing subprocess 21745
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:27:40,859.859 dlc26te6b6pxn0nk-master-0:21771 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:27:41,511.511 dlc26te6b6pxn0nk-master-0:21826 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:41,511.511 dlc26te6b6pxn0nk-master-0:21827 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:41,681.681 dlc26te6b6pxn0nk-master-0:21825 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:41,763.763 dlc26te6b6pxn0nk-master-0:21828 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:42,859.859 dlc26te6b6pxn0nk-master-0:21827 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:27:43,027.027 dlc26te6b6pxn0nk-master-0:21828 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:27:43,392.392 dlc26te6b6pxn0nk-master-0:21826 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:27:43,400.400 dlc26te6b6pxn0nk-master-0:21825 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 21825
Killing subprocess 21826
Killing subprocess 21827
Killing subprocess 21828
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:27:46,641.641 dlc26te6b6pxn0nk-master-0:21854 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:27:47,294.294 dlc26te6b6pxn0nk-master-0:21910 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:47,297.297 dlc26te6b6pxn0nk-master-0:21912 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:47,378.378 dlc26te6b6pxn0nk-master-0:21909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:47,381.381 dlc26te6b6pxn0nk-master-0:21911 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:48,447.447 dlc26te6b6pxn0nk-master-0:21911 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:27:49,232.232 dlc26te6b6pxn0nk-master-0:21910 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:27:49,236.236 dlc26te6b6pxn0nk-master-0:21912 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:27:49,237.237 dlc26te6b6pxn0nk-master-0:21909 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 21909
Killing subprocess 21910
Killing subprocess 21911
Killing subprocess 21912
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:27:52,419.419 dlc26te6b6pxn0nk-master-0:21938 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:27:53,071.071 dlc26te6b6pxn0nk-master-0:21992 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:53,071.071 dlc26te6b6pxn0nk-master-0:21995 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:53,071.071 dlc26te6b6pxn0nk-master-0:21994 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:53,071.071 dlc26te6b6pxn0nk-master-0:21993 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:54,127.127 dlc26te6b6pxn0nk-master-0:21993 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:27:55,104.104 dlc26te6b6pxn0nk-master-0:21994 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:27:55,116.116 dlc26te6b6pxn0nk-master-0:21995 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:27:55,119.119 dlc26te6b6pxn0nk-master-0:21992 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 21992
Killing subprocess 21993
Killing subprocess 21994
Killing subprocess 21995
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:27:58,219.219 dlc26te6b6pxn0nk-master-0:22021 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:27:58,946.946 dlc26te6b6pxn0nk-master-0:22076 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:58,959.959 dlc26te6b6pxn0nk-master-0:22077 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:59,137.137 dlc26te6b6pxn0nk-master-0:22075 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:27:59,137.137 dlc26te6b6pxn0nk-master-0:22078 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:01,357.357 dlc26te6b6pxn0nk-master-0:22077 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:01,358.358 dlc26te6b6pxn0nk-master-0:22076 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:01,547.547 dlc26te6b6pxn0nk-master-0:22078 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:01,554.554 dlc26te6b6pxn0nk-master-0:22075 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22075
Killing subprocess 22076
Killing subprocess 22077
Killing subprocess 22078
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:28:04,013.013 dlc26te6b6pxn0nk-master-0:22104 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:04,649.649 dlc26te6b6pxn0nk-master-0:22159 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:04,649.649 dlc26te6b6pxn0nk-master-0:22160 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:04,650.650 dlc26te6b6pxn0nk-master-0:22161 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:04,656.656 dlc26te6b6pxn0nk-master-0:22158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:05,708.708 dlc26te6b6pxn0nk-master-0:22161 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:05,709.709 dlc26te6b6pxn0nk-master-0:22160 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:06,693.693 dlc26te6b6pxn0nk-master-0:22159 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:06,700.700 dlc26te6b6pxn0nk-master-0:22158 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22158
Killing subprocess 22159
Killing subprocess 22160
Killing subprocess 22161
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:28:09,926.926 dlc26te6b6pxn0nk-master-0:22187 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:10,561.561 dlc26te6b6pxn0nk-master-0:22244 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:10,564.564 dlc26te6b6pxn0nk-master-0:22242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:10,650.650 dlc26te6b6pxn0nk-master-0:22241 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:10,658.658 dlc26te6b6pxn0nk-master-0:22243 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:12,024.024 dlc26te6b6pxn0nk-master-0:22243 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:12,487.487 dlc26te6b6pxn0nk-master-0:22244 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:12,490.490 dlc26te6b6pxn0nk-master-0:22242 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:12,500.500 dlc26te6b6pxn0nk-master-0:22241 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22241
Killing subprocess 22242
Killing subprocess 22243
Killing subprocess 22244
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:28:15,698.698 dlc26te6b6pxn0nk-master-0:22270 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:16,340.340 dlc26te6b6pxn0nk-master-0:22325 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:16,340.340 dlc26te6b6pxn0nk-master-0:22324 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:16,345.345 dlc26te6b6pxn0nk-master-0:22327 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:16,346.346 dlc26te6b6pxn0nk-master-0:22326 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:17,379.379 dlc26te6b6pxn0nk-master-0:22326 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:17,381.381 dlc26te6b6pxn0nk-master-0:22327 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:17,383.383 dlc26te6b6pxn0nk-master-0:22325 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:17,386.386 dlc26te6b6pxn0nk-master-0:22324 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22324
Killing subprocess 22325
Killing subprocess 22326
Killing subprocess 22327
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:28:20,505.505 dlc26te6b6pxn0nk-master-0:22353 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:21,163.163 dlc26te6b6pxn0nk-master-0:22408 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:21,216.216 dlc26te6b6pxn0nk-master-0:22410 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:21,228.228 dlc26te6b6pxn0nk-master-0:22409 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:21,280.280 dlc26te6b6pxn0nk-master-0:22407 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:22,683.683 dlc26te6b6pxn0nk-master-0:22409 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:23,036.036 dlc26te6b6pxn0nk-master-0:22408 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:23,141.141 dlc26te6b6pxn0nk-master-0:22410 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:23,149.149 dlc26te6b6pxn0nk-master-0:22407 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22407
Killing subprocess 22408
Killing subprocess 22409
Killing subprocess 22410
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:28:26,320.320 dlc26te6b6pxn0nk-master-0:22436 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:27,021.021 dlc26te6b6pxn0nk-master-0:22492 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:27,052.052 dlc26te6b6pxn0nk-master-0:22493 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:27,131.131 dlc26te6b6pxn0nk-master-0:22491 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:27,220.220 dlc26te6b6pxn0nk-master-0:22494 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:28,417.417 dlc26te6b6pxn0nk-master-0:22493 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:28,510.510 dlc26te6b6pxn0nk-master-0:22494 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:28,900.900 dlc26te6b6pxn0nk-master-0:22492 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:28,901.901 dlc26te6b6pxn0nk-master-0:22491 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22491
Killing subprocess 22492
Killing subprocess 22493
Killing subprocess 22494
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:28:32,096.096 dlc26te6b6pxn0nk-master-0:22520 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:32,737.737 dlc26te6b6pxn0nk-master-0:22577 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:32,737.737 dlc26te6b6pxn0nk-master-0:22576 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:32,738.738 dlc26te6b6pxn0nk-master-0:22575 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:32,738.738 dlc26te6b6pxn0nk-master-0:22574 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:33,784.784 dlc26te6b6pxn0nk-master-0:22575 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:33,792.792 dlc26te6b6pxn0nk-master-0:22576 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:33,793.793 dlc26te6b6pxn0nk-master-0:22577 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:33,794.794 dlc26te6b6pxn0nk-master-0:22574 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22574
Killing subprocess 22575
Killing subprocess 22576
Killing subprocess 22577
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:28:36,905.905 dlc26te6b6pxn0nk-master-0:22603 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:37,633.633 dlc26te6b6pxn0nk-master-0:22658 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:37,633.633 dlc26te6b6pxn0nk-master-0:22659 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:37,807.807 dlc26te6b6pxn0nk-master-0:22660 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:37,807.807 dlc26te6b6pxn0nk-master-0:22657 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:39,660.660 dlc26te6b6pxn0nk-master-0:22658 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:39,661.661 dlc26te6b6pxn0nk-master-0:22659 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:40,198.198 dlc26te6b6pxn0nk-master-0:22660 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:40,202.202 dlc26te6b6pxn0nk-master-0:22657 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22657
Killing subprocess 22658
Killing subprocess 22659
Killing subprocess 22660
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:28:42,695.695 dlc26te6b6pxn0nk-master-0:22686 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:43,430.430 dlc26te6b6pxn0nk-master-0:22742 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:43,434.434 dlc26te6b6pxn0nk-master-0:22743 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:43,435.435 dlc26te6b6pxn0nk-master-0:22740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:43,436.436 dlc26te6b6pxn0nk-master-0:22741 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:44,814.814 dlc26te6b6pxn0nk-master-0:22742 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:44,815.815 dlc26te6b6pxn0nk-master-0:22741 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:45,379.379 dlc26te6b6pxn0nk-master-0:22743 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:45,387.387 dlc26te6b6pxn0nk-master-0:22740 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22740
Killing subprocess 22741
Killing subprocess 22742
Killing subprocess 22743
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:28:48,505.505 dlc26te6b6pxn0nk-master-0:22769 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:49,249.249 dlc26te6b6pxn0nk-master-0:22824 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:49,249.249 dlc26te6b6pxn0nk-master-0:22825 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:49,425.425 dlc26te6b6pxn0nk-master-0:22826 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:49,425.425 dlc26te6b6pxn0nk-master-0:22823 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:51,617.617 dlc26te6b6pxn0nk-master-0:22825 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:51,622.622 dlc26te6b6pxn0nk-master-0:22824 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:51,797.797 dlc26te6b6pxn0nk-master-0:22826 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:51,801.801 dlc26te6b6pxn0nk-master-0:22823 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22823
Killing subprocess 22824
Killing subprocess 22825
Killing subprocess 22826
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:28:54,309.309 dlc26te6b6pxn0nk-master-0:22852 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:28:55,051.051 dlc26te6b6pxn0nk-master-0:22908 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:55,077.077 dlc26te6b6pxn0nk-master-0:22907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:55,165.165 dlc26te6b6pxn0nk-master-0:22906 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:55,256.256 dlc26te6b6pxn0nk-master-0:22909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:28:56,457.457 dlc26te6b6pxn0nk-master-0:22907 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:28:56,618.618 dlc26te6b6pxn0nk-master-0:22909 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:28:57,148.148 dlc26te6b6pxn0nk-master-0:22908 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:28:57,155.155 dlc26te6b6pxn0nk-master-0:22906 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22906
Killing subprocess 22907
Killing subprocess 22908
Killing subprocess 22909
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:29:00,141.141 dlc26te6b6pxn0nk-master-0:22936 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:00,779.779 dlc26te6b6pxn0nk-master-0:22992 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:00,779.779 dlc26te6b6pxn0nk-master-0:22990 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:00,861.861 dlc26te6b6pxn0nk-master-0:22993 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:00,866.866 dlc26te6b6pxn0nk-master-0:22991 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:01,710.710 dlc26te6b6pxn0nk-master-0:22992 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:02,065.065 dlc26te6b6pxn0nk-master-0:22991 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:02,066.066 dlc26te6b6pxn0nk-master-0:22993 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:02,074.074 dlc26te6b6pxn0nk-master-0:22990 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 22990
Killing subprocess 22991
Killing subprocess 22992
Killing subprocess 22993
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:29:04,931.931 dlc26te6b6pxn0nk-master-0:23019 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:05,569.569 dlc26te6b6pxn0nk-master-0:23075 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:05,573.573 dlc26te6b6pxn0nk-master-0:23073 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:05,655.655 dlc26te6b6pxn0nk-master-0:23076 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:05,657.657 dlc26te6b6pxn0nk-master-0:23074 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:07,155.155 dlc26te6b6pxn0nk-master-0:23074 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:07,160.160 dlc26te6b6pxn0nk-master-0:23076 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:07,488.488 dlc26te6b6pxn0nk-master-0:23075 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:07,493.493 dlc26te6b6pxn0nk-master-0:23073 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23073
Killing subprocess 23074
Killing subprocess 23075
Killing subprocess 23076
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:29:10,764.764 dlc26te6b6pxn0nk-master-0:23102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:11,504.504 dlc26te6b6pxn0nk-master-0:23158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:11,504.504 dlc26te6b6pxn0nk-master-0:23157 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:11,673.673 dlc26te6b6pxn0nk-master-0:23159 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:11,673.673 dlc26te6b6pxn0nk-master-0:23156 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:12,717.717 dlc26te6b6pxn0nk-master-0:23159 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:13,664.664 dlc26te6b6pxn0nk-master-0:23158 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:13,679.679 dlc26te6b6pxn0nk-master-0:23157 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:13,681.681 dlc26te6b6pxn0nk-master-0:23156 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23156
Killing subprocess 23157
Killing subprocess 23158
Killing subprocess 23159
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:29:16,561.561 dlc26te6b6pxn0nk-master-0:23185 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:17,265.265 dlc26te6b6pxn0nk-master-0:23241 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:17,265.265 dlc26te6b6pxn0nk-master-0:23240 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:17,380.380 dlc26te6b6pxn0nk-master-0:23239 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:17,380.380 dlc26te6b6pxn0nk-master-0:23242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:18,455.455 dlc26te6b6pxn0nk-master-0:23241 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:18,572.572 dlc26te6b6pxn0nk-master-0:23242 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:19,133.133 dlc26te6b6pxn0nk-master-0:23240 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:19,143.143 dlc26te6b6pxn0nk-master-0:23239 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23239
Killing subprocess 23240
Killing subprocess 23241
Killing subprocess 23242
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:29:22,332.332 dlc26te6b6pxn0nk-master-0:23268 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:22,976.976 dlc26te6b6pxn0nk-master-0:23323 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:22,976.976 dlc26te6b6pxn0nk-master-0:23322 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:22,976.976 dlc26te6b6pxn0nk-master-0:23324 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:22,977.977 dlc26te6b6pxn0nk-master-0:23325 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:24,031.031 dlc26te6b6pxn0nk-master-0:23324 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:24,038.038 dlc26te6b6pxn0nk-master-0:23323 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:24,039.039 dlc26te6b6pxn0nk-master-0:23325 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:24,044.044 dlc26te6b6pxn0nk-master-0:23322 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23322
Killing subprocess 23323
Killing subprocess 23324
Killing subprocess 23325
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15']' returned non-zero exit status 1.
[2023-01-18 05:29:27,111.111 dlc26te6b6pxn0nk-master-0:23351 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:27,761.761 dlc26te6b6pxn0nk-master-0:23406 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:27,763.763 dlc26te6b6pxn0nk-master-0:23408 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:27,845.845 dlc26te6b6pxn0nk-master-0:23405 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:27,849.849 dlc26te6b6pxn0nk-master-0:23407 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:29,216.216 dlc26te6b6pxn0nk-master-0:23407 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:29,684.684 dlc26te6b6pxn0nk-master-0:23408 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:29,685.685 dlc26te6b6pxn0nk-master-0:23406 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:29,692.692 dlc26te6b6pxn0nk-master-0:23405 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23405
Killing subprocess 23406
Killing subprocess 23407
Killing subprocess 23408
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:29:32,899.899 dlc26te6b6pxn0nk-master-0:23434 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:33,538.538 dlc26te6b6pxn0nk-master-0:23489 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:33,538.538 dlc26te6b6pxn0nk-master-0:23490 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:33,722.722 dlc26te6b6pxn0nk-master-0:23488 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:33,802.802 dlc26te6b6pxn0nk-master-0:23491 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:34,878.878 dlc26te6b6pxn0nk-master-0:23490 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:35,082.082 dlc26te6b6pxn0nk-master-0:23491 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:35,395.395 dlc26te6b6pxn0nk-master-0:23489 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:35,396.396 dlc26te6b6pxn0nk-master-0:23488 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23488
Killing subprocess 23489
Killing subprocess 23490
Killing subprocess 23491
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:29:38,702.702 dlc26te6b6pxn0nk-master-0:23517 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:39,412.412 dlc26te6b6pxn0nk-master-0:23572 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:39,435.435 dlc26te6b6pxn0nk-master-0:23573 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:39,524.524 dlc26te6b6pxn0nk-master-0:23571 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:39,617.617 dlc26te6b6pxn0nk-master-0:23574 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:40,774.774 dlc26te6b6pxn0nk-master-0:23573 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:40,888.888 dlc26te6b6pxn0nk-master-0:23574 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:41,277.277 dlc26te6b6pxn0nk-master-0:23572 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:41,283.283 dlc26te6b6pxn0nk-master-0:23571 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23571
Killing subprocess 23572
Killing subprocess 23573
Killing subprocess 23574
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:29:44,505.505 dlc26te6b6pxn0nk-master-0:23600 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:45,148.148 dlc26te6b6pxn0nk-master-0:23655 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:45,148.148 dlc26te6b6pxn0nk-master-0:23656 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:45,148.148 dlc26te6b6pxn0nk-master-0:23657 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:45,149.149 dlc26te6b6pxn0nk-master-0:23654 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:46,226.226 dlc26te6b6pxn0nk-master-0:23656 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:47,219.219 dlc26te6b6pxn0nk-master-0:23655 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:47,223.223 dlc26te6b6pxn0nk-master-0:23657 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:47,231.231 dlc26te6b6pxn0nk-master-0:23654 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.3-15 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.3-15 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23654
Killing subprocess 23655
Killing subprocess 23656
Killing subprocess 23657
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.3-15', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:29:50,287.287 dlc26te6b6pxn0nk-master-0:23684 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:50,928.928 dlc26te6b6pxn0nk-master-0:23738 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:50,928.928 dlc26te6b6pxn0nk-master-0:23740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:51,015.015 dlc26te6b6pxn0nk-master-0:23741 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:51,028.028 dlc26te6b6pxn0nk-master-0:23739 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:51,957.957 dlc26te6b6pxn0nk-master-0:23740 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:52,407.407 dlc26te6b6pxn0nk-master-0:23741 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:52,409.409 dlc26te6b6pxn0nk-master-0:23739 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:52,409.409 dlc26te6b6pxn0nk-master-0:23738 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23738
Killing subprocess 23739
Killing subprocess 23740
Killing subprocess 23741
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:29:55,085.085 dlc26te6b6pxn0nk-master-0:23767 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:29:55,826.826 dlc26te6b6pxn0nk-master-0:23823 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:55,827.827 dlc26te6b6pxn0nk-master-0:23822 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:55,993.993 dlc26te6b6pxn0nk-master-0:23824 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:55,993.993 dlc26te6b6pxn0nk-master-0:23821 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:29:58,005.005 dlc26te6b6pxn0nk-master-0:23822 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:29:58,009.009 dlc26te6b6pxn0nk-master-0:23823 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:29:58,154.154 dlc26te6b6pxn0nk-master-0:23824 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:29:58,159.159 dlc26te6b6pxn0nk-master-0:23821 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23821
Killing subprocess 23822
Killing subprocess 23823
Killing subprocess 23824
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:30:00,900.900 dlc26te6b6pxn0nk-master-0:23850 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:01,547.547 dlc26te6b6pxn0nk-master-0:23906 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:01,547.547 dlc26te6b6pxn0nk-master-0:23904 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:01,643.643 dlc26te6b6pxn0nk-master-0:23907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:01,647.647 dlc26te6b6pxn0nk-master-0:23905 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:02,499.499 dlc26te6b6pxn0nk-master-0:23906 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:03,057.057 dlc26te6b6pxn0nk-master-0:23905 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:03,058.058 dlc26te6b6pxn0nk-master-0:23907 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:03,066.066 dlc26te6b6pxn0nk-master-0:23904 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23904
Killing subprocess 23905
Killing subprocess 23906
Killing subprocess 23907
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:30:05,707.707 dlc26te6b6pxn0nk-master-0:23933 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:06,442.442 dlc26te6b6pxn0nk-master-0:23988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:06,447.447 dlc26te6b6pxn0nk-master-0:23989 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:06,516.516 dlc26te6b6pxn0nk-master-0:23990 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:06,516.516 dlc26te6b6pxn0nk-master-0:23987 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:07,578.578 dlc26te6b6pxn0nk-master-0:23990 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:07,849.849 dlc26te6b6pxn0nk-master-0:23988 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:07,852.852 dlc26te6b6pxn0nk-master-0:23989 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:07,856.856 dlc26te6b6pxn0nk-master-0:23987 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 23987
Killing subprocess 23988
Killing subprocess 23989
Killing subprocess 23990
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:30:10,500.500 dlc26te6b6pxn0nk-master-0:24016 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:11,144.144 dlc26te6b6pxn0nk-master-0:24071 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:11,145.145 dlc26te6b6pxn0nk-master-0:24072 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:11,315.315 dlc26te6b6pxn0nk-master-0:24073 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:11,398.398 dlc26te6b6pxn0nk-master-0:24070 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:13,013.013 dlc26te6b6pxn0nk-master-0:24071 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:13,183.183 dlc26te6b6pxn0nk-master-0:24073 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:13,487.487 dlc26te6b6pxn0nk-master-0:24072 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:13,490.490 dlc26te6b6pxn0nk-master-0:24070 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24070
Killing subprocess 24071
Killing subprocess 24072
Killing subprocess 24073
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:30:16,287.287 dlc26te6b6pxn0nk-master-0:24099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:17,014.014 dlc26te6b6pxn0nk-master-0:24155 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:17,036.036 dlc26te6b6pxn0nk-master-0:24154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:17,107.107 dlc26te6b6pxn0nk-master-0:24156 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:17,108.108 dlc26te6b6pxn0nk-master-0:24153 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:18,047.047 dlc26te6b6pxn0nk-master-0:24156 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:18,429.429 dlc26te6b6pxn0nk-master-0:24155 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:18,432.432 dlc26te6b6pxn0nk-master-0:24154 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:18,442.442 dlc26te6b6pxn0nk-master-0:24153 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24153
Killing subprocess 24154
Killing subprocess 24155
Killing subprocess 24156
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:30:21,078.078 dlc26te6b6pxn0nk-master-0:24182 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:21,778.778 dlc26te6b6pxn0nk-master-0:24238 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:21,805.805 dlc26te6b6pxn0nk-master-0:24237 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:21,894.894 dlc26te6b6pxn0nk-master-0:24239 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:21,983.983 dlc26te6b6pxn0nk-master-0:24236 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:23,662.662 dlc26te6b6pxn0nk-master-0:24238 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:23,786.786 dlc26te6b6pxn0nk-master-0:24239 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:24,245.245 dlc26te6b6pxn0nk-master-0:24237 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:24,246.246 dlc26te6b6pxn0nk-master-0:24236 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24236
Killing subprocess 24237
Killing subprocess 24238
Killing subprocess 24239
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:30:26,848.848 dlc26te6b6pxn0nk-master-0:24266 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:27,488.488 dlc26te6b6pxn0nk-master-0:24322 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:27,490.490 dlc26te6b6pxn0nk-master-0:24320 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:27,545.545 dlc26te6b6pxn0nk-master-0:24321 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:27,684.684 dlc26te6b6pxn0nk-master-0:24323 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:28,436.436 dlc26te6b6pxn0nk-master-0:24322 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:28,861.861 dlc26te6b6pxn0nk-master-0:24321 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:28,956.956 dlc26te6b6pxn0nk-master-0:24323 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:28,961.961 dlc26te6b6pxn0nk-master-0:24320 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24320
Killing subprocess 24321
Killing subprocess 24322
Killing subprocess 24323
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:30:31,651.651 dlc26te6b6pxn0nk-master-0:24349 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:32,350.350 dlc26te6b6pxn0nk-master-0:24405 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:32,383.383 dlc26te6b6pxn0nk-master-0:24404 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:32,463.463 dlc26te6b6pxn0nk-master-0:24403 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:32,554.554 dlc26te6b6pxn0nk-master-0:24406 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:33,738.738 dlc26te6b6pxn0nk-master-0:24404 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:33,835.835 dlc26te6b6pxn0nk-master-0:24406 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:34,251.251 dlc26te6b6pxn0nk-master-0:24405 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:34,260.260 dlc26te6b6pxn0nk-master-0:24403 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24403
Killing subprocess 24404
Killing subprocess 24405
Killing subprocess 24406
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:30:37,417.417 dlc26te6b6pxn0nk-master-0:24432 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:38,054.054 dlc26te6b6pxn0nk-master-0:24486 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:38,054.054 dlc26te6b6pxn0nk-master-0:24489 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:38,054.054 dlc26te6b6pxn0nk-master-0:24488 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:38,096.096 dlc26te6b6pxn0nk-master-0:24487 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:39,140.140 dlc26te6b6pxn0nk-master-0:24489 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:39,149.149 dlc26te6b6pxn0nk-master-0:24488 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:39,153.153 dlc26te6b6pxn0nk-master-0:24487 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:39,160.160 dlc26te6b6pxn0nk-master-0:24486 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24486
Killing subprocess 24487
Killing subprocess 24488
Killing subprocess 24489
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:30:42,193.193 dlc26te6b6pxn0nk-master-0:24515 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:42,831.831 dlc26te6b6pxn0nk-master-0:24572 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:42,833.833 dlc26te6b6pxn0nk-master-0:24570 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:42,919.919 dlc26te6b6pxn0nk-master-0:24571 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:42,926.926 dlc26te6b6pxn0nk-master-0:24569 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:44,768.768 dlc26te6b6pxn0nk-master-0:24572 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:44,770.770 dlc26te6b6pxn0nk-master-0:24570 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:45,271.271 dlc26te6b6pxn0nk-master-0:24571 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:45,277.277 dlc26te6b6pxn0nk-master-0:24569 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
Traceback (most recent call last):
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24569
Killing subprocess 24570
Killing subprocess 24571
Killing subprocess 24572
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:30:47,995.995 dlc26te6b6pxn0nk-master-0:24598 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:48,638.638 dlc26te6b6pxn0nk-master-0:24654 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:48,644.644 dlc26te6b6pxn0nk-master-0:24653 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:48,813.813 dlc26te6b6pxn0nk-master-0:24655 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:48,892.892 dlc26te6b6pxn0nk-master-0:24652 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:50,516.516 dlc26te6b6pxn0nk-master-0:24654 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:50,683.683 dlc26te6b6pxn0nk-master-0:24655 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:51,004.004 dlc26te6b6pxn0nk-master-0:24653 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:51,007.007 dlc26te6b6pxn0nk-master-0:24652 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24652
Killing subprocess 24653
Killing subprocess 24654
Killing subprocess 24655
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:30:53,762.762 dlc26te6b6pxn0nk-master-0:24681 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:30:54,406.406 dlc26te6b6pxn0nk-master-0:24737 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:54,406.406 dlc26te6b6pxn0nk-master-0:24736 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:54,492.492 dlc26te6b6pxn0nk-master-0:24738 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:54,498.498 dlc26te6b6pxn0nk-master-0:24735 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:30:55,523.523 dlc26te6b6pxn0nk-master-0:24738 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:30:56,342.342 dlc26te6b6pxn0nk-master-0:24737 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:30:56,346.346 dlc26te6b6pxn0nk-master-0:24736 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:30:56,353.353 dlc26te6b6pxn0nk-master-0:24735 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24735
Killing subprocess 24736
Killing subprocess 24737
Killing subprocess 24738
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:30:59,528.528 dlc26te6b6pxn0nk-master-0:24765 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:00,212.212 dlc26te6b6pxn0nk-master-0:24820 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:00,221.221 dlc26te6b6pxn0nk-master-0:24822 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:00,292.292 dlc26te6b6pxn0nk-master-0:24819 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:00,297.297 dlc26te6b6pxn0nk-master-0:24821 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:02,146.146 dlc26te6b6pxn0nk-master-0:24822 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:02,150.150 dlc26te6b6pxn0nk-master-0:24820 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:02,629.629 dlc26te6b6pxn0nk-master-0:24821 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:02,633.633 dlc26te6b6pxn0nk-master-0:24819 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24819
Killing subprocess 24820
Killing subprocess 24821
Killing subprocess 24822
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:31:05,329.329 dlc26te6b6pxn0nk-master-0:24848 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:05,968.968 dlc26te6b6pxn0nk-master-0:24903 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:05,970.970 dlc26te6b6pxn0nk-master-0:24904 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:06,154.154 dlc26te6b6pxn0nk-master-0:24902 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:06,234.234 dlc26te6b6pxn0nk-master-0:24905 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:07,317.317 dlc26te6b6pxn0nk-master-0:24903 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:07,484.484 dlc26te6b6pxn0nk-master-0:24905 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:07,851.851 dlc26te6b6pxn0nk-master-0:24904 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:07,855.855 dlc26te6b6pxn0nk-master-0:24902 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24902
Killing subprocess 24903
Killing subprocess 24904
Killing subprocess 24905
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '2', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:31:11,103.103 dlc26te6b6pxn0nk-master-0:24931 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:11,745.745 dlc26te6b6pxn0nk-master-0:24987 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:11,745.745 dlc26te6b6pxn0nk-master-0:24985 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:11,835.835 dlc26te6b6pxn0nk-master-0:24986 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:11,836.836 dlc26te6b6pxn0nk-master-0:24988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:13,003.003 dlc26te6b6pxn0nk-master-0:24988 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:13,005.005 dlc26te6b6pxn0nk-master-0:24986 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:13,686.686 dlc26te6b6pxn0nk-master-0:24987 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:13,694.694 dlc26te6b6pxn0nk-master-0:24985 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 24985
Killing subprocess 24986
Killing subprocess 24987
Killing subprocess 24988
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '32', '--accumulate_num', '1', '--lr', '2e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:31:16,886.886 dlc26te6b6pxn0nk-master-0:25014 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:17,531.531 dlc26te6b6pxn0nk-master-0:25068 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:17,567.567 dlc26te6b6pxn0nk-master-0:25071 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:17,615.615 dlc26te6b6pxn0nk-master-0:25069 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:17,618.618 dlc26te6b6pxn0nk-master-0:25070 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:18,443.443 dlc26te6b6pxn0nk-master-0:25071 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:18,806.806 dlc26te6b6pxn0nk-master-0:25069 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:18,807.807 dlc26te6b6pxn0nk-master-0:25070 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:18,815.815 dlc26te6b6pxn0nk-master-0:25068 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25068
Killing subprocess 25069
Killing subprocess 25070
Killing subprocess 25071
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:31:21,656.656 dlc26te6b6pxn0nk-master-0:25097 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:22,295.295 dlc26te6b6pxn0nk-master-0:25152 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:22,295.295 dlc26te6b6pxn0nk-master-0:25151 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:22,295.295 dlc26te6b6pxn0nk-master-0:25154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:22,295.295 dlc26te6b6pxn0nk-master-0:25153 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:24,319.319 dlc26te6b6pxn0nk-master-0:25154 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:24,342.342 dlc26te6b6pxn0nk-master-0:25152 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:24,347.347 dlc26te6b6pxn0nk-master-0:25153 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:24,351.351 dlc26te6b6pxn0nk-master-0:25151 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25151
Killing subprocess 25152
Killing subprocess 25153
Killing subprocess 25154
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:31:27,433.433 dlc26te6b6pxn0nk-master-0:25180 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:28,084.084 dlc26te6b6pxn0nk-master-0:25236 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:28,091.091 dlc26te6b6pxn0nk-master-0:25237 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:28,168.168 dlc26te6b6pxn0nk-master-0:25234 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:28,173.173 dlc26te6b6pxn0nk-master-0:25235 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:29,989.989 dlc26te6b6pxn0nk-master-0:25236 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:30,001.001 dlc26te6b6pxn0nk-master-0:25237 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:30,562.562 dlc26te6b6pxn0nk-master-0:25235 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:30,567.567 dlc26te6b6pxn0nk-master-0:25234 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25234
Killing subprocess 25235
Killing subprocess 25236
Killing subprocess 25237
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:31:33,256.256 dlc26te6b6pxn0nk-master-0:25263 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:33,897.897 dlc26te6b6pxn0nk-master-0:25319 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:33,897.897 dlc26te6b6pxn0nk-master-0:25318 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:34,065.065 dlc26te6b6pxn0nk-master-0:25320 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:34,149.149 dlc26te6b6pxn0nk-master-0:25317 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:35,773.773 dlc26te6b6pxn0nk-master-0:25319 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:35,932.932 dlc26te6b6pxn0nk-master-0:25320 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:36,255.255 dlc26te6b6pxn0nk-master-0:25318 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:36,263.263 dlc26te6b6pxn0nk-master-0:25317 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25317
Killing subprocess 25318
Killing subprocess 25319
Killing subprocess 25320
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25']' returned non-zero exit status 1.
[2023-01-18 05:31:39,031.031 dlc26te6b6pxn0nk-master-0:25346 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:39,670.670 dlc26te6b6pxn0nk-master-0:25400 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:39,670.670 dlc26te6b6pxn0nk-master-0:25401 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:39,670.670 dlc26te6b6pxn0nk-master-0:25403 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:39,670.670 dlc26te6b6pxn0nk-master-0:25402 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:40,728.728 dlc26te6b6pxn0nk-master-0:25402 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:41,716.716 dlc26te6b6pxn0nk-master-0:25401 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:41,721.721 dlc26te6b6pxn0nk-master-0:25403 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:41,721.721 dlc26te6b6pxn0nk-master-0:25400 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25400
Killing subprocess 25401
Killing subprocess 25402
Killing subprocess 25403
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:31:44,806.806 dlc26te6b6pxn0nk-master-0:25429 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:45,447.447 dlc26te6b6pxn0nk-master-0:25486 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:45,447.447 dlc26te6b6pxn0nk-master-0:25485 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:45,533.533 dlc26te6b6pxn0nk-master-0:25484 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:45,547.547 dlc26te6b6pxn0nk-master-0:25483 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:47,400.400 dlc26te6b6pxn0nk-master-0:25485 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:47,400.400 dlc26te6b6pxn0nk-master-0:25486 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:47,927.927 dlc26te6b6pxn0nk-master-0:25484 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:47,933.933 dlc26te6b6pxn0nk-master-0:25483 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25483
Killing subprocess 25484
Killing subprocess 25485
Killing subprocess 25486
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '5', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:31:50,615.615 dlc26te6b6pxn0nk-master-0:25513 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:51,346.346 dlc26te6b6pxn0nk-master-0:25569 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:51,357.357 dlc26te6b6pxn0nk-master-0:25568 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:51,530.530 dlc26te6b6pxn0nk-master-0:25567 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:51,530.530 dlc26te6b6pxn0nk-master-0:25570 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:53,719.719 dlc26te6b6pxn0nk-master-0:25569 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:53,720.720 dlc26te6b6pxn0nk-master-0:25568 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:53,839.839 dlc26te6b6pxn0nk-master-0:25570 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:53,846.846 dlc26te6b6pxn0nk-master-0:25567 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25567
Killing subprocess 25568
Killing subprocess 25569
Killing subprocess 25570
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '2', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
[2023-01-18 05:31:56,421.421 dlc26te6b6pxn0nk-master-0:25596 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 05:31:57,060.060 dlc26te6b6pxn0nk-master-0:25651 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:57,060.060 dlc26te6b6pxn0nk-master-0:25652 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:57,261.261 dlc26te6b6pxn0nk-master-0:25650 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:57,334.334 dlc26te6b6pxn0nk-master-0:25653 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 05:31:58,404.404 dlc26te6b6pxn0nk-master-0:25651 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 05:31:58,571.571 dlc26te6b6pxn0nk-master-0:25653 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 05:31:58,940.940 dlc26te6b6pxn0nk-master-0:25652 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 05:31:58,948.948 dlc26te6b6pxn0nk-master-0:25650 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.5-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/workspace/mtt/main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/root/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /root/data/yts/saved_models/v4.3.5-25 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Killing subprocess 25650
Killing subprocess 25651
Killing subprocess 25652
Killing subprocess 25653
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/pai/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/pai/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/pai/bin/python', '-u', '/root/workspace/mtt/main.py', '--local_rank=3', '--system', '/root/data/yts', '--task', 'mosei', '--dont_show', '--output_file', 'mosei3.csv', '--last_conv_layer', 'no', '--epochs', '50', '--batch_size', '24', '--accumulate_num', '1', '--lr', '1e-5', '--model', 'saved_models/v4.3.5-25', '--seed', '3407']' returned non-zero exit status 1.
