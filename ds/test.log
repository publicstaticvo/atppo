[2022-12-30 14:19:50,177.177 dlc14ajn5vhvxm4z-master-0:29 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 14:19:50,940.940 dlc14ajn5vhvxm4z-master-0:96 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:19:50,940.940 dlc14ajn5vhvxm4z-master-0:97 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:19:51,044.044 dlc14ajn5vhvxm4z-master-0:95 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:19:51,044.044 dlc14ajn5vhvxm4z-master-0:98 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:19:52,522.522 dlc14ajn5vhvxm4z-master-0:96 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 14:19:52,523.523 dlc14ajn5vhvxm4z-master-0:98 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 14:19:53,518.518 dlc14ajn5vhvxm4z-master-0:97 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 14:19:53,521.521 dlc14ajn5vhvxm4z-master-0:95 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 20 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.2.bias', 'end_prediction_head.2.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.2.bias', 'selection_head.0.bias', 'selection_head.2.bias', 'selection_head.2.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.2.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.2.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.0.weight', 'start_prediction_head.2.weight', 'selection_head.2.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.2.bias', 'selection_head.2.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['start_prediction_head.2.bias', 'selection_head.2.bias', 'selection_head.2.weight', 'start_prediction_head.2.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'selection_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.2.bias', 'selection_head.2.bias', 'end_prediction_head.2.weight', 'selection_head.2.weight', 'mam_head.bias', 'selection_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.bias', 'start_prediction_head.2.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:95:95 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:97:97 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:98:98 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:96:96 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5263), 0.535542490646713, 0.8484005563282336, tensor(2.1514)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5249), 0.5371459112773918, 0.8484005563282336, tensor(2.1608)]
[tensor(-0.5249), 0.5371459112773918, 0.8484005563282336, tensor(2.1608)]
[tensor(-0.5187), 0.5451630144307856, 0.8484005563282336, tensor(2.2071)]
[tensor(-0.5187), 0.5451630144307856, 0.8484005563282336, tensor(2.2071)]
[2022-12-30 14:32:52,739.739 dlc14ajn5vhvxm4z-master-0:170 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 14:32:53,363.363 dlc14ajn5vhvxm4z-master-0:237 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:32:53,364.364 dlc14ajn5vhvxm4z-master-0:238 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:32:53,473.473 dlc14ajn5vhvxm4z-master-0:236 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:32:53,473.473 dlc14ajn5vhvxm4z-master-0:239 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:32:55,191.191 dlc14ajn5vhvxm4z-master-0:238 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 14:32:55,349.349 dlc14ajn5vhvxm4z-master-0:237 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 14:32:55,351.351 dlc14ajn5vhvxm4z-master-0:239 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 14:32:55,355.355 dlc14ajn5vhvxm4z-master-0:236 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 20 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.2.weight', 'end_prediction_head.0.weight', 'start_prediction_head.2.weight', 'mlm_head.dense.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.2.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.2.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.0.bias', 'selection_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.2.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.2.weight', 'start_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.2.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.2.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.2.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.0.bias', 'start_prediction_head.2.bias', 'mlm_head.dense.weight', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.2.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.0.weight', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.2.weight', 'selection_head.2.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.2.weight', 'selection_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.weight', 'audio_encoder.audio_sep', 'selection_head.2.weight', 'selection_head.2.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.2.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:236:236 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:239:239 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:237:237 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:238:238 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.6541), 0.4024585783003741, 0.7990264255910987, tensor(1.3582)]
[tensor(-0.5377), 0.5232495991448424, 0.8609179415855355, tensor(2.0785)]
[tensor(-0.5029), 0.5440940673436665, 0.8609179415855355, tensor(2.2176)]
[tensor(-0.5029), 0.5440940673436665, 0.866481223922114, tensor(2.2176)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5029), 0.5440940673436665, 0.866481223922114, tensor(2.2176)]
[2022-12-30 14:44:46,207.207 dlc14ajn5vhvxm4z-master-0:311 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 14:44:46,833.833 dlc14ajn5vhvxm4z-master-0:379 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:44:46,834.834 dlc14ajn5vhvxm4z-master-0:378 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:44:47,030.030 dlc14ajn5vhvxm4z-master-0:380 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:44:47,099.099 dlc14ajn5vhvxm4z-master-0:377 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 14:44:48,649.649 dlc14ajn5vhvxm4z-master-0:379 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 14:44:48,864.864 dlc14ajn5vhvxm4z-master-0:380 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 14:44:49,102.102 dlc14ajn5vhvxm4z-master-0:378 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 14:44:49,104.104 dlc14ajn5vhvxm4z-master-0:377 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 20 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.2.bias', 'end_prediction_head.2.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.2.weight', 'end_prediction_head.2.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.0.weight', 'start_prediction_head.2.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.2.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.2.bias', 'mam_head.decoder.weight', 'end_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.2.weight', 'start_prediction_head.2.weight', 'mlm_head.bias', 'selection_head.0.weight', 'selection_head.0.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'start_prediction_head.2.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.2.weight', 'mam_head.dense.weight', 'start_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.2.weight', 'mam_head.layer_norm.weight', 'selection_head.2.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.2.bias', 'mam_head.bias', 'end_prediction_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['end_prediction_head.2.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.2.bias', 'end_prediction_head.2.bias', 'mlm_head.decoder.bias', 'selection_head.2.weight', 'mam_head.dense.weight', 'selection_head.2.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'selection_head.0.weight', 'mam_head.bias', 'selection_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:377:377 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:379:379 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:380:380 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:378:378 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5431), 0.5387493319080705, 0.8317107093184979, tensor(2.1506)]
[tensor(-0.5313), 0.5387493319080705, 0.8421418636995828, tensor(2.1506)]
[tensor(-0.5255), 0.5408872260823089, 0.8442280945757997, tensor(2.1789)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5255), 0.5408872260823089, 0.8442280945757997, tensor(2.1789)]
[tensor(-0.5228), 0.5408872260823089, 0.8511821974965229, tensor(2.1789)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5228), 0.5408872260823089, 0.8511821974965229, tensor(2.1789)]
[tensor(-0.5228), 0.5408872260823089, 0.8511821974965229, tensor(2.1789)]
[tensor(-0.5184), 0.5408872260823089, 0.8525730180806675, tensor(2.1789)]
[tensor(-0.5184), 0.5408872260823089, 0.8525730180806675, tensor(2.1789)]
[tensor(-0.5184), 0.5408872260823089, 0.8525730180806675, tensor(2.1789)]
[tensor(-0.5184), 0.5408872260823089, 0.8525730180806675, tensor(2.1789)]
[tensor(-0.5184), 0.5408872260823089, 0.8525730180806675, tensor(2.1789)]
[tensor(-0.5184), 0.5408872260823089, 0.8525730180806675, tensor(2.1789)]
early stopping at 13
[2022-12-30 15:15:32,734.734 dlc14ajn5vhvxm4z-master-0:474 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 15:15:33,368.368 dlc14ajn5vhvxm4z-master-0:541 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:15:33,368.368 dlc14ajn5vhvxm4z-master-0:542 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:15:33,475.475 dlc14ajn5vhvxm4z-master-0:543 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:15:33,475.475 dlc14ajn5vhvxm4z-master-0:540 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:15:35,228.228 dlc14ajn5vhvxm4z-master-0:541 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 15:15:35,229.229 dlc14ajn5vhvxm4z-master-0:542 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 15:15:35,323.323 dlc14ajn5vhvxm4z-master-0:543 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 15:15:35,329.329 dlc14ajn5vhvxm4z-master-0:540 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 20 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.0.weight', 'selection_head.2.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.2.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.2.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.2.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.2.bias', 'end_prediction_head.0.bias', 'selection_head.2.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.2.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.2.bias', 'mam_head.bias', 'selection_head.2.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.2.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.2.weight', 'end_prediction_head.2.bias', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.0.bias', 'start_prediction_head.2.bias', 'selection_head.2.bias', 'audio_encoder.audio_sep', 'start_prediction_head.2.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.2.bias', 'mlm_head.decoder.weight', 'start_prediction_head.2.bias', 'selection_head.2.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.2.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.2.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:540:540 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:541:541 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:542:542 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:543:543 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5335), 0.5318011758417959, 0.8407510431154381, tensor(2.1255)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5335), 0.538214858364511, 0.8407510431154381, tensor(2.1545)]
[tensor(-0.5137), 0.547300908605024, 0.8546592489568846, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8546592489568846, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8546592489568846, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8560500695410292, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8560500695410292, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8560500695410292, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8560500695410292, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8560500695410292, tensor(2.2228)]
[tensor(-0.5137), 0.547300908605024, 0.8560500695410292, tensor(2.2228)]
early stopping at 11
[2022-12-30 15:41:53,020.020 dlc14ajn5vhvxm4z-master-0:631 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 15:41:53,641.641 dlc14ajn5vhvxm4z-master-0:699 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:41:53,641.641 dlc14ajn5vhvxm4z-master-0:698 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:41:53,830.830 dlc14ajn5vhvxm4z-master-0:697 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:41:53,902.902 dlc14ajn5vhvxm4z-master-0:700 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:41:54,879.879 dlc14ajn5vhvxm4z-master-0:698 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 15:41:55,045.045 dlc14ajn5vhvxm4z-master-0:700 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 15:41:55,449.449 dlc14ajn5vhvxm4z-master-0:699 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 15:41:55,458.458 dlc14ajn5vhvxm4z-master-0:697 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 20 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['selection_head.2.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.2.bias', 'selection_head.0.bias', 'end_prediction_head.2.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.2.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.2.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.2.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.0.weight', 'end_prediction_head.2.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.2.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.2.bias', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.2.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['selection_head.0.weight', 'start_prediction_head.2.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.2.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.weight', 'selection_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'start_prediction_head.0.weight', 'selection_head.2.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.2.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'selection_head.2.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.2.bias', 'end_prediction_head.0.bias', 'start_prediction_head.2.weight', 'mam_head.decoder.bias', 'selection_head.2.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.0.bias', 'end_prediction_head.2.weight', 'start_prediction_head.2.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:697:697 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:700:700 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:698:698 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:699:699 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5669), 0.5168359166221272, 0.8553546592489569, tensor(2.0172)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5359), 0.5237840726884019, 0.8553546592489569, tensor(2.0830)]
[tensor(-0.5068), 0.5398182789951897, 0.8623087621696801, tensor(2.1923)]
[tensor(-0.5068), 0.5494388027792624, 0.8671766342141863, tensor(2.2327)]
[tensor(-0.5068), 0.5494388027792624, 0.8671766342141863, tensor(2.2327)]
[2022-12-30 15:54:09,457.457 dlc14ajn5vhvxm4z-master-0:772 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 15:54:10,160.160 dlc14ajn5vhvxm4z-master-0:841 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:54:10,216.216 dlc14ajn5vhvxm4z-master-0:838 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:54:10,236.236 dlc14ajn5vhvxm4z-master-0:840 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:54:10,303.303 dlc14ajn5vhvxm4z-master-0:839 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 15:54:11,523.523 dlc14ajn5vhvxm4z-master-0:839 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 15:54:12,015.015 dlc14ajn5vhvxm4z-master-0:841 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 15:54:12,100.100 dlc14ajn5vhvxm4z-master-0:840 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 15:54:12,106.106 dlc14ajn5vhvxm4z-master-0:838 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 20 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.2.weight', 'start_prediction_head.2.weight', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.2.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.2.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.2.bias', 'mlm_head.decoder.weight', 'selection_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['selection_head.2.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.2.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.2.weight', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.2.bias', 'mam_head.decoder.weight', 'selection_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.2.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.2.weight', 'mam_head.decoder.bias', 'start_prediction_head.2.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.bias', 'selection_head.2.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.2.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.0.weight', 'selection_head.2.weight', 'start_prediction_head.2.bias', 'selection_head.2.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:838:838 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:841:841 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:840:840 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:839:839 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5208), 0.5387493319080705, 0.8567454798331016, tensor(2.1730)]
[tensor(-0.5208), 0.5387493319080705, 0.8567454798331016, tensor(2.1730)]
[tensor(-0.5208), 0.5387493319080705, 0.8567454798331016, tensor(2.1730)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5208), 0.5387493319080705, 0.8567454798331016, tensor(2.1730)]
[tensor(-0.5023), 0.5537145911277391, 0.8567454798331016, tensor(2.2662)]
[2022-12-30 16:06:33,882.882 dlc14ajn5vhvxm4z-master-0:913 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 16:06:34,489.489 dlc14ajn5vhvxm4z-master-0:981 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:06:34,489.489 dlc14ajn5vhvxm4z-master-0:980 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:06:34,569.569 dlc14ajn5vhvxm4z-master-0:982 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:06:34,588.588 dlc14ajn5vhvxm4z-master-0:979 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:06:36,336.336 dlc14ajn5vhvxm4z-master-0:980 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 16:06:36,337.337 dlc14ajn5vhvxm4z-master-0:981 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 16:06:36,840.840 dlc14ajn5vhvxm4z-master-0:982 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 16:06:36,849.849 dlc14ajn5vhvxm4z-master-0:979 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 20 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.2.bias', 'mlm_head.decoder.weight', 'start_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.2.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.2.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.2.weight', 'audio_encoder.audio_sep', 'selection_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.2.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.2.bias', 'selection_head.2.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.2.bias', 'start_prediction_head.2.weight', 'selection_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.2.weight', 'selection_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.2.bias', 'mlm_head.bias', 'selection_head.2.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.2.weight', 'mam_head.bias', 'start_prediction_head.2.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.bias', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.2.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.2.bias', 'audio_encoder.audio_sep', 'start_prediction_head.2.bias', 'selection_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.2.weight', 'selection_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.2.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.2.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:979:979 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:981:981 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:980:980 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:982:982 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5486), 0.5248530197755211, 0.8407510431154381, tensor(2.0756)]
[tensor(-0.5264), 0.5446285408872261, 0.8574408901251739, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8574408901251739, tensor(2.1967)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.5210), 0.5446285408872261, 0.8574408901251739, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8602225312934632, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8602225312934632, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8609179415855355, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8643949930458971, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8643949930458971, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8643949930458971, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8643949930458971, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8643949930458971, tensor(2.1967)]
[tensor(-0.5210), 0.5446285408872261, 0.8643949930458971, tensor(2.1967)]
early stopping at 13
[2022-12-30 16:37:07,425.425 dlc14ajn5vhvxm4z-master-0:1074 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 16:37:08,058.058 dlc14ajn5vhvxm4z-master-0:1141 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:37:08,096.096 dlc14ajn5vhvxm4z-master-0:1142 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:37:08,251.251 dlc14ajn5vhvxm4z-master-0:1140 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:37:08,306.306 dlc14ajn5vhvxm4z-master-0:1143 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 16:37:09,472.472 dlc14ajn5vhvxm4z-master-0:1143 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 16:37:09,884.884 dlc14ajn5vhvxm4z-master-0:1141 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 16:37:09,912.912 dlc14ajn5vhvxm4z-master-0:1142 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 16:37:09,920.920 dlc14ajn5vhvxm4z-master-0:1140 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 20 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.2.bias', 'selection_head.0.bias', 'start_prediction_head.2.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.2.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'start_prediction_head.2.weight', 'selection_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.2.weight', 'end_prediction_head.2.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.2.weight', 'end_prediction_head.2.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'selection_head.2.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'selection_head.2.weight', 'mam_head.dense.bias', 'selection_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.2.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.2.weight', 'start_prediction_head.2.bias', 'selection_head.2.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.2.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.0.weight', 'start_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.2.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.2.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.2.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:1140:1140 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:1142:1142 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1141:1141 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1143:1143 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5767), 0.5077498663816141, 0.8442280945757997, tensor(1.9621)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5270), 0.5334045964724746, 0.8525730180806675, tensor(2.1400)]
[tensor(-0.5270), 0.5334045964724746, 0.8525730180806675, tensor(2.1400)]
[tensor(-0.5160), 0.5334045964724746, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5334045964724746, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5334045964724746, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5334045964724746, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5334045964724746, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5360769641902726, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5360769641902726, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5360769641902726, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5360769641902726, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5360769641902726, 0.8525730180806675, tensor(2.1511)]
[tensor(-0.5160), 0.5366114377338321, 0.8525730180806675, tensor(2.1631)]
[tensor(-0.5160), 0.5366114377338321, 0.8525730180806675, tensor(2.1631)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5160), 0.5366114377338321, 0.8525730180806675, tensor(2.1631)]
[tensor(-0.5160), 0.5366114377338321, 0.8525730180806675, tensor(2.1631)]
[tensor(-0.5160), 0.5366114377338321, 0.8525730180806675, tensor(2.1631)]
[tensor(-0.5160), 0.5366114377338321, 0.8525730180806675, tensor(2.1631)]
early stopping at 19
[2022-12-30 17:22:06,759.759 dlc14ajn5vhvxm4z-master-0:1253 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 17:22:07,423.423 dlc14ajn5vhvxm4z-master-0:1322 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:22:07,431.431 dlc14ajn5vhvxm4z-master-0:1319 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:22:07,486.486 dlc14ajn5vhvxm4z-master-0:1320 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:22:07,490.490 dlc14ajn5vhvxm4z-master-0:1321 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:22:08,789.789 dlc14ajn5vhvxm4z-master-0:1321 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 17:22:08,791.791 dlc14ajn5vhvxm4z-master-0:1320 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 17:22:09,398.398 dlc14ajn5vhvxm4z-master-0:1322 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 17:22:09,405.405 dlc14ajn5vhvxm4z-master-0:1319 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.2.bias', 'end_prediction_head.0.bias', 'start_prediction_head.2.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.weight', 'start_prediction_head.2.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.2.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['selection_head.2.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'selection_head.0.weight', 'selection_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.2.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.2.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.2.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.0.weight', 'mam_head.dense.bias', 'selection_head.2.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'start_prediction_head.2.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.2.weight', 'selection_head.0.bias', 'start_prediction_head.2.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.2.bias', 'audio_encoder.audio_sep', 'start_prediction_head.2.weight', 'selection_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.2.bias', 'end_prediction_head.2.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.2.bias', 'selection_head.2.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:1319:1319 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:1322:1322 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1321:1321 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1320:1320 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5247), 0.5424906467129877, 0.8636995827538247, tensor(2.1877)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5247), 0.5424906467129877, 0.8636995827538247, tensor(2.1877)]
[tensor(-0.5247), 0.5424906467129877, 0.8636995827538247, tensor(2.1877)]
[tensor(-0.5145), 0.5515766969535008, 0.8636995827538247, tensor(2.2434)]
[tensor(-0.5145), 0.5515766969535008, 0.8636995827538247, tensor(2.2434)]
[2022-12-30 17:32:34,057.057 dlc14ajn5vhvxm4z-master-0:1392 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 17:32:34,699.699 dlc14ajn5vhvxm4z-master-0:1459 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:32:34,699.699 dlc14ajn5vhvxm4z-master-0:1460 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:32:34,808.808 dlc14ajn5vhvxm4z-master-0:1461 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:32:34,809.809 dlc14ajn5vhvxm4z-master-0:1458 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:32:36,547.547 dlc14ajn5vhvxm4z-master-0:1459 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 17:32:36,637.637 dlc14ajn5vhvxm4z-master-0:1460 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 17:32:36,684.684 dlc14ajn5vhvxm4z-master-0:1461 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 17:32:36,687.687 dlc14ajn5vhvxm4z-master-0:1458 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.2.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.0.weight', 'mlm_head.dense.weight', 'selection_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.2.bias', 'mam_head.bias', 'selection_head.2.weight', 'start_prediction_head.2.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.2.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.bias', 'mam_head.decoder.weight', 'selection_head.2.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.2.weight', 'selection_head.2.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.2.weight', 'mam_head.decoder.weight', 'selection_head.2.bias', 'selection_head.0.bias', 'end_prediction_head.2.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.2.bias', 'start_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.2.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'selection_head.0.weight', 'end_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'selection_head.2.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.2.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.0.bias', 'selection_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:1458:1458 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:1460:1460 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1459:1459 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1461:1461 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5233), 0.5387493319080705, 0.847009735744089, tensor(2.1704)]
[tensor(-0.5233), 0.5387493319080705, 0.847009735744089, tensor(2.1704)]
[tensor(-0.5233), 0.5456974879743453, 0.8560500695410292, tensor(2.1984)]
[tensor(-0.5233), 0.5456974879743453, 0.8560500695410292, tensor(2.1984)]
[tensor(-0.5233), 0.5456974879743453, 0.8616133518776078, tensor(2.1984)]
[2022-12-30 17:42:38,370.370 dlc14ajn5vhvxm4z-master-0:1531 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 17:42:38,983.983 dlc14ajn5vhvxm4z-master-0:1598 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:42:38,983.983 dlc14ajn5vhvxm4z-master-0:1600 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:42:39,066.066 dlc14ajn5vhvxm4z-master-0:1597 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:42:39,076.076 dlc14ajn5vhvxm4z-master-0:1599 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:42:40,832.832 dlc14ajn5vhvxm4z-master-0:1600 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 17:42:40,833.833 dlc14ajn5vhvxm4z-master-0:1598 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 17:42:41,348.348 dlc14ajn5vhvxm4z-master-0:1599 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 17:42:41,352.352 dlc14ajn5vhvxm4z-master-0:1597 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['selection_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.2.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.2.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.2.weight', 'mlm_head.dense.weight', 'end_prediction_head.2.weight', 'end_prediction_head.0.bias', 'start_prediction_head.2.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.2.weight', 'end_prediction_head.0.weight', 'selection_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.0.bias', 'selection_head.2.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.2.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.2.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.2.bias', 'start_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'selection_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.2.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.0.bias', 'selection_head.2.bias', 'mam_head.layer_norm.weight', 'selection_head.2.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'start_prediction_head.2.weight', 'start_prediction_head.2.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:1597:1597 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:1598:1598 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1600:1600 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1599:1599 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5203), 0.5494388027792624, 0.8518776077885952, tensor(2.2269)]
[tensor(-0.5203), 0.5494388027792624, 0.8553546592489569, tensor(2.2269)]
[tensor(-0.5203), 0.5494388027792624, 0.8574408901251739, tensor(2.2269)]
[tensor(-0.5203), 0.5494388027792624, 0.8574408901251739, tensor(2.2269)]
[tensor(-0.5203), 0.5494388027792624, 0.8574408901251739, tensor(2.2269)]
[tensor(-0.5203), 0.5494388027792624, 0.8574408901251739, tensor(2.2269)]
[tensor(-0.5203), 0.5494388027792624, 0.8574408901251739, tensor(2.2269)]
[tensor(-0.5203), 0.5494388027792624, 0.8574408901251739, tensor(2.2269)]
early stopping at 8
[2022-12-30 17:58:50,054.054 dlc14ajn5vhvxm4z-master-0:1678 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 17:58:50,777.777 dlc14ajn5vhvxm4z-master-0:1746 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:58:50,783.783 dlc14ajn5vhvxm4z-master-0:1745 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:58:50,800.800 dlc14ajn5vhvxm4z-master-0:1744 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:58:50,800.800 dlc14ajn5vhvxm4z-master-0:1747 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 17:58:51,664.664 dlc14ajn5vhvxm4z-master-0:1747 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 17:58:52,067.067 dlc14ajn5vhvxm4z-master-0:1745 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 17:58:52,068.068 dlc14ajn5vhvxm4z-master-0:1746 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 17:58:52,068.068 dlc14ajn5vhvxm4z-master-0:1744 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
has_audio_cls True multi audio False v2 Trueprompt False bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['selection_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.2.bias', 'selection_head.2.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.2.bias', 'end_prediction_head.0.bias', 'start_prediction_head.2.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['selection_head.2.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.2.weight', 'end_prediction_head.2.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'selection_head.2.bias', 'selection_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.2.weight', 'selection_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'selection_head.2.weight', 'selection_head.2.bias', 'start_prediction_head.2.bias', 'end_prediction_head.2.bias', 'start_prediction_head.2.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.2.bias', 'selection_head.2.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.2.bias', 'mam_head.dense.bias', 'end_prediction_head.2.weight', 'selection_head.2.weight', 'start_prediction_head.0.bias', 'start_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc14ajn5vhvxm4z-master-0:1744:1744 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:1745:1745 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1747:1747 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1746:1746 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5465), 0.5350080171031534, 0.8511821974965229, tensor(2.1285)]
[tensor(-0.5385), 0.5350080171031534, 0.8546592489568846, tensor(2.1285)]
[tensor(-0.5203), 0.5350080171031534, 0.8546592489568846, tensor(2.1387)]
[tensor(-0.5203), 0.5350080171031534, 0.8546592489568846, tensor(2.1387)]
[tensor(-0.5203), 0.5350080171031534, 0.8595271210013908, tensor(2.1387)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5203), 0.5350080171031534, 0.8595271210013908, tensor(2.1387)]
[tensor(-0.5203), 0.5350080171031534, 0.8595271210013908, tensor(2.1387)]
[tensor(-0.5203), 0.5350080171031534, 0.8595271210013908, tensor(2.1387)]
[tensor(-0.5203), 0.5350080171031534, 0.8595271210013908, tensor(2.1387)]
[tensor(-0.5203), 0.5350080171031534, 0.8595271210013908, tensor(2.1387)]
early stopping at 10
[2022-12-30 18:19:30,943.943 dlc14ajn5vhvxm4z-master-0:1830 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 18:19:31,555.555 dlc14ajn5vhvxm4z-master-0:1899 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:19:31,559.559 dlc14ajn5vhvxm4z-master-0:1898 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:19:31,559.559 dlc14ajn5vhvxm4z-master-0:1897 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:19:31,584.584 dlc14ajn5vhvxm4z-master-0:1896 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:19:33,470.470 dlc14ajn5vhvxm4z-master-0:1898 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 18:19:33,482.482 dlc14ajn5vhvxm4z-master-0:1899 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 18:19:33,486.486 dlc14ajn5vhvxm4z-master-0:1897 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 18:19:33,490.490 dlc14ajn5vhvxm4z-master-0:1896 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.2.weight', 'end_prediction_head.2.weight', 'selection_head.2.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.2.bias', 'mlm_head.decoder.weight', 'selection_head.2.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.2.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.2.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.0.weight', 'start_prediction_head.2.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.2.weight', 'mlm_head.layer_norm.bias', 'selection_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.2.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.2.weight', 'end_prediction_head.2.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['end_prediction_head.2.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.2.weight', 'selection_head.2.bias', 'start_prediction_head.2.weight', 'mam_head.layer_norm.weight', 'selection_head.0.bias', 'selection_head.0.weight', 'selection_head.2.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.2.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.2.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.2.bias', 'start_prediction_head.2.weight', 'start_prediction_head.2.bias', 'selection_head.2.weight', 'end_prediction_head.2.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:1896:1896 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:1898:1898 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1899:1899 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:1897:1897 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2828), 0.555956678700361, tensor(1.4970)]
[tensor(-1.2347), 0.605595667870036, tensor(1.7932)]
[tensor(-1.1889), 0.605595667870036, tensor(1.8120)]
[tensor(-1.1889), 0.605595667870036, tensor(1.8120)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1889), 0.605595667870036, tensor(1.8120)]
[2022-12-30 18:41:38,944.944 dlc14ajn5vhvxm4z-master-0:2367 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 18:41:39,576.576 dlc14ajn5vhvxm4z-master-0:2435 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:41:39,605.605 dlc14ajn5vhvxm4z-master-0:2434 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:41:39,677.677 dlc14ajn5vhvxm4z-master-0:2436 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:41:39,763.763 dlc14ajn5vhvxm4z-master-0:2433 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:41:41,403.403 dlc14ajn5vhvxm4z-master-0:2435 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 18:41:41,486.486 dlc14ajn5vhvxm4z-master-0:2436 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 18:41:41,856.856 dlc14ajn5vhvxm4z-master-0:2434 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 18:41:41,858.858 dlc14ajn5vhvxm4z-master-0:2433 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.2.weight', 'mlm_head.dense.weight', 'selection_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.2.bias', 'start_prediction_head.2.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.2.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.2.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.bias', 'end_prediction_head.2.bias', 'start_prediction_head.2.weight', 'end_prediction_head.2.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.2.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.2.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.2.bias', 'mam_head.bias', 'selection_head.2.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.2.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.2.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.2.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.2.weight', 'selection_head.2.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:2433:2433 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:2435:2435 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:2436:2436 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:2434:2434 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2202), 0.5821299638989169, tensor(1.6905)]
[tensor(-1.1696), 0.6064981949458483, tensor(1.8629)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1696), 0.6064981949458483, tensor(1.8629)]
[tensor(-1.1696), 0.6119133574007221, tensor(1.8629)]
[tensor(-1.1696), 0.6119133574007221, tensor(1.8629)]
[2022-12-30 18:59:14,713.713 dlc14ajn5vhvxm4z-master-0:2898 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 18:59:15,330.330 dlc14ajn5vhvxm4z-master-0:2966 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:59:15,330.330 dlc14ajn5vhvxm4z-master-0:2965 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:59:15,438.438 dlc14ajn5vhvxm4z-master-0:2964 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:59:15,438.438 dlc14ajn5vhvxm4z-master-0:2967 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 18:59:17,181.181 dlc14ajn5vhvxm4z-master-0:2966 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 18:59:17,182.182 dlc14ajn5vhvxm4z-master-0:2965 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 18:59:17,283.283 dlc14ajn5vhvxm4z-master-0:2967 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 18:59:17,287.287 dlc14ajn5vhvxm4z-master-0:2964 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.2.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.2.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.2.weight', 'start_prediction_head.2.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.2.bias', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.2.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.2.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.2.weight', 'end_prediction_head.0.weight', 'selection_head.0.weight', 'end_prediction_head.2.weight', 'start_prediction_head.2.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.2.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.0.bias', 'start_prediction_head.2.bias', 'end_prediction_head.2.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.2.bias', 'selection_head.2.bias', 'mlm_head.decoder.weight', 'start_prediction_head.2.weight', 'selection_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.2.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.2.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.2.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.2.weight', 'mlm_head.decoder.weight', 'selection_head.0.bias', 'end_prediction_head.2.weight', 'start_prediction_head.2.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:2964:2964 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:2967:2967 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:2965:2965 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:2966:2966 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6369), 0.4232851985559567, tensor(0.4795)]
[tensor(-1.6338), 0.4232851985559567, tensor(0.4826)]
[tensor(-1.6338), 0.4232851985559567, tensor(0.4826)]
[tensor(-1.6338), 0.4232851985559567, tensor(0.4826)]
[tensor(-1.6338), 0.4232851985559567, tensor(0.4826)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5188), 0.48375451263537905, tensor(0.9000)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4596), 0.5027075812274369, tensor(1.0539)]
[tensor(-1.4596), 0.5027075812274369, tensor(1.0539)]
[tensor(-1.4356), 0.5126353790613718, tensor(1.1276)]
[tensor(-1.3655), 0.5234657039711191, tensor(1.2519)]
[tensor(-1.3343), 0.5523465703971119, tensor(1.4274)]
[tensor(-1.3343), 0.5613718411552346, tensor(1.4629)]
[tensor(-1.3336), 0.5839350180505415, tensor(1.5861)]
[tensor(-1.2935), 0.5947653429602888, tensor(1.6803)]
[tensor(-1.2935), 0.5947653429602888, tensor(1.6803)]
[tensor(-1.2935), 0.6010830324909747, tensor(1.6803)]
[tensor(-1.2935), 0.6010830324909747, tensor(1.6803)]
[tensor(-1.2935), 0.6010830324909747, tensor(1.6803)]
[tensor(-1.2935), 0.6010830324909747, tensor(1.6803)]
[tensor(-1.2935), 0.6010830324909747, tensor(1.6803)]
[tensor(-1.2935), 0.6010830324909747, tensor(1.6803)]
early stopping at 21
[2022-12-30 20:07:45,318.318 dlc14ajn5vhvxm4z-master-0:3492 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 20:07:45,940.940 dlc14ajn5vhvxm4z-master-0:3559 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:07:45,940.940 dlc14ajn5vhvxm4z-master-0:3560 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:07:46,116.116 dlc14ajn5vhvxm4z-master-0:3558 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:07:46,116.116 dlc14ajn5vhvxm4z-master-0:3561 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:07:46,981.981 dlc14ajn5vhvxm4z-master-0:3561 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 20:07:47,868.868 dlc14ajn5vhvxm4z-master-0:3560 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 20:07:47,868.868 dlc14ajn5vhvxm4z-master-0:3559 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 20:07:47,871.871 dlc14ajn5vhvxm4z-master-0:3558 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['start_prediction_head.2.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.2.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.0.bias', 'end_prediction_head.2.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.2.weight', 'start_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.2.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.0.bias', 'start_prediction_head.2.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.2.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.2.weight', 'end_prediction_head.2.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.2.weight', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['selection_head.2.weight', 'mam_head.dense.bias', 'selection_head.0.bias', 'selection_head.2.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.2.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.2.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.2.bias', 'selection_head.2.weight', 'mlm_head.layer_norm.weight', 'selection_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.2.weight', 'end_prediction_head.2.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.weight', 'selection_head.2.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:3558:3558 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:3560:3560 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:3559:3559 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:3561:3561 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2762), 0.5712996389891697, tensor(1.5803)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2439), 0.5947653429602888, tensor(1.7299)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1740), 0.6155234657039711, tensor(1.9036)]
[tensor(-1.1740), 0.6155234657039711, tensor(1.9036)]
[tensor(-1.1740), 0.6155234657039711, tensor(1.9036)]
[tensor(-1.1740), 0.6155234657039711, tensor(1.9036)]
[tensor(-1.1740), 0.6155234657039711, tensor(1.9036)]
[tensor(-1.1740), 0.6155234657039711, tensor(1.9036)]
early stopping at 8
[2022-12-30 20:34:55,587.587 dlc14ajn5vhvxm4z-master-0:4037 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 20:34:56,223.223 dlc14ajn5vhvxm4z-master-0:4104 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:34:56,223.223 dlc14ajn5vhvxm4z-master-0:4105 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:34:56,264.264 dlc14ajn5vhvxm4z-master-0:4103 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:34:56,266.266 dlc14ajn5vhvxm4z-master-0:4106 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:34:57,173.173 dlc14ajn5vhvxm4z-master-0:4106 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 20:34:58,076.076 dlc14ajn5vhvxm4z-master-0:4104 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 20:34:58,147.147 dlc14ajn5vhvxm4z-master-0:4105 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 20:34:58,156.156 dlc14ajn5vhvxm4z-master-0:4103 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['selection_head.2.weight', 'start_prediction_head.2.bias', 'selection_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.2.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.2.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.2.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.2.weight', 'start_prediction_head.2.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.2.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.2.weight', 'mam_head.layer_norm.weight', 'selection_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.2.bias', 'mlm_head.decoder.bias', 'selection_head.2.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['selection_head.2.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.2.bias', 'start_prediction_head.0.weight', 'selection_head.0.bias', 'start_prediction_head.2.weight', 'selection_head.2.weight', 'start_prediction_head.2.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.2.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'selection_head.2.bias', 'selection_head.0.bias', 'mam_head.bias', 'selection_head.2.weight', 'start_prediction_head.2.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:4103:4103 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:4105:4105 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:4106:4106 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:4104:4104 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.3741), 0.5252707581227437, tensor(1.2523)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.2180), 0.6137184115523465, tensor(1.8506)]
[tensor(-1.1710), 0.6137184115523465, tensor(1.8506)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
[tensor(-1.1629), 0.6137184115523465, tensor(1.8506)]
[tensor(-1.1629), 0.6137184115523465, tensor(1.8506)]
[2022-12-30 20:52:45,333.333 dlc14ajn5vhvxm4z-master-0:4570 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 20:52:46,022.022 dlc14ajn5vhvxm4z-master-0:4637 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:52:46,047.047 dlc14ajn5vhvxm4z-master-0:4638 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:52:46,131.131 dlc14ajn5vhvxm4z-master-0:4636 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:52:46,211.211 dlc14ajn5vhvxm4z-master-0:4639 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 20:52:47,299.299 dlc14ajn5vhvxm4z-master-0:4638 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 20:52:47,412.412 dlc14ajn5vhvxm4z-master-0:4639 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 20:52:47,840.840 dlc14ajn5vhvxm4z-master-0:4637 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 20:52:47,849.849 dlc14ajn5vhvxm4z-master-0:4636 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['selection_head.2.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.2.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.0.bias', 'selection_head.0.weight', 'start_prediction_head.2.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.2.weight', 'start_prediction_head.2.bias', 'end_prediction_head.2.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.2.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'selection_head.2.bias', 'mlm_head.dense.weight', 'selection_head.0.bias', 'end_prediction_head.2.bias', 'end_prediction_head.2.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.2.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'selection_head.2.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.2.bias', 'mlm_head.decoder.bias', 'selection_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.2.weight', 'start_prediction_head.2.weight', 'selection_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.2.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.2.bias', 'start_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.0.weight', 'selection_head.2.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.weight', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'mam_head.layer_norm.bias', 'selection_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:4636:4636 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:4638:4638 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:4639:4639 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:4637:4637 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.2530), 0.5712996389891697, tensor(1.6035)]
[tensor(-1.1724), 0.5902527075812274, tensor(1.7789)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.1608), 0.601985559566787, tensor(1.8491)]
[tensor(-1.1608), 0.6064981949458483, tensor(1.8491)]
[tensor(-1.1608), 0.6083032490974729, tensor(1.8491)]
[2022-12-30 21:10:36,176.176 dlc14ajn5vhvxm4z-master-0:5102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 21:10:36,798.798 dlc14ajn5vhvxm4z-master-0:5169 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:10:36,799.799 dlc14ajn5vhvxm4z-master-0:5170 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:10:36,872.872 dlc14ajn5vhvxm4z-master-0:5171 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:10:36,890.890 dlc14ajn5vhvxm4z-master-0:5168 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:10:38,694.694 dlc14ajn5vhvxm4z-master-0:5169 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 21:10:38,701.701 dlc14ajn5vhvxm4z-master-0:5170 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 21:10:38,754.754 dlc14ajn5vhvxm4z-master-0:5171 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 21:10:38,760.760 dlc14ajn5vhvxm4z-master-0:5168 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.2.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.2.bias', 'mlm_head.bias', 'selection_head.2.bias', 'mam_head.layer_norm.bias', 'selection_head.2.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.2.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.0.bias', 'mam_head.bias', 'selection_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['start_prediction_head.2.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.2.bias', 'selection_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.2.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.2.bias', 'mam_head.bias', 'end_prediction_head.2.weight', 'start_prediction_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.2.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.2.weight', 'start_prediction_head.2.bias', 'selection_head.2.bias', 'mlm_head.decoder.bias', 'selection_head.2.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.2.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.2.weight', 'end_prediction_head.2.weight', 'selection_head.0.weight', 'start_prediction_head.2.bias', 'start_prediction_head.2.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.2.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:5168:5168 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:5171:5171 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:5169:5169 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:5170:5170 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-1.3702), 0.5397111913357401, tensor(1.3283)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2148), 0.6001805054151624, tensor(1.7861)]
[tensor(-1.2060), 0.6001805054151624, tensor(1.7861)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1975), 0.6001805054151624, tensor(1.8034)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1975), 0.6001805054151624, tensor(1.8034)]
[tensor(-1.1975), 0.6001805054151624, tensor(1.8034)]
[tensor(-1.1975), 0.6001805054151624, tensor(1.8034)]
[tensor(-1.1975), 0.6001805054151624, tensor(1.8034)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1975), 0.6001805054151624, tensor(1.8034)]
early stopping at 9
[2022-12-30 21:41:16,633.633 dlc14ajn5vhvxm4z-master-0:5647 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 21:41:17,273.273 dlc14ajn5vhvxm4z-master-0:5715 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:41:17,302.302 dlc14ajn5vhvxm4z-master-0:5714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:41:17,376.376 dlc14ajn5vhvxm4z-master-0:5713 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:41:17,462.462 dlc14ajn5vhvxm4z-master-0:5716 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 21:41:18,571.571 dlc14ajn5vhvxm4z-master-0:5714 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 21:41:18,649.649 dlc14ajn5vhvxm4z-master-0:5716 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 21:41:19,099.099 dlc14ajn5vhvxm4z-master-0:5715 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 21:41:19,104.104 dlc14ajn5vhvxm4z-master-0:5713 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.3.3_1 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.2.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.2.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.2.bias', 'mlm_head.dense.bias', 'start_prediction_head.2.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.2.weight', 'mam_head.bias', 'selection_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.0.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.2.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.2.weight', 'selection_head.2.bias', 'selection_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.2.weight', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['selection_head.0.bias', 'mlm_head.bias', 'selection_head.2.bias', 'selection_head.0.weight', 'selection_head.2.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.2.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.2.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.3.3_1 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.2.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.2.weight', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mam_head.decoder.weight', 'selection_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.2.bias', 'selection_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:5713:5713 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:5714:5714 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:5715:5715 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:5716:5716 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.3802), 0.5487364620938628, tensor(1.3635)]
[tensor(-1.2683), 0.5956678700361011, tensor(1.7101)]
[tensor(-1.1837), 0.6110108303249098, tensor(1.8713)]
[tensor(-1.1837), 0.6110108303249098, tensor(1.8713)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.1837), 0.6110108303249098, tensor(1.8713)]
[tensor(-1.1837), 0.6110108303249098, tensor(1.8713)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
[tensor(-1.1837), 0.6110108303249098, tensor(1.8713)]
[tensor(-1.1837), 0.6110108303249098, tensor(1.8713)]
[tensor(-1.1837), 0.6110108303249098, tensor(1.8713)]
early stopping at 9
[2022-12-30 22:11:37,145.145 dlc14ajn5vhvxm4z-master-0:6190 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 22:11:37,773.773 dlc14ajn5vhvxm4z-master-0:6258 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:11:37,822.822 dlc14ajn5vhvxm4z-master-0:6257 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:11:37,972.972 dlc14ajn5vhvxm4z-master-0:6259 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:11:37,990.990 dlc14ajn5vhvxm4z-master-0:6256 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:11:39,629.629 dlc14ajn5vhvxm4z-master-0:6258 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 22:11:39,655.655 dlc14ajn5vhvxm4z-master-0:6257 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 22:11:39,819.819 dlc14ajn5vhvxm4z-master-0:6259 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 22:11:39,820.820 dlc14ajn5vhvxm4z-master-0:6256 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'mam_head.decoder.weight', 'selection_head.2.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.0.bias', 'selection_head.2.weight', 'start_prediction_head.2.bias', 'start_prediction_head.2.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.bias', 'selection_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.2.bias', 'selection_head.0.bias', 'end_prediction_head.2.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.2.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.2.weight', 'start_prediction_head.2.weight', 'selection_head.2.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.2.bias', 'selection_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.2.bias', 'start_prediction_head.2.weight', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'end_prediction_head.2.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.2.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'selection_head.0.bias', 'selection_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.2.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.2.weight', 'selection_head.2.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.2.weight', 'mam_head.dense.bias', 'selection_head.2.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:6256:6256 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:6259:6259 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:6257:6257 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:6258:6258 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2545), 0.6028880866425993, tensor(1.7599)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.2152), 0.6028880866425993, tensor(1.7599)]
[tensor(-1.1504), 0.6028880866425993, tensor(1.8505)]
[tensor(-1.1504), 0.6028880866425993, tensor(1.8505)]
[tensor(-1.1504), 0.6046931407942239, tensor(1.8505)]
[2022-12-30 22:26:57,732.732 dlc14ajn5vhvxm4z-master-0:6720 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 22:26:58,374.374 dlc14ajn5vhvxm4z-master-0:6787 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:26:58,375.375 dlc14ajn5vhvxm4z-master-0:6788 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:26:58,543.543 dlc14ajn5vhvxm4z-master-0:6786 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:26:58,543.543 dlc14ajn5vhvxm4z-master-0:6789 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:26:59,384.384 dlc14ajn5vhvxm4z-master-0:6789 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 22:27:00,233.233 dlc14ajn5vhvxm4z-master-0:6787 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 22:27:00,234.234 dlc14ajn5vhvxm4z-master-0:6788 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 22:27:00,234.234 dlc14ajn5vhvxm4z-master-0:6786 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['start_prediction_head.2.bias', 'selection_head.0.weight', 'start_prediction_head.2.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.2.bias', 'selection_head.2.weight', 'start_prediction_head.0.bias', 'end_prediction_head.2.bias', 'end_prediction_head.2.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.2.weight', 'mlm_head.decoder.bias', 'selection_head.2.weight', 'mam_head.layer_norm.bias', 'selection_head.2.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.2.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.0.weight', 'end_prediction_head.2.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.2.weight', 'selection_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.0.weight', 'mlm_head.bias', 'start_prediction_head.2.weight', 'selection_head.2.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.2.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.2.bias', 'mam_head.layer_norm.bias', 'selection_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.2.weight', 'start_prediction_head.2.weight', 'selection_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'mlm_head.layer_norm.weight', 'selection_head.2.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.2.weight', 'selection_head.0.weight', 'start_prediction_head.2.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:6786:6786 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:6787:6787 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:6788:6788 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:6789:6789 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2244), 0.5830324909747292, tensor(1.6907)]
[tensor(-1.1725), 0.605595667870036, tensor(1.8555)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1644), 0.605595667870036, tensor(1.8591)]
[tensor(-1.1644), 0.605595667870036, tensor(1.8591)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1644), 0.605595667870036, tensor(1.8591)]
[2022-12-30 22:42:19,338.338 dlc14ajn5vhvxm4z-master-0:7248 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 22:42:20,055.055 dlc14ajn5vhvxm4z-master-0:7316 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:42:20,057.057 dlc14ajn5vhvxm4z-master-0:7315 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:42:20,075.075 dlc14ajn5vhvxm4z-master-0:7317 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:42:20,077.077 dlc14ajn5vhvxm4z-master-0:7314 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 22:42:21,340.340 dlc14ajn5vhvxm4z-master-0:7316 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 22:42:21,341.341 dlc14ajn5vhvxm4z-master-0:7315 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 22:42:21,935.935 dlc14ajn5vhvxm4z-master-0:7317 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 22:42:21,938.938 dlc14ajn5vhvxm4z-master-0:7314 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['end_prediction_head.2.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.0.bias', 'selection_head.2.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.2.bias', 'mam_head.bias', 'selection_head.2.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.2.weight', 'start_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.2.weight', 'mlm_head.dense.bias', 'selection_head.2.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'selection_head.0.bias', 'selection_head.2.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.bias', 'start_prediction_head.2.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.2.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.0.weight', 'mlm_head.dense.weight', 'selection_head.2.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.2.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.2.bias', 'mlm_head.decoder.weight', 'selection_head.2.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.2.weight', 'selection_head.2.bias', 'start_prediction_head.0.bias', 'start_prediction_head.2.bias', 'end_prediction_head.0.weight', 'selection_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.2.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:7314:7314 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:7316:7316 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:7317:7317 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:7315:7315 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6462), 0.4232851985559567, tensor(0.4703)]
[tensor(-1.6408), 0.4232851985559567, tensor(0.4756)]
[tensor(-1.6408), 0.4232851985559567, tensor(0.4756)]
[tensor(-1.6344), 0.4232851985559567, tensor(0.4820)]
[tensor(-1.6344), 0.4232851985559567, tensor(0.4820)]
[tensor(-1.5784), 0.44223826714801445, tensor(0.6328)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4895), 0.490072202166065, tensor(0.9609)]
[tensor(-1.4589), 0.4954873646209386, tensor(1.0186)]
[tensor(-1.4323), 0.5036101083032491, tensor(1.0857)]
[tensor(-1.4323), 0.5433212996389891, tensor(1.2591)]
[tensor(-1.3660), 0.5568592057761733, tensor(1.4183)]
[tensor(-1.3369), 0.5667870036101083, tensor(1.4970)]
[tensor(-1.3369), 0.5667870036101083, tensor(1.4970)]
[tensor(-1.3276), 0.5731046931407943, tensor(1.5379)]
[tensor(-1.3276), 0.575812274368231, tensor(1.5379)]
[tensor(-1.3276), 0.575812274368231, tensor(1.5379)]
[tensor(-1.3276), 0.575812274368231, tensor(1.5379)]
[tensor(-1.3276), 0.575812274368231, tensor(1.5379)]
[tensor(-1.3276), 0.575812274368231, tensor(1.5379)]
[tensor(-1.3276), 0.575812274368231, tensor(1.5379)]
early stopping at 20
[2022-12-30 23:38:49,345.345 dlc14ajn5vhvxm4z-master-0:7823 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2022-12-30 23:38:49,994.994 dlc14ajn5vhvxm4z-master-0:7890 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 23:38:50,026.026 dlc14ajn5vhvxm4z-master-0:7891 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 23:38:50,096.096 dlc14ajn5vhvxm4z-master-0:7889 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 23:38:50,182.182 dlc14ajn5vhvxm4z-master-0:7892 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2022-12-30 23:38:51,288.288 dlc14ajn5vhvxm4z-master-0:7891 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-30 23:38:51,378.378 dlc14ajn5vhvxm4z-master-0:7892 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-30 23:38:51,802.802 dlc14ajn5vhvxm4z-master-0:7890 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-30 23:38:51,806.806 dlc14ajn5vhvxm4z-master-0:7889 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v3.2.6 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task meld last_conv_layer group cl_mode no cl_steps 3 prompt True train_mode twentyturn
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
has_audio_cls True multi audio True v2 Trueprompt True bert False
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['selection_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.2.weight', 'mam_head.dense.bias', 'end_prediction_head.2.bias', 'selection_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.2.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.2.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.2.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['start_prediction_head.2.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.2.bias', 'mlm_head.decoder.weight', 'end_prediction_head.2.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.2.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.2.weight', 'selection_head.0.weight', 'selection_head.0.bias', 'selection_head.2.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.0.bias', 'end_prediction_head.2.bias', 'selection_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.2.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'selection_head.2.weight', 'end_prediction_head.2.weight', 'selection_head.2.bias', 'start_prediction_head.2.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v3.2.6 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.2.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.2.bias', 'selection_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.2.bias', 'selection_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.2.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.2.weight', 'mlm_head.bias', 'selection_head.2.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic

dlc14ajn5vhvxm4z-master-0:7889:7889 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc14ajn5vhvxm4z-master-0:7890:7890 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:7891:7891 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc14ajn5vhvxm4z-master-0:7892:7892 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2871), 0.5649819494584838, tensor(1.5378)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1964), 0.601985559566787, tensor(1.8136)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.1964), 0.601985559566787, tensor(1.8136)]
[tensor(-1.1964), 0.601985559566787, tensor(1.8136)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.1964), 0.601985559566787, tensor(1.8136)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
[tensor(-1.1964), 0.601985559566787, tensor(1.8136)]
[tensor(-1.1964), 0.601985559566787, tensor(1.8136)]
early stopping at 7
