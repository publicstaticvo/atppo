Model v4.3.6-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-25 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 16
Model v4.3.6-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 17
Model v4.3.6-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 10
Model v4.3.6-25 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 34
Model v4.3.6-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 30
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 33
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 11
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 19
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0

has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 13
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 11
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 8
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 12
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 8
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 9
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 14
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 7
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 7
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-25 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 9
Model v4.3.6-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-50 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 3
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 31
Model v4.3.6-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 3
Model v4.3.6-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-50 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 29
Model v4.3.6-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
early stopping at 3
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 21
Model v4.3.6-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 9
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 11
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
early stopping at 13
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 11
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 16
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 30
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 6
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 12
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0

has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


early stopping at 14
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 29
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0

fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 7
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-50 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 16
Model v4.3.6-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-75 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 28
Model v4.3.6-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 41
Model v4.3.6-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-75 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 18
Model v4.3.6-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 23
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 26
Model v4.3.6-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['selection_head.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

early stopping at 14
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 12
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 12
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 10
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 7
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 12
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 12
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
early stopping at 3
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0

fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 13
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 15
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 9
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-75 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 6
Model v4.3.6-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-100 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 22
Model v4.3.6-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 17
Model v4.3.6-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-100 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 17
Model v4.3.6-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 19
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Model v4.3.6-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 25
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 43
Model v4.3.6-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 44
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0

has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

early stopping at 16
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 10
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 15
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 14
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 7
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 14
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 8
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 10
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 9
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 7
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 8
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-100 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 6
Model v4.3.6-125 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
Model v4.3.6-125 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 32
Model v4.3.6-125 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 21
Model v4.3.6-125 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-125 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-125 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 50
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-125 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-125 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-125 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 33
Model v4.3.6-125 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 3
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2downstreamv2 mosei 
mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0

fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 10
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 11
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 11
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 11
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 7
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
early stopping at 11
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 19
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 16
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 19
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 18
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-125 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-125 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 16
Model v4.3.6-150 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-150 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 35
Model v4.3.6-150 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 19
Model v4.3.6-150 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-150 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 23
Model v4.3.6-150 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 33
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-150 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 41
Model v4.3.6-150 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-150 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-150 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 10
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 12
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 11
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 11
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 9
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 14
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 8
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 11
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 10
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0

fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 7
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 14
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-150 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-150 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 15
Model v4.3.6-175 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 22
Model v4.3.6-175 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 27
Model v4.3.6-175 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-175 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 19
Model v4.3.6-175 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 23
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
early stopping at 21
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 9
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 19
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 11
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 9
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 14
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 12
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
early stopping at 19
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0

has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 18
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 7
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0

fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 8
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-175 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-175 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
early stopping at 16
Model v4.3.6-200 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-200 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 22
Model v4.3.6-200 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 23
Model v4.3.6-200 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-200 datasize 960 batchsize 16 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 10
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 21
Model v4.3.6-200 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 40
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-200 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-200 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Model v4.3.6-200 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 3
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-200 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0

has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 17
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 15
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
early stopping at 11
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 9
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 7
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
early stopping at 9
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
early stopping at 8
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
early stopping at 3
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 16
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
early stopping at 19
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0

fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 4 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
early stopping at 10
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Model v4.3.6-200 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer group cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of ATModel were not initialized from the model checkpoint at /mnt/shared/public/yts/Audio-Text-Pretraining/models/v4.3.6-200 and are newly initialized: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
NCCL version 2.12.10+cuda11.3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
early stopping at 22
