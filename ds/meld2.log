[2023-01-16 01:11:26,721.721 dlcmzxjb7qmi93pp-master-0:36 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 01:11:27,588.588 dlcmzxjb7qmi93pp-master-0:103 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:11:27,589.589 dlcmzxjb7qmi93pp-master-0:102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:11:27,672.672 dlcmzxjb7qmi93pp-master-0:101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:11:27,674.674 dlcmzxjb7qmi93pp-master-0:104 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:11:29,250.250 dlcmzxjb7qmi93pp-master-0:102 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 01:11:29,252.252 dlcmzxjb7qmi93pp-master-0:103 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 01:11:30,245.245 dlcmzxjb7qmi93pp-master-0:104 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 01:11:30,252.252 dlcmzxjb7qmi93pp-master-0:101 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:101:101 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:103:103 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:104:104 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:102:102 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5412), 0.5275253874933191, 0.8581363004172462, tensor(2.0965)]
[tensor(-0.5343), 0.5280598610368786, 0.8616133518776078, tensor(2.1060)]
[tensor(-0.5132), 0.5312667022982362, 0.8616133518776078, tensor(2.1431)]
[tensor(-0.5132), 0.5318011758417959, 0.8616133518776078, tensor(2.1431)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5132), 0.5318011758417959, 0.8616133518776078, tensor(2.1431)]
[2023-01-16 01:22:43,182.182 dlcmzxjb7qmi93pp-master-0:179 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 01:22:43,818.818 dlcmzxjb7qmi93pp-master-0:245 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:22:43,851.851 dlcmzxjb7qmi93pp-master-0:247 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:22:43,905.905 dlcmzxjb7qmi93pp-master-0:244 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:22:43,908.908 dlcmzxjb7qmi93pp-master-0:246 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:22:44,947.947 dlcmzxjb7qmi93pp-master-0:246 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 01:22:45,649.649 dlcmzxjb7qmi93pp-master-0:245 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 01:22:45,717.717 dlcmzxjb7qmi93pp-master-0:247 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 01:22:45,719.719 dlcmzxjb7qmi93pp-master-0:244 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:244:244 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:247:247 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:245:245 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:246:246 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5272), 0.5424906467129877, 0.8511821974965229, tensor(2.1852)]
[tensor(-0.5186), 0.5430251202565473, 0.8595271210013908, tensor(2.1965)]
[tensor(-0.5075), 0.5585248530197755, 0.8595271210013908, tensor(2.2851)]
[tensor(-0.5058), 0.5585248530197755, 0.8643949930458971, tensor(2.2851)]
[tensor(-0.5058), 0.5585248530197755, 0.8643949930458971, tensor(2.2851)]
[2023-01-16 01:33:15,565.565 dlcmzxjb7qmi93pp-master-0:323 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 01:33:16,215.215 dlcmzxjb7qmi93pp-master-0:388 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:33:16,231.231 dlcmzxjb7qmi93pp-master-0:390 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:33:16,283.283 dlcmzxjb7qmi93pp-master-0:389 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:33:16,289.289 dlcmzxjb7qmi93pp-master-0:391 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:33:17,607.607 dlcmzxjb7qmi93pp-master-0:391 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 01:33:17,609.609 dlcmzxjb7qmi93pp-master-0:389 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 01:33:18,127.127 dlcmzxjb7qmi93pp-master-0:390 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 01:33:18,134.134 dlcmzxjb7qmi93pp-master-0:388 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:388:388 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:390:390 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:391:391 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:389:389 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5255), 0.532870122928915, 0.8511821974965229, tensor(2.1389)]
[tensor(-0.5255), 0.532870122928915, 0.8511821974965229, tensor(2.1389)]
[tensor(-0.5177), 0.5494388027792624, 0.8671766342141863, tensor(2.2295)]
[tensor(-0.5026), 0.5531801175841796, 0.8671766342141863, tensor(2.2633)]
[tensor(-0.5026), 0.5531801175841796, 0.8671766342141863, tensor(2.2633)]
[tensor(-0.5026), 0.5531801175841796, 0.8671766342141863, tensor(2.2633)]
[tensor(-0.5026), 0.5531801175841796, 0.8671766342141863, tensor(2.2633)]
[tensor(-0.5026), 0.5531801175841796, 0.8671766342141863, tensor(2.2633)]
[tensor(-0.5026), 0.5531801175841796, 0.8671766342141863, tensor(2.2633)]
early stopping at 9
[2023-01-16 01:52:19,421.421 dlcmzxjb7qmi93pp-master-0:478 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 01:52:20,055.055 dlcmzxjb7qmi93pp-master-0:546 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:20,055.055 dlcmzxjb7qmi93pp-master-0:543 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:20,055.055 dlcmzxjb7qmi93pp-master-0:545 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:20,068.068 dlcmzxjb7qmi93pp-master-0:544 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 01:52:21,030.030 dlcmzxjb7qmi93pp-master-0:546 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 01:52:21,032.032 dlcmzxjb7qmi93pp-master-0:545 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 01:52:21,033.033 dlcmzxjb7qmi93pp-master-0:544 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 01:52:21,037.037 dlcmzxjb7qmi93pp-master-0:543 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:543:543 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:544:544 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:546:546 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:545:545 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5449), 0.5264564404061999, 0.8643949930458971, tensor(2.0874)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5351), 0.532870122928915, 0.8643949930458971, tensor(2.1293)]
[tensor(-0.5039), 0.5494388027792624, 0.8643949930458971, tensor(2.2433)]
[tensor(-0.5039), 0.5494388027792624, 0.8643949930458971, tensor(2.2433)]
[tensor(-0.5039), 0.5494388027792624, 0.8643949930458971, tensor(2.2433)]
[tensor(-0.5039), 0.5494388027792624, 0.8643949930458971, tensor(2.2433)]
[tensor(-0.5039), 0.5494388027792624, 0.8643949930458971, tensor(2.2433)]
[tensor(-0.5039), 0.5494388027792624, 0.8643949930458971, tensor(2.2433)]
early stopping at 8
[2023-01-16 02:08:30,123.123 dlcmzxjb7qmi93pp-master-0:629 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 02:08:30,745.745 dlcmzxjb7qmi93pp-master-0:695 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:08:30,745.745 dlcmzxjb7qmi93pp-master-0:694 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:08:30,825.825 dlcmzxjb7qmi93pp-master-0:696 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:08:30,835.835 dlcmzxjb7qmi93pp-master-0:697 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:08:31,672.672 dlcmzxjb7qmi93pp-master-0:695 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 02:08:32,171.171 dlcmzxjb7qmi93pp-master-0:696 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 02:08:32,294.294 dlcmzxjb7qmi93pp-master-0:697 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 02:08:32,299.299 dlcmzxjb7qmi93pp-master-0:694 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:694:694 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:695:695 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:696:696 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:697:697 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5327), 0.5467664350614645, 0.8560500695410292, tensor(2.2011)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5229), 0.5467664350614645, 0.8595271210013908, tensor(2.2011)]
[tensor(-0.5229), 0.5467664350614645, 0.8630041724617524, tensor(2.2011)]
[tensor(-0.5229), 0.5467664350614645, 0.8630041724617524, tensor(2.2011)]
[tensor(-0.5229), 0.5467664350614645, 0.8630041724617524, tensor(2.2011)]
[2023-01-16 02:19:14,481.481 dlcmzxjb7qmi93pp-master-0:773 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 02:19:15,109.109 dlcmzxjb7qmi93pp-master-0:839 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:19:15,110.110 dlcmzxjb7qmi93pp-master-0:840 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:19:15,205.205 dlcmzxjb7qmi93pp-master-0:841 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:19:15,220.220 dlcmzxjb7qmi93pp-master-0:838 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:19:17,115.115 dlcmzxjb7qmi93pp-master-0:840 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 02:19:17,122.122 dlcmzxjb7qmi93pp-master-0:839 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 02:19:17,149.149 dlcmzxjb7qmi93pp-master-0:841 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 02:19:17,159.159 dlcmzxjb7qmi93pp-master-0:838 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:838:838 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:841:841 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:839:839 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:840:840 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5185), 0.5387493319080705, 0.8657858136300417, tensor(2.1752)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5185), 0.5387493319080705, 0.8657858136300417, tensor(2.1752)]
[tensor(-0.5185), 0.5387493319080705, 0.8657858136300417, tensor(2.1752)]
[tensor(-0.5185), 0.5387493319080705, 0.8657858136300417, tensor(2.1752)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5149), 0.5398182789951897, 0.8741307371349096, tensor(2.1842)]
[2023-01-16 02:29:57,834.834 dlcmzxjb7qmi93pp-master-0:915 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 02:29:58,449.449 dlcmzxjb7qmi93pp-master-0:982 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:29:58,449.449 dlcmzxjb7qmi93pp-master-0:981 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:29:58,530.530 dlcmzxjb7qmi93pp-master-0:983 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:29:58,540.540 dlcmzxjb7qmi93pp-master-0:980 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:30:00,337.337 dlcmzxjb7qmi93pp-master-0:981 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 02:30:00,340.340 dlcmzxjb7qmi93pp-master-0:982 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 02:30:00,807.807 dlcmzxjb7qmi93pp-master-0:983 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 02:30:00,812.812 dlcmzxjb7qmi93pp-master-0:980 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:980:980 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:982:982 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:981:981 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:983:983 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5522), 0.5152324959914484, 0.8379694019471489, tensor(2.0239)]
[tensor(-0.5522), 0.5152324959914484, 0.8553546592489569, tensor(2.0239)]
[tensor(-0.5258), 0.5323356493853554, 0.8574408901251739, tensor(2.1359)]
[tensor(-0.5093), 0.5606627471940139, 0.8574408901251739, tensor(2.2940)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5093), 0.5606627471940139, 0.8574408901251739, tensor(2.2940)]
[tensor(-0.5093), 0.5606627471940139, 0.8574408901251739, tensor(2.2940)]
[tensor(-0.5093), 0.5606627471940139, 0.8574408901251739, tensor(2.2940)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5093), 0.5606627471940139, 0.8574408901251739, tensor(2.2940)]
[tensor(-0.5093), 0.5606627471940139, 0.8574408901251739, tensor(2.2940)]
early stopping at 9
[2023-01-16 02:48:33,654.654 dlcmzxjb7qmi93pp-master-0:1070 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 02:48:34,285.285 dlcmzxjb7qmi93pp-master-0:1138 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:48:34,285.285 dlcmzxjb7qmi93pp-master-0:1135 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:48:34,285.285 dlcmzxjb7qmi93pp-master-0:1136 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:48:34,287.287 dlcmzxjb7qmi93pp-master-0:1137 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 02:48:35,221.221 dlcmzxjb7qmi93pp-master-0:1138 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 02:48:35,222.222 dlcmzxjb7qmi93pp-master-0:1136 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 02:48:35,223.223 dlcmzxjb7qmi93pp-master-0:1137 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 02:48:35,231.231 dlcmzxjb7qmi93pp-master-0:1135 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:1135:1135 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:1138:1138 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1136:1136 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1137:1137 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5423), 0.5285943345804383, 0.8581363004172462, tensor(2.1006)]
[tensor(-0.5415), 0.530197755211117, 0.8643949930458971, tensor(2.1095)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
[tensor(-0.5193), 0.5424906467129877, 0.8643949930458971, tensor(2.1931)]
early stopping at 10
[2023-01-16 03:09:03,576.576 dlcmzxjb7qmi93pp-master-0:1228 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 03:09:04,198.198 dlcmzxjb7qmi93pp-master-0:1295 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:09:04,200.200 dlcmzxjb7qmi93pp-master-0:1294 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:09:04,356.356 dlcmzxjb7qmi93pp-master-0:1293 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:09:04,432.432 dlcmzxjb7qmi93pp-master-0:1296 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:09:05,478.478 dlcmzxjb7qmi93pp-master-0:1294 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 03:09:05,589.589 dlcmzxjb7qmi93pp-master-0:1296 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 03:09:06,032.032 dlcmzxjb7qmi93pp-master-0:1295 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 03:09:06,041.041 dlcmzxjb7qmi93pp-master-0:1293 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:1293:1293 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:1295:1295 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1294:1294 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1296:1296 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5400), 0.5307322287546766, 0.8532684283727399, tensor(2.1136)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5147), 0.5574559059326564, 0.8532684283727399, tensor(2.2726)]
[Mon Jan 16 03:14:31 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5018), 0.5574559059326564, 0.8581363004172462, tensor(2.2726)]
[tensor(-0.5018), 0.5574559059326564, 0.8581363004172462, tensor(2.2726)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5018), 0.5574559059326564, 0.8595271210013908, tensor(2.2726)]
[2023-01-16 03:19:17,892.892 dlcmzxjb7qmi93pp-master-0:1370 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 03:19:18,513.513 dlcmzxjb7qmi93pp-master-0:1435 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:18,514.514 dlcmzxjb7qmi93pp-master-0:1437 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:18,666.666 dlcmzxjb7qmi93pp-master-0:1438 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:18,667.667 dlcmzxjb7qmi93pp-master-0:1436 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:19:19,373.373 dlcmzxjb7qmi93pp-master-0:1437 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 03:19:19,556.556 dlcmzxjb7qmi93pp-master-0:1438 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 03:19:19,557.557 dlcmzxjb7qmi93pp-master-0:1436 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 03:19:19,563.563 dlcmzxjb7qmi93pp-master-0:1435 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:1435:1435 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:1436:1436 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1438:1438 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1437:1437 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5252), 0.5296632816675575, 0.8435326842837274, tensor(2.1231)]
[tensor(-0.5134), 0.5424906467129877, 0.849095966620306, tensor(2.1991)]
[tensor(-0.5134), 0.5424906467129877, 0.8546592489568846, tensor(2.1991)]
[tensor(-0.5134), 0.5424906467129877, 0.8546592489568846, tensor(2.1991)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5134), 0.5424906467129877, 0.8546592489568846, tensor(2.1991)]
[2023-01-16 03:29:24,184.184 dlcmzxjb7qmi93pp-master-0:1513 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 03:29:24,811.811 dlcmzxjb7qmi93pp-master-0:1578 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:29:24,811.811 dlcmzxjb7qmi93pp-master-0:1580 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:29:24,811.811 dlcmzxjb7qmi93pp-master-0:1579 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:29:24,814.814 dlcmzxjb7qmi93pp-master-0:1581 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:29:25,777.777 dlcmzxjb7qmi93pp-master-0:1580 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 03:29:25,780.780 dlcmzxjb7qmi93pp-master-0:1579 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 03:29:25,784.784 dlcmzxjb7qmi93pp-master-0:1581 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 03:29:25,788.788 dlcmzxjb7qmi93pp-master-0:1578 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:1578:1578 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:1580:1580 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1579:1579 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1581:1581 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5280), 0.5360769641902726, 0.8553546592489569, tensor(2.1524)]
[tensor(-0.5101), 0.5435595938001069, 0.8602225312934632, tensor(2.2077)]
[Mon Jan 16 03:34:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5101), 0.5435595938001069, 0.8602225312934632, tensor(2.2077)]
[tensor(-0.5101), 0.5440940673436665, 0.8602225312934632, tensor(2.2077)]
[tensor(-0.5101), 0.5440940673436665, 0.8602225312934632, tensor(2.2077)]
[tensor(-0.5101), 0.5440940673436665, 0.8602225312934632, tensor(2.2077)]
[tensor(-0.5101), 0.5440940673436665, 0.8602225312934632, tensor(2.2077)]
[tensor(-0.5101), 0.5440940673436665, 0.8678720445062587, tensor(2.2077)]
[tensor(-0.5101), 0.5456974879743453, 0.8678720445062587, tensor(2.2113)]
[tensor(-0.5101), 0.5456974879743453, 0.8678720445062587, tensor(2.2113)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5101), 0.5456974879743453, 0.8678720445062587, tensor(2.2113)]
[Mon Jan 16 03:52:26 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5101), 0.5456974879743453, 0.8678720445062587, tensor(2.2113)]
[tensor(-0.5101), 0.5456974879743453, 0.8678720445062587, tensor(2.2113)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5101), 0.5456974879743453, 0.8678720445062587, tensor(2.2113)]
early stopping at 14
[2023-01-16 03:57:13,530.530 dlcmzxjb7qmi93pp-master-0:1681 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 03:57:14,151.151 dlcmzxjb7qmi93pp-master-0:1747 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:57:14,152.152 dlcmzxjb7qmi93pp-master-0:1746 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:57:14,233.233 dlcmzxjb7qmi93pp-master-0:1749 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:57:14,238.238 dlcmzxjb7qmi93pp-master-0:1748 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 03:57:15,025.025 dlcmzxjb7qmi93pp-master-0:1747 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 03:57:15,526.526 dlcmzxjb7qmi93pp-master-0:1749 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 03:57:15,528.528 dlcmzxjb7qmi93pp-master-0:1748 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 03:57:15,531.531 dlcmzxjb7qmi93pp-master-0:1746 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:1746:1746 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:1747:1747 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1749:1749 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1748:1748 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5273), 0.5350080171031534, 0.8504867872044506, tensor(2.1478)]
[tensor(-0.5241), 0.5440940673436665, 0.8539638386648123, tensor(2.1964)]
[tensor(-0.5241), 0.5440940673436665, 0.8539638386648123, tensor(2.1964)]
[tensor(-0.5093), 0.5483698556921432, 0.8539638386648123, tensor(2.2325)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5093), 0.5483698556921432, 0.8539638386648123, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8574408901251739, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8574408901251739, tensor(2.2325)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5093), 0.5483698556921432, 0.8574408901251739, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8574408901251739, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8574408901251739, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8574408901251739, tensor(2.2325)]
early stopping at 11
[2023-01-16 04:19:18,509.509 dlcmzxjb7qmi93pp-master-0:1842 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 04:19:19,117.117 dlcmzxjb7qmi93pp-master-0:1907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:19:19,124.124 dlcmzxjb7qmi93pp-master-0:1909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:19:19,199.199 dlcmzxjb7qmi93pp-master-0:1910 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:19:19,206.206 dlcmzxjb7qmi93pp-master-0:1908 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:19:20,028.028 dlcmzxjb7qmi93pp-master-0:1909 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 04:19:20,099.099 dlcmzxjb7qmi93pp-master-0:1910 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 04:19:20,111.111 dlcmzxjb7qmi93pp-master-0:1908 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 04:19:20,118.118 dlcmzxjb7qmi93pp-master-0:1907 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:1907:1907 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:1909:1909 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1908:1908 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:1910:1910 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5964), 0.47728487439871725, 0.8546592489568846, tensor(1.7900)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5373), 0.5312667022982362, 0.8623087621696801, tensor(2.1190)]
[tensor(-0.5277), 0.5376803848209514, 0.8623087621696801, tensor(2.1607)]
[tensor(-0.4998), 0.5510422234099412, 0.8643949930458971, tensor(2.2554)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.4998), 0.5521111704970604, 0.8643949930458971, tensor(2.2554)]
[2023-01-16 04:29:23,881.881 dlcmzxjb7qmi93pp-master-0:1984 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 04:29:24,502.502 dlcmzxjb7qmi93pp-master-0:2051 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:29:24,503.503 dlcmzxjb7qmi93pp-master-0:2050 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:29:24,649.649 dlcmzxjb7qmi93pp-master-0:2052 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:29:24,649.649 dlcmzxjb7qmi93pp-master-0:2049 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:29:25,512.512 dlcmzxjb7qmi93pp-master-0:2052 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 04:29:26,315.315 dlcmzxjb7qmi93pp-master-0:2051 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 04:29:26,362.362 dlcmzxjb7qmi93pp-master-0:2050 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 04:29:26,369.369 dlcmzxjb7qmi93pp-master-0:2049 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:2049:2049 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:2052:2052 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2051:2051 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2050:2050 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5897), 0.4917156600748263, 0.8358831710709318, tensor(1.8689)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5274), 0.5360769641902726, 0.8609179415855355, tensor(2.1530)]
[tensor(-0.5274), 0.5360769641902726, 0.8609179415855355, tensor(2.1530)]
[tensor(-0.5274), 0.5360769641902726, 0.8609179415855355, tensor(2.1530)]
[tensor(-0.5271), 0.5360769641902726, 0.8609179415855355, tensor(2.1530)]
[2023-01-16 04:39:58,244.244 dlcmzxjb7qmi93pp-master-0:2126 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 04:39:58,905.905 dlcmzxjb7qmi93pp-master-0:2191 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:39:58,906.906 dlcmzxjb7qmi93pp-master-0:2192 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:39:58,986.986 dlcmzxjb7qmi93pp-master-0:2194 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:39:58,992.992 dlcmzxjb7qmi93pp-master-0:2193 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:39:59,768.768 dlcmzxjb7qmi93pp-master-0:2192 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 04:40:00,269.269 dlcmzxjb7qmi93pp-master-0:2194 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 04:40:00,269.269 dlcmzxjb7qmi93pp-master-0:2193 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 04:40:00,275.275 dlcmzxjb7qmi93pp-master-0:2191 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:2191:2191 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:2193:2193 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2194:2194 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2192:2192 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5261), 0.5440940673436665, 0.8539638386648123, tensor(2.1944)]
[tensor(-0.5207), 0.5440940673436665, 0.8741307371349096, tensor(2.1944)]
[tensor(-0.5207), 0.5440940673436665, 0.8741307371349096, tensor(2.1944)]
[tensor(-0.5082), 0.5467664350614645, 0.8741307371349096, tensor(2.2257)]
[tensor(-0.5082), 0.5467664350614645, 0.8741307371349096, tensor(2.2257)]
[Mon Jan 16 04:51:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5082), 0.5467664350614645, 0.8741307371349096, tensor(2.2257)]
[tensor(-0.5082), 0.5467664350614645, 0.8741307371349096, tensor(2.2257)]
[tensor(-0.5082), 0.5467664350614645, 0.8741307371349096, tensor(2.2257)]
[tensor(-0.5082), 0.5467664350614645, 0.8741307371349096, tensor(2.2257)]
early stopping at 9
[2023-01-16 04:58:18,030.030 dlcmzxjb7qmi93pp-master-0:2281 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 04:58:18,638.638 dlcmzxjb7qmi93pp-master-0:2348 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:58:18,668.668 dlcmzxjb7qmi93pp-master-0:2347 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:58:18,737.737 dlcmzxjb7qmi93pp-master-0:2349 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:58:18,824.824 dlcmzxjb7qmi93pp-master-0:2346 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 04:58:20,473.473 dlcmzxjb7qmi93pp-master-0:2348 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 04:58:20,538.538 dlcmzxjb7qmi93pp-master-0:2349 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 04:58:20,943.943 dlcmzxjb7qmi93pp-master-0:2347 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 04:58:20,951.951 dlcmzxjb7qmi93pp-master-0:2346 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:2346:2346 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:2347:2347 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2349:2349 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2348:2348 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5408), 0.530197755211117, 0.8546592489568846, tensor(2.1102)]
[tensor(-0.5305), 0.538214858364511, 0.8595271210013908, tensor(2.1605)]
[tensor(-0.5305), 0.538214858364511, 0.8609179415855355, tensor(2.1605)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5171), 0.5462319615179049, 0.8609179415855355, tensor(2.2140)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5171), 0.5462319615179049, 0.8623087621696801, tensor(2.2140)]
[tensor(-0.5171), 0.5462319615179049, 0.8623087621696801, tensor(2.2140)]
[tensor(-0.5171), 0.5462319615179049, 0.8623087621696801, tensor(2.2140)]
[tensor(-0.5171), 0.5462319615179049, 0.8623087621696801, tensor(2.2140)]
[tensor(-0.5171), 0.5462319615179049, 0.8623087621696801, tensor(2.2140)]
[tensor(-0.5171), 0.5462319615179049, 0.8623087621696801, tensor(2.2140)]
early stopping at 10
[2023-01-16 05:18:30,918.918 dlcmzxjb7qmi93pp-master-0:2438 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 05:18:31,539.539 dlcmzxjb7qmi93pp-master-0:2505 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:18:31,569.569 dlcmzxjb7qmi93pp-master-0:2504 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:18:31,640.640 dlcmzxjb7qmi93pp-master-0:2506 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:18:31,726.726 dlcmzxjb7qmi93pp-master-0:2503 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:18:33,377.377 dlcmzxjb7qmi93pp-master-0:2505 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 05:18:33,485.485 dlcmzxjb7qmi93pp-master-0:2506 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 05:18:33,841.841 dlcmzxjb7qmi93pp-master-0:2504 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 05:18:33,843.843 dlcmzxjb7qmi93pp-master-0:2503 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:2503:2503 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:2506:2506 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2505:2505 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2504:2504 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5440), 0.5291288081239979, 0.8567454798331016, tensor(2.1016)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5432), 0.5291288081239979, 0.8567454798331016, tensor(2.1016)]
[tensor(-0.5432), 0.5291288081239979, 0.8567454798331016, tensor(2.1016)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5178), 0.5296632816675575, 0.8567454798331016, tensor(2.1305)]
[tensor(-0.5106), 0.538214858364511, 0.8581363004172462, tensor(2.1805)]
[2023-01-16 05:28:54,277.277 dlcmzxjb7qmi93pp-master-0:2581 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 05:28:54,900.900 dlcmzxjb7qmi93pp-master-0:2648 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:28:54,938.938 dlcmzxjb7qmi93pp-master-0:2647 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:28:55,056.056 dlcmzxjb7qmi93pp-master-0:2646 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:28:55,147.147 dlcmzxjb7qmi93pp-master-0:2649 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:28:56,362.362 dlcmzxjb7qmi93pp-master-0:2649 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 05:28:56,717.717 dlcmzxjb7qmi93pp-master-0:2648 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 05:28:56,746.746 dlcmzxjb7qmi93pp-master-0:2647 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 05:28:56,751.751 dlcmzxjb7qmi93pp-master-0:2646 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:2646:2646 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:2647:2647 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2648:2648 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2649:2649 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5211), 0.5430251202565473, 0.8560500695410292, tensor(2.1940)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5211), 0.5430251202565473, 0.8581363004172462, tensor(2.1940)]
[tensor(-0.5211), 0.5435595938001069, 0.8581363004172462, tensor(2.1940)]
[tensor(-0.5211), 0.5435595938001069, 0.8595271210013908, tensor(2.1940)]
[tensor(-0.5139), 0.5435595938001069, 0.8595271210013908, tensor(2.1940)]
[2023-01-16 05:39:13,611.611 dlcmzxjb7qmi93pp-master-0:2723 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 05:39:14,223.223 dlcmzxjb7qmi93pp-master-0:2790 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:39:14,224.224 dlcmzxjb7qmi93pp-master-0:2788 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:39:14,305.305 dlcmzxjb7qmi93pp-master-0:2789 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:39:14,311.311 dlcmzxjb7qmi93pp-master-0:2791 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:39:15,388.388 dlcmzxjb7qmi93pp-master-0:2789 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 05:39:15,389.389 dlcmzxjb7qmi93pp-master-0:2791 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 05:39:16,073.073 dlcmzxjb7qmi93pp-master-0:2790 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 05:39:16,080.080 dlcmzxjb7qmi93pp-master-0:2788 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:2788:2788 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:2789:2789 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2790:2790 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2791:2791 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5301), 0.532870122928915, 0.8609179415855355, tensor(2.1343)]
[tensor(-0.5249), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5249), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5147), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
[tensor(-0.5147), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5147), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
[tensor(-0.5147), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
[tensor(-0.5147), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
[tensor(-0.5147), 0.5462319615179049, 0.8678720445062587, tensor(2.2062)]
early stopping at 9
[2023-01-16 05:58:24,436.436 dlcmzxjb7qmi93pp-master-0:2879 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 05:58:25,047.047 dlcmzxjb7qmi93pp-master-0:2946 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:58:25,054.054 dlcmzxjb7qmi93pp-master-0:2945 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:58:25,236.236 dlcmzxjb7qmi93pp-master-0:2944 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:58:25,312.312 dlcmzxjb7qmi93pp-master-0:2947 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 05:58:26,292.292 dlcmzxjb7qmi93pp-master-0:2945 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 05:58:26,461.461 dlcmzxjb7qmi93pp-master-0:2947 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 05:58:26,860.860 dlcmzxjb7qmi93pp-master-0:2946 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 05:58:26,863.863 dlcmzxjb7qmi93pp-master-0:2944 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:2944:2944 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:2947:2947 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2945:2945 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:2946:2946 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5331), 0.5312667022982362, 0.8567454798331016, tensor(2.1232)]
[tensor(-0.5331), 0.5312667022982362, 0.8567454798331016, tensor(2.1232)]
[tensor(-0.5125), 0.5435595938001069, 0.8616133518776078, tensor(2.2053)]
[tensor(-0.5125), 0.5435595938001069, 0.8630041724617524, tensor(2.2053)]
[tensor(-0.5125), 0.5435595938001069, 0.8630041724617524, tensor(2.2053)]
[tensor(-0.5125), 0.5435595938001069, 0.8630041724617524, tensor(2.2053)]
[tensor(-0.5125), 0.5435595938001069, 0.8630041724617524, tensor(2.2053)]
[tensor(-0.5125), 0.5435595938001069, 0.8630041724617524, tensor(2.2053)]
[tensor(-0.5125), 0.5435595938001069, 0.8630041724617524, tensor(2.2053)]
early stopping at 9
[2023-01-16 06:17:28,249.249 dlcmzxjb7qmi93pp-master-0:3035 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 06:17:28,864.864 dlcmzxjb7qmi93pp-master-0:3100 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:17:28,865.865 dlcmzxjb7qmi93pp-master-0:3102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:17:28,942.942 dlcmzxjb7qmi93pp-master-0:3103 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:17:28,948.948 dlcmzxjb7qmi93pp-master-0:3101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:17:29,719.719 dlcmzxjb7qmi93pp-master-0:3102 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 06:17:30,264.264 dlcmzxjb7qmi93pp-master-0:3103 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 06:17:30,267.267 dlcmzxjb7qmi93pp-master-0:3101 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 06:17:30,273.273 dlcmzxjb7qmi93pp-master-0:3100 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:3100:3100 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:3101:3101 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3102:3102 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3103:3103 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.6382), 0.4318546231961518, 0.8386648122392212, tensor(1.5211)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5488), 0.5232495991448424, 0.8428372739916551, tensor(2.0675)]
[tensor(-0.5072), 0.5424906467129877, 0.8657858136300417, tensor(2.2053)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5072), 0.5424906467129877, 0.8657858136300417, tensor(2.2053)]
[tensor(-0.5072), 0.5424906467129877, 0.8657858136300417, tensor(2.2053)]
[2023-01-16 06:27:35,546.546 dlcmzxjb7qmi93pp-master-0:3177 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 06:27:36,158.158 dlcmzxjb7qmi93pp-master-0:3243 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:27:36,159.159 dlcmzxjb7qmi93pp-master-0:3245 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:27:36,237.237 dlcmzxjb7qmi93pp-master-0:3242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:27:36,248.248 dlcmzxjb7qmi93pp-master-0:3244 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:27:37,582.582 dlcmzxjb7qmi93pp-master-0:3244 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 06:27:38,031.031 dlcmzxjb7qmi93pp-master-0:3245 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 06:27:38,037.037 dlcmzxjb7qmi93pp-master-0:3243 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 06:27:38,047.047 dlcmzxjb7qmi93pp-master-0:3242 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:3242:3242 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:3244:3244 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3243:3243 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3245:3245 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5276), 0.538214858364511, 0.8581363004172462, tensor(2.1635)]
[tensor(-0.5185), 0.538214858364511, 0.8706536856745479, tensor(2.1635)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5067), 0.538214858364511, 0.8706536856745479, tensor(2.1763)]
[tensor(-0.5067), 0.538214858364511, 0.8706536856745479, tensor(2.1763)]
[tensor(-0.5067), 0.538214858364511, 0.8706536856745479, tensor(2.1763)]
[2023-01-16 06:38:00,887.887 dlcmzxjb7qmi93pp-master-0:3319 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 06:38:01,513.513 dlcmzxjb7qmi93pp-master-0:3386 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:38:01,513.513 dlcmzxjb7qmi93pp-master-0:3385 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:38:01,713.713 dlcmzxjb7qmi93pp-master-0:3384 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:38:01,764.764 dlcmzxjb7qmi93pp-master-0:3387 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:38:02,886.886 dlcmzxjb7qmi93pp-master-0:3385 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 06:38:02,930.930 dlcmzxjb7qmi93pp-master-0:3387 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 06:38:03,330.330 dlcmzxjb7qmi93pp-master-0:3386 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 06:38:03,332.332 dlcmzxjb7qmi93pp-master-0:3384 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:3384:3384 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:3386:3386 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3385:3385 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3387:3387 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5432), 0.5205772314270444, 0.8560500695410292, tensor(2.0596)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5311), 0.535542490646713, 0.8567454798331016, tensor(2.1466)]
[tensor(-0.5229), 0.5371459112773918, 0.8602225312934632, tensor(2.1628)]
[tensor(-0.5188), 0.5371459112773918, 0.8602225312934632, tensor(2.1628)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5100), 0.5408872260823089, 0.8643949930458971, tensor(2.1945)]
[tensor(-0.5100), 0.5408872260823089, 0.8643949930458971, tensor(2.1945)]
[tensor(-0.5100), 0.5408872260823089, 0.8643949930458971, tensor(2.1945)]
[tensor(-0.5100), 0.5408872260823089, 0.8643949930458971, tensor(2.1945)]
[tensor(-0.5100), 0.5408872260823089, 0.8643949930458971, tensor(2.1945)]
[tensor(-0.5100), 0.5408872260823089, 0.8643949930458971, tensor(2.1945)]
early stopping at 10
[2023-01-16 06:58:09,798.798 dlcmzxjb7qmi93pp-master-0:3477 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 06:58:10,411.411 dlcmzxjb7qmi93pp-master-0:3545 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:58:10,411.411 dlcmzxjb7qmi93pp-master-0:3542 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:58:10,489.489 dlcmzxjb7qmi93pp-master-0:3544 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:58:10,506.506 dlcmzxjb7qmi93pp-master-0:3543 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 06:58:11,270.270 dlcmzxjb7qmi93pp-master-0:3545 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 06:58:11,794.794 dlcmzxjb7qmi93pp-master-0:3544 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 06:58:11,795.795 dlcmzxjb7qmi93pp-master-0:3543 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 06:58:11,804.804 dlcmzxjb7qmi93pp-master-0:3542 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:3542:3542 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:3545:3545 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3544:3544 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3543:3543 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5286), 0.5435595938001069, 0.8567454798331016, tensor(2.1891)]
[tensor(-0.5281), 0.5435595938001069, 0.8671766342141863, tensor(2.1891)]
[tensor(-0.5235), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
[tensor(-0.5235), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
[tensor(-0.5207), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
[tensor(-0.5207), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
[tensor(-0.5207), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
[tensor(-0.5207), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
[tensor(-0.5207), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
[tensor(-0.5207), 0.5435595938001069, 0.868567454798331, tensor(2.1891)]
early stopping at 10
[2023-01-16 07:18:10,671.671 dlcmzxjb7qmi93pp-master-0:3634 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 07:18:11,284.284 dlcmzxjb7qmi93pp-master-0:3701 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:18:11,304.304 dlcmzxjb7qmi93pp-master-0:3702 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:18:11,367.367 dlcmzxjb7qmi93pp-master-0:3699 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:18:11,376.376 dlcmzxjb7qmi93pp-master-0:3700 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:18:13,095.095 dlcmzxjb7qmi93pp-master-0:3701 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 07:18:13,134.134 dlcmzxjb7qmi93pp-master-0:3702 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 07:18:13,669.669 dlcmzxjb7qmi93pp-master-0:3700 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 07:18:13,672.672 dlcmzxjb7qmi93pp-master-0:3699 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:3699:3699 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:3701:3701 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3700:3700 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3702:3702 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5560), 0.5216461785141635, 0.8365785813630042, tensor(2.0522)]
[tensor(-0.5534), 0.5216461785141635, 0.8560500695410292, tensor(2.0522)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5534), 0.5216461785141635, 0.8560500695410292, tensor(2.0522)]
[tensor(-0.5059), 0.5467664350614645, 0.8609179415855355, tensor(2.2280)]
[tensor(-0.5059), 0.5467664350614645, 0.8609179415855355, tensor(2.2280)]
[2023-01-16 07:28:48,002.002 dlcmzxjb7qmi93pp-master-0:3777 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 07:28:48,609.609 dlcmzxjb7qmi93pp-master-0:3842 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:28:48,610.610 dlcmzxjb7qmi93pp-master-0:3844 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:28:48,610.610 dlcmzxjb7qmi93pp-master-0:3845 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:28:48,612.612 dlcmzxjb7qmi93pp-master-0:3843 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:28:49,549.549 dlcmzxjb7qmi93pp-master-0:3844 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 07:28:50,540.540 dlcmzxjb7qmi93pp-master-0:3843 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 07:28:50,546.546 dlcmzxjb7qmi93pp-master-0:3845 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 07:28:50,550.550 dlcmzxjb7qmi93pp-master-0:3842 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:3842:3842 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:3844:3844 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3845:3845 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3843:3843 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5245), 0.5318011758417959, 0.8553546592489569, tensor(2.1345)]
[tensor(-0.5116), 0.5435595938001069, 0.8706536856745479, tensor(2.2062)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5116), 0.5435595938001069, 0.8706536856745479, tensor(2.2062)]
[tensor(-0.5116), 0.5435595938001069, 0.8706536856745479, tensor(2.2062)]
[tensor(-0.5060), 0.5435595938001069, 0.8706536856745479, tensor(2.2062)]
[2023-01-16 07:39:17,307.307 dlcmzxjb7qmi93pp-master-0:3920 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 07:39:17,933.933 dlcmzxjb7qmi93pp-master-0:3985 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:39:17,990.990 dlcmzxjb7qmi93pp-master-0:3986 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:39:18,017.017 dlcmzxjb7qmi93pp-master-0:3987 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:39:18,019.019 dlcmzxjb7qmi93pp-master-0:3988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 07:39:18,826.826 dlcmzxjb7qmi93pp-master-0:3986 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 07:39:19,293.293 dlcmzxjb7qmi93pp-master-0:3987 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 07:39:19,295.295 dlcmzxjb7qmi93pp-master-0:3988 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 07:39:19,295.295 dlcmzxjb7qmi93pp-master-0:3985 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:3985:3985 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:3986:3986 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3987:3987 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:3988:3988 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5536), 0.518439337252806, 0.8351877607788595, tensor(2.0386)]
[tensor(-0.5269), 0.530197755211117, 0.8477051460361613, tensor(2.1241)]
[tensor(-0.5165), 0.5403527525387494, 0.866481223922114, tensor(2.1852)]
[tensor(-0.5079), 0.5451630144307856, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5079), 0.5451630144307856, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5079), 0.5451630144307856, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5079), 0.5505077498663816, 0.866481223922114, tensor(2.2390)]
[tensor(-0.5079), 0.5505077498663816, 0.866481223922114, tensor(2.2390)]
[tensor(-0.5079), 0.5505077498663816, 0.866481223922114, tensor(2.2390)]
[tensor(-0.5079), 0.5505077498663816, 0.866481223922114, tensor(2.2390)]
[tensor(-0.5079), 0.5505077498663816, 0.866481223922114, tensor(2.2390)]
[tensor(-0.5079), 0.5505077498663816, 0.866481223922114, tensor(2.2390)]
early stopping at 12
[2023-01-16 08:03:20,398.398 dlcmzxjb7qmi93pp-master-0:4083 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 08:03:21,017.017 dlcmzxjb7qmi93pp-master-0:4150 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:03:21,050.050 dlcmzxjb7qmi93pp-master-0:4149 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:03:21,128.128 dlcmzxjb7qmi93pp-master-0:4148 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:03:21,199.199 dlcmzxjb7qmi93pp-master-0:4151 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:03:22,303.303 dlcmzxjb7qmi93pp-master-0:4149 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 08:03:22,397.397 dlcmzxjb7qmi93pp-master-0:4151 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 08:03:22,839.839 dlcmzxjb7qmi93pp-master-0:4150 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 08:03:22,843.843 dlcmzxjb7qmi93pp-master-0:4148 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:4148:4148 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:4150:4150 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4151:4151 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4149:4149 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5352), 0.5371459112773918, 0.8456189151599444, tensor(2.1505)]
[tensor(-0.5352), 0.5424906467129877, 0.8553546592489569, tensor(2.1727)]
[tensor(-0.5015), 0.5446285408872261, 0.8553546592489569, tensor(2.2217)]
[tensor(-0.5015), 0.5446285408872261, 0.8581363004172462, tensor(2.2217)]
[tensor(-0.5015), 0.5446285408872261, 0.8602225312934632, tensor(2.2217)]
[tensor(-0.5015), 0.5446285408872261, 0.8602225312934632, tensor(2.2217)]
[tensor(-0.5015), 0.5456974879743453, 0.8602225312934632, tensor(2.2217)]
[tensor(-0.5015), 0.5494388027792624, 0.8602225312934632, tensor(2.2338)]
[tensor(-0.5015), 0.5494388027792624, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.5494388027792624, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.5494388027792624, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.5494388027792624, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.5494388027792624, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.549973276322822, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.549973276322822, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.549973276322822, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.549973276322822, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.549973276322822, 0.8609179415855355, tensor(2.2338)]
[tensor(-0.5015), 0.549973276322822, 0.8609179415855355, tensor(2.2338)]
early stopping at 19
[2023-01-16 08:41:02,301.301 dlcmzxjb7qmi93pp-master-0:4266 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 08:41:02,916.916 dlcmzxjb7qmi93pp-master-0:4333 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:41:02,919.919 dlcmzxjb7qmi93pp-master-0:4332 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:41:02,998.998 dlcmzxjb7qmi93pp-master-0:4334 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:41:03,010.010 dlcmzxjb7qmi93pp-master-0:4331 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:41:04,737.737 dlcmzxjb7qmi93pp-master-0:4332 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 08:41:04,881.881 dlcmzxjb7qmi93pp-master-0:4333 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 08:41:05,285.285 dlcmzxjb7qmi93pp-master-0:4334 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 08:41:05,289.289 dlcmzxjb7qmi93pp-master-0:4331 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.bias', 'selection_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:4331:4331 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:4332:4332 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4333:4333 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4334:4334 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5993), 0.4917156600748263, 0.8595271210013908, tensor(1.8593)]
[tensor(-0.5189), 0.5505077498663816, 0.8657858136300417, tensor(2.2336)]
[tensor(-0.5180), 0.5505077498663816, 0.8657858136300417, tensor(2.2336)]
[tensor(-0.5065), 0.5505077498663816, 0.8706536856745479, tensor(2.2461)]
[tensor(-0.5065), 0.5505077498663816, 0.8706536856745479, tensor(2.2461)]
[2023-01-16 08:51:28,656.656 dlcmzxjb7qmi93pp-master-0:4409 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 08:51:29,273.273 dlcmzxjb7qmi93pp-master-0:4476 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:51:29,314.314 dlcmzxjb7qmi93pp-master-0:4475 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:51:29,364.364 dlcmzxjb7qmi93pp-master-0:4477 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:51:29,438.438 dlcmzxjb7qmi93pp-master-0:4474 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 08:51:31,128.128 dlcmzxjb7qmi93pp-master-0:4476 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 08:51:31,250.250 dlcmzxjb7qmi93pp-master-0:4477 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 08:51:31,579.579 dlcmzxjb7qmi93pp-master-0:4475 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 08:51:31,589.589 dlcmzxjb7qmi93pp-master-0:4474 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:4474:4474 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:4477:4477 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4476:4476 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4475:4475 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5362), 0.535542490646713, 0.8630041724617524, tensor(2.1415)]
[tensor(-0.5157), 0.5414216996258685, 0.8678720445062587, tensor(2.1914)]
[tensor(-0.5107), 0.5462319615179049, 0.8678720445062587, tensor(2.2205)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.5107), 0.5462319615179049, 0.8678720445062587, tensor(2.2205)]
[tensor(-0.5107), 0.5547835382148584, 0.8699582753824756, tensor(2.2630)]
[2023-01-16 09:01:52,973.973 dlcmzxjb7qmi93pp-master-0:4551 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 09:01:53,587.587 dlcmzxjb7qmi93pp-master-0:4616 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:01:53,587.587 dlcmzxjb7qmi93pp-master-0:4619 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:01:53,667.667 dlcmzxjb7qmi93pp-master-0:4617 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:01:53,672.672 dlcmzxjb7qmi93pp-master-0:4618 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:01:54,522.522 dlcmzxjb7qmi93pp-master-0:4619 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 09:01:54,927.927 dlcmzxjb7qmi93pp-master-0:4618 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 09:01:54,929.929 dlcmzxjb7qmi93pp-master-0:4617 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 09:01:54,936.936 dlcmzxjb7qmi93pp-master-0:4616 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:4616:4616 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:4618:4618 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4617:4617 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4619:4619 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5335), 0.5318011758417959, 0.8379694019471489, tensor(2.1255)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5335), 0.5318011758417959, 0.8546592489568846, tensor(2.1255)]
[tensor(-0.5250), 0.5408872260823089, 0.8616133518776078, tensor(2.1794)]
[tensor(-0.5087), 0.5456974879743453, 0.8616133518776078, tensor(2.2198)]
[tensor(-0.5087), 0.5456974879743453, 0.8616133518776078, tensor(2.2198)]
[tensor(-0.5087), 0.5478353821485836, 0.8616133518776078, tensor(2.2198)]
[tensor(-0.5087), 0.5478353821485836, 0.8616133518776078, tensor(2.2198)]
[tensor(-0.5087), 0.5478353821485836, 0.8616133518776078, tensor(2.2198)]
[tensor(-0.5087), 0.5478353821485836, 0.8616133518776078, tensor(2.2198)]
[tensor(-0.5087), 0.5478353821485836, 0.8616133518776078, tensor(2.2198)]
[tensor(-0.5087), 0.5478353821485836, 0.8616133518776078, tensor(2.2198)]
early stopping at 11
[2023-01-16 09:23:53,991.991 dlcmzxjb7qmi93pp-master-0:4711 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 09:23:54,607.607 dlcmzxjb7qmi93pp-master-0:4776 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:23:54,607.607 dlcmzxjb7qmi93pp-master-0:4779 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:23:54,607.607 dlcmzxjb7qmi93pp-master-0:4777 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:23:54,608.608 dlcmzxjb7qmi93pp-master-0:4778 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:23:55,682.682 dlcmzxjb7qmi93pp-master-0:4777 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 09:23:55,686.686 dlcmzxjb7qmi93pp-master-0:4779 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 09:23:55,696.696 dlcmzxjb7qmi93pp-master-0:4778 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 09:23:55,698.698 dlcmzxjb7qmi93pp-master-0:4776 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:4776:4776 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:4778:4778 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4779:4779 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4777:4777 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5305), 0.5505077498663816, 0.8560500695410292, tensor(2.2220)]
[tensor(-0.5273), 0.5505077498663816, 0.8720445062586927, tensor(2.2220)]
[tensor(-0.5040), 0.5574559059326564, 0.8720445062586927, tensor(2.2832)]
[tensor(-0.5040), 0.5574559059326564, 0.8720445062586927, tensor(2.2832)]
[tensor(-0.5040), 0.5574559059326564, 0.8720445062586927, tensor(2.2832)]
[tensor(-0.5040), 0.5574559059326564, 0.8720445062586927, tensor(2.2832)]
[tensor(-0.5040), 0.5574559059326564, 0.8720445062586927, tensor(2.2832)]
[tensor(-0.5040), 0.5574559059326564, 0.8720445062586927, tensor(2.2832)]
early stopping at 8
[2023-01-16 09:39:49,674.674 dlcmzxjb7qmi93pp-master-0:4862 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 09:39:50,332.332 dlcmzxjb7qmi93pp-master-0:4927 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:39:50,366.366 dlcmzxjb7qmi93pp-master-0:4929 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:39:50,414.414 dlcmzxjb7qmi93pp-master-0:4930 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:39:50,550.550 dlcmzxjb7qmi93pp-master-0:4928 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:39:51,351.351 dlcmzxjb7qmi93pp-master-0:4928 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 09:39:51,634.634 dlcmzxjb7qmi93pp-master-0:4929 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 09:39:51,669.669 dlcmzxjb7qmi93pp-master-0:4930 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 09:39:51,673.673 dlcmzxjb7qmi93pp-master-0:4927 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:4927:4927 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:4928:4928 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4930:4930 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:4929:4929 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5150), 0.5446285408872261, 0.8532684283727399, tensor(2.2082)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5150), 0.5446285408872261, 0.8581363004172462, tensor(2.2082)]
[Mon Jan 16 09:45:17 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5133), 0.5446285408872261, 0.8581363004172462, tensor(2.2082)]
[tensor(-0.5007), 0.5483698556921432, 0.8581363004172462, tensor(2.2412)]
[tensor(-0.5007), 0.5585248530197755, 0.8581363004172462, tensor(2.2842)]
[2023-01-16 09:50:04,020.020 dlcmzxjb7qmi93pp-master-0:5004 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 09:50:04,636.636 dlcmzxjb7qmi93pp-master-0:5072 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:50:04,636.636 dlcmzxjb7qmi93pp-master-0:5071 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:50:04,822.822 dlcmzxjb7qmi93pp-master-0:5073 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:50:04,892.892 dlcmzxjb7qmi93pp-master-0:5070 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 09:50:06,495.495 dlcmzxjb7qmi93pp-master-0:5072 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 09:50:06,643.643 dlcmzxjb7qmi93pp-master-0:5073 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 09:50:06,889.889 dlcmzxjb7qmi93pp-master-0:5071 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 09:50:06,896.896 dlcmzxjb7qmi93pp-master-0:5070 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:5070:5070 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:5072:5072 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5071:5071 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5073:5073 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5224), 0.5318011758417959, 0.8546592489568846, tensor(2.1366)]
[tensor(-0.5099), 0.5430251202565473, 0.8546592489568846, tensor(2.2052)]
[tensor(-0.5086), 0.5505077498663816, 0.8616133518776078, tensor(2.2439)]
[tensor(-0.5086), 0.5505077498663816, 0.8616133518776078, tensor(2.2439)]
[tensor(-0.5082), 0.5505077498663816, 0.8616133518776078, tensor(2.2439)]
[2023-01-16 10:00:29,377.377 dlcmzxjb7qmi93pp-master-0:5147 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 10:00:30,003.003 dlcmzxjb7qmi93pp-master-0:5212 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:00:30,003.003 dlcmzxjb7qmi93pp-master-0:5215 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:00:30,030.030 dlcmzxjb7qmi93pp-master-0:5214 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:00:30,295.295 dlcmzxjb7qmi93pp-master-0:5213 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:00:31,411.411 dlcmzxjb7qmi93pp-master-0:5213 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 10:00:31,822.822 dlcmzxjb7qmi93pp-master-0:5215 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 10:00:31,859.859 dlcmzxjb7qmi93pp-master-0:5214 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 10:00:31,865.865 dlcmzxjb7qmi93pp-master-0:5212 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:5212:5212 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:5215:5215 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5213:5213 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5214:5214 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5284), 0.5371459112773918, 0.8525730180806675, tensor(2.1574)]
[tensor(-0.5139), 0.5483698556921432, 0.8539638386648123, tensor(2.2280)]
[Mon Jan 16 10:05:52 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5135), 0.5505077498663816, 0.8602225312934632, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8609179415855355, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8616133518776078, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8616133518776078, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8616133518776078, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8616133518776078, tensor(2.2390)]
[tensor(-0.5135), 0.5505077498663816, 0.8616133518776078, tensor(2.2390)]
[Mon Jan 16 10:33:57 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5135), 0.5505077498663816, 0.8616133518776078, tensor(2.2390)]
early stopping at 17
[2023-01-16 10:35:30,176.176 dlcmzxjb7qmi93pp-master-0:5327 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 10:35:30,792.792 dlcmzxjb7qmi93pp-master-0:5395 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:35:30,811.811 dlcmzxjb7qmi93pp-master-0:5393 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:35:30,877.877 dlcmzxjb7qmi93pp-master-0:5392 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:35:30,882.882 dlcmzxjb7qmi93pp-master-0:5394 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:35:32,167.167 dlcmzxjb7qmi93pp-master-0:5394 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 10:35:32,601.601 dlcmzxjb7qmi93pp-master-0:5395 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 10:35:32,629.629 dlcmzxjb7qmi93pp-master-0:5393 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 10:35:32,631.631 dlcmzxjb7qmi93pp-master-0:5392 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:5392:5392 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:5395:5395 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5394:5394 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5393:5393 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5446), 0.518439337252806, 0.8463143254520167, tensor(2.0476)]
[tensor(-0.5207), 0.5360769641902726, 0.8623087621696801, tensor(2.1597)]
[tensor(-0.5182), 0.5456974879743453, 0.8623087621696801, tensor(2.2102)]
[tensor(-0.5075), 0.5590593265633351, 0.8623087621696801, tensor(2.2878)]
[tensor(-0.5075), 0.5590593265633351, 0.8623087621696801, tensor(2.2878)]
[tensor(-0.5075), 0.5590593265633351, 0.8623087621696801, tensor(2.2878)]
[tensor(-0.5075), 0.5590593265633351, 0.8623087621696801, tensor(2.2878)]
[tensor(-0.5075), 0.5590593265633351, 0.8623087621696801, tensor(2.2878)]
[tensor(-0.5075), 0.5590593265633351, 0.8623087621696801, tensor(2.2878)]
early stopping at 9
[2023-01-16 10:53:26,998.998 dlcmzxjb7qmi93pp-master-0:5481 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 10:53:27,622.622 dlcmzxjb7qmi93pp-master-0:5548 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:53:27,652.652 dlcmzxjb7qmi93pp-master-0:5547 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:53:27,735.735 dlcmzxjb7qmi93pp-master-0:5546 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:53:27,809.809 dlcmzxjb7qmi93pp-master-0:5549 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 10:53:28,919.919 dlcmzxjb7qmi93pp-master-0:5547 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 10:53:28,998.998 dlcmzxjb7qmi93pp-master-0:5549 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 10:53:29,439.439 dlcmzxjb7qmi93pp-master-0:5548 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 10:53:29,440.440 dlcmzxjb7qmi93pp-master-0:5546 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:5546:5546 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:5548:5548 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5549:5549 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5547:5547 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5436), 0.5120256547300909, 0.8358831710709318, tensor(2.0165)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5119), 0.5424906467129877, 0.8532684283727399, tensor(2.2006)]
[tensor(-0.5119), 0.5424906467129877, 0.8602225312934632, tensor(2.2006)]
[tensor(-0.5119), 0.5424906467129877, 0.8650904033379694, tensor(2.2006)]
[Mon Jan 16 11:01:44 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5119), 0.5424906467129877, 0.8692628650904033, tensor(2.2006)]
[2023-01-16 11:03:43,355.355 dlcmzxjb7qmi93pp-master-0:5623 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 11:03:43,961.961 dlcmzxjb7qmi93pp-master-0:5688 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:03:43,962.962 dlcmzxjb7qmi93pp-master-0:5689 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:03:44,047.047 dlcmzxjb7qmi93pp-master-0:5690 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:03:44,055.055 dlcmzxjb7qmi93pp-master-0:5691 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:03:45,328.328 dlcmzxjb7qmi93pp-master-0:5690 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 11:03:45,329.329 dlcmzxjb7qmi93pp-master-0:5691 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 11:03:45,840.840 dlcmzxjb7qmi93pp-master-0:5689 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 11:03:45,845.845 dlcmzxjb7qmi93pp-master-0:5688 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:5688:5688 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:5691:5691 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5689:5689 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5690:5690 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5603), 0.5195082843399251, 0.8602225312934632, tensor(2.0373)]
[tensor(-0.5462), 0.5248530197755211, 0.8616133518776078, tensor(2.0780)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5004), 0.549973276322822, 0.8616133518776078, tensor(2.2495)]
[tensor(-0.5004), 0.549973276322822, 0.866481223922114, tensor(2.2495)]
[tensor(-0.5004), 0.549973276322822, 0.866481223922114, tensor(2.2495)]
[2023-01-16 11:13:50,689.689 dlcmzxjb7qmi93pp-master-0:5765 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 11:13:51,307.307 dlcmzxjb7qmi93pp-master-0:5831 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:13:51,307.307 dlcmzxjb7qmi93pp-master-0:5832 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:13:51,502.502 dlcmzxjb7qmi93pp-master-0:5830 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:13:51,567.567 dlcmzxjb7qmi93pp-master-0:5833 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:13:52,565.565 dlcmzxjb7qmi93pp-master-0:5831 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 11:13:52,760.760 dlcmzxjb7qmi93pp-master-0:5833 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 11:13:53,242.242 dlcmzxjb7qmi93pp-master-0:5832 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 11:13:53,245.245 dlcmzxjb7qmi93pp-master-0:5830 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:5830:5830 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:5833:5833 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5831:5831 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5832:5832 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5352), 0.5318011758417959, 0.8497913769123783, tensor(2.1238)]
[tensor(-0.5158), 0.5387493319080705, 0.8616133518776078, tensor(2.1779)]
[tensor(-0.5085), 0.5430251202565473, 0.8616133518776078, tensor(2.2066)]
[tensor(-0.5085), 0.5462319615179049, 0.8616133518776078, tensor(2.2207)]
[tensor(-0.5085), 0.5462319615179049, 0.8636995827538247, tensor(2.2207)]
[Mon Jan 16 11:25:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5085), 0.5462319615179049, 0.8636995827538247, tensor(2.2207)]
[tensor(-0.5085), 0.5462319615179049, 0.8636995827538247, tensor(2.2207)]
[tensor(-0.5085), 0.5462319615179049, 0.8636995827538247, tensor(2.2207)]
[tensor(-0.5085), 0.5462319615179049, 0.8636995827538247, tensor(2.2207)]
[tensor(-0.5085), 0.5462319615179049, 0.8636995827538247, tensor(2.2207)]
early stopping at 10
[2023-01-16 11:34:22,631.631 dlcmzxjb7qmi93pp-master-0:5923 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 11:34:23,256.256 dlcmzxjb7qmi93pp-master-0:5991 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:23,256.256 dlcmzxjb7qmi93pp-master-0:5989 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:23,257.257 dlcmzxjb7qmi93pp-master-0:5988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:23,264.264 dlcmzxjb7qmi93pp-master-0:5990 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:34:24,261.261 dlcmzxjb7qmi93pp-master-0:5991 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 11:34:24,263.263 dlcmzxjb7qmi93pp-master-0:5989 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 11:34:24,265.265 dlcmzxjb7qmi93pp-master-0:5990 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 11:34:24,269.269 dlcmzxjb7qmi93pp-master-0:5988 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:5988:5988 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:5989:5989 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5991:5991 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:5990:5990 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5314), 0.535542490646713, 0.8588317107093185, tensor(2.1463)]
[tensor(-0.5268), 0.535542490646713, 0.8602225312934632, tensor(2.1463)]
[tensor(-0.5268), 0.535542490646713, 0.8602225312934632, tensor(2.1463)]
[tensor(-0.5062), 0.5430251202565473, 0.8602225312934632, tensor(2.2090)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5062), 0.5430251202565473, 0.8602225312934632, tensor(2.2090)]
[tensor(-0.5062), 0.5430251202565473, 0.8602225312934632, tensor(2.2090)]
[tensor(-0.5062), 0.5494388027792624, 0.8602225312934632, tensor(2.2175)]
[tensor(-0.5062), 0.5494388027792624, 0.8602225312934632, tensor(2.2175)]
[tensor(-0.5062), 0.5494388027792624, 0.8602225312934632, tensor(2.2175)]
[tensor(-0.5062), 0.5494388027792624, 0.8602225312934632, tensor(2.2175)]
[tensor(-0.5062), 0.5494388027792624, 0.8602225312934632, tensor(2.2175)]
[tensor(-0.5062), 0.5494388027792624, 0.8602225312934632, tensor(2.2175)]
early stopping at 12
[2023-01-16 11:58:29,829.829 dlcmzxjb7qmi93pp-master-0:6086 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 11:58:30,457.457 dlcmzxjb7qmi93pp-master-0:6153 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:58:30,489.489 dlcmzxjb7qmi93pp-master-0:6152 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:58:30,540.540 dlcmzxjb7qmi93pp-master-0:6151 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:58:30,607.607 dlcmzxjb7qmi93pp-master-0:6154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 11:58:31,443.443 dlcmzxjb7qmi93pp-master-0:6152 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 11:58:31,573.573 dlcmzxjb7qmi93pp-master-0:6154 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 11:58:32,319.319 dlcmzxjb7qmi93pp-master-0:6153 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 11:58:32,321.321 dlcmzxjb7qmi93pp-master-0:6151 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:6151:6151 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:6152:6152 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6153:6153 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6154:6154 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5219), 0.5408872260823089, 0.8546592489568846, tensor(2.1826)]
[tensor(-0.5219), 0.5408872260823089, 0.8630041724617524, tensor(2.1826)]
[tensor(-0.5219), 0.5408872260823089, 0.8630041724617524, tensor(2.1826)]
[tensor(-0.5126), 0.5408872260823089, 0.8630041724617524, tensor(2.1826)]
[tensor(-0.5126), 0.5408872260823089, 0.8630041724617524, tensor(2.1826)]
[2023-01-16 12:08:42,198.198 dlcmzxjb7qmi93pp-master-0:6228 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 12:08:42,817.817 dlcmzxjb7qmi93pp-master-0:6294 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:08:42,818.818 dlcmzxjb7qmi93pp-master-0:6295 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:08:42,985.985 dlcmzxjb7qmi93pp-master-0:6296 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:08:42,987.987 dlcmzxjb7qmi93pp-master-0:6293 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:08:44,709.709 dlcmzxjb7qmi93pp-master-0:6295 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 12:08:44,713.713 dlcmzxjb7qmi93pp-master-0:6294 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 12:08:44,919.919 dlcmzxjb7qmi93pp-master-0:6296 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 12:08:44,922.922 dlcmzxjb7qmi93pp-master-0:6293 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:6293:6293 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:6296:6296 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6295:6295 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6294:6294 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5153), 0.5478353821485836, 0.8546592489568846, tensor(2.2238)]
[tensor(-0.5153), 0.5478353821485836, 0.8636995827538247, tensor(2.2238)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5053), 0.5547835382148584, 0.8643949930458971, tensor(2.2687)]
[tensor(-0.5053), 0.5547835382148584, 0.8643949930458971, tensor(2.2687)]
[tensor(-0.5053), 0.5547835382148584, 0.8643949930458971, tensor(2.2687)]
[2023-01-16 12:18:59,547.547 dlcmzxjb7qmi93pp-master-0:6371 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 12:19:00,164.164 dlcmzxjb7qmi93pp-master-0:6439 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:19:00,164.164 dlcmzxjb7qmi93pp-master-0:6437 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:19:00,164.164 dlcmzxjb7qmi93pp-master-0:6438 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:19:00,164.164 dlcmzxjb7qmi93pp-master-0:6436 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:19:01,240.240 dlcmzxjb7qmi93pp-master-0:6439 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 12:19:01,242.242 dlcmzxjb7qmi93pp-master-0:6438 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 12:19:01,243.243 dlcmzxjb7qmi93pp-master-0:6437 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 12:19:01,245.245 dlcmzxjb7qmi93pp-master-0:6436 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:6436:6436 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:6437:6437 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6439:6439 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6438:6438 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5536), 0.5034740780331374, 0.8407510431154381, tensor(1.9637)]
[tensor(-0.5185), 0.5446285408872261, 0.8616133518776078, tensor(2.2047)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5185), 0.5446285408872261, 0.8616133518776078, tensor(2.2047)]
[tensor(-0.5102), 0.5489043292357029, 0.8616133518776078, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8623087621696801, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8623087621696801, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8630041724617524, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8650904033379694, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8650904033379694, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8650904033379694, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8650904033379694, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8650904033379694, tensor(2.2343)]
[tensor(-0.5102), 0.5489043292357029, 0.8650904033379694, tensor(2.2343)]
early stopping at 13
[2023-01-16 12:45:15,812.812 dlcmzxjb7qmi93pp-master-0:6537 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 12:45:16,427.427 dlcmzxjb7qmi93pp-master-0:6605 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:45:16,428.428 dlcmzxjb7qmi93pp-master-0:6602 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:45:16,508.508 dlcmzxjb7qmi93pp-master-0:6604 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:45:16,514.514 dlcmzxjb7qmi93pp-master-0:6603 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 12:45:17,466.466 dlcmzxjb7qmi93pp-master-0:6605 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 12:45:17,860.860 dlcmzxjb7qmi93pp-master-0:6603 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 12:45:17,861.861 dlcmzxjb7qmi93pp-master-0:6604 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 12:45:17,865.865 dlcmzxjb7qmi93pp-master-0:6602 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:6602:6602 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:6604:6604 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6603:6603 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6605:6605 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5372), 0.5350080171031534, 0.8484005563282336, tensor(2.1378)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5324), 0.5350080171031534, 0.8574408901251739, tensor(2.1378)]
[tensor(-0.5108), 0.5478353821485836, 0.8643949930458971, tensor(2.2284)]
[tensor(-0.5108), 0.5478353821485836, 0.8643949930458971, tensor(2.2284)]
[tensor(-0.5108), 0.5478353821485836, 0.8643949930458971, tensor(2.2284)]
[tensor(-0.5108), 0.5478353821485836, 0.8643949930458971, tensor(2.2284)]
[tensor(-0.5108), 0.5478353821485836, 0.8643949930458971, tensor(2.2284)]
[tensor(-0.5108), 0.5478353821485836, 0.8643949930458971, tensor(2.2284)]
early stopping at 8
[2023-01-16 13:01:28,473.473 dlcmzxjb7qmi93pp-master-0:6689 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 13:01:29,089.089 dlcmzxjb7qmi93pp-master-0:6754 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:01:29,089.089 dlcmzxjb7qmi93pp-master-0:6755 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:01:29,167.167 dlcmzxjb7qmi93pp-master-0:6756 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:01:29,180.180 dlcmzxjb7qmi93pp-master-0:6757 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:01:30,229.229 dlcmzxjb7qmi93pp-master-0:6756 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 13:01:30,231.231 dlcmzxjb7qmi93pp-master-0:6757 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 13:01:30,977.977 dlcmzxjb7qmi93pp-master-0:6755 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 13:01:30,982.982 dlcmzxjb7qmi93pp-master-0:6754 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:6754:6754 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:6757:6757 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6755:6755 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6756:6756 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5284), 0.5446285408872261, 0.8588317107093185, tensor(2.1948)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5156), 0.5494388027792624, 0.8623087621696801, tensor(2.2316)]
[tensor(-0.5046), 0.5494388027792624, 0.8692628650904033, tensor(2.2316)]
[tensor(-0.5046), 0.5494388027792624, 0.8692628650904033, tensor(2.2316)]
[tensor(-0.5046), 0.5494388027792624, 0.8692628650904033, tensor(2.2316)]
[2023-01-16 13:11:42,817.817 dlcmzxjb7qmi93pp-master-0:6831 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 13:11:43,434.434 dlcmzxjb7qmi93pp-master-0:6897 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:11:43,434.434 dlcmzxjb7qmi93pp-master-0:6899 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:11:43,436.436 dlcmzxjb7qmi93pp-master-0:6896 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:11:43,512.512 dlcmzxjb7qmi93pp-master-0:6898 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:11:44,397.397 dlcmzxjb7qmi93pp-master-0:6898 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 13:11:45,362.362 dlcmzxjb7qmi93pp-master-0:6899 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 13:11:45,369.369 dlcmzxjb7qmi93pp-master-0:6897 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 13:11:45,370.370 dlcmzxjb7qmi93pp-master-0:6896 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:6896:6896 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:6899:6899 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6897:6897 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:6898:6898 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5503), 0.5350080171031534, 0.8532684283727399, tensor(2.1247)]
[tensor(-0.5122), 0.5510422234099412, 0.8671766342141863, tensor(2.2430)]
[tensor(-0.5122), 0.5510422234099412, 0.8671766342141863, tensor(2.2430)]
[tensor(-0.5122), 0.5510422234099412, 0.8671766342141863, tensor(2.2430)]
[tensor(-0.5122), 0.5510422234099412, 0.8671766342141863, tensor(2.2430)]
[2023-01-16 13:22:12,177.177 dlcmzxjb7qmi93pp-master-0:6973 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 13:22:12,952.952 dlcmzxjb7qmi93pp-master-0:7039 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:22:12,962.962 dlcmzxjb7qmi93pp-master-0:7038 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:22:13,035.035 dlcmzxjb7qmi93pp-master-0:7040 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:22:13,035.035 dlcmzxjb7qmi93pp-master-0:7041 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:22:14,139.139 dlcmzxjb7qmi93pp-master-0:7040 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 13:22:14,141.141 dlcmzxjb7qmi93pp-master-0:7041 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 13:22:14,889.889 dlcmzxjb7qmi93pp-master-0:7039 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 13:22:14,893.893 dlcmzxjb7qmi93pp-master-0:7038 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:7038:7038 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:7039:7039 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7041:7041 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7040:7040 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5303), 0.5350080171031534, 0.8497913769123783, tensor(2.1447)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5210), 0.5483698556921432, 0.8609179415855355, tensor(2.2208)]
[tensor(-0.5146), 0.5483698556921432, 0.8650904033379694, tensor(2.2208)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5146), 0.5483698556921432, 0.8650904033379694, tensor(2.2208)]
[tensor(-0.5093), 0.5483698556921432, 0.8650904033379694, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8650904033379694, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8650904033379694, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8650904033379694, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8650904033379694, tensor(2.2325)]
[tensor(-0.5093), 0.5483698556921432, 0.8650904033379694, tensor(2.2325)]
early stopping at 10
[2023-01-16 13:42:37,145.145 dlcmzxjb7qmi93pp-master-0:7131 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 13:42:37,851.851 dlcmzxjb7qmi93pp-master-0:7196 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:42:37,864.864 dlcmzxjb7qmi93pp-master-0:7197 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:42:37,873.873 dlcmzxjb7qmi93pp-master-0:7198 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:42:37,875.875 dlcmzxjb7qmi93pp-master-0:7199 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 13:42:38,865.865 dlcmzxjb7qmi93pp-master-0:7197 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 13:42:39,762.762 dlcmzxjb7qmi93pp-master-0:7198 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 13:42:39,787.787 dlcmzxjb7qmi93pp-master-0:7199 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 13:42:39,794.794 dlcmzxjb7qmi93pp-master-0:7196 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:7196:7196 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:7197:7197 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7199:7199 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7198:7198 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5229), 0.538214858364511, 0.8650904033379694, tensor(2.1682)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5222), 0.5398182789951897, 0.866481223922114, tensor(2.1769)]
[tensor(-0.5219), 0.5398182789951897, 0.8699582753824756, tensor(2.1769)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5219), 0.5398182789951897, 0.8699582753824756, tensor(2.1769)]
[tensor(-0.5219), 0.5478353821485836, 0.8699582753824756, tensor(2.2158)]
[tensor(-0.5219), 0.5478353821485836, 0.8699582753824756, tensor(2.2158)]
[tensor(-0.5219), 0.5478353821485836, 0.8699582753824756, tensor(2.2158)]
[tensor(-0.5219), 0.5478353821485836, 0.8699582753824756, tensor(2.2158)]
[tensor(-0.5219), 0.5478353821485836, 0.8699582753824756, tensor(2.2158)]
[tensor(-0.5219), 0.5478353821485836, 0.8699582753824756, tensor(2.2158)]
early stopping at 10
[2023-01-16 14:02:59,090.090 dlcmzxjb7qmi93pp-master-0:7288 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 14:02:59,712.712 dlcmzxjb7qmi93pp-master-0:7353 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:02:59,712.712 dlcmzxjb7qmi93pp-master-0:7356 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:02:59,712.712 dlcmzxjb7qmi93pp-master-0:7355 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:02:59,720.720 dlcmzxjb7qmi93pp-master-0:7354 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:03:00,709.709 dlcmzxjb7qmi93pp-master-0:7355 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 14:03:00,713.713 dlcmzxjb7qmi93pp-master-0:7354 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 14:03:00,717.717 dlcmzxjb7qmi93pp-master-0:7356 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 14:03:00,724.724 dlcmzxjb7qmi93pp-master-0:7353 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:7353:7353 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:7354:7354 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7355:7355 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7356:7356 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5554), 0.51309460181721, 0.8539638386648123, tensor(2.0101)]
[tensor(-0.5156), 0.5456974879743453, 0.8602225312934632, tensor(2.2129)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5156), 0.5456974879743453, 0.8602225312934632, tensor(2.2129)]
[tensor(-0.5156), 0.5456974879743453, 0.8602225312934632, tensor(2.2129)]
[tensor(-0.5156), 0.5456974879743453, 0.8602225312934632, tensor(2.2129)]
[2023-01-16 14:13:35,446.446 dlcmzxjb7qmi93pp-master-0:7431 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 14:13:36,081.081 dlcmzxjb7qmi93pp-master-0:7497 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:13:36,086.086 dlcmzxjb7qmi93pp-master-0:7499 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:13:36,133.133 dlcmzxjb7qmi93pp-master-0:7498 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:13:36,208.208 dlcmzxjb7qmi93pp-master-0:7496 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:13:37,965.965 dlcmzxjb7qmi93pp-master-0:7499 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 14:13:37,965.965 dlcmzxjb7qmi93pp-master-0:7497 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 14:13:38,391.391 dlcmzxjb7qmi93pp-master-0:7498 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 14:13:38,391.391 dlcmzxjb7qmi93pp-master-0:7496 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:7496:7496 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:7499:7499 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7498:7498 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7497:7497 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5065), 0.5494388027792624, 0.8643949930458971, tensor(2.2407)]
[tensor(-0.5065), 0.5494388027792624, 0.8643949930458971, tensor(2.2407)]
[tensor(-0.5065), 0.5494388027792624, 0.8643949930458971, tensor(2.2407)]
[tensor(-0.5065), 0.5494388027792624, 0.8643949930458971, tensor(2.2407)]
[tensor(-0.5065), 0.5494388027792624, 0.8643949930458971, tensor(2.2407)]
[2023-01-16 14:24:06,830.830 dlcmzxjb7qmi93pp-master-0:7574 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 14:24:07,460.460 dlcmzxjb7qmi93pp-master-0:7642 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:24:07,461.461 dlcmzxjb7qmi93pp-master-0:7641 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:24:07,572.572 dlcmzxjb7qmi93pp-master-0:7640 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:24:07,572.572 dlcmzxjb7qmi93pp-master-0:7639 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:24:09,363.363 dlcmzxjb7qmi93pp-master-0:7642 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 14:24:09,368.368 dlcmzxjb7qmi93pp-master-0:7641 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 14:24:09,446.446 dlcmzxjb7qmi93pp-master-0:7640 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 14:24:09,447.447 dlcmzxjb7qmi93pp-master-0:7639 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:7639:7639 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:7642:7642 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7640:7640 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7641:7641 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5235), 0.5440940673436665, 0.8511821974965229, tensor(2.1970)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5235), 0.5440940673436665, 0.8602225312934632, tensor(2.1970)]
[tensor(-0.5208), 0.5440940673436665, 0.8657858136300417, tensor(2.1970)]
[tensor(-0.5094), 0.5505077498663816, 0.8657858136300417, tensor(2.2432)]
[tensor(-0.5094), 0.5505077498663816, 0.8657858136300417, tensor(2.2432)]
[tensor(-0.5094), 0.5505077498663816, 0.8657858136300417, tensor(2.2432)]
[tensor(-0.5094), 0.5505077498663816, 0.8657858136300417, tensor(2.2432)]
[tensor(-0.5094), 0.5505077498663816, 0.8657858136300417, tensor(2.2432)]
[tensor(-0.5094), 0.5505077498663816, 0.8657858136300417, tensor(2.2432)]
early stopping at 9
[2023-01-16 14:42:46,680.680 dlcmzxjb7qmi93pp-master-0:7729 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 14:42:47,313.313 dlcmzxjb7qmi93pp-master-0:7795 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:42:47,316.316 dlcmzxjb7qmi93pp-master-0:7797 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:42:47,332.332 dlcmzxjb7qmi93pp-master-0:7794 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:42:47,343.343 dlcmzxjb7qmi93pp-master-0:7796 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 14:42:49,302.302 dlcmzxjb7qmi93pp-master-0:7795 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 14:42:49,332.332 dlcmzxjb7qmi93pp-master-0:7796 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 14:42:49,334.334 dlcmzxjb7qmi93pp-master-0:7797 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 14:42:49,338.338 dlcmzxjb7qmi93pp-master-0:7794 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:7794:7794 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:7796:7796 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7797:7797 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7795:7795 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5437), 0.5334045964724746, 0.847009735744089, tensor(2.1234)]
[tensor(-0.5437), 0.5334045964724746, 0.8504867872044506, tensor(2.1234)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5179), 0.5392838054516301, 0.8574408901251739, tensor(2.1785)]
[tensor(-0.5179), 0.5392838054516301, 0.8574408901251739, tensor(2.1785)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
[tensor(-0.5179), 0.5467664350614645, 0.8574408901251739, tensor(2.2085)]
early stopping at 12
[2023-01-16 15:06:46,962.962 dlcmzxjb7qmi93pp-master-0:7892 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 15:06:47,609.609 dlcmzxjb7qmi93pp-master-0:7959 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:06:47,614.614 dlcmzxjb7qmi93pp-master-0:7957 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:06:47,615.615 dlcmzxjb7qmi93pp-master-0:7960 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:06:47,623.623 dlcmzxjb7qmi93pp-master-0:7958 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:06:48,691.691 dlcmzxjb7qmi93pp-master-0:7959 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 15:06:48,691.691 dlcmzxjb7qmi93pp-master-0:7960 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 15:06:49,531.531 dlcmzxjb7qmi93pp-master-0:7958 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 15:06:49,540.540 dlcmzxjb7qmi93pp-master-0:7957 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:7957:7957 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:7960:7960 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7959:7959 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:7958:7958 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5554), 0.521111704970604, 0.8386648122392212, tensor(2.0502)]
[tensor(-0.5146), 0.5515766969535008, 0.8623087621696801, tensor(2.2433)]
[tensor(-0.5146), 0.5515766969535008, 0.866481223922114, tensor(2.2433)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5073), 0.5515766969535008, 0.866481223922114, tensor(2.2433)]
[tensor(-0.4924), 0.5563869588455371, 0.8678720445062587, tensor(2.2895)]
[2023-01-16 15:17:00,269.269 dlcmzxjb7qmi93pp-master-0:8034 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 15:17:00,893.893 dlcmzxjb7qmi93pp-master-0:8101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:17:00,893.893 dlcmzxjb7qmi93pp-master-0:8102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:17:00,899.899 dlcmzxjb7qmi93pp-master-0:8099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:17:00,915.915 dlcmzxjb7qmi93pp-master-0:8100 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:17:01,875.875 dlcmzxjb7qmi93pp-master-0:8101 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 15:17:02,751.751 dlcmzxjb7qmi93pp-master-0:8102 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 15:17:02,868.868 dlcmzxjb7qmi93pp-master-0:8100 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 15:17:02,876.876 dlcmzxjb7qmi93pp-master-0:8099 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:8099:8099 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:8102:8102 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8101:8101 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8100:8100 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5246), 0.5312667022982362, 0.8602225312934632, tensor(2.1318)]
[tensor(-0.5132), 0.5483698556921432, 0.8699582753824756, tensor(2.2286)]
[tensor(-0.5010), 0.5489043292357029, 0.8699582753824756, tensor(2.2435)]
[tensor(-0.5010), 0.5489043292357029, 0.8699582753824756, tensor(2.2435)]
[tensor(-0.5010), 0.549973276322822, 0.8699582753824756, tensor(2.2435)]
[2023-01-16 15:27:15,598.598 dlcmzxjb7qmi93pp-master-0:8177 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 15:27:16,224.224 dlcmzxjb7qmi93pp-master-0:8244 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:27:16,225.225 dlcmzxjb7qmi93pp-master-0:8242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:27:16,226.226 dlcmzxjb7qmi93pp-master-0:8245 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:27:16,228.228 dlcmzxjb7qmi93pp-master-0:8243 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:27:17,243.243 dlcmzxjb7qmi93pp-master-0:8244 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 15:27:17,252.252 dlcmzxjb7qmi93pp-master-0:8243 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 15:27:17,255.255 dlcmzxjb7qmi93pp-master-0:8245 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 15:27:17,264.264 dlcmzxjb7qmi93pp-master-0:8242 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:8242:8242 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:8244:8244 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8245:8245 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8243:8243 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5286), 0.5243185462319615, 0.8477051460361613, tensor(2.0930)]
[tensor(-0.5234), 0.5371459112773918, 0.8553546592489569, tensor(2.1623)]
[tensor(-0.5199), 0.5462319615179049, 0.8616133518776078, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8616133518776078, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8616133518776078, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8630041724617524, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8630041724617524, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8636995827538247, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8636995827538247, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8636995827538247, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8636995827538247, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8636995827538247, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8636995827538247, tensor(2.2113)]
[tensor(-0.5117), 0.5462319615179049, 0.8636995827538247, tensor(2.2113)]
early stopping at 14
[2023-01-16 15:55:34,959.959 dlcmzxjb7qmi93pp-master-0:8346 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 15:55:35,584.584 dlcmzxjb7qmi93pp-master-0:8413 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:55:35,585.585 dlcmzxjb7qmi93pp-master-0:8411 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:55:35,585.585 dlcmzxjb7qmi93pp-master-0:8412 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:55:35,589.589 dlcmzxjb7qmi93pp-master-0:8414 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 15:55:37,560.560 dlcmzxjb7qmi93pp-master-0:8414 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 15:55:37,570.570 dlcmzxjb7qmi93pp-master-0:8413 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 15:55:37,578.578 dlcmzxjb7qmi93pp-master-0:8412 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 15:55:37,582.582 dlcmzxjb7qmi93pp-master-0:8411 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:8411:8411 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:8412:8412 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8413:8413 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8414:8414 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5431), 0.5296632816675575, 0.8609179415855355, tensor(2.1052)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5370), 0.5296632816675575, 0.8623087621696801, tensor(2.1052)]
[tensor(-0.5085), 0.5456974879743453, 0.8623087621696801, tensor(2.2200)]
[tensor(-0.5085), 0.5462319615179049, 0.8623087621696801, tensor(2.2200)]
[tensor(-0.5085), 0.5462319615179049, 0.8643949930458971, tensor(2.2200)]
[tensor(-0.5085), 0.5462319615179049, 0.8643949930458971, tensor(2.2200)]
[tensor(-0.5085), 0.5462319615179049, 0.8643949930458971, tensor(2.2200)]
[tensor(-0.5085), 0.5462319615179049, 0.8643949930458971, tensor(2.2200)]
[tensor(-0.5085), 0.5462319615179049, 0.8643949930458971, tensor(2.2200)]
[tensor(-0.5085), 0.5462319615179049, 0.8643949930458971, tensor(2.2200)]
early stopping at 10
[2023-01-16 16:15:46,924.924 dlcmzxjb7qmi93pp-master-0:8503 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 16:15:47,559.559 dlcmzxjb7qmi93pp-master-0:8568 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:15:47,562.562 dlcmzxjb7qmi93pp-master-0:8571 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:15:47,564.564 dlcmzxjb7qmi93pp-master-0:8570 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:15:47,580.580 dlcmzxjb7qmi93pp-master-0:8569 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:15:48,649.649 dlcmzxjb7qmi93pp-master-0:8570 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 16:15:48,651.651 dlcmzxjb7qmi93pp-master-0:8569 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 16:15:49,639.639 dlcmzxjb7qmi93pp-master-0:8571 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 16:15:49,647.647 dlcmzxjb7qmi93pp-master-0:8568 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:8568:8568 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:8571:8571 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8569:8569 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8570:8570 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5303), 0.5392838054516301, 0.8539638386648123, tensor(2.1661)]
[tensor(-0.5134), 0.5392838054516301, 0.868567454798331, tensor(2.1777)]
[tensor(-0.5130), 0.5424906467129877, 0.868567454798331, tensor(2.1994)]
[tensor(-0.5130), 0.5424906467129877, 0.868567454798331, tensor(2.1994)]
[tensor(-0.5094), 0.5456974879743453, 0.868567454798331, tensor(2.2191)]
[2023-01-16 16:26:00,260.260 dlcmzxjb7qmi93pp-master-0:8645 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 16:26:00,880.880 dlcmzxjb7qmi93pp-master-0:8713 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:26:00,880.880 dlcmzxjb7qmi93pp-master-0:8711 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:26:00,880.880 dlcmzxjb7qmi93pp-master-0:8710 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:26:00,881.881 dlcmzxjb7qmi93pp-master-0:8712 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:26:01,862.862 dlcmzxjb7qmi93pp-master-0:8713 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 16:26:01,864.864 dlcmzxjb7qmi93pp-master-0:8712 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 16:26:02,852.852 dlcmzxjb7qmi93pp-master-0:8711 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 16:26:02,862.862 dlcmzxjb7qmi93pp-master-0:8710 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:8710:8710 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:8712:8712 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8711:8711 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8713:8713 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5361), 0.5269909139497595, 0.8477051460361613, tensor(2.0989)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5212), 0.5392838054516301, 0.8497913769123783, tensor(2.1753)]
[tensor(-0.5212), 0.5392838054516301, 0.8609179415855355, tensor(2.1753)]
[tensor(-0.5093), 0.5531801175841796, 0.8623087621696801, tensor(2.2566)]
[tensor(-0.5048), 0.5531801175841796, 0.8623087621696801, tensor(2.2566)]
[2023-01-16 16:36:17,606.606 dlcmzxjb7qmi93pp-master-0:8788 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 16:36:18,429.429 dlcmzxjb7qmi93pp-master-0:8855 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:36:18,430.430 dlcmzxjb7qmi93pp-master-0:8854 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:36:18,445.445 dlcmzxjb7qmi93pp-master-0:8853 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:36:18,445.445 dlcmzxjb7qmi93pp-master-0:8856 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:36:20,382.382 dlcmzxjb7qmi93pp-master-0:8855 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 16:36:20,391.391 dlcmzxjb7qmi93pp-master-0:8856 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 16:36:20,396.396 dlcmzxjb7qmi93pp-master-0:8854 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 16:36:20,399.399 dlcmzxjb7qmi93pp-master-0:8853 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:8853:8853 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:8855:8855 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8856:8856 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:8854:8854 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5253), 0.5408872260823089, 0.8546592489568846, tensor(2.1792)]
[tensor(-0.5084), 0.5515766969535008, 0.8574408901251739, tensor(2.2494)]
[Mon Jan 16 16:41:46 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5084), 0.5515766969535008, 0.8595271210013908, tensor(2.2494)]
[tensor(-0.5084), 0.5515766969535008, 0.8595271210013908, tensor(2.2494)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5084), 0.5515766969535008, 0.8595271210013908, tensor(2.2494)]
[tensor(-0.5084), 0.5515766969535008, 0.8595271210013908, tensor(2.2494)]
[tensor(-0.5084), 0.5515766969535008, 0.8595271210013908, tensor(2.2494)]
[tensor(-0.5084), 0.5515766969535008, 0.8595271210013908, tensor(2.2494)]
early stopping at 8
[2023-01-16 16:52:33,324.324 dlcmzxjb7qmi93pp-master-0:8939 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 16:52:33,961.961 dlcmzxjb7qmi93pp-master-0:9005 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:52:33,965.965 dlcmzxjb7qmi93pp-master-0:9006 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:52:34,045.045 dlcmzxjb7qmi93pp-master-0:9004 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:52:34,058.058 dlcmzxjb7qmi93pp-master-0:9007 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 16:52:34,950.950 dlcmzxjb7qmi93pp-master-0:9007 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 16:52:35,864.864 dlcmzxjb7qmi93pp-master-0:9006 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 16:52:35,869.869 dlcmzxjb7qmi93pp-master-0:9005 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 16:52:35,870.870 dlcmzxjb7qmi93pp-master-0:9004 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:9004:9004 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:9006:9006 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9005:9005 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9007:9007 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5229), 0.5414216996258685, 0.8504867872044506, tensor(2.1842)]
[tensor(-0.5127), 0.5440940673436665, 0.8650904033379694, tensor(2.2077)]
[tensor(-0.5127), 0.5440940673436665, 0.8650904033379694, tensor(2.2077)]
[tensor(-0.5067), 0.55264564404062, 0.8650904033379694, tensor(2.2565)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5067), 0.55264564404062, 0.8650904033379694, tensor(2.2565)]
[tensor(-0.5067), 0.5542490646712988, 0.8650904033379694, tensor(2.2624)]
[tensor(-0.5067), 0.5542490646712988, 0.8650904033379694, tensor(2.2624)]
[tensor(-0.5067), 0.5542490646712988, 0.8650904033379694, tensor(2.2624)]
[tensor(-0.5067), 0.5542490646712988, 0.8650904033379694, tensor(2.2624)]
[tensor(-0.5067), 0.5542490646712988, 0.8650904033379694, tensor(2.2624)]
[tensor(-0.5067), 0.5542490646712988, 0.8650904033379694, tensor(2.2624)]
early stopping at 11
[2023-01-16 17:14:39,352.352 dlcmzxjb7qmi93pp-master-0:9099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 17:14:39,982.982 dlcmzxjb7qmi93pp-master-0:9167 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:14:39,983.983 dlcmzxjb7qmi93pp-master-0:9166 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:14:40,062.062 dlcmzxjb7qmi93pp-master-0:9164 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:14:40,073.073 dlcmzxjb7qmi93pp-master-0:9165 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:14:41,850.850 dlcmzxjb7qmi93pp-master-0:9166 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 17:14:42,084.084 dlcmzxjb7qmi93pp-master-0:9167 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 17:14:42,089.089 dlcmzxjb7qmi93pp-master-0:9165 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 17:14:42,092.092 dlcmzxjb7qmi93pp-master-0:9164 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:9164:9164 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:9165:9165 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9166:9166 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9167:9167 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5379), 0.5221806520577231, 0.8560500695410292, tensor(2.0730)]
[tensor(-0.5379), 0.5221806520577231, 0.8560500695410292, tensor(2.0730)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5379), 0.5221806520577231, 0.8560500695410292, tensor(2.0730)]
[tensor(-0.5144), 0.5424906467129877, 0.8630041724617524, tensor(2.1980)]
[Mon Jan 16 17:23:09 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5144), 0.5424906467129877, 0.8630041724617524, tensor(2.1980)]
[2023-01-16 17:25:10,700.700 dlcmzxjb7qmi93pp-master-0:9242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 17:25:11,337.337 dlcmzxjb7qmi93pp-master-0:9309 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:25:11,338.338 dlcmzxjb7qmi93pp-master-0:9308 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:25:11,411.411 dlcmzxjb7qmi93pp-master-0:9307 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:25:11,416.416 dlcmzxjb7qmi93pp-master-0:9310 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:25:12,710.710 dlcmzxjb7qmi93pp-master-0:9310 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 17:25:13,262.262 dlcmzxjb7qmi93pp-master-0:9308 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 17:25:13,350.350 dlcmzxjb7qmi93pp-master-0:9309 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 17:25:13,356.356 dlcmzxjb7qmi93pp-master-0:9307 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:9307:9307 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:9310:9310 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9308:9308 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9309:9309 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5315), 0.5205772314270444, 0.866481223922114, tensor(2.0713)]
[tensor(-0.5159), 0.5435595938001069, 0.866481223922114, tensor(2.2019)]
[tensor(-0.5159), 0.5435595938001069, 0.868567454798331, tensor(2.2019)]
[tensor(-0.5159), 0.5435595938001069, 0.868567454798331, tensor(2.2019)]
[tensor(-0.5051), 0.5489043292357029, 0.868567454798331, tensor(2.2395)]
[2023-01-16 17:35:33,086.086 dlcmzxjb7qmi93pp-master-0:9384 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 17:35:33,713.713 dlcmzxjb7qmi93pp-master-0:9451 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:35:33,714.714 dlcmzxjb7qmi93pp-master-0:9450 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:35:33,717.717 dlcmzxjb7qmi93pp-master-0:9452 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:35:33,721.721 dlcmzxjb7qmi93pp-master-0:9449 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 17:35:34,720.720 dlcmzxjb7qmi93pp-master-0:9451 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 17:35:34,725.725 dlcmzxjb7qmi93pp-master-0:9452 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 17:35:35,708.708 dlcmzxjb7qmi93pp-master-0:9450 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 17:35:35,718.718 dlcmzxjb7qmi93pp-master-0:9449 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:9449:9449 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:9450:9450 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9452:9452 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9451:9451 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5401), 0.5232495991448424, 0.8484005563282336, tensor(2.0762)]
[tensor(-0.5214), 0.5350080171031534, 0.8560500695410292, tensor(2.1536)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5119), 0.5483698556921432, 0.8560500695410292, tensor(2.2299)]
[tensor(-0.5091), 0.5483698556921432, 0.8636995827538247, tensor(2.2299)]
[tensor(-0.5091), 0.5483698556921432, 0.8636995827538247, tensor(2.2299)]
[Mon Jan 16 17:47:06 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5091), 0.5483698556921432, 0.8650904033379694, tensor(2.2299)]
[tensor(-0.5077), 0.5510422234099412, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5510422234099412, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5510422234099412, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5510422234099412, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5510422234099412, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5515766969535008, 0.8650904033379694, tensor(2.2475)]
[Mon Jan 16 18:00:30 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5077), 0.5515766969535008, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5515766969535008, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5515766969535008, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5515766969535008, 0.8650904033379694, tensor(2.2475)]
[tensor(-0.5077), 0.5515766969535008, 0.8650904033379694, tensor(2.2475)]
early stopping at 17
[2023-01-16 18:10:01,836.836 dlcmzxjb7qmi93pp-master-0:9563 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 18:10:02,472.472 dlcmzxjb7qmi93pp-master-0:9629 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:10:02,485.485 dlcmzxjb7qmi93pp-master-0:9630 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:10:02,543.543 dlcmzxjb7qmi93pp-master-0:9628 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:10:02,544.544 dlcmzxjb7qmi93pp-master-0:9631 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:10:03,469.469 dlcmzxjb7qmi93pp-master-0:9630 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 18:10:04,364.364 dlcmzxjb7qmi93pp-master-0:9629 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 18:10:04,444.444 dlcmzxjb7qmi93pp-master-0:9631 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 18:10:04,450.450 dlcmzxjb7qmi93pp-master-0:9628 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:9628:9628 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:9630:9630 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9631:9631 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9629:9629 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5864), 0.5061464457509354, 0.8143254520166898, tensor(1.9443)]
[tensor(-0.5401), 0.5376803848209514, 0.8497913769123783, tensor(2.1483)]
[tensor(-0.5401), 0.5376803848209514, 0.8497913769123783, tensor(2.1483)]
[tensor(-0.5401), 0.5376803848209514, 0.8497913769123783, tensor(2.1483)]
[tensor(-0.5355), 0.5376803848209514, 0.8616133518776078, tensor(2.1483)]
[tensor(-0.5281), 0.5387493319080705, 0.8616133518776078, tensor(2.1656)]
[tensor(-0.5222), 0.5521111704970604, 0.8636995827538247, tensor(2.2383)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5222), 0.5521111704970604, 0.8643949930458971, tensor(2.2383)]
[tensor(-0.5222), 0.5521111704970604, 0.8643949930458971, tensor(2.2383)]
[tensor(-0.5222), 0.5521111704970604, 0.8643949930458971, tensor(2.2383)]
[Mon Jan 16 18:31:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5222), 0.5521111704970604, 0.8643949930458971, tensor(2.2383)]
[tensor(-0.5222), 0.5521111704970604, 0.8643949930458971, tensor(2.2383)]
[tensor(-0.5222), 0.5521111704970604, 0.8643949930458971, tensor(2.2383)]
[tensor(-0.5222), 0.5521111704970604, 0.8643949930458971, tensor(2.2383)]
early stopping at 14
[2023-01-16 18:38:29,305.305 dlcmzxjb7qmi93pp-master-0:9733 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 18:38:29,921.921 dlcmzxjb7qmi93pp-master-0:9800 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:38:29,978.978 dlcmzxjb7qmi93pp-master-0:9799 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:38:29,984.984 dlcmzxjb7qmi93pp-master-0:9798 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:38:30,059.059 dlcmzxjb7qmi93pp-master-0:9801 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:38:31,163.163 dlcmzxjb7qmi93pp-master-0:9799 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 18:38:31,207.207 dlcmzxjb7qmi93pp-master-0:9801 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 18:38:31,755.755 dlcmzxjb7qmi93pp-master-0:9800 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 18:38:31,761.761 dlcmzxjb7qmi93pp-master-0:9798 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:9798:9798 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:9801:9801 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9799:9799 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9800:9800 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5172), 0.5339390700160342, 0.8588317107093185, tensor(2.1525)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5172), 0.5339390700160342, 0.8588317107093185, tensor(2.1525)]
[tensor(-0.5161), 0.5446285408872261, 0.8588317107093185, tensor(2.2071)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5104), 0.5478353821485836, 0.8588317107093185, tensor(2.2288)]
[tensor(-0.5104), 0.5478353821485836, 0.8609179415855355, tensor(2.2288)]
[2023-01-16 18:48:44,678.678 dlcmzxjb7qmi93pp-master-0:9875 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 18:48:45,296.296 dlcmzxjb7qmi93pp-master-0:9941 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:48:45,296.296 dlcmzxjb7qmi93pp-master-0:9942 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:48:45,297.297 dlcmzxjb7qmi93pp-master-0:9940 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:48:45,299.299 dlcmzxjb7qmi93pp-master-0:9943 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:48:46,264.264 dlcmzxjb7qmi93pp-master-0:9943 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 18:48:46,266.266 dlcmzxjb7qmi93pp-master-0:9941 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 18:48:46,267.267 dlcmzxjb7qmi93pp-master-0:9942 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 18:48:46,277.277 dlcmzxjb7qmi93pp-master-0:9940 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:9940:9940 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:9943:9943 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9942:9942 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:9941:9941 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5198), 0.5344735435595938, 0.8630041724617524, tensor(2.1526)]
[tensor(-0.5122), 0.5467664350614645, 0.868567454798331, tensor(2.2216)]
[tensor(-0.5122), 0.547300908605024, 0.868567454798331, tensor(2.2243)]
[tensor(-0.5122), 0.5505077498663816, 0.8706536856745479, tensor(2.2345)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5122), 0.5505077498663816, 0.8706536856745479, tensor(2.2345)]
[2023-01-16 18:59:38,081.081 dlcmzxjb7qmi93pp-master-0:10018 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 18:59:38,729.729 dlcmzxjb7qmi93pp-master-0:10084 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:59:38,759.759 dlcmzxjb7qmi93pp-master-0:10085 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:59:38,806.806 dlcmzxjb7qmi93pp-master-0:10083 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:59:38,890.890 dlcmzxjb7qmi93pp-master-0:10086 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 18:59:39,714.714 dlcmzxjb7qmi93pp-master-0:10085 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 18:59:39,861.861 dlcmzxjb7qmi93pp-master-0:10086 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 18:59:40,580.580 dlcmzxjb7qmi93pp-master-0:10084 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 18:59:40,580.580 dlcmzxjb7qmi93pp-master-0:10083 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:10083:10083 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:10085:10085 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10086:10086 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10084:10084 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5244), 0.5419561731694281, 0.8595271210013908, tensor(2.1854)]
[tensor(-0.5244), 0.5419561731694281, 0.8595271210013908, tensor(2.1854)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5216), 0.5419561731694281, 0.8595271210013908, tensor(2.1854)]
[tensor(-0.5047), 0.5451630144307856, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5451630144307856, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5451630144307856, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5451630144307856, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5451630144307856, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5478353821485836, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5478353821485836, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5478353821485836, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5478353821485836, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5478353821485836, 0.8657858136300417, tensor(2.2211)]
[tensor(-0.5047), 0.5478353821485836, 0.8657858136300417, tensor(2.2211)]
early stopping at 14
[2023-01-16 19:27:43,490.490 dlcmzxjb7qmi93pp-master-0:10187 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 19:27:44,140.140 dlcmzxjb7qmi93pp-master-0:10254 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:27:44,140.140 dlcmzxjb7qmi93pp-master-0:10255 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:27:44,140.140 dlcmzxjb7qmi93pp-master-0:10253 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:27:44,141.141 dlcmzxjb7qmi93pp-master-0:10252 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:27:45,059.059 dlcmzxjb7qmi93pp-master-0:10254 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 19:27:45,140.140 dlcmzxjb7qmi93pp-master-0:10255 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 19:27:45,144.144 dlcmzxjb7qmi93pp-master-0:10253 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 19:27:45,147.147 dlcmzxjb7qmi93pp-master-0:10252 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:10252:10252 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:10255:10255 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10253:10253 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10254:10254 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5309), 0.5376803848209514, 0.8602225312934632, tensor(2.1575)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5290), 0.5376803848209514, 0.8602225312934632, tensor(2.1575)]
[tensor(-0.5138), 0.5414216996258685, 0.8616133518776078, tensor(2.1933)]
[tensor(-0.5138), 0.5414216996258685, 0.8616133518776078, tensor(2.1933)]
[tensor(-0.5138), 0.5483698556921432, 0.8616133518776078, tensor(2.2242)]
[tensor(-0.5138), 0.5483698556921432, 0.8616133518776078, tensor(2.2242)]
[tensor(-0.5138), 0.5483698556921432, 0.8616133518776078, tensor(2.2242)]
[tensor(-0.5138), 0.5483698556921432, 0.8616133518776078, tensor(2.2242)]
[tensor(-0.5138), 0.5483698556921432, 0.8616133518776078, tensor(2.2242)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5138), 0.5483698556921432, 0.8616133518776078, tensor(2.2242)]
early stopping at 10
[2023-01-16 19:48:06,484.484 dlcmzxjb7qmi93pp-master-0:10344 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 19:48:07,120.120 dlcmzxjb7qmi93pp-master-0:10410 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:48:07,120.120 dlcmzxjb7qmi93pp-master-0:10411 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:48:07,122.122 dlcmzxjb7qmi93pp-master-0:10412 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:48:07,129.129 dlcmzxjb7qmi93pp-master-0:10409 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:48:08,090.090 dlcmzxjb7qmi93pp-master-0:10411 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 19:48:08,092.092 dlcmzxjb7qmi93pp-master-0:10410 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 19:48:09,067.067 dlcmzxjb7qmi93pp-master-0:10412 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 19:48:09,071.071 dlcmzxjb7qmi93pp-master-0:10409 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:10409:10409 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:10412:10412 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10410:10410 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10411:10411 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5930), 0.5077498663816141, 0.8532684283727399, tensor(1.9458)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5207), 0.5371459112773918, 0.8539638386648123, tensor(2.1650)]
[tensor(-0.5207), 0.5398182789951897, 0.8539638386648123, tensor(2.1733)]
[tensor(-0.5162), 0.5430251202565473, 0.8630041724617524, tensor(2.1989)]
[tensor(-0.5162), 0.5430251202565473, 0.8630041724617524, tensor(2.1989)]
[2023-01-16 19:58:16,859.859 dlcmzxjb7qmi93pp-master-0:10487 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 19:58:17,484.484 dlcmzxjb7qmi93pp-master-0:10554 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:58:17,485.485 dlcmzxjb7qmi93pp-master-0:10555 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:58:17,487.487 dlcmzxjb7qmi93pp-master-0:10552 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:58:17,488.488 dlcmzxjb7qmi93pp-master-0:10553 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 19:58:18,561.561 dlcmzxjb7qmi93pp-master-0:10553 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 19:58:18,562.562 dlcmzxjb7qmi93pp-master-0:10555 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 19:58:19,547.547 dlcmzxjb7qmi93pp-master-0:10554 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 19:58:19,551.551 dlcmzxjb7qmi93pp-master-0:10552 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:10552:10552 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:10553:10553 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10554:10554 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10555:10555 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5239), 0.5360769641902726, 0.8623087621696801, tensor(2.1565)]
[tensor(-0.5212), 0.5451630144307856, 0.8706536856745479, tensor(2.2046)]
[tensor(-0.5078), 0.5478353821485836, 0.8706536856745479, tensor(2.2314)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5078), 0.5478353821485836, 0.8706536856745479, tensor(2.2314)]
[tensor(-0.5078), 0.5510422234099412, 0.8706536856745479, tensor(2.2447)]
[2023-01-16 20:08:43,233.233 dlcmzxjb7qmi93pp-master-0:10629 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 20:08:43,856.856 dlcmzxjb7qmi93pp-master-0:10695 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:08:43,856.856 dlcmzxjb7qmi93pp-master-0:10694 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:08:43,934.934 dlcmzxjb7qmi93pp-master-0:10696 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:08:43,942.942 dlcmzxjb7qmi93pp-master-0:10697 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:08:45,203.203 dlcmzxjb7qmi93pp-master-0:10696 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 20:08:45,204.204 dlcmzxjb7qmi93pp-master-0:10697 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 20:08:45,727.727 dlcmzxjb7qmi93pp-master-0:10695 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 20:08:45,727.727 dlcmzxjb7qmi93pp-master-0:10694 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:10694:10694 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:10697:10697 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10695:10695 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10696:10696 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5337), 0.5350080171031534, 0.849095966620306, tensor(2.1414)]
[tensor(-0.5229), 0.5430251202565473, 0.8595271210013908, tensor(2.1923)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5143), 0.5430251202565473, 0.8616133518776078, tensor(2.1923)]
[tensor(-0.5143), 0.5430251202565473, 0.8616133518776078, tensor(2.1948)]
[tensor(-0.5084), 0.5521111704970604, 0.8616133518776078, tensor(2.2521)]
[tensor(-0.5084), 0.5521111704970604, 0.8616133518776078, tensor(2.2521)]
[tensor(-0.5084), 0.5521111704970604, 0.8616133518776078, tensor(2.2521)]
[tensor(-0.5084), 0.5521111704970604, 0.8616133518776078, tensor(2.2521)]
[tensor(-0.5084), 0.5521111704970604, 0.8616133518776078, tensor(2.2521)]
[tensor(-0.5084), 0.5521111704970604, 0.8616133518776078, tensor(2.2521)]
early stopping at 10
[2023-01-16 20:29:24,177.177 dlcmzxjb7qmi93pp-master-0:10787 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 20:29:24,799.799 dlcmzxjb7qmi93pp-master-0:10852 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:29:24,800.800 dlcmzxjb7qmi93pp-master-0:10854 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:29:24,802.802 dlcmzxjb7qmi93pp-master-0:10855 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:29:24,803.803 dlcmzxjb7qmi93pp-master-0:10853 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:29:25,774.774 dlcmzxjb7qmi93pp-master-0:10854 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 20:29:26,769.769 dlcmzxjb7qmi93pp-master-0:10855 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 20:29:26,771.771 dlcmzxjb7qmi93pp-master-0:10853 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 20:29:26,775.775 dlcmzxjb7qmi93pp-master-0:10852 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:10852:10852 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:10854:10854 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10853:10853 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:10855:10855 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5203), 0.5419561731694281, 0.8623087621696801, tensor(2.1895)]
[tensor(-0.5203), 0.5419561731694281, 0.8643949930458971, tensor(2.1895)]
[tensor(-0.5078), 0.5505077498663816, 0.8643949930458971, tensor(2.2447)]
[tensor(-0.5078), 0.5505077498663816, 0.8657858136300417, tensor(2.2447)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5078), 0.5505077498663816, 0.8692628650904033, tensor(2.2447)]
[tensor(-0.5078), 0.5505077498663816, 0.8692628650904033, tensor(2.2447)]
[tensor(-0.5078), 0.5505077498663816, 0.8692628650904033, tensor(2.2447)]
[tensor(-0.5078), 0.5505077498663816, 0.8692628650904033, tensor(2.2447)]
[tensor(-0.5078), 0.5505077498663816, 0.8692628650904033, tensor(2.2447)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5078), 0.5505077498663816, 0.8692628650904033, tensor(2.2447)]
early stopping at 10
[2023-01-16 20:51:48,190.190 dlcmzxjb7qmi93pp-master-0:10948 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 20:51:48,826.826 dlcmzxjb7qmi93pp-master-0:11014 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:51:48,826.826 dlcmzxjb7qmi93pp-master-0:11015 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:51:48,985.985 dlcmzxjb7qmi93pp-master-0:11013 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:51:48,992.992 dlcmzxjb7qmi93pp-master-0:11016 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 20:51:50,689.689 dlcmzxjb7qmi93pp-master-0:11015 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 20:51:50,694.694 dlcmzxjb7qmi93pp-master-0:11014 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 20:51:50,855.855 dlcmzxjb7qmi93pp-master-0:11016 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 20:51:50,860.860 dlcmzxjb7qmi93pp-master-0:11013 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:11013:11013 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:11014:11014 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11015:11015 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11016:11016 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5590), 0.5098877605558525, 0.8268428372739917, tensor(1.9904)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5192), 0.5440940673436665, 0.8602225312934632, tensor(2.2013)]
[tensor(-0.5018), 0.5462319615179049, 0.8678720445062587, tensor(2.2294)]
[tensor(-0.5018), 0.5462319615179049, 0.8678720445062587, tensor(2.2294)]
[tensor(-0.5018), 0.5462319615179049, 0.8692628650904033, tensor(2.2294)]
[2023-01-16 21:02:25,550.550 dlcmzxjb7qmi93pp-master-0:11091 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 21:02:26,166.166 dlcmzxjb7qmi93pp-master-0:11157 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:02:26,166.166 dlcmzxjb7qmi93pp-master-0:11156 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:02:26,167.167 dlcmzxjb7qmi93pp-master-0:11159 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:02:26,167.167 dlcmzxjb7qmi93pp-master-0:11158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:02:27,155.155 dlcmzxjb7qmi93pp-master-0:11159 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 21:02:28,146.146 dlcmzxjb7qmi93pp-master-0:11157 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 21:02:28,150.150 dlcmzxjb7qmi93pp-master-0:11158 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 21:02:28,155.155 dlcmzxjb7qmi93pp-master-0:11156 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:11156:11156 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:11159:11159 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11158:11158 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11157:11157 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5457), 0.5109567076429716, 0.8643949930458971, tensor(2.0091)]
[tensor(-0.5457), 0.5371459112773918, 0.8643949930458971, tensor(2.1394)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5091), 0.5456974879743453, 0.8699582753824756, tensor(2.2193)]
[tensor(-0.5091), 0.5456974879743453, 0.872739916550765, tensor(2.2193)]
[tensor(-0.5091), 0.5489043292357029, 0.872739916550765, tensor(2.2326)]
[2023-01-16 21:13:52,985.985 dlcmzxjb7qmi93pp-master-0:11235 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 21:13:53,760.760 dlcmzxjb7qmi93pp-master-0:11303 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:13:53,760.760 dlcmzxjb7qmi93pp-master-0:11302 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:13:53,762.762 dlcmzxjb7qmi93pp-master-0:11300 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:13:53,777.777 dlcmzxjb7qmi93pp-master-0:11301 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:13:54,719.719 dlcmzxjb7qmi93pp-master-0:11302 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 21:13:55,704.704 dlcmzxjb7qmi93pp-master-0:11301 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 21:13:55,714.714 dlcmzxjb7qmi93pp-master-0:11303 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 21:13:55,720.720 dlcmzxjb7qmi93pp-master-0:11300 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:11300:11300 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:11301:11301 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11302:11302 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11303:11303 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5206), 0.5430251202565473, 0.8609179415855355, tensor(2.1945)]
[tensor(-0.5206), 0.5430251202565473, 0.8609179415855355, tensor(2.1945)]
[tensor(-0.5206), 0.5430251202565473, 0.8623087621696801, tensor(2.1945)]
[tensor(-0.5157), 0.5483698556921432, 0.8623087621696801, tensor(2.2262)]
[tensor(-0.5157), 0.5483698556921432, 0.8623087621696801, tensor(2.2262)]
[tensor(-0.5157), 0.5483698556921432, 0.8623087621696801, tensor(2.2262)]
[tensor(-0.5157), 0.5483698556921432, 0.8623087621696801, tensor(2.2262)]
[tensor(-0.5157), 0.5483698556921432, 0.8623087621696801, tensor(2.2262)]
[tensor(-0.5157), 0.5483698556921432, 0.8623087621696801, tensor(2.2262)]
early stopping at 9
[2023-01-16 21:32:02,820.820 dlcmzxjb7qmi93pp-master-0:11389 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 21:32:03,435.435 dlcmzxjb7qmi93pp-master-0:11455 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:32:03,435.435 dlcmzxjb7qmi93pp-master-0:11457 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:32:03,439.439 dlcmzxjb7qmi93pp-master-0:11456 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:32:03,440.440 dlcmzxjb7qmi93pp-master-0:11454 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:32:04,418.418 dlcmzxjb7qmi93pp-master-0:11455 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 21:32:04,419.419 dlcmzxjb7qmi93pp-master-0:11457 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 21:32:05,401.401 dlcmzxjb7qmi93pp-master-0:11456 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 21:32:05,404.404 dlcmzxjb7qmi93pp-master-0:11454 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:11454:11454 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:11456:11456 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11455:11455 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11457:11457 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5563), 0.5259219668626403, 0.8407510431154381, tensor(2.0734)]
[tensor(-0.5359), 0.5344735435595938, 0.8504867872044506, tensor(2.1365)]
[tensor(-0.5080), 0.5489043292357029, 0.8518776077885952, tensor(2.2365)]
[tensor(-0.5080), 0.5489043292357029, 0.8609179415855355, tensor(2.2365)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5080), 0.5489043292357029, 0.8609179415855355, tensor(2.2365)]
[tensor(-0.5080), 0.5489043292357029, 0.8609179415855355, tensor(2.2365)]
[tensor(-0.5080), 0.5489043292357029, 0.8609179415855355, tensor(2.2365)]
[tensor(-0.5080), 0.5489043292357029, 0.8609179415855355, tensor(2.2365)]
[tensor(-0.5080), 0.5489043292357029, 0.8609179415855355, tensor(2.2365)]
early stopping at 9
[2023-01-16 21:50:19,618.618 dlcmzxjb7qmi93pp-master-0:11544 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 21:50:20,237.237 dlcmzxjb7qmi93pp-master-0:11612 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:50:20,268.268 dlcmzxjb7qmi93pp-master-0:11609 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:50:20,316.316 dlcmzxjb7qmi93pp-master-0:11611 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:50:20,322.322 dlcmzxjb7qmi93pp-master-0:11610 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 21:50:21,460.460 dlcmzxjb7qmi93pp-master-0:11610 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 21:50:21,465.465 dlcmzxjb7qmi93pp-master-0:11611 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 21:50:22,127.127 dlcmzxjb7qmi93pp-master-0:11612 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 21:50:22,131.131 dlcmzxjb7qmi93pp-master-0:11609 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:11609:11609 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:11610:11610 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11612:11612 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11611:11611 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5196), 0.5414216996258685, 0.8609179415855355, tensor(2.1875)]
[tensor(-0.5196), 0.5414216996258685, 0.8609179415855355, tensor(2.1875)]
[tensor(-0.5196), 0.5414216996258685, 0.8650904033379694, tensor(2.1875)]
[tensor(-0.5016), 0.55264564404062, 0.8650904033379694, tensor(2.2616)]
[tensor(-0.5016), 0.55264564404062, 0.8650904033379694, tensor(2.2616)]
[2023-01-16 22:00:42,976.976 dlcmzxjb7qmi93pp-master-0:11686 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 22:00:43,585.585 dlcmzxjb7qmi93pp-master-0:11752 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:00:43,585.585 dlcmzxjb7qmi93pp-master-0:11753 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:00:43,792.792 dlcmzxjb7qmi93pp-master-0:11751 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:00:43,851.851 dlcmzxjb7qmi93pp-master-0:11754 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:00:44,852.852 dlcmzxjb7qmi93pp-master-0:11753 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 22:00:45,015.015 dlcmzxjb7qmi93pp-master-0:11754 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 22:00:45,490.490 dlcmzxjb7qmi93pp-master-0:11752 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 22:00:45,498.498 dlcmzxjb7qmi93pp-master-0:11751 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:11751:11751 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:11754:11754 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11753:11753 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11752:11752 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5257), 0.5494388027792624, 0.8553546592489569, tensor(2.2215)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5189), 0.5494388027792624, 0.8630041724617524, tensor(2.2257)]
[tensor(-0.5052), 0.5494388027792624, 0.8699582753824756, tensor(2.2394)]
[tensor(-0.5052), 0.5494388027792624, 0.8699582753824756, tensor(2.2394)]
[tensor(-0.5052), 0.5494388027792624, 0.8699582753824756, tensor(2.2394)]
[2023-01-16 22:11:08,329.329 dlcmzxjb7qmi93pp-master-0:11829 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 22:11:08,961.961 dlcmzxjb7qmi93pp-master-0:11895 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:11:08,976.976 dlcmzxjb7qmi93pp-master-0:11896 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:11:09,036.036 dlcmzxjb7qmi93pp-master-0:11894 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:11:09,041.041 dlcmzxjb7qmi93pp-master-0:11897 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:11:09,958.958 dlcmzxjb7qmi93pp-master-0:11897 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 22:11:10,839.839 dlcmzxjb7qmi93pp-master-0:11895 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 22:11:10,866.866 dlcmzxjb7qmi93pp-master-0:11896 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 22:11:10,868.868 dlcmzxjb7qmi93pp-master-0:11894 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:11894:11894 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:11897:11897 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11896:11896 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:11895:11895 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.5430), 0.5344735435595938, 0.8428372739916551, tensor(2.1294)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5411), 0.5366114377338321, 0.8560500695410292, tensor(2.1419)]
[tensor(-0.5411), 0.5435595938001069, 0.8588317107093185, tensor(2.1760)]
[tensor(-0.5304), 0.5435595938001069, 0.8588317107093185, tensor(2.1794)]
[tensor(-0.5304), 0.5435595938001069, 0.8588317107093185, tensor(2.1794)]
[tensor(-0.5304), 0.5435595938001069, 0.8588317107093185, tensor(2.1794)]
[tensor(-0.5304), 0.5435595938001069, 0.8588317107093185, tensor(2.1794)]
[tensor(-0.5304), 0.5435595938001069, 0.8588317107093185, tensor(2.1794)]
[tensor(-0.5304), 0.5435595938001069, 0.8588317107093185, tensor(2.1794)]
early stopping at 9
[2023-01-16 22:29:30,161.161 dlcmzxjb7qmi93pp-master-0:11983 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 22:29:30,787.787 dlcmzxjb7qmi93pp-master-0:12051 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:29:30,798.798 dlcmzxjb7qmi93pp-master-0:12050 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:29:30,866.866 dlcmzxjb7qmi93pp-master-0:12048 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:29:30,870.870 dlcmzxjb7qmi93pp-master-0:12049 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:29:32,666.666 dlcmzxjb7qmi93pp-master-0:12051 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 22:29:32,668.668 dlcmzxjb7qmi93pp-master-0:12050 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 22:29:32,985.985 dlcmzxjb7qmi93pp-master-0:12049 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 22:29:32,987.987 dlcmzxjb7qmi93pp-master-0:12048 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:12048:12048 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:12050:12050 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12051:12051 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12049:12049 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5659), 0.5098877605558525, 0.8386648122392212, tensor(1.9836)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5659), 0.5179048637092464, 0.8463143254520167, tensor(2.0150)]
[tensor(-0.5384), 0.538214858364511, 0.8463143254520167, tensor(2.1527)]
[tensor(-0.5384), 0.538214858364511, 0.8525730180806675, tensor(2.1527)]
[tensor(-0.5384), 0.538214858364511, 0.8525730180806675, tensor(2.1527)]
[tensor(-0.5384), 0.538214858364511, 0.8525730180806675, tensor(2.1527)]
[tensor(-0.5384), 0.538214858364511, 0.8525730180806675, tensor(2.1527)]
[tensor(-0.5384), 0.538214858364511, 0.8525730180806675, tensor(2.1527)]
[tensor(-0.5384), 0.538214858364511, 0.8525730180806675, tensor(2.1527)]
[tensor(-0.5384), 0.538214858364511, 0.8525730180806675, tensor(2.1527)]
early stopping at 10
[2023-01-16 22:51:53,245.245 dlcmzxjb7qmi93pp-master-0:12143 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 22:51:53,860.860 dlcmzxjb7qmi93pp-master-0:12211 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:51:53,861.861 dlcmzxjb7qmi93pp-master-0:12208 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:51:53,941.941 dlcmzxjb7qmi93pp-master-0:12210 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:51:53,953.953 dlcmzxjb7qmi93pp-master-0:12209 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 22:51:55,370.370 dlcmzxjb7qmi93pp-master-0:12210 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 22:51:55,372.372 dlcmzxjb7qmi93pp-master-0:12209 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 22:51:55,730.730 dlcmzxjb7qmi93pp-master-0:12211 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 22:51:55,731.731 dlcmzxjb7qmi93pp-master-0:12208 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:12208:12208 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:12211:12211 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12209:12209 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12210:12210 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5322), 0.5478353821485836, 0.8518776077885952, tensor(2.2070)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5322), 0.5478353821485836, 0.8588317107093185, tensor(2.2070)]
[Mon Jan 16 22:57:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5287), 0.5478353821485836, 0.8643949930458971, tensor(2.2070)]
[tensor(-0.5287), 0.5478353821485836, 0.8643949930458971, tensor(2.2070)]
[tensor(-0.5287), 0.5478353821485836, 0.8643949930458971, tensor(2.2070)]
[2023-01-16 23:02:08,559.559 dlcmzxjb7qmi93pp-master-0:12285 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 23:02:09,189.189 dlcmzxjb7qmi93pp-master-0:12350 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:02:09,190.190 dlcmzxjb7qmi93pp-master-0:12351 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:02:09,190.190 dlcmzxjb7qmi93pp-master-0:12352 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:02:09,193.193 dlcmzxjb7qmi93pp-master-0:12353 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:02:10,217.217 dlcmzxjb7qmi93pp-master-0:12353 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 23:02:10,245.245 dlcmzxjb7qmi93pp-master-0:12351 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 23:02:11,208.208 dlcmzxjb7qmi93pp-master-0:12352 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 23:02:11,208.208 dlcmzxjb7qmi93pp-master-0:12350 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:12350:12350 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:12351:12351 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12353:12353 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12352:12352 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5590), 0.5173703901656868, 0.8581363004172462, tensor(2.0278)]
[tensor(-0.5162), 0.538214858364511, 0.8581363004172462, tensor(2.1749)]
[tensor(-0.5018), 0.5478353821485836, 0.868567454798331, tensor(2.2374)]
[tensor(-0.5018), 0.5478353821485836, 0.868567454798331, tensor(2.2374)]
[tensor(-0.5018), 0.5478353821485836, 0.868567454798331, tensor(2.2374)]
[2023-01-16 23:12:22,971.971 dlcmzxjb7qmi93pp-master-0:12428 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 23:12:23,598.598 dlcmzxjb7qmi93pp-master-0:12495 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:12:23,598.598 dlcmzxjb7qmi93pp-master-0:12493 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:12:23,600.600 dlcmzxjb7qmi93pp-master-0:12496 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:12:23,608.608 dlcmzxjb7qmi93pp-master-0:12494 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:12:25,569.569 dlcmzxjb7qmi93pp-master-0:12495 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 23:12:25,581.581 dlcmzxjb7qmi93pp-master-0:12494 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 23:12:25,584.584 dlcmzxjb7qmi93pp-master-0:12496 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 23:12:25,590.590 dlcmzxjb7qmi93pp-master-0:12493 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:12493:12493 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:12495:12495 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12494:12494 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12496:12496 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5612), 0.5061464457509354, 0.8435326842837274, tensor(1.9696)]
[tensor(-0.5265), 0.5424906467129877, 0.8609179415855355, tensor(2.1860)]
[Mon Jan 16 23:17:50 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5265), 0.5424906467129877, 0.8609179415855355, tensor(2.1860)]
[tensor(-0.5227), 0.5424906467129877, 0.8616133518776078, tensor(2.1860)]
[tensor(-0.5227), 0.5424906467129877, 0.8616133518776078, tensor(2.1860)]
[tensor(-0.5227), 0.5424906467129877, 0.8657858136300417, tensor(2.1860)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5202), 0.5424906467129877, 0.8657858136300417, tensor(2.1860)]
[tensor(-0.5202), 0.5424906467129877, 0.8657858136300417, tensor(2.1860)]
[tensor(-0.5202), 0.5424906467129877, 0.8657858136300417, tensor(2.1860)]
[tensor(-0.5202), 0.5424906467129877, 0.8657858136300417, tensor(2.1860)]
[tensor(-0.5202), 0.5424906467129877, 0.8657858136300417, tensor(2.1860)]
[tensor(-0.5202), 0.5424906467129877, 0.8657858136300417, tensor(2.1860)]
early stopping at 12
[2023-01-16 23:36:44,161.161 dlcmzxjb7qmi93pp-master-0:12591 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 23:36:44,790.790 dlcmzxjb7qmi93pp-master-0:12657 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:36:44,791.791 dlcmzxjb7qmi93pp-master-0:12658 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:36:44,963.963 dlcmzxjb7qmi93pp-master-0:12659 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:36:44,966.966 dlcmzxjb7qmi93pp-master-0:12656 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:36:46,659.659 dlcmzxjb7qmi93pp-master-0:12658 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 23:36:46,689.689 dlcmzxjb7qmi93pp-master-0:12657 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 23:36:46,840.840 dlcmzxjb7qmi93pp-master-0:12659 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 23:36:46,843.843 dlcmzxjb7qmi93pp-master-0:12656 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:12656:12656 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:12658:12658 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12659:12659 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12657:12657 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5479), 0.5221806520577231, 0.849095966620306, tensor(2.0630)]
[tensor(-0.5376), 0.5414216996258685, 0.8560500695410292, tensor(2.1695)]
[tensor(-0.5376), 0.5414216996258685, 0.8560500695410292, tensor(2.1695)]
[tensor(-0.5191), 0.547300908605024, 0.8560500695410292, tensor(2.2175)]
[tensor(-0.5191), 0.547300908605024, 0.8560500695410292, tensor(2.2175)]
[tensor(-0.5191), 0.547300908605024, 0.8567454798331016, tensor(2.2175)]
[tensor(-0.5191), 0.547300908605024, 0.8567454798331016, tensor(2.2175)]
[tensor(-0.5191), 0.547300908605024, 0.8567454798331016, tensor(2.2175)]
[tensor(-0.5191), 0.547300908605024, 0.8567454798331016, tensor(2.2175)]
[tensor(-0.5191), 0.547300908605024, 0.8567454798331016, tensor(2.2175)]
[tensor(-0.5191), 0.547300908605024, 0.8567454798331016, tensor(2.2175)]
early stopping at 11
[2023-01-16 23:58:52,206.206 dlcmzxjb7qmi93pp-master-0:12751 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-16 23:58:52,827.827 dlcmzxjb7qmi93pp-master-0:12817 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:58:52,828.828 dlcmzxjb7qmi93pp-master-0:12816 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:58:52,905.905 dlcmzxjb7qmi93pp-master-0:12818 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:58:52,915.915 dlcmzxjb7qmi93pp-master-0:12819 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-16 23:58:54,292.292 dlcmzxjb7qmi93pp-master-0:12818 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-16 23:58:54,294.294 dlcmzxjb7qmi93pp-master-0:12819 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-16 23:58:54,732.732 dlcmzxjb7qmi93pp-master-0:12817 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-16 23:58:54,738.738 dlcmzxjb7qmi93pp-master-0:12816 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:12816:12816 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:12817:12817 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12819:12819 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12818:12818 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5325), 0.5296632816675575, 0.849095966620306, tensor(2.1158)]
[tensor(-0.5209), 0.5350080171031534, 0.8609179415855355, tensor(2.1542)]
[tensor(-0.5142), 0.5350080171031534, 0.8636995827538247, tensor(2.1555)]
[tensor(-0.5041), 0.5414216996258685, 0.8678720445062587, tensor(2.2031)]
[Tue Jan 17 00:07:13 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5041), 0.5494388027792624, 0.8678720445062587, tensor(2.2343)]
[2023-01-17 00:09:11,590.590 dlcmzxjb7qmi93pp-master-0:12893 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:09:12,209.209 dlcmzxjb7qmi93pp-master-0:12960 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:09:12,211.211 dlcmzxjb7qmi93pp-master-0:12959 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:09:12,393.393 dlcmzxjb7qmi93pp-master-0:12958 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:09:12,393.393 dlcmzxjb7qmi93pp-master-0:12961 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:09:13,320.320 dlcmzxjb7qmi93pp-master-0:12961 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:09:14,088.088 dlcmzxjb7qmi93pp-master-0:12959 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:09:14,089.089 dlcmzxjb7qmi93pp-master-0:12960 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:09:14,091.091 dlcmzxjb7qmi93pp-master-0:12958 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:12958:12958 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:12961:12961 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12960:12960 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:12959:12959 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5699), 0.4922501336183859, 0.8567454798331016, tensor(1.8913)]
[tensor(-0.5188), 0.5403527525387494, 0.8567454798331016, tensor(2.1830)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5188), 0.5403527525387494, 0.8636995827538247, tensor(2.1830)]
[tensor(-0.5188), 0.5403527525387494, 0.8636995827538247, tensor(2.1830)]
[tensor(-0.5188), 0.5403527525387494, 0.8636995827538247, tensor(2.1830)]
[2023-01-17 00:19:38,959.959 dlcmzxjb7qmi93pp-master-0:13036 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:19:39,579.579 dlcmzxjb7qmi93pp-master-0:13101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:19:39,580.580 dlcmzxjb7qmi93pp-master-0:13104 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:19:39,580.580 dlcmzxjb7qmi93pp-master-0:13103 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:19:39,584.584 dlcmzxjb7qmi93pp-master-0:13102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:19:40,592.592 dlcmzxjb7qmi93pp-master-0:13104 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:19:40,593.593 dlcmzxjb7qmi93pp-master-0:13102 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:19:41,587.587 dlcmzxjb7qmi93pp-master-0:13103 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:19:41,592.592 dlcmzxjb7qmi93pp-master-0:13101 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:13101:13101 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:13104:13104 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13102:13102 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13103:13103 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5377), 0.549973276322822, 0.8525730180806675, tensor(2.2122)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5261), 0.549973276322822, 0.8525730180806675, tensor(2.2122)]
[tensor(-0.5261), 0.549973276322822, 0.8630041724617524, tensor(2.2122)]
[tensor(-0.5100), 0.549973276322822, 0.866481223922114, tensor(2.2122)]
[tensor(-0.5100), 0.549973276322822, 0.866481223922114, tensor(2.2122)]
[tensor(-0.5100), 0.549973276322822, 0.866481223922114, tensor(2.2122)]
[tensor(-0.5100), 0.549973276322822, 0.866481223922114, tensor(2.2122)]
[tensor(-0.5100), 0.549973276322822, 0.866481223922114, tensor(2.2122)]
[tensor(-0.5100), 0.549973276322822, 0.866481223922114, tensor(2.2122)]
early stopping at 9
[2023-01-17 00:37:52,804.804 dlcmzxjb7qmi93pp-master-0:13190 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:37:53,439.439 dlcmzxjb7qmi93pp-master-0:13258 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:37:53,445.445 dlcmzxjb7qmi93pp-master-0:13256 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:37:53,511.511 dlcmzxjb7qmi93pp-master-0:13257 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:37:53,532.532 dlcmzxjb7qmi93pp-master-0:13255 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:37:55,335.335 dlcmzxjb7qmi93pp-master-0:13256 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:37:55,338.338 dlcmzxjb7qmi93pp-master-0:13258 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:37:55,873.873 dlcmzxjb7qmi93pp-master-0:13257 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:37:55,876.876 dlcmzxjb7qmi93pp-master-0:13255 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:13255:13255 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:13258:13258 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13257:13257 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13256:13256 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5386), 0.5280598610368786, 0.8623087621696801, tensor(2.1017)]
[tensor(-0.5309), 0.5398182789951897, 0.8623087621696801, tensor(2.1681)]
[tensor(-0.5309), 0.5398182789951897, 0.8623087621696801, tensor(2.1681)]
[tensor(-0.5189), 0.5451630144307856, 0.8623087621696801, tensor(2.2069)]
[tensor(-0.5189), 0.5451630144307856, 0.8623087621696801, tensor(2.2069)]
[tensor(-0.5189), 0.5451630144307856, 0.8623087621696801, tensor(2.2069)]
[tensor(-0.5189), 0.5451630144307856, 0.8623087621696801, tensor(2.2069)]
[tensor(-0.5189), 0.5451630144307856, 0.8623087621696801, tensor(2.2069)]
[tensor(-0.5189), 0.5451630144307856, 0.8623087621696801, tensor(2.2069)]
early stopping at 9
[2023-01-17 00:56:01,658.658 dlcmzxjb7qmi93pp-master-0:13344 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 00:56:02,282.282 dlcmzxjb7qmi93pp-master-0:13410 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:56:02,284.284 dlcmzxjb7qmi93pp-master-0:13412 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:56:02,357.357 dlcmzxjb7qmi93pp-master-0:13411 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:56:02,366.366 dlcmzxjb7qmi93pp-master-0:13409 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 00:56:03,653.653 dlcmzxjb7qmi93pp-master-0:13411 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 00:56:04,219.219 dlcmzxjb7qmi93pp-master-0:13410 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 00:56:04,258.258 dlcmzxjb7qmi93pp-master-0:13412 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 00:56:04,267.267 dlcmzxjb7qmi93pp-master-0:13409 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:13409:13409 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:13411:13411 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13412:13412 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13410:13410 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5425), 0.5237840726884019, 0.8504867872044506, tensor(2.0764)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5334), 0.5237840726884019, 0.8595271210013908, tensor(2.0856)]
[tensor(-0.5186), 0.5387493319080705, 0.8602225312934632, tensor(2.1752)]
[tensor(-0.5186), 0.5440940673436665, 0.8602225312934632, tensor(2.1975)]
[tensor(-0.5186), 0.5440940673436665, 0.8630041724617524, tensor(2.1975)]
[2023-01-17 01:06:15,025.025 dlcmzxjb7qmi93pp-master-0:13487 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 01:06:15,648.648 dlcmzxjb7qmi93pp-master-0:13552 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:06:15,649.649 dlcmzxjb7qmi93pp-master-0:13554 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:06:15,656.656 dlcmzxjb7qmi93pp-master-0:13555 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:06:15,657.657 dlcmzxjb7qmi93pp-master-0:13553 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:06:16,666.666 dlcmzxjb7qmi93pp-master-0:13555 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 01:06:17,640.640 dlcmzxjb7qmi93pp-master-0:13553 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 01:06:17,660.660 dlcmzxjb7qmi93pp-master-0:13554 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 01:06:17,668.668 dlcmzxjb7qmi93pp-master-0:13552 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:13552:13552 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:13554:13554 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13553:13553 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13555:13555 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5365), 0.538214858364511, 0.8497913769123783, tensor(2.1545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5219), 0.5398182789951897, 0.8616133518776078, tensor(2.1772)]
[tensor(-0.5143), 0.5451630144307856, 0.8671766342141863, tensor(2.2115)]
[tensor(-0.5143), 0.5451630144307856, 0.8671766342141863, tensor(2.2115)]
[tensor(-0.5143), 0.5451630144307856, 0.8671766342141863, tensor(2.2115)]
[2023-01-17 01:16:37,392.392 dlcmzxjb7qmi93pp-master-0:13629 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 01:16:38,019.019 dlcmzxjb7qmi93pp-master-0:13694 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:16:38,095.095 dlcmzxjb7qmi93pp-master-0:13696 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:16:38,102.102 dlcmzxjb7qmi93pp-master-0:13695 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:16:38,154.154 dlcmzxjb7qmi93pp-master-0:13697 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:16:39,365.365 dlcmzxjb7qmi93pp-master-0:13695 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 01:16:39,366.366 dlcmzxjb7qmi93pp-master-0:13696 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 01:16:39,963.963 dlcmzxjb7qmi93pp-master-0:13697 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 01:16:39,965.965 dlcmzxjb7qmi93pp-master-0:13694 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:13694:13694 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:13695:13695 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13696:13696 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13697:13697 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5684), 0.4975948690539818, 0.8463143254520167, tensor(1.9196)]
[tensor(-0.5373), 0.5307322287546766, 0.8602225312934632, tensor(2.1164)]
[tensor(-0.5256), 0.532870122928915, 0.8609179415855355, tensor(2.1387)]
[tensor(-0.5233), 0.5435595938001069, 0.866481223922114, tensor(2.1945)]
[tensor(-0.5233), 0.5435595938001069, 0.866481223922114, tensor(2.1945)]
[tensor(-0.5233), 0.5435595938001069, 0.866481223922114, tensor(2.1945)]
[tensor(-0.5233), 0.5435595938001069, 0.866481223922114, tensor(2.1945)]
[tensor(-0.5233), 0.5435595938001069, 0.866481223922114, tensor(2.1945)]
[tensor(-0.5233), 0.5435595938001069, 0.866481223922114, tensor(2.1945)]
early stopping at 9
[2023-01-17 01:35:01,204.204 dlcmzxjb7qmi93pp-master-0:13784 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 01:35:01,839.839 dlcmzxjb7qmi93pp-master-0:13852 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:35:01,840.840 dlcmzxjb7qmi93pp-master-0:13849 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:35:01,919.919 dlcmzxjb7qmi93pp-master-0:13851 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:35:01,919.919 dlcmzxjb7qmi93pp-master-0:13850 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:35:02,790.790 dlcmzxjb7qmi93pp-master-0:13852 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 01:35:03,219.219 dlcmzxjb7qmi93pp-master-0:13851 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 01:35:03,220.220 dlcmzxjb7qmi93pp-master-0:13850 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 01:35:03,221.221 dlcmzxjb7qmi93pp-master-0:13849 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:13849:13849 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:13852:13852 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13851:13851 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:13850:13850 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5327), 0.5366114377338321, 0.8539638386648123, tensor(2.1503)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5236), 0.5366114377338321, 0.8636995827538247, tensor(2.1567)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5120), 0.547300908605024, 0.8643949930458971, tensor(2.2245)]
[tensor(-0.5120), 0.547300908605024, 0.8643949930458971, tensor(2.2245)]
[tensor(-0.5120), 0.547300908605024, 0.8650904033379694, tensor(2.2245)]
[tensor(-0.5120), 0.547300908605024, 0.8650904033379694, tensor(2.2245)]
[tensor(-0.5120), 0.547300908605024, 0.8650904033379694, tensor(2.2245)]
[tensor(-0.5120), 0.547300908605024, 0.8650904033379694, tensor(2.2245)]
[tensor(-0.5120), 0.547300908605024, 0.8650904033379694, tensor(2.2245)]
[tensor(-0.5120), 0.547300908605024, 0.8650904033379694, tensor(2.2245)]
early stopping at 10
[2023-01-17 01:57:31,279.279 dlcmzxjb7qmi93pp-master-0:13945 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 01:57:31,929.929 dlcmzxjb7qmi93pp-master-0:14011 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:57:31,946.946 dlcmzxjb7qmi93pp-master-0:14012 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:57:32,051.051 dlcmzxjb7qmi93pp-master-0:14010 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:57:32,051.051 dlcmzxjb7qmi93pp-master-0:14013 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 01:57:33,829.829 dlcmzxjb7qmi93pp-master-0:14012 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 01:57:33,830.830 dlcmzxjb7qmi93pp-master-0:14011 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 01:57:33,913.913 dlcmzxjb7qmi93pp-master-0:14013 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 01:57:33,916.916 dlcmzxjb7qmi93pp-master-0:14010 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:14010:14010 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:14013:14013 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14012:14012 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14011:14011 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5370), 0.5424906467129877, 0.8560500695410292, tensor(2.1754)]
[tensor(-0.5370), 0.5424906467129877, 0.8678720445062587, tensor(2.1754)]
[tensor(-0.5168), 0.5424906467129877, 0.8678720445062587, tensor(2.1770)]
[tensor(-0.5138), 0.5424906467129877, 0.8678720445062587, tensor(2.1853)]
[tensor(-0.5138), 0.5424906467129877, 0.8713490959666204, tensor(2.1853)]
[2023-01-17 02:07:46,618.618 dlcmzxjb7qmi93pp-master-0:14087 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 02:07:47,368.368 dlcmzxjb7qmi93pp-master-0:14153 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:07:47,369.369 dlcmzxjb7qmi93pp-master-0:14155 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:07:47,369.369 dlcmzxjb7qmi93pp-master-0:14154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:07:47,372.372 dlcmzxjb7qmi93pp-master-0:14152 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:07:48,418.418 dlcmzxjb7qmi93pp-master-0:14154 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 02:07:49,317.317 dlcmzxjb7qmi93pp-master-0:14155 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 02:07:49,348.348 dlcmzxjb7qmi93pp-master-0:14153 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 02:07:49,353.353 dlcmzxjb7qmi93pp-master-0:14152 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:14152:14152 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:14153:14153 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14155:14155 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14154:14154 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5181), 0.547300908605024, 0.8595271210013908, tensor(2.2184)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5166), 0.549973276322822, 0.874826147426982, tensor(2.2333)]
[tensor(-0.5064), 0.549973276322822, 0.874826147426982, tensor(2.2333)]
[tensor(-0.5064), 0.549973276322822, 0.874826147426982, tensor(2.2333)]
[tensor(-0.5064), 0.549973276322822, 0.874826147426982, tensor(2.2333)]
[2023-01-17 02:17:56,939.939 dlcmzxjb7qmi93pp-master-0:14229 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 02:17:57,551.551 dlcmzxjb7qmi93pp-master-0:14295 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:17:57,551.551 dlcmzxjb7qmi93pp-master-0:14296 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:17:57,745.745 dlcmzxjb7qmi93pp-master-0:14294 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:17:57,822.822 dlcmzxjb7qmi93pp-master-0:14297 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:17:58,788.788 dlcmzxjb7qmi93pp-master-0:14296 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 02:17:58,982.982 dlcmzxjb7qmi93pp-master-0:14297 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 02:17:59,368.368 dlcmzxjb7qmi93pp-master-0:14295 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 02:17:59,371.371 dlcmzxjb7qmi93pp-master-0:14294 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:14294:14294 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:14297:14297 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14296:14296 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14295:14295 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[tensor(-0.5269), 0.5408872260823089, 0.8595271210013908, tensor(2.1775)]
[tensor(-0.5207), 0.547300908605024, 0.868567454798331, tensor(2.2158)]
[tensor(-0.5207), 0.547300908605024, 0.8720445062586927, tensor(2.2158)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5139), 0.549973276322822, 0.8720445062586927, tensor(2.2360)]
[tensor(-0.5139), 0.549973276322822, 0.8720445062586927, tensor(2.2360)]
[tensor(-0.5139), 0.549973276322822, 0.8720445062586927, tensor(2.2360)]
[tensor(-0.5139), 0.549973276322822, 0.8720445062586927, tensor(2.2360)]
[tensor(-0.5139), 0.549973276322822, 0.8720445062586927, tensor(2.2360)]
[tensor(-0.5139), 0.549973276322822, 0.8720445062586927, tensor(2.2360)]
early stopping at 9
[2023-01-17 02:36:07,761.761 dlcmzxjb7qmi93pp-master-0:14384 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 02:36:08,382.382 dlcmzxjb7qmi93pp-master-0:14449 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:36:08,383.383 dlcmzxjb7qmi93pp-master-0:14452 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:36:08,383.383 dlcmzxjb7qmi93pp-master-0:14450 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:36:08,387.387 dlcmzxjb7qmi93pp-master-0:14451 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 02:36:09,332.332 dlcmzxjb7qmi93pp-master-0:14451 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 02:36:10,322.322 dlcmzxjb7qmi93pp-master-0:14450 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 02:36:10,331.331 dlcmzxjb7qmi93pp-master-0:14452 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 02:36:10,334.334 dlcmzxjb7qmi93pp-master-0:14449 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:14449:14449 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:14451:14451 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14452:14452 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14450:14450 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/pai/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5229), 0.547300908605024, 0.8581363004172462, tensor(2.2136)]
[tensor(-0.5229), 0.547300908605024, 0.8609179415855355, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8609179415855355, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8650904033379694, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8650904033379694, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8650904033379694, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.866481223922114, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.866481223922114, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.866481223922114, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.866481223922114, tensor(2.2136)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.5176), 0.547300908605024, 0.8678720445062587, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8678720445062587, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8678720445062587, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8678720445062587, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8678720445062587, tensor(2.2136)]
[tensor(-0.5176), 0.547300908605024, 0.8678720445062587, tensor(2.2136)]
early stopping at 16
[2023-01-17 03:08:22,412.412 dlcmzxjb7qmi93pp-master-0:14559 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 03:08:23,037.037 dlcmzxjb7qmi93pp-master-0:14626 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:08:23,037.037 dlcmzxjb7qmi93pp-master-0:14627 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:08:23,037.037 dlcmzxjb7qmi93pp-master-0:14625 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:08:23,039.039 dlcmzxjb7qmi93pp-master-0:14624 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:08:24,001.001 dlcmzxjb7qmi93pp-master-0:14626 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 03:08:24,990.990 dlcmzxjb7qmi93pp-master-0:14625 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 03:08:24,997.997 dlcmzxjb7qmi93pp-master-0:14627 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 03:08:24,998.998 dlcmzxjb7qmi93pp-master-0:14624 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:14624:14624 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:14626:14626 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14627:14627 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14625:14625 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7786), 0.4462854088722608, 0.6481223922114048, tensor(1.4529)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-17 03:19:27,802.802 dlcmzxjb7qmi93pp-master-0:14703 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 03:19:28,427.427 dlcmzxjb7qmi93pp-master-0:14769 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:19:28,427.427 dlcmzxjb7qmi93pp-master-0:14768 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:19:28,429.429 dlcmzxjb7qmi93pp-master-0:14770 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:19:28,435.435 dlcmzxjb7qmi93pp-master-0:14771 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:19:29,392.392 dlcmzxjb7qmi93pp-master-0:14770 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 03:19:29,393.393 dlcmzxjb7qmi93pp-master-0:14769 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 03:19:30,383.383 dlcmzxjb7qmi93pp-master-0:14771 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 03:19:30,384.384 dlcmzxjb7qmi93pp-master-0:14768 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:14768:14768 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:14769:14769 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14771:14771 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14770:14770 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-17 03:29:44,144.144 dlcmzxjb7qmi93pp-master-0:14845 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 03:29:44,784.784 dlcmzxjb7qmi93pp-master-0:14912 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:29:44,786.786 dlcmzxjb7qmi93pp-master-0:14910 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:29:44,984.984 dlcmzxjb7qmi93pp-master-0:14913 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:29:44,984.984 dlcmzxjb7qmi93pp-master-0:14911 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 03:29:45,853.853 dlcmzxjb7qmi93pp-master-0:14911 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 03:29:45,857.857 dlcmzxjb7qmi93pp-master-0:14913 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 03:29:46,634.634 dlcmzxjb7qmi93pp-master-0:14912 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 03:29:46,636.636 dlcmzxjb7qmi93pp-master-0:14910 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:14910:14910 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:14912:14912 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14911:14911 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:14913:14913 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7779), 0.4462854088722608, 0.6481223922114048, tensor(1.4535)]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[2023-01-17 05:09:12,654.654 dlcmzxjb7qmi93pp-master-0:15120 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 05:09:13,268.268 dlcmzxjb7qmi93pp-master-0:15186 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:09:13,268.268 dlcmzxjb7qmi93pp-master-0:15185 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:09:13,268.268 dlcmzxjb7qmi93pp-master-0:15187 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:09:13,274.274 dlcmzxjb7qmi93pp-master-0:15188 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 05:09:14,247.247 dlcmzxjb7qmi93pp-master-0:15187 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 05:09:15,236.236 dlcmzxjb7qmi93pp-master-0:15188 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 05:09:15,242.242 dlcmzxjb7qmi93pp-master-0:15186 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 05:09:15,250.250 dlcmzxjb7qmi93pp-master-0:15185 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:15185:15185 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:15187:15187 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15188:15188 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15186:15186 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[tensor(-0.7758), 0.4462854088722608, 0.6481223922114048, tensor(1.4557)]
[2023-01-17 06:50:04,340.340 dlcmzxjb7qmi93pp-master-0:15398 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 06:50:04,948.948 dlcmzxjb7qmi93pp-master-0:15466 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:50:04,948.948 dlcmzxjb7qmi93pp-master-0:15467 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:50:05,030.030 dlcmzxjb7qmi93pp-master-0:15465 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:50:05,037.037 dlcmzxjb7qmi93pp-master-0:15464 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 06:50:06,821.821 dlcmzxjb7qmi93pp-master-0:15467 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 06:50:06,822.822 dlcmzxjb7qmi93pp-master-0:15466 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 06:50:07,326.326 dlcmzxjb7qmi93pp-master-0:15465 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 06:50:07,329.329 dlcmzxjb7qmi93pp-master-0:15464 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:15464:15464 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:15466:15466 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15465:15465 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15467:15467 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-17 07:00:49,721.721 dlcmzxjb7qmi93pp-master-0:15542 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 07:00:50,337.337 dlcmzxjb7qmi93pp-master-0:15609 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:00:50,380.380 dlcmzxjb7qmi93pp-master-0:15607 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:00:50,414.414 dlcmzxjb7qmi93pp-master-0:15610 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:00:50,426.426 dlcmzxjb7qmi93pp-master-0:15608 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:00:51,339.339 dlcmzxjb7qmi93pp-master-0:15608 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 07:00:51,357.357 dlcmzxjb7qmi93pp-master-0:15610 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 07:00:52,162.162 dlcmzxjb7qmi93pp-master-0:15609 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 07:00:52,170.170 dlcmzxjb7qmi93pp-master-0:15607 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:15607:15607 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:15609:15609 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15608:15608 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15610:15610 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-17 07:11:58,113.113 dlcmzxjb7qmi93pp-master-0:15685 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 07:11:58,738.738 dlcmzxjb7qmi93pp-master-0:15751 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:11:58,738.738 dlcmzxjb7qmi93pp-master-0:15752 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:11:58,849.849 dlcmzxjb7qmi93pp-master-0:15753 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:11:58,853.853 dlcmzxjb7qmi93pp-master-0:15750 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 07:12:00,610.610 dlcmzxjb7qmi93pp-master-0:15752 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 07:12:00,611.611 dlcmzxjb7qmi93pp-master-0:15751 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 07:12:00,720.720 dlcmzxjb7qmi93pp-master-0:15753 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 07:12:00,725.725 dlcmzxjb7qmi93pp-master-0:15750 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:15750:15750 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:15753:15753 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15751:15751 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:15752:15752 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7777), 0.4462854088722608, 0.6481223922114048, tensor(1.4538)]
[tensor(-0.7777), 0.4462854088722608, 0.6481223922114048, tensor(1.4538)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7378), 0.4462854088722608, 0.6815020862308763, tensor(1.4937)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7378), 0.4462854088722608, 0.6815020862308763, tensor(1.4937)]
[tensor(-0.7364), 0.4462854088722608, 0.6835883171070932, tensor(1.4937)]
[tensor(-0.7364), 0.4462854088722608, 0.7162726008344924, tensor(1.4937)]
[tensor(-0.7364), 0.4462854088722608, 0.7162726008344924, tensor(1.4937)]
[tensor(-0.7364), 0.4462854088722608, 0.7162726008344924, tensor(1.4937)]
[tensor(-0.7364), 0.4462854088722608, 0.7162726008344924, tensor(1.4937)]
[tensor(-0.7364), 0.4462854088722608, 0.7162726008344924, tensor(1.4937)]
[tensor(-0.7364), 0.4462854088722608, 0.7162726008344924, tensor(1.4937)]
[tensor(-0.7364), 0.4478888295029396, 0.7162726008344924, tensor(1.4937)]
[tensor(-0.7364), 0.4478888295029396, 0.721835883171071, tensor(1.4937)]
[tensor(-0.7364), 0.4478888295029396, 0.721835883171071, tensor(1.4937)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7364), 0.4478888295029396, 0.721835883171071, tensor(1.4937)]
[tensor(-0.7364), 0.4478888295029396, 0.721835883171071, tensor(1.4937)]
[tensor(-0.7364), 0.4478888295029396, 0.721835883171071, tensor(1.4937)]
[tensor(-0.7364), 0.4478888295029396, 0.721835883171071, tensor(1.4937)]
early stopping at 42
[2023-01-17 08:35:36,788.788 dlcmzxjb7qmi93pp-master-0:15938 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 08:35:37,404.404 dlcmzxjb7qmi93pp-master-0:16004 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:35:37,404.404 dlcmzxjb7qmi93pp-master-0:16005 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:35:37,501.501 dlcmzxjb7qmi93pp-master-0:16006 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:35:37,517.517 dlcmzxjb7qmi93pp-master-0:16003 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 08:35:39,279.279 dlcmzxjb7qmi93pp-master-0:16005 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 08:35:39,280.280 dlcmzxjb7qmi93pp-master-0:16004 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 08:35:39,340.340 dlcmzxjb7qmi93pp-master-0:16006 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 08:35:39,340.340 dlcmzxjb7qmi93pp-master-0:16003 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:16003:16003 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:16005:16005 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16006:16006 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16004:16004 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6481223922114048, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6488178025034771, tensor(1.4734)]
[tensor(-0.7580), 0.4462854088722608, 0.6488178025034771, tensor(1.4734)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7344), 0.4462854088722608, 0.6995827538247567, tensor(1.4970)]
[tensor(-0.7344), 0.4462854088722608, 0.6995827538247567, tensor(1.4970)]
[tensor(-0.7344), 0.4462854088722608, 0.6995827538247567, tensor(1.4970)]
[tensor(-0.7344), 0.4462854088722608, 0.6995827538247567, tensor(1.4970)]
[tensor(-0.7344), 0.4462854088722608, 0.6995827538247567, tensor(1.4970)]
[tensor(-0.7344), 0.4462854088722608, 0.6995827538247567, tensor(1.4970)]
[tensor(-0.7344), 0.4462854088722608, 0.6995827538247567, tensor(1.4970)]
[2023-01-17 10:14:46,482.482 dlcmzxjb7qmi93pp-master-0:16214 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 10:14:47,107.107 dlcmzxjb7qmi93pp-master-0:16281 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:14:47,117.117 dlcmzxjb7qmi93pp-master-0:16280 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:14:47,189.189 dlcmzxjb7qmi93pp-master-0:16282 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:14:47,190.190 dlcmzxjb7qmi93pp-master-0:16279 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:14:49,023.023 dlcmzxjb7qmi93pp-master-0:16280 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 10:14:49,028.028 dlcmzxjb7qmi93pp-master-0:16281 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 10:14:49,159.159 dlcmzxjb7qmi93pp-master-0:16282 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 10:14:49,161.161 dlcmzxjb7qmi93pp-master-0:16279 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:16279:16279 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:16280:16280 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16282:16282 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16281:16281 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7751), 0.4462854088722608, 0.6474269819193325, tensor(1.4563)]
[tensor(-0.7749), 0.4462854088722608, 0.6474269819193325, tensor(1.4566)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-17 10:24:59,829.829 dlcmzxjb7qmi93pp-master-0:16356 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 10:25:00,466.466 dlcmzxjb7qmi93pp-master-0:16423 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:25:00,502.502 dlcmzxjb7qmi93pp-master-0:16422 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:25:00,546.546 dlcmzxjb7qmi93pp-master-0:16424 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:25:00,623.623 dlcmzxjb7qmi93pp-master-0:16421 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:25:02,309.309 dlcmzxjb7qmi93pp-master-0:16423 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 10:25:02,437.437 dlcmzxjb7qmi93pp-master-0:16424 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 10:25:02,496.496 dlcmzxjb7qmi93pp-master-0:16422 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 10:25:02,498.498 dlcmzxjb7qmi93pp-master-0:16421 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:16421:16421 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:16422:16422 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16423:16423 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16424:16424 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-17 10:35:14,208.208 dlcmzxjb7qmi93pp-master-0:16498 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 10:35:14,904.904 dlcmzxjb7qmi93pp-master-0:16566 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:35:14,950.950 dlcmzxjb7qmi93pp-master-0:16565 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:35:15,007.007 dlcmzxjb7qmi93pp-master-0:16567 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:35:15,009.009 dlcmzxjb7qmi93pp-master-0:16564 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 10:35:16,867.867 dlcmzxjb7qmi93pp-master-0:16565 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 10:35:16,902.902 dlcmzxjb7qmi93pp-master-0:16567 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 10:35:16,940.940 dlcmzxjb7qmi93pp-master-0:16566 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 10:35:16,949.949 dlcmzxjb7qmi93pp-master-0:16564 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:16564:16564 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:16566:16566 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16567:16567 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16565:16565 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7751), 0.4462854088722608, 0.6474269819193325, tensor(1.4563)]
[tensor(-0.7750), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[Tue Jan 17 10:40:50 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7750), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7750), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Tue Jan 17 11:38:07 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-17 12:15:10,828.828 dlcmzxjb7qmi93pp-master-0:16775 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 12:15:11,453.453 dlcmzxjb7qmi93pp-master-0:16842 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:15:11,457.457 dlcmzxjb7qmi93pp-master-0:16840 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:15:11,512.512 dlcmzxjb7qmi93pp-master-0:16843 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:15:11,576.576 dlcmzxjb7qmi93pp-master-0:16841 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 12:15:12,778.778 dlcmzxjb7qmi93pp-master-0:16843 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 12:15:12,805.805 dlcmzxjb7qmi93pp-master-0:16841 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 12:15:13,341.341 dlcmzxjb7qmi93pp-master-0:16842 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 12:15:13,345.345 dlcmzxjb7qmi93pp-master-0:16840 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:16840:16840 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:16841:16841 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16843:16843 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:16842:16842 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Tue Jan 17 12:54:27 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Tue Jan 17 13:15:52 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Tue Jan 17 13:21:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[Tue Jan 17 13:41:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[Tue Jan 17 13:49:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[tensor(-0.7663), 0.4462854088722608, 0.7141863699582753, tensor(1.4651)]
[2023-01-17 13:55:08,318.318 dlcmzxjb7qmi93pp-master-0:17052 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 13:55:08,947.947 dlcmzxjb7qmi93pp-master-0:17117 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:55:08,948.948 dlcmzxjb7qmi93pp-master-0:17120 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:55:08,948.948 dlcmzxjb7qmi93pp-master-0:17118 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:55:08,953.953 dlcmzxjb7qmi93pp-master-0:17119 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 13:55:09,957.957 dlcmzxjb7qmi93pp-master-0:17118 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 13:55:10,941.941 dlcmzxjb7qmi93pp-master-0:17120 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 13:55:10,947.947 dlcmzxjb7qmi93pp-master-0:17119 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 13:55:10,953.953 dlcmzxjb7qmi93pp-master-0:17117 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:17117:17117 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:17118:17118 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17119:17119 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17120:17120 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Tue Jan 17 14:03:38 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-17 14:05:36,701.701 dlcmzxjb7qmi93pp-master-0:17194 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 14:05:37,314.314 dlcmzxjb7qmi93pp-master-0:17262 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:05:37,314.314 dlcmzxjb7qmi93pp-master-0:17259 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:05:37,314.314 dlcmzxjb7qmi93pp-master-0:17261 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:05:37,316.316 dlcmzxjb7qmi93pp-master-0:17260 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:05:39,337.337 dlcmzxjb7qmi93pp-master-0:17262 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 14:05:39,340.340 dlcmzxjb7qmi93pp-master-0:17261 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 14:05:39,344.344 dlcmzxjb7qmi93pp-master-0:17260 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 14:05:39,347.347 dlcmzxjb7qmi93pp-master-0:17259 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:17259:17259 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:17260:17260 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17262:17262 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17261:17261 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[2023-01-17 14:16:05,080.080 dlcmzxjb7qmi93pp-master-0:17337 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 14:16:05,714.714 dlcmzxjb7qmi93pp-master-0:17405 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:16:05,719.719 dlcmzxjb7qmi93pp-master-0:17402 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:16:05,723.723 dlcmzxjb7qmi93pp-master-0:17404 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:16:05,733.733 dlcmzxjb7qmi93pp-master-0:17403 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 14:16:06,923.923 dlcmzxjb7qmi93pp-master-0:17404 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 14:16:06,927.927 dlcmzxjb7qmi93pp-master-0:17405 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 14:16:07,905.905 dlcmzxjb7qmi93pp-master-0:17403 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 14:16:07,914.914 dlcmzxjb7qmi93pp-master-0:17402 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:17402:17402 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:17403:17403 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17404:17404 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17405:17405 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7747), 0.4462854088722608, 0.6474269819193325, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6474269819193325, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6474269819193325, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6474269819193325, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Tue Jan 17 14:27:30 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 14:40:43 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 14:55:19 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 14:57:11 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:00:50 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:02:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:04:38 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:07:29 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:09:19 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:17:18 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:19:18 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:20:33 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:36:51 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:41:25 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:42:59 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Tue Jan 17 15:45:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[2023-01-17 15:56:44,792.792 dlcmzxjb7qmi93pp-master-0:17615 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 15:56:45,410.410 dlcmzxjb7qmi93pp-master-0:17682 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:56:45,410.410 dlcmzxjb7qmi93pp-master-0:17681 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:56:45,622.622 dlcmzxjb7qmi93pp-master-0:17683 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:56:45,624.624 dlcmzxjb7qmi93pp-master-0:17680 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 15:56:46,485.485 dlcmzxjb7qmi93pp-master-0:17683 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 15:56:47,291.291 dlcmzxjb7qmi93pp-master-0:17682 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 15:56:47,300.300 dlcmzxjb7qmi93pp-master-0:17681 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 15:56:47,306.306 dlcmzxjb7qmi93pp-master-0:17680 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:17680:17680 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:17683:17683 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17682:17682 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17681:17681 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6488178025034771, tensor(1.4569)]
[2023-01-17 17:37:15,579.579 dlcmzxjb7qmi93pp-master-0:17892 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 17:37:16,218.218 dlcmzxjb7qmi93pp-master-0:17958 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:37:16,219.219 dlcmzxjb7qmi93pp-master-0:17959 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:37:16,295.295 dlcmzxjb7qmi93pp-master-0:17960 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:37:16,300.300 dlcmzxjb7qmi93pp-master-0:17957 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:37:18,499.499 dlcmzxjb7qmi93pp-master-0:17958 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 17:37:18,509.509 dlcmzxjb7qmi93pp-master-0:17959 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 17:37:18,946.946 dlcmzxjb7qmi93pp-master-0:17960 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 17:37:18,948.948 dlcmzxjb7qmi93pp-master-0:17957 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:17957:17957 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:17960:17960 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17959:17959 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:17958:17958 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7777), 0.4462854088722608, 0.6481223922114048, tensor(1.4537)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-17 17:47:33,931.931 dlcmzxjb7qmi93pp-master-0:18034 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 17:47:34,562.562 dlcmzxjb7qmi93pp-master-0:18099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:47:34,563.563 dlcmzxjb7qmi93pp-master-0:18100 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:47:34,566.566 dlcmzxjb7qmi93pp-master-0:18102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:47:34,567.567 dlcmzxjb7qmi93pp-master-0:18101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:47:35,600.600 dlcmzxjb7qmi93pp-master-0:18102 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 17:47:35,602.602 dlcmzxjb7qmi93pp-master-0:18101 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 17:47:36,447.447 dlcmzxjb7qmi93pp-master-0:18100 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 17:47:36,457.457 dlcmzxjb7qmi93pp-master-0:18099 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:18099:18099 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:18101:18101 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18102:18102 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18100:18100 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-17 17:57:52,297.297 dlcmzxjb7qmi93pp-master-0:18176 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 17:57:52,928.928 dlcmzxjb7qmi93pp-master-0:18243 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:52,928.928 dlcmzxjb7qmi93pp-master-0:18241 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:52,929.929 dlcmzxjb7qmi93pp-master-0:18242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:52,961.961 dlcmzxjb7qmi93pp-master-0:18244 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 17:57:53,925.925 dlcmzxjb7qmi93pp-master-0:18243 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 17:57:53,928.928 dlcmzxjb7qmi93pp-master-0:18244 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 17:57:53,930.930 dlcmzxjb7qmi93pp-master-0:18242 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 17:57:53,936.936 dlcmzxjb7qmi93pp-master-0:18241 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:18241:18241 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:18244:18244 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18243:18243 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18242:18242 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7774), 0.4462854088722608, 0.6474269819193325, tensor(1.4540)]
[tensor(-0.7773), 0.4462854088722608, 0.6474269819193325, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[2023-01-17 19:37:58,827.827 dlcmzxjb7qmi93pp-master-0:18453 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 19:37:59,447.447 dlcmzxjb7qmi93pp-master-0:18521 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:37:59,447.447 dlcmzxjb7qmi93pp-master-0:18519 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:37:59,447.447 dlcmzxjb7qmi93pp-master-0:18518 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:37:59,449.449 dlcmzxjb7qmi93pp-master-0:18520 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 19:38:00,396.396 dlcmzxjb7qmi93pp-master-0:18520 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 19:38:00,397.397 dlcmzxjb7qmi93pp-master-0:18519 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 19:38:01,391.391 dlcmzxjb7qmi93pp-master-0:18521 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 19:38:01,397.397 dlcmzxjb7qmi93pp-master-0:18518 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:18518:18518 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:18521:18521 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18519:18519 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18520:18520 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7774), 0.4462854088722608, 0.6474269819193325, tensor(1.4540)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7253), 0.4494922501336184, 0.7482614742698191, tensor(1.5222)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.6868), 0.46125066809192944, 0.7719054242002782, tensor(1.6195)]
[tensor(-0.6820), 0.46125066809192944, 0.7719054242002782, tensor(1.6195)]
[tensor(-0.6728), 0.46125066809192944, 0.7719054242002782, tensor(1.6195)]
[tensor(-0.6622), 0.46125066809192944, 0.7788595271210014, tensor(1.6195)]
[tensor(-0.6568), 0.46125066809192944, 0.7788595271210014, tensor(1.6361)]
[tensor(-0.6534), 0.4649919828968466, 0.7788595271210014, tensor(1.6715)]
[tensor(-0.6462), 0.4692677712453234, 0.7809457579972183, tensor(1.7001)]
[tensor(-0.6351), 0.4692677712453234, 0.7872044506258693, tensor(1.7113)]
[tensor(-0.6351), 0.4692677712453234, 0.7878998609179416, tensor(1.7113)]
[tensor(-0.6351), 0.4692677712453234, 0.7878998609179416, tensor(1.7113)]
[tensor(-0.6248), 0.4901122394441475, 0.8004172461752433, tensor(1.8258)]
[tensor(-0.6248), 0.4901122394441475, 0.8004172461752433, tensor(1.8258)]
[tensor(-0.6248), 0.4901122394441475, 0.8004172461752433, tensor(1.8258)]
[tensor(-0.6248), 0.4901122394441475, 0.8004172461752433, tensor(1.8258)]
[tensor(-0.6248), 0.4901122394441475, 0.8004172461752433, tensor(1.8258)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.6248), 0.4901122394441475, 0.8004172461752433, tensor(1.8258)]
early stopping at 45
[2023-01-17 21:16:13,191.191 dlcmzxjb7qmi93pp-master-0:18724 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 21:16:13,893.893 dlcmzxjb7qmi93pp-master-0:18790 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:16:13,894.894 dlcmzxjb7qmi93pp-master-0:18791 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:16:14,020.020 dlcmzxjb7qmi93pp-master-0:18789 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:16:14,021.021 dlcmzxjb7qmi93pp-master-0:18792 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:16:14,916.916 dlcmzxjb7qmi93pp-master-0:18792 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 21:16:15,819.819 dlcmzxjb7qmi93pp-master-0:18791 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 21:16:15,821.821 dlcmzxjb7qmi93pp-master-0:18790 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 21:16:15,828.828 dlcmzxjb7qmi93pp-master-0:18789 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:18789:18789 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:18792:18792 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18791:18791 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18790:18790 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-17 21:26:38,569.569 dlcmzxjb7qmi93pp-master-0:18866 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 21:26:39,195.195 dlcmzxjb7qmi93pp-master-0:18933 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:26:39,195.195 dlcmzxjb7qmi93pp-master-0:18934 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:26:39,195.195 dlcmzxjb7qmi93pp-master-0:18932 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:26:39,203.203 dlcmzxjb7qmi93pp-master-0:18931 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:26:40,228.228 dlcmzxjb7qmi93pp-master-0:18932 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 21:26:41,205.205 dlcmzxjb7qmi93pp-master-0:18933 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 21:26:41,225.225 dlcmzxjb7qmi93pp-master-0:18934 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 21:26:41,231.231 dlcmzxjb7qmi93pp-master-0:18931 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:18931:18931 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:18932:18932 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18934:18934 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:18933:18933 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-17 21:38:02,970.970 dlcmzxjb7qmi93pp-master-0:19011 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 21:38:03,599.599 dlcmzxjb7qmi93pp-master-0:19079 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:38:03,599.599 dlcmzxjb7qmi93pp-master-0:19076 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:38:03,600.600 dlcmzxjb7qmi93pp-master-0:19078 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:38:03,610.610 dlcmzxjb7qmi93pp-master-0:19077 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 21:38:05,600.600 dlcmzxjb7qmi93pp-master-0:19079 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 21:38:05,603.603 dlcmzxjb7qmi93pp-master-0:19078 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 21:38:05,605.605 dlcmzxjb7qmi93pp-master-0:19077 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 21:38:05,608.608 dlcmzxjb7qmi93pp-master-0:19076 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:19076:19076 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:19079:19079 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19078:19078 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19077:19077 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7777), 0.4462854088722608, 0.6481223922114048, tensor(1.4537)]
[tensor(-0.7776), 0.4462854088722608, 0.6481223922114048, tensor(1.4538)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7744), 0.4462854088722608, 0.6599443671766342, tensor(1.4570)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7330), 0.4462854088722608, 0.7169680111265647, tensor(1.4985)]
[tensor(-0.7265), 0.4462854088722608, 0.741307371349096, tensor(1.4985)]
[tensor(-0.7119), 0.4462854088722608, 0.7482614742698191, tensor(1.4985)]
[tensor(-0.7064), 0.4462854088722608, 0.7482614742698191, tensor(1.4985)]
[tensor(-0.7064), 0.4462854088722608, 0.7600834492350487, tensor(1.4985)]
[tensor(-0.6926), 0.4462854088722608, 0.7621696801112656, tensor(1.4985)]
[tensor(-0.6588), 0.4580438268305719, 0.7684283727399166, tensor(1.6314)]
[tensor(-0.6588), 0.4580438268305719, 0.7691237830319889, tensor(1.6314)]
[tensor(-0.6511), 0.4649919828968466, 0.7691237830319889, tensor(1.6739)]
[tensor(-0.6511), 0.4649919828968466, 0.7767732962447844, tensor(1.6739)]
[tensor(-0.6399), 0.4649919828968466, 0.7837273991655076, tensor(1.6851)]
[tensor(-0.6399), 0.4649919828968466, 0.7837273991655076, tensor(1.6851)]
[tensor(-0.6391), 0.4853019775521112, 0.7837273991655076, tensor(1.7874)]
[tensor(-0.6391), 0.4853019775521112, 0.7837273991655076, tensor(1.7874)]
[tensor(-0.6259), 0.4853019775521112, 0.7837273991655076, tensor(1.7926)]
[tensor(-0.6259), 0.4853019775521112, 0.7837273991655076, tensor(1.7926)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.6259), 0.4853019775521112, 0.7837273991655076, tensor(1.7926)]
[tensor(-0.6259), 0.4853019775521112, 0.7858136300417247, tensor(1.7926)]
[tensor(-0.6259), 0.4853019775521112, 0.7927677329624478, tensor(1.7926)]
[tensor(-0.6169), 0.4853019775521112, 0.7927677329624478, tensor(1.8069)]
[tensor(-0.6169), 0.4853019775521112, 0.7927677329624478, tensor(1.8069)]
[tensor(-0.6169), 0.4906467129877071, 0.7927677329624478, tensor(1.8362)]
[tensor(-0.6169), 0.4906467129877071, 0.7927677329624478, tensor(1.8362)]
[tensor(-0.6169), 0.4906467129877071, 0.7927677329624478, tensor(1.8362)]
[tensor(-0.6169), 0.4906467129877071, 0.7927677329624478, tensor(1.8362)]
[tensor(-0.6169), 0.4906467129877071, 0.7927677329624478, tensor(1.8362)]
[tensor(-0.6169), 0.4906467129877071, 0.7927677329624478, tensor(1.8362)]
early stopping at 40
[2023-01-17 22:59:26,438.438 dlcmzxjb7qmi93pp-master-0:19260 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-17 22:59:27,059.059 dlcmzxjb7qmi93pp-master-0:19326 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:59:27,060.060 dlcmzxjb7qmi93pp-master-0:19325 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:59:27,060.060 dlcmzxjb7qmi93pp-master-0:19328 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:59:27,098.098 dlcmzxjb7qmi93pp-master-0:19327 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-17 22:59:27,998.998 dlcmzxjb7qmi93pp-master-0:19328 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-17 22:59:28,005.005 dlcmzxjb7qmi93pp-master-0:19327 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-17 22:59:28,982.982 dlcmzxjb7qmi93pp-master-0:19326 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-17 22:59:28,990.990 dlcmzxjb7qmi93pp-master-0:19325 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:19325:19325 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:19327:19327 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19328:19328 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19326:19326 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7357), 0.4462854088722608, 0.7148817802503477, tensor(1.4583)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7114), 0.4462854088722608, 0.741307371349096, tensor(1.5147)]
[tensor(-0.6920), 0.4462854088722608, 0.7573018080667594, tensor(1.5147)]
[tensor(-0.6750), 0.45269909139497594, 0.7573018080667594, tensor(1.5885)]
[tensor(-0.6629), 0.4649919828968466, 0.7712100139082059, tensor(1.6621)]
[tensor(-0.6590), 0.481560662747194, 0.7767732962447844, tensor(1.7488)]
[tensor(-0.6428), 0.481560662747194, 0.7767732962447844, tensor(1.7650)]
[tensor(-0.6428), 0.481560662747194, 0.7767732962447844, tensor(1.7650)]
[tensor(-0.6428), 0.48583645109567075, 0.7767732962447844, tensor(1.7859)]
[tensor(-0.6263), 0.4911811865312667, 0.780250347705146, tensor(1.8296)]
[tensor(-0.6197), 0.4927846071619455, 0.7892906815020863, tensor(1.8443)]
[tensor(-0.6147), 0.5018706574024586, 0.7983310152990264, tensor(1.8947)]
[tensor(-0.6147), 0.5018706574024586, 0.7983310152990264, tensor(1.8947)]
[tensor(-0.6147), 0.5018706574024586, 0.7983310152990264, tensor(1.8947)]
[tensor(-0.6147), 0.5018706574024586, 0.7983310152990264, tensor(1.8947)]
[tensor(-0.6147), 0.5018706574024586, 0.7983310152990264, tensor(1.8947)]
[tensor(-0.6147), 0.5018706574024586, 0.7983310152990264, tensor(1.8947)]
early stopping at 48
[2023-01-18 00:35:18,685.685 dlcmzxjb7qmi93pp-master-0:19530 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 00:35:19,315.315 dlcmzxjb7qmi93pp-master-0:19598 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:35:19,315.315 dlcmzxjb7qmi93pp-master-0:19597 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:35:19,393.393 dlcmzxjb7qmi93pp-master-0:19595 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:35:19,404.404 dlcmzxjb7qmi93pp-master-0:19596 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:35:20,611.611 dlcmzxjb7qmi93pp-master-0:19596 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 00:35:21,195.195 dlcmzxjb7qmi93pp-master-0:19598 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 00:35:21,195.195 dlcmzxjb7qmi93pp-master-0:19597 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 00:35:21,204.204 dlcmzxjb7qmi93pp-master-0:19595 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:19595:19595 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:19596:19596 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19597:19597 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19598:19598 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.8189), 0.4462854088722608, 0.3518776077885953, tensor(1.4125)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-18 00:45:37,054.054 dlcmzxjb7qmi93pp-master-0:19673 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 00:45:37,670.670 dlcmzxjb7qmi93pp-master-0:19739 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:45:37,670.670 dlcmzxjb7qmi93pp-master-0:19740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:45:37,670.670 dlcmzxjb7qmi93pp-master-0:19741 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:45:37,683.683 dlcmzxjb7qmi93pp-master-0:19738 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:45:38,649.649 dlcmzxjb7qmi93pp-master-0:19741 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 00:45:39,636.636 dlcmzxjb7qmi93pp-master-0:19740 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 00:45:39,644.644 dlcmzxjb7qmi93pp-master-0:19739 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 00:45:39,650.650 dlcmzxjb7qmi93pp-master-0:19738 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:19738:19738 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:19740:19740 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19739:19739 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19741:19741 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-18 00:56:52,447.447 dlcmzxjb7qmi93pp-master-0:19817 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 00:56:53,067.067 dlcmzxjb7qmi93pp-master-0:19883 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:56:53,068.068 dlcmzxjb7qmi93pp-master-0:19885 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:56:53,070.070 dlcmzxjb7qmi93pp-master-0:19884 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:56:53,070.070 dlcmzxjb7qmi93pp-master-0:19882 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 00:56:54,022.022 dlcmzxjb7qmi93pp-master-0:19884 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 00:56:55,013.013 dlcmzxjb7qmi93pp-master-0:19883 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 00:56:55,020.020 dlcmzxjb7qmi93pp-master-0:19885 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 00:56:55,024.024 dlcmzxjb7qmi93pp-master-0:19882 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:19882:19882 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:19884:19884 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19885:19885 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:19883:19883 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7782), 0.4462854088722608, 0.6481223922114048, tensor(1.4532)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[2023-01-18 02:36:06,886.886 dlcmzxjb7qmi93pp-master-0:20093 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 02:36:07,504.504 dlcmzxjb7qmi93pp-master-0:20161 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:36:07,504.504 dlcmzxjb7qmi93pp-master-0:20159 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:36:07,505.505 dlcmzxjb7qmi93pp-master-0:20158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:36:07,514.514 dlcmzxjb7qmi93pp-master-0:20160 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 02:36:08,475.475 dlcmzxjb7qmi93pp-master-0:20161 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 02:36:08,477.477 dlcmzxjb7qmi93pp-master-0:20160 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 02:36:08,478.478 dlcmzxjb7qmi93pp-master-0:20159 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 02:36:08,482.482 dlcmzxjb7qmi93pp-master-0:20158 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:20158:20158 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:20161:20161 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20160:20160 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20159:20159 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[2023-01-18 04:15:18,501.501 dlcmzxjb7qmi93pp-master-0:20368 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 04:15:19,120.120 dlcmzxjb7qmi93pp-master-0:20434 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:15:19,121.121 dlcmzxjb7qmi93pp-master-0:20435 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:15:19,220.220 dlcmzxjb7qmi93pp-master-0:20436 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:15:19,227.227 dlcmzxjb7qmi93pp-master-0:20433 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:15:20,973.973 dlcmzxjb7qmi93pp-master-0:20435 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 04:15:21,110.110 dlcmzxjb7qmi93pp-master-0:20436 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 04:15:21,309.309 dlcmzxjb7qmi93pp-master-0:20434 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 04:15:21,314.314 dlcmzxjb7qmi93pp-master-0:20433 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:20433:20433 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:20434:20434 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20436:20436 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20435:20435 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-18 04:25:38,908.908 dlcmzxjb7qmi93pp-master-0:20511 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 04:25:39,576.576 dlcmzxjb7qmi93pp-master-0:20578 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:25:39,610.610 dlcmzxjb7qmi93pp-master-0:20577 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:25:39,615.615 dlcmzxjb7qmi93pp-master-0:20576 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:25:39,696.696 dlcmzxjb7qmi93pp-master-0:20579 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:25:40,882.882 dlcmzxjb7qmi93pp-master-0:20577 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 04:25:41,019.019 dlcmzxjb7qmi93pp-master-0:20579 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 04:25:41,426.426 dlcmzxjb7qmi93pp-master-0:20578 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 04:25:41,431.431 dlcmzxjb7qmi93pp-master-0:20576 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:20576:20576 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:20579:20579 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20577:20577 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20578:20578 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-18 04:35:56,240.240 dlcmzxjb7qmi93pp-master-0:20654 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 04:35:56,850.850 dlcmzxjb7qmi93pp-master-0:20721 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:35:56,850.850 dlcmzxjb7qmi93pp-master-0:20720 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:35:56,930.930 dlcmzxjb7qmi93pp-master-0:20722 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:35:56,943.943 dlcmzxjb7qmi93pp-master-0:20719 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 04:35:58,723.723 dlcmzxjb7qmi93pp-master-0:20721 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 04:35:58,724.724 dlcmzxjb7qmi93pp-master-0:20720 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 04:35:59,182.182 dlcmzxjb7qmi93pp-master-0:20722 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 04:35:59,192.192 dlcmzxjb7qmi93pp-master-0:20719 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:20719:20719 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:20720:20720 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20722:20722 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20721:20721 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7778), 0.4462854088722608, 0.6481223922114048, tensor(1.4537)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[2023-01-18 06:16:24,823.823 dlcmzxjb7qmi93pp-master-0:20931 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 06:16:25,446.446 dlcmzxjb7qmi93pp-master-0:20999 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 06:16:25,447.447 dlcmzxjb7qmi93pp-master-0:20996 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 06:16:25,448.448 dlcmzxjb7qmi93pp-master-0:20998 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 06:16:25,457.457 dlcmzxjb7qmi93pp-master-0:20997 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 06:16:26,398.398 dlcmzxjb7qmi93pp-master-0:20998 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 06:16:26,399.399 dlcmzxjb7qmi93pp-master-0:20997 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 06:16:27,350.350 dlcmzxjb7qmi93pp-master-0:20999 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 06:16:27,359.359 dlcmzxjb7qmi93pp-master-0:20996 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:20996:20996 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:20999:20999 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20998:20998 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:20997:20997 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[2023-01-18 07:55:45,531.531 dlcmzxjb7qmi93pp-master-0:21207 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 07:55:46,140.140 dlcmzxjb7qmi93pp-master-0:21272 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 07:55:46,142.142 dlcmzxjb7qmi93pp-master-0:21274 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 07:55:46,147.147 dlcmzxjb7qmi93pp-master-0:21273 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 07:55:46,152.152 dlcmzxjb7qmi93pp-master-0:21275 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 07:55:47,112.112 dlcmzxjb7qmi93pp-master-0:21274 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 07:55:47,114.114 dlcmzxjb7qmi93pp-master-0:21273 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 07:55:48,097.097 dlcmzxjb7qmi93pp-master-0:21275 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 07:55:48,103.103 dlcmzxjb7qmi93pp-master-0:21272 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:21272:21272 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:21274:21274 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21273:21273 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21275:21275 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[Wed Jan 18 08:01:11 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-18 08:06:00,896.896 dlcmzxjb7qmi93pp-master-0:21350 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 08:06:01,526.526 dlcmzxjb7qmi93pp-master-0:21415 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:06:01,527.527 dlcmzxjb7qmi93pp-master-0:21416 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:06:01,528.528 dlcmzxjb7qmi93pp-master-0:21417 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:06:01,534.534 dlcmzxjb7qmi93pp-master-0:21418 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:06:02,580.580 dlcmzxjb7qmi93pp-master-0:21417 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 08:06:03,525.525 dlcmzxjb7qmi93pp-master-0:21416 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 08:06:03,575.575 dlcmzxjb7qmi93pp-master-0:21418 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 08:06:03,580.580 dlcmzxjb7qmi93pp-master-0:21415 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:21415:21415 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:21417:21417 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21418:21418 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21416:21416 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-18 08:16:21,326.326 dlcmzxjb7qmi93pp-master-0:21492 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 08:16:21,949.949 dlcmzxjb7qmi93pp-master-0:21557 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:16:21,949.949 dlcmzxjb7qmi93pp-master-0:21559 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:16:21,950.950 dlcmzxjb7qmi93pp-master-0:21560 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:16:21,955.955 dlcmzxjb7qmi93pp-master-0:21558 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 08:16:23,879.879 dlcmzxjb7qmi93pp-master-0:21559 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 08:16:23,885.885 dlcmzxjb7qmi93pp-master-0:21558 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 08:16:23,902.902 dlcmzxjb7qmi93pp-master-0:21560 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 08:16:23,911.911 dlcmzxjb7qmi93pp-master-0:21557 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:21557:21557 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:21559:21559 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21560:21560 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21558:21558 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7762), 0.4436130411544629, 0.6474269819193325, tensor(1.4419)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[Wed Jan 18 08:21:47 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 08:48:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 08:55:43 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 08:57:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 09:12:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 09:17:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 09:19:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 09:26:45 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 09:32:31 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 09:34:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 09:41:15 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7445), 0.4462854088722608, 0.7127955493741307, tensor(1.4869)]
[Wed Jan 18 09:42:47 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7445), 0.4462854088722608, 0.7127955493741307, tensor(1.4869)]
[tensor(-0.7375), 0.4462854088722608, 0.7239221140472879, tensor(1.4869)]
[tensor(-0.7018), 0.45911277391769106, 0.7517385257301809, tensor(1.5937)]
[tensor(-0.7018), 0.46125066809192944, 0.7517385257301809, tensor(1.6017)]
[Wed Jan 18 09:50:57 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.6984), 0.46125066809192944, 0.7573018080667594, tensor(1.6017)]
[tensor(-0.6841), 0.4681988241582042, 0.7573018080667594, tensor(1.6569)]
[tensor(-0.6778), 0.4681988241582042, 0.7573018080667594, tensor(1.6569)]
[2023-01-18 09:56:17,170.170 dlcmzxjb7qmi93pp-master-0:21769 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 09:56:17,798.798 dlcmzxjb7qmi93pp-master-0:21837 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 09:56:17,812.812 dlcmzxjb7qmi93pp-master-0:21836 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 09:56:17,851.851 dlcmzxjb7qmi93pp-master-0:21835 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 09:56:17,856.856 dlcmzxjb7qmi93pp-master-0:21834 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 09:56:18,934.934 dlcmzxjb7qmi93pp-master-0:21837 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 09:56:18,935.935 dlcmzxjb7qmi93pp-master-0:21836 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 09:56:19,930.930 dlcmzxjb7qmi93pp-master-0:21835 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 09:56:19,935.935 dlcmzxjb7qmi93pp-master-0:21834 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:21834:21834 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:21836:21836 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21835:21835 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:21837:21837 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[Wed Jan 18 10:35:45 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 10:57:24 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[Wed Jan 18 11:03:14 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[Wed Jan 18 11:22:44 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[Wed Jan 18 11:30:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[2023-01-18 11:36:10,661.661 dlcmzxjb7qmi93pp-master-0:22046 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 11:36:11,288.288 dlcmzxjb7qmi93pp-master-0:22112 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:36:11,318.318 dlcmzxjb7qmi93pp-master-0:22113 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:36:11,396.396 dlcmzxjb7qmi93pp-master-0:22111 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:36:11,462.462 dlcmzxjb7qmi93pp-master-0:22114 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:36:12,289.289 dlcmzxjb7qmi93pp-master-0:22113 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 11:36:12,400.400 dlcmzxjb7qmi93pp-master-0:22114 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 11:36:13,128.128 dlcmzxjb7qmi93pp-master-0:22112 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 11:36:13,129.129 dlcmzxjb7qmi93pp-master-0:22111 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
downstreamv2 mosei
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:22111:22111 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:22113:22113 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22114:22114 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22112:22112 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 11:44:29 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-18 11:46:29,014.014 dlcmzxjb7qmi93pp-master-0:22188 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 11:46:29,648.648 dlcmzxjb7qmi93pp-master-0:22255 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:46:29,648.648 dlcmzxjb7qmi93pp-master-0:22254 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:46:29,791.791 dlcmzxjb7qmi93pp-master-0:22256 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:46:29,792.792 dlcmzxjb7qmi93pp-master-0:22253 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:46:30,660.660 dlcmzxjb7qmi93pp-master-0:22256 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 11:46:31,538.538 dlcmzxjb7qmi93pp-master-0:22255 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 11:46:31,541.541 dlcmzxjb7qmi93pp-master-0:22254 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 11:46:31,548.548 dlcmzxjb7qmi93pp-master-0:22253 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:22253:22253 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:22256:22256 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22255:22255 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22254:22254 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-18 11:56:53,359.359 dlcmzxjb7qmi93pp-master-0:22331 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 11:56:53,978.978 dlcmzxjb7qmi93pp-master-0:22398 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:56:53,981.981 dlcmzxjb7qmi93pp-master-0:22396 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:56:53,981.981 dlcmzxjb7qmi93pp-master-0:22397 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:56:53,983.983 dlcmzxjb7qmi93pp-master-0:22399 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 11:56:55,923.923 dlcmzxjb7qmi93pp-master-0:22397 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 11:56:55,936.936 dlcmzxjb7qmi93pp-master-0:22399 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 11:56:55,937.937 dlcmzxjb7qmi93pp-master-0:22398 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 11:56:55,942.942 dlcmzxjb7qmi93pp-master-0:22396 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:22396:22396 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:22398:22398 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22397:22397 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22399:22399 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[Wed Jan 18 12:05:23 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 12:29:55 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 12:36:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 12:38:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 12:41:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Wed Jan 18 12:43:26 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 12:48:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 12:50:25 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 12:56:38 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 12:58:30 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:00:30 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:01:45 2023] [cudaHostAllocator] allocates 1.95 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:12:00 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:15:40 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:17:55 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:22:27 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:24:01 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Wed Jan 18 13:26:23 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[2023-01-18 13:37:27,868.868 dlcmzxjb7qmi93pp-master-0:22609 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 13:37:28,490.490 dlcmzxjb7qmi93pp-master-0:22674 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 13:37:28,490.490 dlcmzxjb7qmi93pp-master-0:22677 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 13:37:28,490.490 dlcmzxjb7qmi93pp-master-0:22676 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 13:37:28,491.491 dlcmzxjb7qmi93pp-master-0:22675 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 13:37:29,453.453 dlcmzxjb7qmi93pp-master-0:22676 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 13:37:29,455.455 dlcmzxjb7qmi93pp-master-0:22677 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 13:37:29,456.456 dlcmzxjb7qmi93pp-master-0:22675 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 13:37:29,460.460 dlcmzxjb7qmi93pp-master-0:22674 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:22674:22674 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:22677:22677 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22675:22675 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22676:22676 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[Wed Jan 18 13:58:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[Wed Jan 18 14:17:11 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[Wed Jan 18 14:20:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[Wed Jan 18 14:50:37 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-18 15:17:18,210.210 dlcmzxjb7qmi93pp-master-0:22885 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 15:17:18,827.827 dlcmzxjb7qmi93pp-master-0:22951 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:17:18,828.828 dlcmzxjb7qmi93pp-master-0:22952 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:17:19,026.026 dlcmzxjb7qmi93pp-master-0:22953 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:17:19,026.026 dlcmzxjb7qmi93pp-master-0:22950 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:17:20,095.095 dlcmzxjb7qmi93pp-master-0:22953 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 15:17:20,674.674 dlcmzxjb7qmi93pp-master-0:22952 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 15:17:20,675.675 dlcmzxjb7qmi93pp-master-0:22951 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 15:17:20,676.676 dlcmzxjb7qmi93pp-master-0:22950 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:22950:22950 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:22953:22953 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22951:22951 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:22952:22952 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-18 15:27:27,568.568 dlcmzxjb7qmi93pp-master-0:23028 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 15:27:28,212.212 dlcmzxjb7qmi93pp-master-0:23095 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:27:28,257.257 dlcmzxjb7qmi93pp-master-0:23093 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:27:28,264.264 dlcmzxjb7qmi93pp-master-0:23094 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:27:28,346.346 dlcmzxjb7qmi93pp-master-0:23096 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:27:29,519.519 dlcmzxjb7qmi93pp-master-0:23094 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 15:27:29,569.569 dlcmzxjb7qmi93pp-master-0:23096 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 15:27:30,028.028 dlcmzxjb7qmi93pp-master-0:23095 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 15:27:30,029.029 dlcmzxjb7qmi93pp-master-0:23093 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:23093:23093 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:23095:23095 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23096:23096 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23094:23094 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-18 15:37:46,913.913 dlcmzxjb7qmi93pp-master-0:23170 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 15:37:47,533.533 dlcmzxjb7qmi93pp-master-0:23235 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:37:47,533.533 dlcmzxjb7qmi93pp-master-0:23238 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:37:47,614.614 dlcmzxjb7qmi93pp-master-0:23236 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:37:47,624.624 dlcmzxjb7qmi93pp-master-0:23237 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 15:37:48,400.400 dlcmzxjb7qmi93pp-master-0:23238 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 15:37:48,886.886 dlcmzxjb7qmi93pp-master-0:23236 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 15:37:48,888.888 dlcmzxjb7qmi93pp-master-0:23237 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 15:37:48,889.889 dlcmzxjb7qmi93pp-master-0:23235 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:23235:23235 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:23237:23237 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23236:23236 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23238:23238 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7776), 0.4462854088722608, 0.6481223922114048, tensor(1.4538)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
[tensor(-0.7767), 0.4462854088722608, 0.6481223922114048, tensor(1.4547)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7448), 0.4462854088722608, 0.6481223922114048, tensor(1.4866)]
[tensor(-0.7398), 0.4462854088722608, 0.6815020862308763, tensor(1.4866)]
[2023-01-18 17:18:23,581.581 dlcmzxjb7qmi93pp-master-0:23447 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 17:18:24,205.205 dlcmzxjb7qmi93pp-master-0:23514 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:18:24,205.205 dlcmzxjb7qmi93pp-master-0:23512 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:18:24,440.440 dlcmzxjb7qmi93pp-master-0:23513 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:18:24,440.440 dlcmzxjb7qmi93pp-master-0:23515 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:18:25,288.288 dlcmzxjb7qmi93pp-master-0:23513 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 17:18:25,289.289 dlcmzxjb7qmi93pp-master-0:23515 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 17:18:26,019.019 dlcmzxjb7qmi93pp-master-0:23514 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 17:18:26,023.023 dlcmzxjb7qmi93pp-master-0:23512 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:23512:23512 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:23513:23513 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23514:23514 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23515:23515 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6474269819193325, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7691), 0.4462854088722608, 0.6863699582753825, tensor(1.4623)]
[tensor(-0.7563), 0.4462854088722608, 0.7009735744089013, tensor(1.4751)]
[tensor(-0.7311), 0.4462854088722608, 0.7260083449235049, tensor(1.4751)]
[tensor(-0.6824), 0.4580438268305719, 0.7545201668984701, tensor(1.6078)]
[tensor(-0.6824), 0.4580438268305719, 0.7677329624478443, tensor(1.6078)]
[tensor(-0.6722), 0.4580438268305719, 0.7698191933240612, tensor(1.6180)]
[tensor(-0.6517), 0.46766435061464456, 0.7788595271210014, tensor(1.6866)]
[tensor(-0.6517), 0.46766435061464456, 0.7788595271210014, tensor(1.6866)]
[tensor(-0.6517), 0.4804917156600748, 0.7788595271210014, tensor(1.7425)]
[tensor(-0.6421), 0.4804917156600748, 0.7878998609179416, tensor(1.7425)]
[tensor(-0.6326), 0.4804917156600748, 0.7878998609179416, tensor(1.7425)]
[tensor(-0.6287), 0.4804917156600748, 0.7892906815020863, tensor(1.7550)]
[tensor(-0.6287), 0.4804917156600748, 0.7892906815020863, tensor(1.7550)]
[tensor(-0.6287), 0.4804917156600748, 0.7892906815020863, tensor(1.7626)]
[tensor(-0.6230), 0.4917156600748263, 0.7892906815020863, tensor(1.8356)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.6230), 0.4917156600748263, 0.7892906815020863, tensor(1.8356)]
[tensor(-0.6230), 0.4917156600748263, 0.7920723226703755, tensor(1.8356)]
[tensor(-0.6156), 0.4922501336183859, 0.7920723226703755, tensor(1.8456)]
[tensor(-0.6156), 0.4922501336183859, 0.7920723226703755, tensor(1.8456)]
[2023-01-18 18:58:08,131.131 dlcmzxjb7qmi93pp-master-0:23724 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 18:58:08,748.748 dlcmzxjb7qmi93pp-master-0:23790 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:58:08,748.748 dlcmzxjb7qmi93pp-master-0:23791 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:58:08,851.851 dlcmzxjb7qmi93pp-master-0:23789 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:58:08,852.852 dlcmzxjb7qmi93pp-master-0:23792 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:58:09,859.859 dlcmzxjb7qmi93pp-master-0:23792 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 18:58:10,719.719 dlcmzxjb7qmi93pp-master-0:23791 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 18:58:10,730.730 dlcmzxjb7qmi93pp-master-0:23790 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 18:58:10,738.738 dlcmzxjb7qmi93pp-master-0:23789 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:23789:23789 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:23790:23790 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23791:23791 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23792:23792 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7775), 0.4462854088722608, 0.6474269819193325, tensor(1.4539)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-18 19:08:43,497.497 dlcmzxjb7qmi93pp-master-0:23867 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 19:08:44,113.113 dlcmzxjb7qmi93pp-master-0:23934 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:08:44,113.113 dlcmzxjb7qmi93pp-master-0:23933 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:08:44,263.263 dlcmzxjb7qmi93pp-master-0:23935 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:08:44,298.298 dlcmzxjb7qmi93pp-master-0:23932 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:08:46,080.080 dlcmzxjb7qmi93pp-master-0:23934 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 19:08:46,082.082 dlcmzxjb7qmi93pp-master-0:23933 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 19:08:46,537.537 dlcmzxjb7qmi93pp-master-0:23935 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 19:08:46,546.546 dlcmzxjb7qmi93pp-master-0:23932 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:23932:23932 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:23933:23933 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23935:23935 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:23934:23934 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7774), 0.4462854088722608, 0.6474269819193325, tensor(1.4540)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[2023-01-18 19:19:31,890.890 dlcmzxjb7qmi93pp-master-0:24010 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 19:19:32,509.509 dlcmzxjb7qmi93pp-master-0:24075 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:32,510.510 dlcmzxjb7qmi93pp-master-0:24078 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:32,593.593 dlcmzxjb7qmi93pp-master-0:24077 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:32,600.600 dlcmzxjb7qmi93pp-master-0:24076 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:33,865.865 dlcmzxjb7qmi93pp-master-0:24076 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 19:19:33,866.866 dlcmzxjb7qmi93pp-master-0:24077 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 19:19:34,396.396 dlcmzxjb7qmi93pp-master-0:24078 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 19:19:34,401.401 dlcmzxjb7qmi93pp-master-0:24075 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:24075:24075 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:24076:24076 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24078:24078 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24077:24077 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7779), 0.4462854088722608, 0.6481223922114048, tensor(1.4535)]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[2023-01-18 21:01:02,600.600 dlcmzxjb7qmi93pp-master-0:24290 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 21:01:03,253.253 dlcmzxjb7qmi93pp-master-0:24356 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:01:03,275.275 dlcmzxjb7qmi93pp-master-0:24355 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:01:03,334.334 dlcmzxjb7qmi93pp-master-0:24358 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:01:03,346.346 dlcmzxjb7qmi93pp-master-0:24357 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:01:04,587.587 dlcmzxjb7qmi93pp-master-0:24357 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 21:01:04,589.589 dlcmzxjb7qmi93pp-master-0:24358 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 21:01:05,134.134 dlcmzxjb7qmi93pp-master-0:24356 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 21:01:05,140.140 dlcmzxjb7qmi93pp-master-0:24355 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:24355:24355 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:24357:24357 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24356:24356 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24358:24358 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6467315716272601, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6474269819193325, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7737), 0.4462854088722608, 0.6988873435326843, tensor(1.4578)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7324), 0.4462854088722608, 0.719749652294854, tensor(1.4990)]
[tensor(-0.7324), 0.4462854088722608, 0.719749652294854, tensor(1.4990)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.7324), 0.4537680384820951, 0.719749652294854, tensor(1.5152)]
[tensor(-0.7057), 0.4537680384820951, 0.7545201668984701, tensor(1.5152)]
[tensor(-0.6786), 0.45911277391769106, 0.758692628650904, tensor(1.6170)]
[tensor(-0.6786), 0.45911277391769106, 0.758692628650904, tensor(1.6170)]
[tensor(-0.6786), 0.45911277391769106, 0.7642559109874826, tensor(1.6170)]
[tensor(-0.6786), 0.45911277391769106, 0.7677329624478443, tensor(1.6170)]
[tensor(-0.6717), 0.47140566541956175, 0.7677329624478443, tensor(1.6854)]
[tensor(-0.6606), 0.47140566541956175, 0.7739916550764951, tensor(1.6854)]
[tensor(-0.6603), 0.47140566541956175, 0.7809457579972183, tensor(1.6854)]
[tensor(-0.6603), 0.47140566541956175, 0.7844228094575799, tensor(1.6854)]
[tensor(-0.6603), 0.47140566541956175, 0.7844228094575799, tensor(1.6854)]
[tensor(-0.6566), 0.47140566541956175, 0.7899860917941586, tensor(1.6854)]
[tensor(-0.6335), 0.4826296098343132, 0.7899860917941586, tensor(1.7796)]
[tensor(-0.6335), 0.4826296098343132, 0.7899860917941586, tensor(1.7796)]
[tensor(-0.6335), 0.4826296098343132, 0.7899860917941586, tensor(1.7796)]
[tensor(-0.6335), 0.4826296098343132, 0.7899860917941586, tensor(1.7796)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.6222), 0.4826296098343132, 0.7899860917941586, tensor(1.7883)]
[tensor(-0.6222), 0.4826296098343132, 0.7899860917941586, tensor(1.7883)]
[tensor(-0.6222), 0.4826296098343132, 0.7899860917941586, tensor(1.7883)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.6222), 0.4826296098343132, 0.7899860917941586, tensor(1.7883)]
[tensor(-0.6222), 0.4826296098343132, 0.7899860917941586, tensor(1.7883)]
[tensor(-0.6222), 0.4826296098343132, 0.7899860917941586, tensor(1.7883)]
early stopping at 45
[2023-01-18 22:29:51,470.470 dlcmzxjb7qmi93pp-master-0:24550 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 22:29:52,081.081 dlcmzxjb7qmi93pp-master-0:24618 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:29:52,081.081 dlcmzxjb7qmi93pp-master-0:24617 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:29:52,161.161 dlcmzxjb7qmi93pp-master-0:24616 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:29:52,166.166 dlcmzxjb7qmi93pp-master-0:24615 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:29:53,984.984 dlcmzxjb7qmi93pp-master-0:24618 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 22:29:53,986.986 dlcmzxjb7qmi93pp-master-0:24617 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 22:29:54,427.427 dlcmzxjb7qmi93pp-master-0:24616 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 22:29:54,431.431 dlcmzxjb7qmi93pp-master-0:24615 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:24615:24615 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:24616:24616 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24617:24617 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24618:24618 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-18 22:40:44,854.854 dlcmzxjb7qmi93pp-master-0:24693 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 22:40:45,471.471 dlcmzxjb7qmi93pp-master-0:24760 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:40:45,471.471 dlcmzxjb7qmi93pp-master-0:24759 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:40:45,715.715 dlcmzxjb7qmi93pp-master-0:24758 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:40:45,715.715 dlcmzxjb7qmi93pp-master-0:24761 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:40:47,316.316 dlcmzxjb7qmi93pp-master-0:24760 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 22:40:47,317.317 dlcmzxjb7qmi93pp-master-0:24759 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 22:40:48,008.008 dlcmzxjb7qmi93pp-master-0:24761 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 22:40:48,012.012 dlcmzxjb7qmi93pp-master-0:24758 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:24758:24758 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:24759:24759 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24761:24761 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24760:24760 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[2023-01-18 22:52:25,340.340 dlcmzxjb7qmi93pp-master-0:24838 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 22:52:25,979.979 dlcmzxjb7qmi93pp-master-0:24904 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:52:26,039.039 dlcmzxjb7qmi93pp-master-0:24903 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:52:26,040.040 dlcmzxjb7qmi93pp-master-0:24905 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:52:26,116.116 dlcmzxjb7qmi93pp-master-0:24906 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:52:27,296.296 dlcmzxjb7qmi93pp-master-0:24905 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 22:52:27,346.346 dlcmzxjb7qmi93pp-master-0:24906 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 22:52:27,791.791 dlcmzxjb7qmi93pp-master-0:24904 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 22:52:27,797.797 dlcmzxjb7qmi93pp-master-0:24903 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:24903:24903 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:24905:24905 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24904:24904 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:24906:24906 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7782), 0.4462854088722608, 0.6481223922114048, tensor(1.4532)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[2023-01-19 00:36:10,935.935 dlcmzxjb7qmi93pp-master-0:25120 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 00:36:11,551.551 dlcmzxjb7qmi93pp-master-0:25186 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:36:11,580.580 dlcmzxjb7qmi93pp-master-0:25187 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:36:11,664.664 dlcmzxjb7qmi93pp-master-0:25185 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:36:11,739.739 dlcmzxjb7qmi93pp-master-0:25188 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:36:12,827.827 dlcmzxjb7qmi93pp-master-0:25187 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 00:36:12,926.926 dlcmzxjb7qmi93pp-master-0:25188 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 00:36:13,453.453 dlcmzxjb7qmi93pp-master-0:25186 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 00:36:13,461.461 dlcmzxjb7qmi93pp-master-0:25185 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:25185:25185 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:25188:25188 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25186:25186 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25187:25187 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7768), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[2023-01-19 02:21:45,785.785 dlcmzxjb7qmi93pp-master-0:25406 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 02:21:46,404.404 dlcmzxjb7qmi93pp-master-0:25472 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:21:46,405.405 dlcmzxjb7qmi93pp-master-0:25473 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:21:46,583.583 dlcmzxjb7qmi93pp-master-0:25471 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:21:46,586.586 dlcmzxjb7qmi93pp-master-0:25474 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:21:47,893.893 dlcmzxjb7qmi93pp-master-0:25474 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 02:21:48,266.266 dlcmzxjb7qmi93pp-master-0:25472 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 02:21:48,270.270 dlcmzxjb7qmi93pp-master-0:25473 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 02:21:48,277.277 dlcmzxjb7qmi93pp-master-0:25471 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:25471:25471 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:25472:25472 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25473:25473 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25474:25474 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-19 02:32:21,158.158 dlcmzxjb7qmi93pp-master-0:25548 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 02:32:21,782.782 dlcmzxjb7qmi93pp-master-0:25614 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:32:21,815.815 dlcmzxjb7qmi93pp-master-0:25615 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:32:21,888.888 dlcmzxjb7qmi93pp-master-0:25613 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:32:21,965.965 dlcmzxjb7qmi93pp-master-0:25616 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:32:23,145.145 dlcmzxjb7qmi93pp-master-0:25615 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 02:32:23,233.233 dlcmzxjb7qmi93pp-master-0:25616 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 02:32:23,594.594 dlcmzxjb7qmi93pp-master-0:25614 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 02:32:23,594.594 dlcmzxjb7qmi93pp-master-0:25613 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:25613:25613 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:25614:25614 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25616:25616 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25615:25615 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7777), 0.4462854088722608, 0.6481223922114048, tensor(1.4537)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-19 02:43:46,568.568 dlcmzxjb7qmi93pp-master-0:25693 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 02:43:47,218.218 dlcmzxjb7qmi93pp-master-0:25760 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:43:47,285.285 dlcmzxjb7qmi93pp-master-0:25759 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:43:47,311.311 dlcmzxjb7qmi93pp-master-0:25761 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:43:47,457.457 dlcmzxjb7qmi93pp-master-0:25758 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:43:49,058.058 dlcmzxjb7qmi93pp-master-0:25760 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 02:43:49,127.127 dlcmzxjb7qmi93pp-master-0:25761 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 02:43:49,611.611 dlcmzxjb7qmi93pp-master-0:25759 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 02:43:49,612.612 dlcmzxjb7qmi93pp-master-0:25758 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:25758:25758 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:25760:25760 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25761:25761 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:25759:25759 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7776), 0.4462854088722608, 0.6481223922114048, tensor(1.4538)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[2023-01-19 04:27:26,318.318 dlcmzxjb7qmi93pp-master-0:25975 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 04:27:26,927.927 dlcmzxjb7qmi93pp-master-0:26043 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:27:26,927.927 dlcmzxjb7qmi93pp-master-0:26042 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:27:26,928.928 dlcmzxjb7qmi93pp-master-0:26040 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:27:26,939.939 dlcmzxjb7qmi93pp-master-0:26041 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:27:27,851.851 dlcmzxjb7qmi93pp-master-0:26043 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 04:27:27,852.852 dlcmzxjb7qmi93pp-master-0:26042 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 04:27:28,841.841 dlcmzxjb7qmi93pp-master-0:26041 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 04:27:28,851.851 dlcmzxjb7qmi93pp-master-0:26040 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:26040:26040 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:26041:26041 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26042:26042 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26043:26043 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[2023-01-19 06:07:26,878.878 dlcmzxjb7qmi93pp-master-0:26252 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 06:07:27,497.497 dlcmzxjb7qmi93pp-master-0:26318 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:07:27,497.497 dlcmzxjb7qmi93pp-master-0:26320 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:07:27,498.498 dlcmzxjb7qmi93pp-master-0:26317 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:07:27,525.525 dlcmzxjb7qmi93pp-master-0:26319 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:07:28,535.535 dlcmzxjb7qmi93pp-master-0:26319 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 06:07:29,388.388 dlcmzxjb7qmi93pp-master-0:26320 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 06:07:29,390.390 dlcmzxjb7qmi93pp-master-0:26318 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 06:07:29,392.392 dlcmzxjb7qmi93pp-master-0:26317 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:26317:26317 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:26318:26318 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26320:26320 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26319:26319 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[Thu Jan 19 06:12:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-19 06:17:36,166.166 dlcmzxjb7qmi93pp-master-0:26394 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 06:17:36,785.785 dlcmzxjb7qmi93pp-master-0:26459 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:17:36,827.827 dlcmzxjb7qmi93pp-master-0:26460 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:17:36,860.860 dlcmzxjb7qmi93pp-master-0:26462 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:17:36,867.867 dlcmzxjb7qmi93pp-master-0:26461 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:17:37,711.711 dlcmzxjb7qmi93pp-master-0:26460 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 06:17:37,966.966 dlcmzxjb7qmi93pp-master-0:26461 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 06:17:37,984.984 dlcmzxjb7qmi93pp-master-0:26462 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 06:17:37,993.993 dlcmzxjb7qmi93pp-master-0:26459 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:26459:26459 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:26461:26461 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26460:26460 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26462:26462 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[2023-01-19 06:27:58,475.475 dlcmzxjb7qmi93pp-master-0:26536 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 06:27:59,096.096 dlcmzxjb7qmi93pp-master-0:26602 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:27:59,126.126 dlcmzxjb7qmi93pp-master-0:26603 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:27:59,194.194 dlcmzxjb7qmi93pp-master-0:26604 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:27:59,280.280 dlcmzxjb7qmi93pp-master-0:26601 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 06:28:00,944.944 dlcmzxjb7qmi93pp-master-0:26602 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 06:28:01,023.023 dlcmzxjb7qmi93pp-master-0:26604 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 06:28:01,389.389 dlcmzxjb7qmi93pp-master-0:26603 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 06:28:01,391.391 dlcmzxjb7qmi93pp-master-0:26601 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:26601:26601 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:26602:26602 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26603:26603 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26604:26604 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7753), 0.4452164617851416, 0.6474269819193325, tensor(1.4508)]
[tensor(-0.7749), 0.4462854088722608, 0.6474269819193325, tensor(1.4565)]
[Thu Jan 19 06:33:31 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:01:37 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:08:13 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:10:04 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:23:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:25:01 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:29:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:31:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[Thu Jan 19 07:35:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:39:15 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:47:14 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 07:54:00 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[Thu Jan 19 08:05:17 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-19 08:09:00,061.061 dlcmzxjb7qmi93pp-master-0:26812 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 08:09:00,755.755 dlcmzxjb7qmi93pp-master-0:26878 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 08:09:00,789.789 dlcmzxjb7qmi93pp-master-0:26880 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 08:09:00,814.814 dlcmzxjb7qmi93pp-master-0:26879 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 08:09:00,825.825 dlcmzxjb7qmi93pp-master-0:26877 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 08:09:02,005.005 dlcmzxjb7qmi93pp-master-0:26878 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 08:09:02,045.045 dlcmzxjb7qmi93pp-master-0:26879 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 08:09:02,605.605 dlcmzxjb7qmi93pp-master-0:26880 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 08:09:02,606.606 dlcmzxjb7qmi93pp-master-0:26877 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:26877:26877 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:26880:26880 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26878:26878 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:26879:26879 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7751), 0.4462854088722608, 0.6481223922114048, tensor(1.4563)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 08:49:12 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 09:10:39 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 09:16:28 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Thu Jan 19 09:35:52 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4572)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4572)]
[Thu Jan 19 09:44:03 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4572)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4572)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4572)]
[2023-01-19 09:49:15,602.602 dlcmzxjb7qmi93pp-master-0:27087 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 09:49:16,216.216 dlcmzxjb7qmi93pp-master-0:27154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:49:16,216.216 dlcmzxjb7qmi93pp-master-0:27153 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:49:16,396.396 dlcmzxjb7qmi93pp-master-0:27155 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:49:16,396.396 dlcmzxjb7qmi93pp-master-0:27152 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:49:18,111.111 dlcmzxjb7qmi93pp-master-0:27153 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 09:49:18,112.112 dlcmzxjb7qmi93pp-master-0:27154 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 09:49:18,251.251 dlcmzxjb7qmi93pp-master-0:27155 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 09:49:18,258.258 dlcmzxjb7qmi93pp-master-0:27152 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:27152:27152 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:27153:27153 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27154:27154 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27155:27155 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7750), 0.4462854088722608, 0.6467315716272601, tensor(1.4564)]
[tensor(-0.7749), 0.4462854088722608, 0.6474269819193325, tensor(1.4565)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[Thu Jan 19 09:57:32 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-19 09:59:30,964.964 dlcmzxjb7qmi93pp-master-0:27229 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 09:59:31,679.679 dlcmzxjb7qmi93pp-master-0:27296 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:59:31,685.685 dlcmzxjb7qmi93pp-master-0:27295 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:59:31,725.725 dlcmzxjb7qmi93pp-master-0:27297 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:59:31,761.761 dlcmzxjb7qmi93pp-master-0:27294 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 09:59:33,486.486 dlcmzxjb7qmi93pp-master-0:27295 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 09:59:33,538.538 dlcmzxjb7qmi93pp-master-0:27297 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 09:59:33,931.931 dlcmzxjb7qmi93pp-master-0:27296 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 09:59:33,933.933 dlcmzxjb7qmi93pp-master-0:27294 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:27294:27294 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:27295:27295 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27297:27297 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27296:27296 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-19 10:09:44,300.300 dlcmzxjb7qmi93pp-master-0:27371 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 10:09:44,914.914 dlcmzxjb7qmi93pp-master-0:27439 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 10:09:44,954.954 dlcmzxjb7qmi93pp-master-0:27438 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 10:09:44,992.992 dlcmzxjb7qmi93pp-master-0:27437 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 10:09:45,078.078 dlcmzxjb7qmi93pp-master-0:27440 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 10:09:46,201.201 dlcmzxjb7qmi93pp-master-0:27438 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 10:09:46,341.341 dlcmzxjb7qmi93pp-master-0:27440 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 10:09:46,720.720 dlcmzxjb7qmi93pp-master-0:27439 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 10:09:46,723.723 dlcmzxjb7qmi93pp-master-0:27437 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:27437:27437 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:27438:27438 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27440:27440 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27439:27439 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[Thu Jan 19 10:21:16 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[Thu Jan 19 10:34:20 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 10:48:51 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 10:50:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 10:54:06 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 10:55:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[Thu Jan 19 10:57:52 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:00:25 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:02:13 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:12:07 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:13:21 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:29:17 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:33:44 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:35:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:37:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Thu Jan 19 11:43:25 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[2023-01-19 11:48:39,860.860 dlcmzxjb7qmi93pp-master-0:27646 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 11:48:40,485.485 dlcmzxjb7qmi93pp-master-0:27713 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 11:48:40,515.515 dlcmzxjb7qmi93pp-master-0:27712 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 11:48:40,615.615 dlcmzxjb7qmi93pp-master-0:27711 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 11:48:40,672.672 dlcmzxjb7qmi93pp-master-0:27714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 11:48:41,772.772 dlcmzxjb7qmi93pp-master-0:27712 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 11:48:41,866.866 dlcmzxjb7qmi93pp-master-0:27714 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 11:48:42,428.428 dlcmzxjb7qmi93pp-master-0:27713 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 11:48:42,432.432 dlcmzxjb7qmi93pp-master-0:27711 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:27711:27711 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:27712:27712 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27714:27714 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27713:27713 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 12:09:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Thu Jan 19 12:27:57 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[2023-01-19 13:27:58,381.381 dlcmzxjb7qmi93pp-master-0:27923 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 13:27:59,014.014 dlcmzxjb7qmi93pp-master-0:27989 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:27:59,014.014 dlcmzxjb7qmi93pp-master-0:27990 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:27:59,177.177 dlcmzxjb7qmi93pp-master-0:27991 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:27:59,258.258 dlcmzxjb7qmi93pp-master-0:27988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:28:00,846.846 dlcmzxjb7qmi93pp-master-0:27990 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 13:28:00,972.972 dlcmzxjb7qmi93pp-master-0:27991 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 13:28:01,261.261 dlcmzxjb7qmi93pp-master-0:27989 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 13:28:01,262.262 dlcmzxjb7qmi93pp-master-0:27988 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:27988:27988 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:27991:27991 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27990:27990 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:27989:27989 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7773), 0.4462854088722608, 0.6474269819193325, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6474269819193325, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-19 13:38:33,720.720 dlcmzxjb7qmi93pp-master-0:28065 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 13:38:34,345.345 dlcmzxjb7qmi93pp-master-0:28132 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:38:34,345.345 dlcmzxjb7qmi93pp-master-0:28131 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:38:34,441.441 dlcmzxjb7qmi93pp-master-0:28133 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:38:34,456.456 dlcmzxjb7qmi93pp-master-0:28130 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:38:36,221.221 dlcmzxjb7qmi93pp-master-0:28131 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 13:38:36,223.223 dlcmzxjb7qmi93pp-master-0:28132 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 13:38:36,261.261 dlcmzxjb7qmi93pp-master-0:28133 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 13:38:36,270.270 dlcmzxjb7qmi93pp-master-0:28130 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:28130:28130 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:28131:28131 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28133:28133 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28132:28132 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-19 13:48:41,066.066 dlcmzxjb7qmi93pp-master-0:28207 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 13:48:41,681.681 dlcmzxjb7qmi93pp-master-0:28273 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:48:41,683.683 dlcmzxjb7qmi93pp-master-0:28274 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:48:41,878.878 dlcmzxjb7qmi93pp-master-0:28275 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:48:41,952.952 dlcmzxjb7qmi93pp-master-0:28272 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 13:48:43,487.487 dlcmzxjb7qmi93pp-master-0:28274 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 13:48:43,696.696 dlcmzxjb7qmi93pp-master-0:28275 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 13:48:43,947.947 dlcmzxjb7qmi93pp-master-0:28273 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 13:48:43,948.948 dlcmzxjb7qmi93pp-master-0:28272 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:28272:28272 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:28275:28275 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28274:28274 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28273:28273 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[2023-01-19 15:30:49,796.796 dlcmzxjb7qmi93pp-master-0:28488 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 15:30:50,432.432 dlcmzxjb7qmi93pp-master-0:28553 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 15:30:50,438.438 dlcmzxjb7qmi93pp-master-0:28555 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 15:30:50,509.509 dlcmzxjb7qmi93pp-master-0:28554 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 15:30:50,513.513 dlcmzxjb7qmi93pp-master-0:28556 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 15:30:51,758.758 dlcmzxjb7qmi93pp-master-0:28556 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 15:30:51,759.759 dlcmzxjb7qmi93pp-master-0:28554 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 15:30:52,300.300 dlcmzxjb7qmi93pp-master-0:28555 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 15:30:52,304.304 dlcmzxjb7qmi93pp-master-0:28553 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:28553:28553 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:28556:28556 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28555:28555 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28554:28554 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7652), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7285), 0.4462854088722608, 0.7454798331015299, tensor(1.4815)]
[tensor(-0.6870), 0.4564404061998931, 0.7670375521557719, tensor(1.5952)]
[tensor(-0.6870), 0.4564404061998931, 0.7691237830319889, tensor(1.5952)]
[tensor(-0.6747), 0.464457509353287, 0.7691237830319889, tensor(1.6476)]
[tensor(-0.6653), 0.464457509353287, 0.7691237830319889, tensor(1.6476)]
[tensor(-0.6614), 0.464457509353287, 0.782336578581363, tensor(1.6608)]
[tensor(-0.6614), 0.47888829502939606, 0.782336578581363, tensor(1.7331)]
[tensor(-0.6428), 0.47888829502939606, 0.782336578581363, tensor(1.7437)]
[2023-01-19 17:10:00,990.990 dlcmzxjb7qmi93pp-master-0:28763 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 17:10:01,657.657 dlcmzxjb7qmi93pp-master-0:28829 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:10:01,715.715 dlcmzxjb7qmi93pp-master-0:28828 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:10:01,715.715 dlcmzxjb7qmi93pp-master-0:28831 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:10:01,779.779 dlcmzxjb7qmi93pp-master-0:28830 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:10:02,913.913 dlcmzxjb7qmi93pp-master-0:28829 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 17:10:02,989.989 dlcmzxjb7qmi93pp-master-0:28830 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 17:10:03,586.586 dlcmzxjb7qmi93pp-master-0:28831 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 17:10:03,590.590 dlcmzxjb7qmi93pp-master-0:28828 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:28828:28828 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:28831:28831 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28829:28829 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28830:28830 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7774), 0.4462854088722608, 0.6474269819193325, tensor(1.4540)]
[tensor(-0.7773), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[2023-01-19 17:20:18,332.332 dlcmzxjb7qmi93pp-master-0:28905 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 17:20:18,950.950 dlcmzxjb7qmi93pp-master-0:28972 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:20:18,950.950 dlcmzxjb7qmi93pp-master-0:28973 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:20:19,031.031 dlcmzxjb7qmi93pp-master-0:28971 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:20:19,041.041 dlcmzxjb7qmi93pp-master-0:28970 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:20:20,348.348 dlcmzxjb7qmi93pp-master-0:28971 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 17:20:20,820.820 dlcmzxjb7qmi93pp-master-0:28972 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 17:20:20,821.821 dlcmzxjb7qmi93pp-master-0:28973 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 17:20:20,822.822 dlcmzxjb7qmi93pp-master-0:28970 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:28970:28970 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:28973:28973 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28971:28971 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:28972:28972 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6474269819193325, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[2023-01-19 17:30:33,677.677 dlcmzxjb7qmi93pp-master-0:29048 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 17:30:34,302.302 dlcmzxjb7qmi93pp-master-0:29114 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:30:34,353.353 dlcmzxjb7qmi93pp-master-0:29113 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:30:34,368.368 dlcmzxjb7qmi93pp-master-0:29115 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:30:34,428.428 dlcmzxjb7qmi93pp-master-0:29116 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 17:30:35,630.630 dlcmzxjb7qmi93pp-master-0:29115 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 17:30:35,678.678 dlcmzxjb7qmi93pp-master-0:29116 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 17:30:36,124.124 dlcmzxjb7qmi93pp-master-0:29114 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 17:30:36,132.132 dlcmzxjb7qmi93pp-master-0:29113 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:29113:29113 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:29114:29114 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29116:29116 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29115:29115 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7778), 0.4462854088722608, 0.6474269819193325, tensor(1.4536)]
[tensor(-0.7773), 0.4462854088722608, 0.6474269819193325, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6488178025034771, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6488178025034771, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6488178025034771, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6488178025034771, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6488178025034771, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6488178025034771, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6488178025034771, tensor(1.4545)]
[2023-01-19 19:13:09,385.385 dlcmzxjb7qmi93pp-master-0:29329 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 19:13:10,003.003 dlcmzxjb7qmi93pp-master-0:29397 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 19:13:10,005.005 dlcmzxjb7qmi93pp-master-0:29396 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 19:13:10,085.085 dlcmzxjb7qmi93pp-master-0:29394 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 19:13:10,097.097 dlcmzxjb7qmi93pp-master-0:29395 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 19:13:11,373.373 dlcmzxjb7qmi93pp-master-0:29395 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 19:13:11,877.877 dlcmzxjb7qmi93pp-master-0:29396 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 19:13:11,878.878 dlcmzxjb7qmi93pp-master-0:29397 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 19:13:11,887.887 dlcmzxjb7qmi93pp-master-0:29394 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:29394:29394 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:29396:29396 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29397:29397 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29395:29395 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6474269819193325, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7468), 0.4462854088722608, 0.6641168289290682, tensor(1.4846)]
[tensor(-0.7217), 0.4462854088722608, 0.7301808066759388, tensor(1.4846)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7155), 0.4462854088722608, 0.7614742698191933, tensor(1.4846)]
[tensor(-0.7067), 0.4462854088722608, 0.7677329624478443, tensor(1.4846)]
[tensor(-0.6998), 0.4462854088722608, 0.7698191933240612, tensor(1.4915)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.6758), 0.44681988241582044, 0.7698191933240612, tensor(1.5583)]
[tensor(-0.6475), 0.4569748797434527, 0.780250347705146, tensor(1.6374)]
[tensor(-0.6475), 0.4569748797434527, 0.7906815020862309, tensor(1.6374)]
[tensor(-0.6475), 0.4569748797434527, 0.7906815020862309, tensor(1.6374)]
[tensor(-0.6475), 0.46125066809192944, 0.7920723226703755, tensor(1.6583)]
[tensor(-0.6463), 0.46392303580972744, 0.7920723226703755, tensor(1.6733)]
[tensor(-0.6294), 0.47888829502939606, 0.7976356050069541, tensor(1.7650)]
[tensor(-0.6294), 0.47888829502939606, 0.7976356050069541, tensor(1.7650)]
[tensor(-0.6294), 0.47888829502939606, 0.7990264255910987, tensor(1.7650)]
[tensor(-0.6294), 0.47888829502939606, 0.7990264255910987, tensor(1.7650)]
[tensor(-0.6294), 0.47888829502939606, 0.7990264255910987, tensor(1.7650)]
[tensor(-0.6294), 0.47888829502939606, 0.7990264255910987, tensor(1.7650)]
[tensor(-0.6294), 0.47888829502939606, 0.805980528511822, tensor(1.7650)]
[tensor(-0.6260), 0.48583645109567075, 0.805980528511822, tensor(1.8032)]
[2023-01-19 20:54:02,677.677 dlcmzxjb7qmi93pp-master-0:29607 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 20:54:03,306.306 dlcmzxjb7qmi93pp-master-0:29675 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 20:54:03,306.306 dlcmzxjb7qmi93pp-master-0:29672 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 20:54:03,306.306 dlcmzxjb7qmi93pp-master-0:29673 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 20:54:03,307.307 dlcmzxjb7qmi93pp-master-0:29674 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 20:54:04,260.260 dlcmzxjb7qmi93pp-master-0:29675 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 20:54:04,261.261 dlcmzxjb7qmi93pp-master-0:29674 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 20:54:05,235.235 dlcmzxjb7qmi93pp-master-0:29673 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 20:54:05,240.240 dlcmzxjb7qmi93pp-master-0:29672 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:29672:29672 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:29673:29673 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29674:29674 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29675:29675 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7777), 0.4462854088722608, 0.6481223922114048, tensor(1.4537)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-19 21:04:28,042.042 dlcmzxjb7qmi93pp-master-0:29750 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 21:04:28,728.728 dlcmzxjb7qmi93pp-master-0:29816 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:04:28,747.747 dlcmzxjb7qmi93pp-master-0:29817 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:04:28,839.839 dlcmzxjb7qmi93pp-master-0:29815 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:04:28,920.920 dlcmzxjb7qmi93pp-master-0:29818 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:04:30,019.019 dlcmzxjb7qmi93pp-master-0:29817 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 21:04:30,140.140 dlcmzxjb7qmi93pp-master-0:29818 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 21:04:30,538.538 dlcmzxjb7qmi93pp-master-0:29816 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 21:04:30,548.548 dlcmzxjb7qmi93pp-master-0:29815 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:29815:29815 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:29816:29816 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29817:29817 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29818:29818 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[2023-01-19 21:14:52,386.386 dlcmzxjb7qmi93pp-master-0:29892 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 21:14:53,028.028 dlcmzxjb7qmi93pp-master-0:29957 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:14:53,031.031 dlcmzxjb7qmi93pp-master-0:29960 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:14:53,031.031 dlcmzxjb7qmi93pp-master-0:29959 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:14:53,032.032 dlcmzxjb7qmi93pp-master-0:29958 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 21:14:53,980.980 dlcmzxjb7qmi93pp-master-0:29960 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 21:14:53,981.981 dlcmzxjb7qmi93pp-master-0:29958 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 21:14:54,972.972 dlcmzxjb7qmi93pp-master-0:29959 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 21:14:54,977.977 dlcmzxjb7qmi93pp-master-0:29957 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:29957:29957 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:29959:29959 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29958:29958 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:29960:29960 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7783), 0.4462854088722608, 0.6481223922114048, tensor(1.4532)]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4540)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[2023-01-19 22:54:31,592.592 dlcmzxjb7qmi93pp-master-0:30169 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 22:54:32,214.214 dlcmzxjb7qmi93pp-master-0:30235 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 22:54:32,215.215 dlcmzxjb7qmi93pp-master-0:30234 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 22:54:32,299.299 dlcmzxjb7qmi93pp-master-0:30237 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 22:54:32,305.305 dlcmzxjb7qmi93pp-master-0:30236 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 22:54:33,094.094 dlcmzxjb7qmi93pp-master-0:30235 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 22:54:33,581.581 dlcmzxjb7qmi93pp-master-0:30236 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 22:54:33,582.582 dlcmzxjb7qmi93pp-master-0:30237 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 22:54:33,591.591 dlcmzxjb7qmi93pp-master-0:30234 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:30234:30234 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:30235:30235 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30237:30237 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30236:30236 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-20 00:33:23,189.189 dlcmzxjb7qmi93pp-master-0:30444 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 00:33:23,810.810 dlcmzxjb7qmi93pp-master-0:30510 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:33:23,810.810 dlcmzxjb7qmi93pp-master-0:30511 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:33:24,007.007 dlcmzxjb7qmi93pp-master-0:30512 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:33:24,007.007 dlcmzxjb7qmi93pp-master-0:30509 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:33:25,671.671 dlcmzxjb7qmi93pp-master-0:30510 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 00:33:25,671.671 dlcmzxjb7qmi93pp-master-0:30511 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 00:33:26,130.130 dlcmzxjb7qmi93pp-master-0:30512 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 00:33:26,136.136 dlcmzxjb7qmi93pp-master-0:30509 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:30509:30509 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:30511:30511 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30512:30512 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30510:30510 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7774), 0.4462854088722608, 0.6474269819193325, tensor(1.4540)]
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-20 00:43:45,560.560 dlcmzxjb7qmi93pp-master-0:30587 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 00:43:46,173.173 dlcmzxjb7qmi93pp-master-0:30654 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:43:46,203.203 dlcmzxjb7qmi93pp-master-0:30653 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:43:46,290.290 dlcmzxjb7qmi93pp-master-0:30652 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:43:46,363.363 dlcmzxjb7qmi93pp-master-0:30655 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:43:47,461.461 dlcmzxjb7qmi93pp-master-0:30653 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 00:43:47,576.576 dlcmzxjb7qmi93pp-master-0:30655 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 00:43:48,078.078 dlcmzxjb7qmi93pp-master-0:30654 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 00:43:48,080.080 dlcmzxjb7qmi93pp-master-0:30652 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:30652:30652 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:30654:30654 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30653:30653 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30655:30655 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6474269819193325, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6474269819193325, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-20 00:53:57,872.872 dlcmzxjb7qmi93pp-master-0:30729 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 00:53:58,499.499 dlcmzxjb7qmi93pp-master-0:30795 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:53:58,499.499 dlcmzxjb7qmi93pp-master-0:30797 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:53:58,529.529 dlcmzxjb7qmi93pp-master-0:30796 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:53:58,688.688 dlcmzxjb7qmi93pp-master-0:30794 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 00:54:00,370.370 dlcmzxjb7qmi93pp-master-0:30795 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 00:54:00,371.371 dlcmzxjb7qmi93pp-master-0:30797 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 00:54:00,881.881 dlcmzxjb7qmi93pp-master-0:30796 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 00:54:00,882.882 dlcmzxjb7qmi93pp-master-0:30794 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:30794:30794 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:30795:30795 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30796:30796 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:30797:30797 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7778), 0.4462854088722608, 0.6481223922114048, tensor(1.4536)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[2023-01-20 02:35:52,118.118 dlcmzxjb7qmi93pp-master-0:31009 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 02:35:52,802.802 dlcmzxjb7qmi93pp-master-0:31075 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 02:35:52,829.829 dlcmzxjb7qmi93pp-master-0:31076 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 02:35:52,905.905 dlcmzxjb7qmi93pp-master-0:31077 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 02:35:52,993.993 dlcmzxjb7qmi93pp-master-0:31074 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 02:35:54,695.695 dlcmzxjb7qmi93pp-master-0:31075 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 02:35:54,720.720 dlcmzxjb7qmi93pp-master-0:31077 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 02:35:55,096.096 dlcmzxjb7qmi93pp-master-0:31076 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 02:35:55,104.104 dlcmzxjb7qmi93pp-master-0:31074 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:31074:31074 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:31075:31075 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31077:31077 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31076:31076 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-20 04:15:46,466.466 dlcmzxjb7qmi93pp-master-0:31286 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 04:15:47,128.128 dlcmzxjb7qmi93pp-master-0:31352 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:15:47,146.146 dlcmzxjb7qmi93pp-master-0:31353 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:15:47,238.238 dlcmzxjb7qmi93pp-master-0:31351 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:15:47,351.351 dlcmzxjb7qmi93pp-master-0:31354 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:15:48,390.390 dlcmzxjb7qmi93pp-master-0:31353 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 04:15:48,541.541 dlcmzxjb7qmi93pp-master-0:31354 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 04:15:48,935.935 dlcmzxjb7qmi93pp-master-0:31352 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 04:15:48,940.940 dlcmzxjb7qmi93pp-master-0:31351 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:31351:31351 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:31353:31353 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31352:31352 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31354:31354 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[Fri Jan 20 04:21:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-20 04:26:20,800.800 dlcmzxjb7qmi93pp-master-0:31428 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 04:26:21,416.416 dlcmzxjb7qmi93pp-master-0:31495 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:26:21,495.495 dlcmzxjb7qmi93pp-master-0:31496 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:26:21,500.500 dlcmzxjb7qmi93pp-master-0:31494 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:26:21,664.664 dlcmzxjb7qmi93pp-master-0:31493 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:26:22,770.770 dlcmzxjb7qmi93pp-master-0:31494 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 04:26:22,774.774 dlcmzxjb7qmi93pp-master-0:31496 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 04:26:23,236.236 dlcmzxjb7qmi93pp-master-0:31495 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 04:26:23,244.244 dlcmzxjb7qmi93pp-master-0:31493 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:31493:31493 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:31494:31494 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31496:31496 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31495:31495 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-20 04:36:29,195.195 dlcmzxjb7qmi93pp-master-0:31571 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 04:36:29,802.802 dlcmzxjb7qmi93pp-master-0:31638 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:36:29,802.802 dlcmzxjb7qmi93pp-master-0:31639 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:36:29,885.885 dlcmzxjb7qmi93pp-master-0:31636 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:36:29,893.893 dlcmzxjb7qmi93pp-master-0:31637 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 04:36:31,001.001 dlcmzxjb7qmi93pp-master-0:31637 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 04:36:31,663.663 dlcmzxjb7qmi93pp-master-0:31639 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 04:36:31,664.664 dlcmzxjb7qmi93pp-master-0:31638 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 04:36:31,666.666 dlcmzxjb7qmi93pp-master-0:31636 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:31636:31636 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:31637:31637 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31639:31639 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31638:31638 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7752), 0.4462854088722608, 0.6481223922114048, tensor(1.4562)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 05:00:20 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 05:16:12 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 05:31:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 05:37:36 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 05:39:40 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 05:43:49 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 06:01:58 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 06:13:10 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-20 06:16:56,670.670 dlcmzxjb7qmi93pp-master-0:31849 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 06:16:57,279.279 dlcmzxjb7qmi93pp-master-0:31916 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 06:16:57,279.279 dlcmzxjb7qmi93pp-master-0:31915 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 06:16:57,280.280 dlcmzxjb7qmi93pp-master-0:31917 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 06:16:57,309.309 dlcmzxjb7qmi93pp-master-0:31914 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 06:16:59,180.180 dlcmzxjb7qmi93pp-master-0:31915 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 06:16:59,184.184 dlcmzxjb7qmi93pp-master-0:31916 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 06:16:59,187.187 dlcmzxjb7qmi93pp-master-0:31917 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 06:16:59,194.194 dlcmzxjb7qmi93pp-master-0:31914 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:31914:31914 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:31916:31916 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31915:31915 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:31917:31917 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7768), 0.4462854088722608, 0.3518776077885953, tensor(1.4546)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 06:56:41 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 07:18:14 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 07:24:04 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 07:44:14 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 07:52:34 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[2023-01-20 07:57:48,147.147 dlcmzxjb7qmi93pp-master-0:32123 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 07:57:48,759.759 dlcmzxjb7qmi93pp-master-0:32189 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 07:57:48,759.759 dlcmzxjb7qmi93pp-master-0:32191 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 07:57:48,839.839 dlcmzxjb7qmi93pp-master-0:32188 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 07:57:48,843.843 dlcmzxjb7qmi93pp-master-0:32190 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 07:57:49,868.868 dlcmzxjb7qmi93pp-master-0:32190 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 07:57:50,613.613 dlcmzxjb7qmi93pp-master-0:32189 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 07:57:50,617.617 dlcmzxjb7qmi93pp-master-0:32191 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 07:57:50,627.627 dlcmzxjb7qmi93pp-master-0:32188 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:32188:32188 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:32191:32191 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32189:32189 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32190:32190 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7749), 0.4462854088722608, 0.6467315716272601, tensor(1.4565)]
[tensor(-0.7749), 0.4462854088722608, 0.6474269819193325, tensor(1.4566)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[2023-01-20 08:07:55,484.484 dlcmzxjb7qmi93pp-master-0:32266 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 08:07:56,096.096 dlcmzxjb7qmi93pp-master-0:32333 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:07:56,097.097 dlcmzxjb7qmi93pp-master-0:32332 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:07:56,285.285 dlcmzxjb7qmi93pp-master-0:32334 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:07:56,285.285 dlcmzxjb7qmi93pp-master-0:32331 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:07:57,161.161 dlcmzxjb7qmi93pp-master-0:32334 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 08:07:57,965.965 dlcmzxjb7qmi93pp-master-0:32333 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 08:07:57,968.968 dlcmzxjb7qmi93pp-master-0:32332 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 08:07:57,969.969 dlcmzxjb7qmi93pp-master-0:32331 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:32331:32331 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:32334:32334 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32333:32333 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32332:32332 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[2023-01-20 08:18:16,851.851 dlcmzxjb7qmi93pp-master-0:32408 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 08:18:17,479.479 dlcmzxjb7qmi93pp-master-0:32476 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:18:17,560.560 dlcmzxjb7qmi93pp-master-0:32475 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:18:17,566.566 dlcmzxjb7qmi93pp-master-0:32473 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:18:17,584.584 dlcmzxjb7qmi93pp-master-0:32474 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 08:18:19,425.425 dlcmzxjb7qmi93pp-master-0:32474 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 08:18:19,452.452 dlcmzxjb7qmi93pp-master-0:32475 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 08:18:19,555.555 dlcmzxjb7qmi93pp-master-0:32476 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 08:18:19,557.557 dlcmzxjb7qmi93pp-master-0:32473 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:32473:32473 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:32476:32476 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32474:32474 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32475:32475 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4565)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[Fri Jan 20 08:26:38 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[Fri Jan 20 08:36:50 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 08:50:35 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 08:57:12 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 08:59:03 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 09:02:42 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 09:04:11 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Fri Jan 20 09:09:00 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 09:10:50 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 09:16:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 09:20:50 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 09:22:04 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[tensor(-0.7745), 0.4462854088722608, 0.6481223922114048, tensor(1.4569)]
[Fri Jan 20 09:32:15 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 09:36:06 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 09:38:18 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 09:42:45 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 09:44:17 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 09:46:39 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[Fri Jan 20 09:52:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[tensor(-0.7744), 0.4462854088722608, 0.6481223922114048, tensor(1.4570)]
[2023-01-20 09:57:51,153.153 dlcmzxjb7qmi93pp-master-0:32684 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 09:57:51,764.764 dlcmzxjb7qmi93pp-master-0:32752 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 09:57:51,764.764 dlcmzxjb7qmi93pp-master-0:32750 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 09:57:51,842.842 dlcmzxjb7qmi93pp-master-0:32751 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 09:57:51,852.852 dlcmzxjb7qmi93pp-master-0:32749 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 09:57:53,613.613 dlcmzxjb7qmi93pp-master-0:32750 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 09:57:53,614.614 dlcmzxjb7qmi93pp-master-0:32752 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 09:57:54,233.233 dlcmzxjb7qmi93pp-master-0:32751 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 09:57:54,242.242 dlcmzxjb7qmi93pp-master-0:32749 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:32749:32749 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:32752:32752 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32751:32751 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:32750:32750 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7749), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4566)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7748), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7747), 0.4462854088722608, 0.6481223922114048, tensor(1.4567)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7746), 0.4462854088722608, 0.6481223922114048, tensor(1.4568)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[tensor(-0.7743), 0.4462854088722608, 0.6481223922114048, tensor(1.4571)]
[2023-01-20 11:37:02,435.435 dlcmzxjb7qmi93pp-master-0:32960 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 11:37:03,064.064 dlcmzxjb7qmi93pp-master-0:33027 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:37:03,103.103 dlcmzxjb7qmi93pp-master-0:33026 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:37:03,140.140 dlcmzxjb7qmi93pp-master-0:33028 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:37:03,215.215 dlcmzxjb7qmi93pp-master-0:33025 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:37:04,910.910 dlcmzxjb7qmi93pp-master-0:33027 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 11:37:04,942.942 dlcmzxjb7qmi93pp-master-0:33028 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 11:37:05,349.349 dlcmzxjb7qmi93pp-master-0:33026 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 11:37:05,354.354 dlcmzxjb7qmi93pp-master-0:33025 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:33025:33025 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:33028:33028 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33027:33027 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33026:33026 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-20 11:47:49,778.778 dlcmzxjb7qmi93pp-master-0:33103 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 11:47:50,402.402 dlcmzxjb7qmi93pp-master-0:33169 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:47:50,402.402 dlcmzxjb7qmi93pp-master-0:33171 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:47:50,402.402 dlcmzxjb7qmi93pp-master-0:33168 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:47:50,402.402 dlcmzxjb7qmi93pp-master-0:33170 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:47:51,352.352 dlcmzxjb7qmi93pp-master-0:33169 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 11:47:51,353.353 dlcmzxjb7qmi93pp-master-0:33171 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 11:47:52,345.345 dlcmzxjb7qmi93pp-master-0:33170 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 11:47:52,350.350 dlcmzxjb7qmi93pp-master-0:33168 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:33168:33168 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:33171:33171 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33170:33170 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33169:33169 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7774), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-20 11:57:57,110.110 dlcmzxjb7qmi93pp-master-0:33246 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 11:57:57,782.782 dlcmzxjb7qmi93pp-master-0:33311 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:57:57,793.793 dlcmzxjb7qmi93pp-master-0:33312 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:57:57,850.850 dlcmzxjb7qmi93pp-master-0:33313 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:57:57,854.854 dlcmzxjb7qmi93pp-master-0:33314 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 11:57:58,777.777 dlcmzxjb7qmi93pp-master-0:33312 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 11:57:59,711.711 dlcmzxjb7qmi93pp-master-0:33314 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 11:57:59,713.713 dlcmzxjb7qmi93pp-master-0:33313 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 11:57:59,714.714 dlcmzxjb7qmi93pp-master-0:33311 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:33311:33311 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:33313:33313 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33314:33314 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33312:33312 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-20 13:39:37,656.656 dlcmzxjb7qmi93pp-master-0:33525 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 13:39:38,302.302 dlcmzxjb7qmi93pp-master-0:33591 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 13:39:38,302.302 dlcmzxjb7qmi93pp-master-0:33590 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 13:39:38,369.369 dlcmzxjb7qmi93pp-master-0:33593 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 13:39:38,410.410 dlcmzxjb7qmi93pp-master-0:33592 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 13:39:39,314.314 dlcmzxjb7qmi93pp-master-0:33591 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 13:39:39,377.377 dlcmzxjb7qmi93pp-master-0:33593 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 13:39:39,382.382 dlcmzxjb7qmi93pp-master-0:33592 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 13:39:39,390.390 dlcmzxjb7qmi93pp-master-0:33590 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:33590:33590 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:33593:33593 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33592:33592 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33591:33591 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6474269819193325, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4546)]
[2023-01-20 15:18:45,923.923 dlcmzxjb7qmi93pp-master-0:33796 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 15:18:46,538.538 dlcmzxjb7qmi93pp-master-0:33863 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:18:46,581.581 dlcmzxjb7qmi93pp-master-0:33862 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:18:46,612.612 dlcmzxjb7qmi93pp-master-0:33864 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:18:46,699.699 dlcmzxjb7qmi93pp-master-0:33861 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:18:48,385.385 dlcmzxjb7qmi93pp-master-0:33863 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 15:18:48,493.493 dlcmzxjb7qmi93pp-master-0:33864 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 15:18:48,834.834 dlcmzxjb7qmi93pp-master-0:33862 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 15:18:48,841.841 dlcmzxjb7qmi93pp-master-0:33861 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:33861:33861 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:33863:33863 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33864:33864 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:33862:33862 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7776), 0.4462854088722608, 0.6481223922114048, tensor(1.4538)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[tensor(-0.7773), 0.4462854088722608, 0.6481223922114048, tensor(1.4541)]
[2023-01-20 15:29:19,275.275 dlcmzxjb7qmi93pp-master-0:33939 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 15:29:19,945.945 dlcmzxjb7qmi93pp-master-0:34005 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:29:19,964.964 dlcmzxjb7qmi93pp-master-0:34006 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:29:19,992.992 dlcmzxjb7qmi93pp-master-0:34007 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:29:20,076.076 dlcmzxjb7qmi93pp-master-0:34004 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:29:21,795.795 dlcmzxjb7qmi93pp-master-0:34005 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 15:29:21,817.817 dlcmzxjb7qmi93pp-master-0:34007 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 15:29:22,208.208 dlcmzxjb7qmi93pp-master-0:34006 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 15:29:22,212.212 dlcmzxjb7qmi93pp-master-0:34004 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:34004:34004 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:34005:34005 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:34007:34007 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:34006:34006 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[2023-01-20 15:40:20,655.655 dlcmzxjb7qmi93pp-master-0:34083 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 15:40:21,268.268 dlcmzxjb7qmi93pp-master-0:34151 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:40:21,269.269 dlcmzxjb7qmi93pp-master-0:34148 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:40:21,352.352 dlcmzxjb7qmi93pp-master-0:34149 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:40:21,359.359 dlcmzxjb7qmi93pp-master-0:34150 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 15:40:22,404.404 dlcmzxjb7qmi93pp-master-0:34149 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 15:40:22,406.406 dlcmzxjb7qmi93pp-master-0:34150 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 15:40:23,137.137 dlcmzxjb7qmi93pp-master-0:34151 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 15:40:23,141.141 dlcmzxjb7qmi93pp-master-0:34148 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:34148:34148 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:34149:34149 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:34150:34150 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:34151:34151 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.7779), 0.4462854088722608, 0.6481223922114048, tensor(1.4535)]
[tensor(-0.7775), 0.4462854088722608, 0.6481223922114048, tensor(1.4539)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4542)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7772), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[tensor(-0.7771), 0.4462854088722608, 0.6481223922114048, tensor(1.4543)]
[2023-01-20 17:19:16,012.012 dlcmzxjb7qmi93pp-master-0:34357 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-20 17:19:16,622.622 dlcmzxjb7qmi93pp-master-0:34425 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 17:19:16,623.623 dlcmzxjb7qmi93pp-master-0:34424 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 17:19:16,706.706 dlcmzxjb7qmi93pp-master-0:34422 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 17:19:16,716.716 dlcmzxjb7qmi93pp-master-0:34423 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-20 17:19:18,018.018 dlcmzxjb7qmi93pp-master-0:34423 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-20 17:19:18,518.518 dlcmzxjb7qmi93pp-master-0:34425 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-20 17:19:18,521.521 dlcmzxjb7qmi93pp-master-0:34424 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-20 17:19:18,523.523 dlcmzxjb7qmi93pp-master-0:34422 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
fusion layers 1
fusion layers 1
fusion layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.1-100 were not used when initializing ATModel: ['response_selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlcmzxjb7qmi93pp-master-0:34422:34422 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlcmzxjb7qmi93pp-master-0:34425:34425 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:34423:34423 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlcmzxjb7qmi93pp-master-0:34424:34424 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7772), 0.4462854088722608, 0.6474269819193325, tensor(1.4543)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4544)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7770), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7769), 0.4462854088722608, 0.6481223922114048, tensor(1.4545)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
[tensor(-0.7766), 0.4462854088722608, 0.6481223922114048, tensor(1.4548)]
