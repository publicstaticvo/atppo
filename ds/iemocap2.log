[2023-01-19 13:50:11,560.560 dsw46436-6bc9877c76-ns84r:81101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0116), 0.5950095969289827, tensor(1.9634)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0116), 0.5950095969289827, tensor(1.9634)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1177)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1177)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9821), 0.6199616122840691, tensor(2.1177)]
[2023-01-19 14:03:54,805.805 dsw46436-6bc9877c76-ns84r:81352 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0781), 0.5662188099808061, tensor(1.7530)]
[tensor(-1.0041), 0.5662188099808061, tensor(1.7983)]
[tensor(-1.0041), 0.5662188099808061, tensor(1.7983)]
[tensor(-1.0041), 0.5758157389635317, tensor(1.7983)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0041), 0.5758157389635317, tensor(1.7983)]
[2023-01-19 14:17:28,116.116 dsw46436-6bc9877c76-ns84r:81596 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0049), 0.6026871401151631, tensor(2.0085)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0049), 0.6026871401151631, tensor(2.0085)]
[tensor(-0.9915), 0.6084452975047985, tensor(2.0508)]
[tensor(-0.9915), 0.6084452975047985, tensor(2.0508)]
[tensor(-0.9915), 0.6084452975047985, tensor(2.0508)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
early stopping at 11
[2023-01-19 14:47:02,797.797 dsw46436-6bc9877c76-ns84r:82102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0626), 0.5470249520153551, tensor(1.6726)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9991), 0.6161228406909789, tensor(1.9855)]
[tensor(-0.9991), 0.6161228406909789, tensor(1.9855)]
[tensor(-0.9991), 0.6161228406909789, tensor(1.9855)]
[tensor(-0.9991), 0.6257197696737045, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
early stopping at 19
[2023-01-19 15:37:48,143.143 dsw46436-6bc9877c76-ns84r:83036 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0230), 0.5662188099808061, tensor(1.8081)]
[tensor(-0.9654), 0.6026871401151631, tensor(2.0481)]
[tensor(-0.9540), 0.6218809980806143, tensor(2.1554)]
[tensor(-0.9540), 0.6218809980806143, tensor(2.1554)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9540), 0.6218809980806143, tensor(2.1554)]
[2023-01-19 15:51:30,108.108 dsw46436-6bc9877c76-ns84r:83286 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0797), 0.5777351247600768, tensor(1.8090)]
[tensor(-1.0657), 0.5777351247600768, tensor(1.8090)]
[tensor(-1.0222), 0.5834932821497121, tensor(1.8953)]
[tensor(-1.0222), 0.6161228406909789, tensor(1.9663)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0222), 0.6161228406909789, tensor(1.9663)]
[2023-01-19 16:04:57,697.697 dsw46436-6bc9877c76-ns84r:83525 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9755), 0.5950095969289827, tensor(1.9995)]
[tensor(-0.9755), 0.5950095969289827, tensor(1.9995)]
[tensor(-0.9640), 0.6161228406909789, tensor(2.1166)]
[tensor(-0.9640), 0.6161228406909789, tensor(2.1166)]
[tensor(-0.9640), 0.6161228406909789, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
early stopping at 11
[2023-01-19 16:35:00,464.464 dsw46436-6bc9877c76-ns84r:84245 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1066), 0.54510556621881, tensor(1.6189)]
[tensor(-1.0304), 0.5930902111324377, tensor(1.9351)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0302), 0.5930902111324377, tensor(1.9351)]
[tensor(-1.0302), 0.5930902111324377, tensor(1.9351)]
[tensor(-1.0302), 0.5930902111324377, tensor(1.9351)]
[tensor(-1.0302), 0.6103646833013435, tensor(1.9498)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0302), 0.6103646833013435, tensor(1.9498)]
[tensor(-1.0302), 0.6103646833013435, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
early stopping at 14
[2023-01-19 17:12:25,421.421 dsw46436-6bc9877c76-ns84r:84909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0106), 0.5930902111324377, tensor(1.9548)]
[tensor(-1.0106), 0.5930902111324377, tensor(1.9548)]
[tensor(-1.0106), 0.6007677543186181, tensor(1.9823)]
[tensor(-1.0106), 0.6007677543186181, tensor(1.9823)]
[tensor(-1.0106), 0.6007677543186181, tensor(1.9823)]
[2023-01-19 17:26:01,350.350 dsw46436-6bc9877c76-ns84r:85152 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0188), 0.5892514395393474, tensor(1.9275)]
[tensor(-0.9975), 0.5892514395393474, tensor(1.9275)]
[tensor(-0.9921), 0.5911708253358925, tensor(1.9637)]
[tensor(-0.9921), 0.5911708253358925, tensor(1.9637)]
[tensor(-0.9921), 0.5911708253358925, tensor(1.9637)]
[2023-01-19 17:39:29,848.848 dsw46436-6bc9877c76-ns84r:85393 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9842), 0.6046065259117083, tensor(2.0388)]
[tensor(-0.9795), 0.6046065259117083, tensor(2.0388)]
[tensor(-0.9622), 0.6180422264875239, tensor(2.1280)]
[tensor(-0.9622), 0.6180422264875239, tensor(2.1280)]
[tensor(-0.9622), 0.6180422264875239, tensor(2.1280)]
[tensor(-0.9622), 0.6679462571976967, tensor(2.3121)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9622), 0.6679462571976967, tensor(2.3121)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9622), 0.6679462571976967, tensor(2.3121)]
[tensor(-0.9622), 0.6679462571976967, tensor(2.3121)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9622), 0.6679462571976967, tensor(2.3121)]
[tensor(-0.9622), 0.6679462571976967, tensor(2.3121)]
early stopping at 11
[2023-01-19 18:08:54,658.658 dsw46436-6bc9877c76-ns84r:85892 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1139), 0.5700575815738963, tensor(1.7364)]
[tensor(-1.0308), 0.6295585412667947, tensor(2.1169)]
[tensor(-0.9910), 0.6295585412667947, tensor(2.1169)]
[tensor(-0.9910), 0.6295585412667947, tensor(2.1169)]
[tensor(-0.9910), 0.6295585412667947, tensor(2.1169)]
[tensor(-0.9910), 0.6295585412667947, tensor(2.1169)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9910), 0.6295585412667947, tensor(2.1169)]
[tensor(-0.9910), 0.6353166986564299, tensor(2.1169)]
[tensor(-0.9910), 0.6353166986564299, tensor(2.1169)]
[tensor(-0.9910), 0.6353166986564299, tensor(2.1169)]
[tensor(-0.9910), 0.6353166986564299, tensor(2.1169)]
[tensor(-0.9910), 0.6353166986564299, tensor(2.1169)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9910), 0.6391554702495201, tensor(2.1169)]
[tensor(-0.9910), 0.6391554702495201, tensor(2.1169)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9910), 0.6391554702495201, tensor(2.1169)]
[tensor(-0.9910), 0.6391554702495201, tensor(2.1169)]
[tensor(-0.9910), 0.6391554702495201, tensor(2.1169)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9910), 0.6391554702495201, tensor(2.1169)]
early stopping at 18
[2023-01-19 18:56:57,374.374 dsw46436-6bc9877c76-ns84r:86696 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9914), 0.5969289827255279, tensor(1.9933)]
[tensor(-0.9914), 0.5969289827255279, tensor(1.9933)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9798), 0.6103646833013435, tensor(2.0720)]
[tensor(-0.9798), 0.6103646833013435, tensor(2.0720)]
[tensor(-0.9798), 0.6103646833013435, tensor(2.0720)]
[2023-01-19 19:10:29,251.251 dsw46436-6bc9877c76-ns84r:86938 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0540), 0.5892514395393474, tensor(1.8922)]
[tensor(-1.0075), 0.5892514395393474, tensor(1.9387)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0075), 0.5892514395393474, tensor(1.9387)]
[tensor(-1.0075), 0.5988483685220729, tensor(1.9387)]
[tensor(-1.0075), 0.5988483685220729, tensor(1.9387)]
[2023-01-19 19:23:55,896.896 dsw46436-6bc9877c76-ns84r:87180 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9637), 0.6161228406909789, tensor(2.1169)]
[tensor(-0.9637), 0.6161228406909789, tensor(2.1169)]
[tensor(-0.9637), 0.6199616122840691, tensor(2.1347)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9637), 0.6199616122840691, tensor(2.1347)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9637), 0.6199616122840691, tensor(2.1347)]
[tensor(-0.9637), 0.6449136276391555, tensor(2.1347)]
[tensor(-0.9637), 0.6449136276391555, tensor(2.1347)]
[tensor(-0.9637), 0.6449136276391555, tensor(2.1347)]
[tensor(-0.9637), 0.6449136276391555, tensor(2.1347)]
[tensor(-0.9637), 0.6449136276391555, tensor(2.1347)]
[tensor(-0.9637), 0.6449136276391555, tensor(2.1347)]
early stopping at 11
[2023-01-19 19:53:05,128.128 dsw46436-6bc9877c76-ns84r:87679 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1650), 0.5220729366602687, tensor(1.4453)]
[tensor(-1.1054), 0.5969289827255279, tensor(1.8792)]
[tensor(-1.0383), 0.5969289827255279, tensor(1.8983)]
[tensor(-1.0383), 0.5969289827255279, tensor(1.8983)]
[tensor(-1.0383), 0.5969289827255279, tensor(1.8983)]
[tensor(-1.0383), 0.5969289827255279, tensor(1.9059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0383), 0.5969289827255279, tensor(1.9059)]
[tensor(-1.0383), 0.6046065259117083, tensor(1.9059)]
[tensor(-1.0383), 0.6046065259117083, tensor(1.9059)]
[tensor(-1.0383), 0.6046065259117083, tensor(1.9059)]
[tensor(-1.0383), 0.6046065259117083, tensor(1.9059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0383), 0.6046065259117083, tensor(1.9059)]
[tensor(-1.0383), 0.6084452975047985, tensor(1.9059)]
[tensor(-1.0383), 0.6084452975047985, tensor(1.9059)]
[tensor(-1.0383), 0.6084452975047985, tensor(1.9059)]
[tensor(-1.0383), 0.6084452975047985, tensor(1.9059)]
[tensor(-1.0383), 0.6180422264875239, tensor(1.9059)]
[tensor(-1.0383), 0.6180422264875239, tensor(1.9059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0383), 0.6180422264875239, tensor(1.9059)]
[tensor(-1.0383), 0.6180422264875239, tensor(1.9059)]
[tensor(-1.0383), 0.6180422264875239, tensor(1.9059)]
[tensor(-1.0383), 0.6180422264875239, tensor(1.9059)]
early stopping at 22
[2023-01-19 20:52:22,474.474 dsw46436-6bc9877c76-ns84r:88664 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0300), 0.5527831094049904, tensor(1.7340)]
[tensor(-1.0149), 0.5604606525911708, tensor(1.7874)]
[tensor(-1.0149), 0.6026871401151631, tensor(1.9934)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0149), 0.6314779270633397, tensor(2.1238)]
[tensor(-1.0149), 0.6314779270633397, tensor(2.1238)]
[2023-01-19 21:14:13,858.858 dsw46436-6bc9877c76-ns84r:89053 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0870), 0.5470249520153551, tensor(1.6481)]
[tensor(-1.0590), 0.5547024952015355, tensor(1.7146)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0464), 0.5796545105566219, tensor(1.8519)]
[tensor(-1.0464), 0.5930902111324377, tensor(1.8519)]
[tensor(-1.0464), 0.5930902111324377, tensor(1.8519)]
[2023-01-19 21:36:02,592.592 dsw46436-6bc9877c76-ns84r:89433 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0112), 0.5700575815738963, tensor(1.8390)]
[tensor(-0.9558), 0.6142034548944337, tensor(2.1152)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9558), 0.6142034548944337, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6238003838771593, tensor(2.1152)]
[tensor(-0.9558), 0.6238003838771593, tensor(2.1152)]
[tensor(-0.9558), 0.6238003838771593, tensor(2.1152)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
early stopping at 21
[2023-01-19 23:07:19,668.668 dsw46436-6bc9877c76-ns84r:90941 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6690), 0.2802303262955854, 0.0]
[tensor(-1.6690), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6690), 0.30902111324376197, 0.0]
early stopping at 3
[2023-01-19 23:20:30,560.560 dsw46436-6bc9877c76-ns84r:91177 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9839), 0.5604606525911708, tensor(1.8184)]
[tensor(-0.9839), 0.6122840690978887, tensor(2.0774)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9839), 0.6122840690978887, tensor(2.0774)]
[tensor(-0.9839), 0.6295585412667947, tensor(2.0774)]
[tensor(-0.9839), 0.6295585412667947, tensor(2.0774)]
[2023-01-19 23:42:40,429.429 dsw46436-6bc9877c76-ns84r:91564 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6975), 0.29942418426103645, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.4609), 0.345489443378119, tensor(0.2665)]
[tensor(-1.2425), 0.43378119001919385, tensor(0.9264)]
[tensor(-1.1825), 0.508637236084453, tensor(1.3607)]
[tensor(-1.1379), 0.508637236084453, tensor(1.3607)]
[2023-01-20 00:04:43,431.431 dsw46436-6bc9877c76-ns84r:91950 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
early stopping at 12
[2023-01-20 00:57:26,194.194 dsw46436-6bc9877c76-ns84r:92830 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6742), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6742), 0.2821497120921305, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4389), 0.33589251439539347, tensor(0.2406)]
[tensor(-1.2091), 0.4875239923224568, tensor(1.2285)]
[tensor(-1.1795), 0.5374280230326296, tensor(1.5076)]
[tensor(-1.1333), 0.5374280230326296, tensor(1.5076)]
[tensor(-1.0792), 0.5489443378119002, tensor(1.6655)]
[tensor(-1.0792), 0.5738963531669866, tensor(1.7566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0792), 0.5796545105566219, tensor(1.7566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0792), 0.5854126679462572, tensor(1.8131)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
early stopping at 16
[2023-01-20 02:07:41,217.217 dsw46436-6bc9877c76-ns84r:93998 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0098), 0.581573896353167, tensor(1.8981)]
[tensor(-0.9832), 0.6084452975047985, tensor(2.0590)]
[tensor(-0.9832), 0.6084452975047985, tensor(2.0590)]
[tensor(-0.9832), 0.6084452975047985, tensor(2.0590)]
[tensor(-0.9832), 0.6257197696737045, tensor(2.0590)]
[2023-01-20 02:29:42,115.115 dsw46436-6bc9877c76-ns84r:94385 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0351), 0.5796545105566219, tensor(1.8632)]
[tensor(-1.0011), 0.5796545105566219, tensor(1.8632)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9984), 0.5892514395393474, tensor(1.9479)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9984), 0.5950095969289827, tensor(1.9479)]
[tensor(-0.9984), 0.5950095969289827, tensor(1.9479)]
[2023-01-20 02:51:38,652.652 dsw46436-6bc9877c76-ns84r:94767 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0159), 0.5508637236084453, tensor(1.7384)]
[tensor(-0.9660), 0.5854126679462572, tensor(1.9610)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9660), 0.5911708253358925, tensor(1.9610)]
[tensor(-0.9660), 0.5911708253358925, tensor(1.9610)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9660), 0.5911708253358925, tensor(1.9610)]
[tensor(-0.9660), 0.6065259117082533, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
early stopping at 12
[2023-01-20 03:44:26,605.605 dsw46436-6bc9877c76-ns84r:95653 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
early stopping at 6
[2023-01-20 04:10:57,010.010 dsw46436-6bc9877c76-ns84r:96113 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0140), 0.5892514395393474, tensor(1.9322)]
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
[2023-01-20 04:32:55,628.628 dsw46436-6bc9877c76-ns84r:96492 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0635), 0.5355086372360844, tensor(1.6140)]
[tensor(-1.0632), 0.5489443378119002, tensor(1.6815)]
[tensor(-1.0632), 0.6026871401151631, tensor(1.9489)]
[tensor(-1.0632), 0.6026871401151631, tensor(1.9489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0632), 0.6084452975047985, tensor(1.9489)]
[2023-01-20 04:54:55,921.921 dsw46436-6bc9877c76-ns84r:96877 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9933), 0.6084452975047985, tensor(2.0489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9812), 0.6084452975047985, tensor(2.0489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6199616122840691, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9812), 0.6257197696737045, tensor(2.0537)]
[tensor(-0.9812), 0.6257197696737045, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
early stopping at 22
[2023-01-20 06:30:41,661.661 dsw46436-6bc9877c76-ns84r:98461 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6702), 0.2802303262955854, 0.0]
[tensor(-1.4585), 0.3857965451055662, tensor(0.4705)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3074), 0.39923224568138194, tensor(0.6888)]
[tensor(-1.1165), 0.5316698656429942, tensor(1.5419)]
[tensor(-1.1165), 0.5316698656429942, tensor(1.5419)]
[tensor(-1.1165), 0.5316698656429942, tensor(1.5419)]
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
[tensor(-1.1165), 0.5585412667946257, tensor(1.5887)]
[tensor(-1.1165), 0.5585412667946257, tensor(1.5887)]
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
[tensor(-1.1165), 0.5719769673704415, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5873320537428023, tensor(1.6440)]
[tensor(-1.1165), 0.5873320537428023, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
early stopping at 37
[2023-01-20 09:09:42,773.773 dsw46436-6bc9877c76-ns84r:101092 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9915), 0.5758157389635317, tensor(1.8876)]
[tensor(-0.9915), 0.5758157389635317, tensor(1.8876)]
[tensor(-0.9915), 0.5892514395393474, tensor(1.8876)]
[tensor(-0.9915), 0.5911708253358925, tensor(1.8876)]
[tensor(-0.9915), 0.5950095969289827, tensor(1.8876)]
[2023-01-20 09:31:13,070.070 dsw46436-6bc9877c76-ns84r:101473 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0015), 0.6007677543186181, tensor(2.0024)]
[tensor(-1.0015), 0.6007677543186181, tensor(2.0024)]
[tensor(-1.0015), 0.6122840690978887, tensor(2.0330)]
[tensor(-1.0015), 0.6295585412667947, tensor(2.0745)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0015), 0.6295585412667947, tensor(2.0745)]
[2023-01-20 09:52:39,689.689 dsw46436-6bc9877c76-ns84r:101857 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0460), 0.5738963531669866, tensor(1.8235)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0284), 0.5738963531669866, tensor(1.8235)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
early stopping at 23
[2023-01-20 11:30:42,870.870 dsw46436-6bc9877c76-ns84r:103500 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2543), 0.43953934740882916, tensor(0.9434)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1462), 0.4817658349328215, tensor(1.2627)]
[tensor(-1.1462), 0.5143953934740882, tensor(1.4165)]
[tensor(-1.1081), 0.5508637236084453, tensor(1.6462)]
[tensor(-1.1081), 0.5508637236084453, tensor(1.6462)]
[tensor(-1.1081), 0.5604606525911708, tensor(1.6462)]
[tensor(-1.1081), 0.5662188099808061, tensor(1.7148)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
early stopping at 17
[2023-01-20 12:43:23,156.156 dsw46436-6bc9877c76-ns84r:104728 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0674), 0.5431861804222649, tensor(1.6486)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9961), 0.5892514395393474, tensor(1.9502)]
[tensor(-0.9961), 0.5950095969289827, tensor(1.9502)]
[tensor(-0.9961), 0.6218809980806143, tensor(2.0729)]
[tensor(-0.9961), 0.6218809980806143, tensor(2.0729)]
[2023-01-20 13:04:46,841.841 dsw46436-6bc9877c76-ns84r:105112 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1079), 0.4990403071017274, tensor(1.3873)]
[tensor(-1.0372), 0.564299424184261, tensor(1.7843)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0352), 0.5854126679462572, tensor(1.8919)]
[tensor(-1.0352), 0.6142034548944337, tensor(1.9760)]
[tensor(-1.0352), 0.6199616122840691, tensor(1.9760)]
[2023-01-20 13:27:33,906.906 dsw46436-6bc9877c76-ns84r:105515 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0041), 0.581573896353167, tensor(1.9038)]
[tensor(-1.0041), 0.581573896353167, tensor(1.9038)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0041), 0.581573896353167, tensor(1.9038)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0041), 0.6065259117082533, tensor(2.0188)]
[tensor(-1.0041), 0.6065259117082533, tensor(2.0188)]
[tensor(-1.0041), 0.6065259117082533, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0041), 0.6142034548944337, tensor(2.0188)]
[tensor(-1.0041), 0.6142034548944337, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0041), 0.6142034548944337, tensor(2.0188)]
[tensor(-1.0041), 0.6199616122840691, tensor(2.0188)]
[tensor(-1.0041), 0.6238003838771593, tensor(2.0188)]
[tensor(-1.0041), 0.6238003838771593, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
early stopping at 23
[2023-01-20 15:06:38,878.878 dsw46436-6bc9877c76-ns84r:107179 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0254), 0.5854126679462572, tensor(1.9017)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5988483685220729, tensor(1.9192)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
early stopping at 13
[2023-01-20 16:03:15,712.712 dsw46436-6bc9877c76-ns84r:108149 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9870), 0.5834932821497121, tensor(1.9305)]
[tensor(-0.9870), 0.5911708253358925, tensor(1.9684)]
[tensor(-0.9870), 0.5988483685220729, tensor(1.9684)]
[tensor(-0.9870), 0.6295585412667947, tensor(2.0583)]
[tensor(-0.9870), 0.6353166986564299, tensor(2.0583)]
[2023-01-20 16:25:07,900.900 dsw46436-6bc9877c76-ns84r:108539 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0209), 0.5585412667946257, tensor(1.7718)]
[tensor(-1.0085), 0.5777351247600768, tensor(1.8802)]
[tensor(-1.0085), 0.5892514395393474, tensor(1.9090)]
[tensor(-1.0085), 0.6142034548944337, tensor(1.9536)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0085), 0.6142034548944337, tensor(1.9536)]
[2023-01-20 16:46:56,578.578 dsw46436-6bc9877c76-ns84r:108935 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0477), 0.5911708253358925, tensor(1.9082)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9888), 0.5911708253358925, tensor(1.9575)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9815), 0.6007677543186181, tensor(2.0224)]
[tensor(-0.9815), 0.6007677543186181, tensor(2.0224)]
[tensor(-0.9815), 0.6007677543186181, tensor(2.0224)]
[tensor(-0.9815), 0.6007677543186181, tensor(2.0224)]
[tensor(-0.9815), 0.6007677543186181, tensor(2.0224)]
[tensor(-0.9815), 0.6007677543186181, tensor(2.0224)]
early stopping at 8
[2023-01-20 17:21:33,467.467 dsw46436-6bc9877c76-ns84r:109538 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0604), 0.581573896353167, tensor(1.8474)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0077), 0.581573896353167, tensor(1.8905)]
[tensor(-1.0077), 0.581573896353167, tensor(1.8905)]
[tensor(-1.0077), 0.6084452975047985, tensor(1.9899)]
[tensor(-1.0077), 0.6084452975047985, tensor(1.9899)]
[tensor(-1.0077), 0.6103646833013435, tensor(1.9899)]
[tensor(-1.0077), 0.6295585412667947, tensor(1.9899)]
[tensor(-1.0077), 0.6295585412667947, tensor(1.9899)]
[tensor(-1.0077), 0.6295585412667947, tensor(1.9899)]
[tensor(-1.0077), 0.6295585412667947, tensor(1.9899)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0077), 0.6295585412667947, tensor(1.9899)]
[tensor(-1.0077), 0.6295585412667947, tensor(1.9899)]
early stopping at 12
[2023-01-20 18:12:42,525.525 dsw46436-6bc9877c76-ns84r:110416 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0091), 0.5681381957773513, tensor(1.8316)]
[tensor(-0.9684), 0.6218809980806143, tensor(2.1410)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9684), 0.6218809980806143, tensor(2.1410)]
[tensor(-0.9684), 0.6218809980806143, tensor(2.1410)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9684), 0.6218809980806143, tensor(2.1410)]
[2023-01-20 18:34:07,174.174 dsw46436-6bc9877c76-ns84r:110799 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0548), 0.54510556621881, tensor(1.6707)]
[tensor(-1.0348), 0.54510556621881, tensor(1.6707)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0348), 0.5758157389635317, tensor(1.8301)]
[tensor(-1.0348), 0.5988483685220729, tensor(1.8788)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0348), 0.5988483685220729, tensor(1.8788)]
[2023-01-20 18:55:29,116.116 dsw46436-6bc9877c76-ns84r:111184 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0203), 0.5892514395393474, tensor(1.9260)]
[tensor(-0.9774), 0.6046065259117083, tensor(2.0456)]
[tensor(-0.9774), 0.6046065259117083, tensor(2.0456)]
[tensor(-0.9774), 0.6238003838771593, tensor(2.1232)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9774), 0.6238003838771593, tensor(2.1232)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9774), 0.6238003838771593, tensor(2.1232)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9774), 0.6238003838771593, tensor(2.1232)]
[tensor(-0.9774), 0.6238003838771593, tensor(2.1232)]
[tensor(-0.9774), 0.6238003838771593, tensor(2.1232)]
early stopping at 9
[2023-01-20 19:33:54,655.655 dsw46436-6bc9877c76-ns84r:111851 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6789), 0.2802303262955854, 0.0]
[tensor(-1.6789), 0.2802303262955854, 0.0]
[tensor(-1.6636), 0.2802303262955854, 0.0]
early stopping at 3
[2023-01-20 19:46:52,745.745 dsw46436-6bc9877c76-ns84r:112089 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0018), 0.6046065259117083, tensor(2.0213)]
[tensor(-0.9851), 0.6046065259117083, tensor(2.0213)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9851), 0.6295585412667947, tensor(2.1615)]
[tensor(-0.9851), 0.6295585412667947, tensor(2.1615)]
[tensor(-0.9851), 0.6295585412667947, tensor(2.1615)]
[2023-01-20 20:00:11,448.448 dsw46436-6bc9877c76-ns84r:112340 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9859), 0.5988483685220729, tensor(2.0083)]
[tensor(-0.9859), 0.5988483685220729, tensor(2.0083)]
[tensor(-0.9609), 0.6046065259117083, tensor(2.0621)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9609), 0.6487523992322457, tensor(2.2297)]
[tensor(-0.9609), 0.6487523992322457, tensor(2.2297)]
[2023-01-20 20:13:35,818.818 dsw46436-6bc9877c76-ns84r:112591 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9541), 0.6065259117082533, tensor(2.0785)]
[tensor(-0.9460), 0.6218809980806143, tensor(2.1634)]
[tensor(-0.9460), 0.6218809980806143, tensor(2.1634)]
[tensor(-0.9460), 0.6218809980806143, tensor(2.1634)]
[tensor(-0.9460), 0.6218809980806143, tensor(2.1634)]
[2023-01-20 20:26:58,225.225 dsw46436-6bc9877c76-ns84r:112842 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9726), 0.6122840690978887, tensor(2.0888)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9567), 0.6122840690978887, tensor(2.0888)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9567), 0.6122840690978887, tensor(2.0888)]
[tensor(-0.9567), 0.6122840690978887, tensor(2.0888)]
[tensor(-0.9567), 0.6122840690978887, tensor(2.0888)]
[2023-01-20 20:40:21,342.342 dsw46436-6bc9877c76-ns84r:113094 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9672), 0.6276391554702495, tensor(2.1710)]
[tensor(-0.9672), 0.6276391554702495, tensor(2.1710)]
[tensor(-0.9672), 0.6276391554702495, tensor(2.1710)]
[tensor(-0.9672), 0.6276391554702495, tensor(2.1710)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9672), 0.6276391554702495, tensor(2.1710)]
[2023-01-20 20:53:52,647.647 dsw46436-6bc9877c76-ns84r:113345 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9457), 0.6180422264875239, tensor(2.1445)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9457), 0.6180422264875239, tensor(2.1445)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9457), 0.6180422264875239, tensor(2.1445)]
[tensor(-0.9457), 0.6180422264875239, tensor(2.1445)]
[tensor(-0.9457), 0.6180422264875239, tensor(2.1445)]
[2023-01-20 21:07:24,177.177 dsw46436-6bc9877c76-ns84r:113578 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9606), 0.6180422264875239, tensor(2.1296)]
[tensor(-0.9606), 0.6180422264875239, tensor(2.1296)]
[tensor(-0.9588), 0.6180422264875239, tensor(2.1314)]
[tensor(-0.9588), 0.6180422264875239, tensor(2.1314)]
[tensor(-0.9588), 0.6180422264875239, tensor(2.1314)]
[2023-01-20 21:20:51,424.424 dsw46436-6bc9877c76-ns84r:113625 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9612), 0.6161228406909789, tensor(2.1195)]
[tensor(-0.9386), 0.6161228406909789, tensor(2.1195)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9353), 0.6276391554702495, tensor(2.2029)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9353), 0.6276391554702495, tensor(2.2029)]
[tensor(-0.9353), 0.6276391554702495, tensor(2.2029)]
[2023-01-20 21:34:03,228.228 dsw46436-6bc9877c76-ns84r:113672 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9411), 0.5988483685220729, tensor(2.0532)]
[tensor(-0.9411), 0.5988483685220729, tensor(2.0532)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9411), 0.6142034548944337, tensor(2.1154)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9411), 0.6142034548944337, tensor(2.1154)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9411), 0.6142034548944337, tensor(2.1154)]
[2023-01-20 21:47:19,640.640 dsw46436-6bc9877c76-ns84r:113719 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0137), 0.5950095969289827, tensor(1.9613)]
[tensor(-0.9815), 0.5969289827255279, tensor(2.0032)]
[tensor(-0.9752), 0.6007677543186181, tensor(2.0286)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9752), 0.6007677543186181, tensor(2.0286)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9752), 0.6084452975047985, tensor(2.0286)]
[2023-01-20 22:00:30,550.550 dsw46436-6bc9877c76-ns84r:113766 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1765), 0.5047984644913628, tensor(1.3475)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9509), 0.6122840690978887, tensor(2.1105)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9509), 0.6142034548944337, tensor(2.1105)]
[tensor(-0.9509), 0.6142034548944337, tensor(2.1105)]
[tensor(-0.9509), 0.6142034548944337, tensor(2.1105)]
[2023-01-20 22:13:47,417.417 dsw46436-6bc9877c76-ns84r:113813 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9906), 0.5950095969289827, tensor(1.9845)]
[tensor(-0.9906), 0.5950095969289827, tensor(1.9845)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9786), 0.6026871401151631, tensor(2.0348)]
[tensor(-0.9786), 0.6142034548944337, tensor(2.0418)]
[tensor(-0.9786), 0.6142034548944337, tensor(2.0418)]
[2023-01-20 22:26:56,966.966 dsw46436-6bc9877c76-ns84r:113859 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9760), 0.5969289827255279, tensor(2.0086)]
[tensor(-0.9546), 0.6161228406909789, tensor(2.1260)]
[tensor(-0.9504), 0.6161228406909789, tensor(2.1302)]
[tensor(-0.9504), 0.6161228406909789, tensor(2.1302)]
[tensor(-0.9504), 0.6161228406909789, tensor(2.1302)]
[2023-01-20 22:40:06,016.016 dsw46436-6bc9877c76-ns84r:113906 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9848), 0.6142034548944337, tensor(2.0863)]
[tensor(-0.9848), 0.6142034548944337, tensor(2.0863)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9709), 0.6142034548944337, tensor(2.0905)]
[tensor(-0.9709), 0.6161228406909789, tensor(2.0905)]
[tensor(-0.9709), 0.6161228406909789, tensor(2.0905)]
[2023-01-20 22:53:16,611.611 dsw46436-6bc9877c76-ns84r:113953 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9689), 0.6065259117082533, tensor(2.0637)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9689), 0.6065259117082533, tensor(2.0637)]
[tensor(-0.9424), 0.6142034548944337, tensor(2.1286)]
[tensor(-0.9424), 0.6295585412667947, tensor(2.1998)]
[tensor(-0.9424), 0.6295585412667947, tensor(2.1998)]
[2023-01-20 23:06:30,464.464 dsw46436-6bc9877c76-ns84r:114000 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9872), 0.6046065259117083, tensor(2.0358)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9872), 0.6046065259117083, tensor(2.0358)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9872), 0.6046065259117083, tensor(2.0358)]
[tensor(-0.9872), 0.6046065259117083, tensor(2.0358)]
[tensor(-0.9872), 0.6046065259117083, tensor(2.0358)]
[2023-01-20 23:19:43,384.384 dsw46436-6bc9877c76-ns84r:114047 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9806), 0.5930902111324377, tensor(1.9848)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9334), 0.6065259117082533, tensor(2.0992)]
[tensor(-0.9334), 0.6065259117082533, tensor(2.0992)]
[tensor(-0.9334), 0.6065259117082533, tensor(2.0992)]
[tensor(-0.9334), 0.6065259117082533, tensor(2.0992)]
[2023-01-20 23:40:56,794.794 dsw46436-6bc9877c76-ns84r:114105 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9952), 0.581573896353167, tensor(1.9126)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9952), 0.581573896353167, tensor(1.9126)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
[tensor(-0.9952), 0.6218809980806143, tensor(2.0589)]
[tensor(-0.9952), 0.6218809980806143, tensor(2.0589)]
[tensor(-0.9952), 0.6218809980806143, tensor(2.0589)]
[2023-01-21 00:02:00,986.986 dsw46436-6bc9877c76-ns84r:114164 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9610), 0.5969289827255279, tensor(2.0236)]
[tensor(-0.9252), 0.6276391554702495, tensor(2.2130)]
[tensor(-0.9252), 0.6276391554702495, tensor(2.2130)]
[tensor(-0.9252), 0.6276391554702495, tensor(2.2130)]
[tensor(-0.9252), 0.6276391554702495, tensor(2.2130)]
[2023-01-21 00:23:05,438.438 dsw46436-6bc9877c76-ns84r:114223 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9945), 0.5873320537428023, tensor(1.9421)]
[tensor(-0.9945), 0.5873320537428023, tensor(1.9421)]
[tensor(-0.9945), 0.6065259117082533, tensor(2.0159)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9945), 0.6065259117082533, tensor(2.0159)]
[tensor(-0.9945), 0.6065259117082533, tensor(2.0159)]
[2023-01-21 00:44:14,391.391 dsw46436-6bc9877c76-ns84r:114281 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9540), 0.6046065259117083, tensor(2.0691)]
[tensor(-0.9358), 0.6161228406909789, tensor(2.1448)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9358), 0.6161228406909789, tensor(2.1448)]
[tensor(-0.9358), 0.6180422264875239, tensor(2.1448)]
[tensor(-0.9358), 0.6180422264875239, tensor(2.1448)]
[2023-01-21 01:05:13,454.454 dsw46436-6bc9877c76-ns84r:114340 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9707), 0.6065259117082533, tensor(2.0619)]
[tensor(-0.9707), 0.6065259117082533, tensor(2.0619)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9707), 0.6314779270633397, tensor(2.1556)]
[tensor(-0.9707), 0.6314779270633397, tensor(2.1556)]
[tensor(-0.9707), 0.6314779270633397, tensor(2.1556)]
[2023-01-21 01:26:10,829.829 dsw46436-6bc9877c76-ns84r:114398 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9487), 0.6065259117082533, tensor(2.0839)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9162), 0.6391554702495201, tensor(2.2796)]
[tensor(-0.9162), 0.6391554702495201, tensor(2.2796)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9162), 0.6391554702495201, tensor(2.2796)]
[tensor(-0.9162), 0.6391554702495201, tensor(2.2796)]
[2023-01-21 01:47:17,901.901 dsw46436-6bc9877c76-ns84r:114457 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9922), 0.5911708253358925, tensor(1.9637)]
[tensor(-0.9922), 0.5911708253358925, tensor(1.9637)]
[tensor(-0.9922), 0.6353166986564299, tensor(2.1737)]
[tensor(-0.9922), 0.6353166986564299, tensor(2.1737)]
[tensor(-0.9922), 0.6353166986564299, tensor(2.1737)]
[2023-01-21 02:08:28,878.878 dsw46436-6bc9877c76-ns84r:114516 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9899), 0.6065259117082533, tensor(2.0428)]
[tensor(-0.9381), 0.6218809980806143, tensor(2.1713)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9381), 0.6238003838771593, tensor(2.1713)]
[tensor(-0.9381), 0.6410748560460653, tensor(2.1713)]
[tensor(-0.9381), 0.6410748560460653, tensor(2.1713)]
[2023-01-21 02:29:31,748.748 dsw46436-6bc9877c76-ns84r:114574 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9705), 0.581573896353167, tensor(1.9374)]
[tensor(-0.9705), 0.581573896353167, tensor(1.9374)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9705), 0.6065259117082533, tensor(2.0038)]
[tensor(-0.9705), 0.6065259117082533, tensor(2.0038)]
[tensor(-0.9705), 0.6065259117082533, tensor(2.0038)]
[2023-01-21 02:50:39,264.264 dsw46436-6bc9877c76-ns84r:114633 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0037), 0.5873320537428023, tensor(1.9329)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9556), 0.5950095969289827, tensor(2.0194)]
[tensor(-0.9556), 0.6238003838771593, tensor(2.1380)]
[tensor(-0.9556), 0.6238003838771593, tensor(2.1380)]
[tensor(-0.9556), 0.6238003838771593, tensor(2.1380)]
[2023-01-21 03:11:48,517.517 dsw46436-6bc9877c76-ns84r:114692 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9973), 0.5738963531669866, tensor(1.8722)]
[tensor(-0.9841), 0.5738963531669866, tensor(1.8758)]
[tensor(-0.9841), 0.6007677543186181, tensor(1.9977)]
[tensor(-0.9841), 0.6103646833013435, tensor(2.0168)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9841), 0.6122840690978887, tensor(2.0168)]
[2023-01-21 03:32:51,264.264 dsw46436-6bc9877c76-ns84r:114750 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9589), 0.6103646833013435, tensor(2.0930)]
[tensor(-0.9285), 0.6449136276391555, tensor(2.2960)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9285), 0.6449136276391555, tensor(2.2960)]
[tensor(-0.9285), 0.6449136276391555, tensor(2.2960)]
[tensor(-0.9285), 0.6449136276391555, tensor(2.2960)]
[2023-01-21 03:53:58,430.430 dsw46436-6bc9877c76-ns84r:114809 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0885), 0.5508637236084453, tensor(1.6658)]
[tensor(-1.0088), 0.5719769673704415, tensor(1.8511)]
[tensor(-1.0088), 0.5854126679462572, tensor(1.9097)]
[tensor(-1.0088), 0.5854126679462572, tensor(1.9097)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0088), 0.5854126679462572, tensor(1.9097)]
[2023-01-21 04:14:55,598.598 dsw46436-6bc9877c76-ns84r:114867 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9638), 0.6084452975047985, tensor(2.0785)]
[tensor(-0.9270), 0.6295585412667947, tensor(2.2208)]
[tensor(-0.9270), 0.6295585412667947, tensor(2.2208)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9270), 0.6372360844529751, tensor(2.2208)]
[tensor(-0.9270), 0.6372360844529751, tensor(2.2208)]
[2023-01-21 04:36:07,658.658 dsw46436-6bc9877c76-ns84r:114926 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 12 epochs 5 lr 1.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0436), 0.5719769673704415, tensor(1.8163)]
[tensor(-1.0055), 0.5719769673704415, tensor(1.8448)]
[tensor(-1.0055), 0.6238003838771593, tensor(2.0867)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0055), 0.6238003838771593, tensor(2.0867)]
[tensor(-1.0055), 0.6238003838771593, tensor(2.0867)]
