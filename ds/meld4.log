[2023-01-18 16:32:42,008.008 dlc1n60lj7mhywta-master-0:33 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 16:32:42,741.741 dlc1n60lj7mhywta-master-0:101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:32:42,805.805 dlc1n60lj7mhywta-master-0:100 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:32:42,815.815 dlc1n60lj7mhywta-master-0:99 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:32:42,818.818 dlc1n60lj7mhywta-master-0:102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:32:44,553.553 dlc1n60lj7mhywta-master-0:100 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 16:32:44,561.561 dlc1n60lj7mhywta-master-0:102 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 16:32:44,569.569 dlc1n60lj7mhywta-master-0:101 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 16:32:44,576.576 dlc1n60lj7mhywta-master-0:99 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:99:99 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:101:101 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:100:100 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:102:102 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5836), 0.5066809192944949, 0.8609179415855355, tensor(1.9498)]
[tensor(-0.5162), 0.5505077498663816, 0.8630041724617524, tensor(2.2364)]
[tensor(-0.5162), 0.5505077498663816, 0.8630041724617524, tensor(2.2364)]
[tensor(-0.5162), 0.5505077498663816, 0.8630041724617524, tensor(2.2364)]
[tensor(-0.5162), 0.5505077498663816, 0.8650904033379694, tensor(2.2364)]
[2023-01-18 16:44:37,522.522 dlc1n60lj7mhywta-master-0:178 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 16:44:38,143.143 dlc1n60lj7mhywta-master-0:245 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:44:38,143.143 dlc1n60lj7mhywta-master-0:247 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:44:38,143.143 dlc1n60lj7mhywta-master-0:246 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:44:38,144.144 dlc1n60lj7mhywta-master-0:244 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:44:39,212.212 dlc1n60lj7mhywta-master-0:245 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 16:44:39,216.216 dlc1n60lj7mhywta-master-0:246 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 16:44:40,168.168 dlc1n60lj7mhywta-master-0:247 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 16:44:40,170.170 dlc1n60lj7mhywta-master-0:244 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:244:244 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:247:247 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:246:246 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:245:245 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5423), 0.535542490646713, 0.8581363004172462, tensor(2.1354)]
[tensor(-0.5095), 0.5579903794762159, 0.8581363004172462, tensor(2.2805)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5090), 0.5579903794762159, 0.8581363004172462, tensor(2.2805)]
[tensor(-0.5090), 0.5579903794762159, 0.8588317107093185, tensor(2.2805)]
[tensor(-0.5090), 0.5579903794762159, 0.8602225312934632, tensor(2.2805)]
[2023-01-18 16:55:43,954.954 dlc1n60lj7mhywta-master-0:321 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 16:55:44,571.571 dlc1n60lj7mhywta-master-0:389 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:55:44,572.572 dlc1n60lj7mhywta-master-0:387 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:55:44,662.662 dlc1n60lj7mhywta-master-0:390 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:55:44,662.662 dlc1n60lj7mhywta-master-0:388 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 16:55:45,715.715 dlc1n60lj7mhywta-master-0:390 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 16:55:45,745.745 dlc1n60lj7mhywta-master-0:388 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 16:55:46,494.494 dlc1n60lj7mhywta-master-0:389 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 16:55:46,501.501 dlc1n60lj7mhywta-master-0:387 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:387:387 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:389:389 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:388:388 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:390:390 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5265), 0.5387493319080705, 0.8567454798331016, tensor(2.1673)]
[tensor(-0.5212), 0.5398182789951897, 0.8567454798331016, tensor(2.1779)]
[tensor(-0.5212), 0.5435595938001069, 0.8574408901251739, tensor(2.1937)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5067), 0.5483698556921432, 0.8574408901251739, tensor(2.2351)]
[tensor(-0.5067), 0.5483698556921432, 0.8574408901251739, tensor(2.2351)]
[tensor(-0.5067), 0.5483698556921432, 0.8574408901251739, tensor(2.2351)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5067), 0.5483698556921432, 0.8574408901251739, tensor(2.2351)]
[tensor(-0.5067), 0.5483698556921432, 0.8574408901251739, tensor(2.2351)]
[tensor(-0.5067), 0.5483698556921432, 0.8574408901251739, tensor(2.2351)]
early stopping at 9
[2023-01-18 17:13:32,730.730 dlc1n60lj7mhywta-master-0:474 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 17:13:33,341.341 dlc1n60lj7mhywta-master-0:541 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:13:33,342.342 dlc1n60lj7mhywta-master-0:540 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:13:33,349.349 dlc1n60lj7mhywta-master-0:542 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:13:33,403.403 dlc1n60lj7mhywta-master-0:543 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:13:34,557.557 dlc1n60lj7mhywta-master-0:543 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 17:13:35,237.237 dlc1n60lj7mhywta-master-0:541 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 17:13:35,239.239 dlc1n60lj7mhywta-master-0:542 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 17:13:35,241.241 dlc1n60lj7mhywta-master-0:540 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:540:540 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:541:541 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:543:543 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:542:542 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5410), 0.535542490646713, 0.8525730180806675, tensor(2.1367)]
[tensor(-0.5283), 0.5483698556921432, 0.8567454798331016, tensor(2.2135)]
[tensor(-0.5104), 0.5483698556921432, 0.8636995827538247, tensor(2.2135)]
[tensor(-0.5104), 0.5483698556921432, 0.8636995827538247, tensor(2.2135)]
[tensor(-0.5089), 0.5483698556921432, 0.8636995827538247, tensor(2.2330)]
[tensor(-0.5089), 0.5483698556921432, 0.8636995827538247, tensor(2.2330)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5089), 0.5483698556921432, 0.8636995827538247, tensor(2.2330)]
[tensor(-0.5089), 0.5483698556921432, 0.8636995827538247, tensor(2.2330)]
[tensor(-0.5089), 0.5483698556921432, 0.8636995827538247, tensor(2.2330)]
[tensor(-0.5089), 0.5483698556921432, 0.8636995827538247, tensor(2.2330)]
early stopping at 10
[2023-01-18 17:34:05,713.713 dlc1n60lj7mhywta-master-0:630 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 17:34:06,328.328 dlc1n60lj7mhywta-master-0:697 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:34:06,329.329 dlc1n60lj7mhywta-master-0:696 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:34:06,329.329 dlc1n60lj7mhywta-master-0:699 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:34:06,331.331 dlc1n60lj7mhywta-master-0:698 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:34:07,370.370 dlc1n60lj7mhywta-master-0:697 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 17:34:07,372.372 dlc1n60lj7mhywta-master-0:698 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 17:34:07,376.376 dlc1n60lj7mhywta-master-0:699 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 17:34:07,378.378 dlc1n60lj7mhywta-master-0:696 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:696:696 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:698:698 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:697:697 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:699:699 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5736), 0.5216461785141635, 0.8296244784422809, tensor(2.0346)]
[tensor(-0.5279), 0.5403527525387494, 0.8630041724617524, tensor(2.1739)]
[tensor(-0.5279), 0.5403527525387494, 0.8630041724617524, tensor(2.1739)]
[tensor(-0.5180), 0.5403527525387494, 0.8630041724617524, tensor(2.1837)]
[tensor(-0.5180), 0.5403527525387494, 0.8630041724617524, tensor(2.1837)]
[2023-01-18 17:44:17,050.050 dlc1n60lj7mhywta-master-0:773 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 17:44:17,865.865 dlc1n60lj7mhywta-master-0:842 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:44:17,868.868 dlc1n60lj7mhywta-master-0:841 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:44:17,869.869 dlc1n60lj7mhywta-master-0:840 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:44:17,898.898 dlc1n60lj7mhywta-master-0:839 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:44:19,858.858 dlc1n60lj7mhywta-master-0:840 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 17:44:19,864.864 dlc1n60lj7mhywta-master-0:841 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 17:44:19,868.868 dlc1n60lj7mhywta-master-0:842 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 17:44:19,877.877 dlc1n60lj7mhywta-master-0:839 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:839:839 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:841:841 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:842:842 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:840:840 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5319), 0.5451630144307856, 0.8511821974965229, tensor(2.1939)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5319), 0.5451630144307856, 0.8511821974965229, tensor(2.1939)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5208), 0.5451630144307856, 0.8511821974965229, tensor(2.1939)]
[tensor(-0.5208), 0.5542490646712988, 0.8602225312934632, tensor(2.2477)]
[tensor(-0.5208), 0.5542490646712988, 0.8602225312934632, tensor(2.2477)]
[2023-01-18 17:54:29,413.413 dlc1n60lj7mhywta-master-0:916 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 17:54:30,017.017 dlc1n60lj7mhywta-master-0:984 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:54:30,017.017 dlc1n60lj7mhywta-master-0:982 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:54:30,102.102 dlc1n60lj7mhywta-master-0:983 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:54:30,110.110 dlc1n60lj7mhywta-master-0:985 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 17:54:30,930.930 dlc1n60lj7mhywta-master-0:984 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 17:54:31,441.441 dlc1n60lj7mhywta-master-0:983 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 17:54:31,445.445 dlc1n60lj7mhywta-master-0:985 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 17:54:31,446.446 dlc1n60lj7mhywta-master-0:982 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:982:982 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:984:984 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:985:985 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:983:983 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.6426), 0.4746125066809193, 0.8240611961057024, tensor(1.7304)]
[tensor(-0.5430), 0.5291288081239979, 0.8532684283727399, tensor(2.1026)]
[tensor(-0.5318), 0.5371459112773918, 0.8553546592489569, tensor(2.1539)]
[tensor(-0.5127), 0.5521111704970604, 0.8553546592489569, tensor(2.2478)]
[tensor(-0.5127), 0.5521111704970604, 0.8553546592489569, tensor(2.2478)]
[tensor(-0.5127), 0.5521111704970604, 0.8553546592489569, tensor(2.2478)]
[tensor(-0.5127), 0.5521111704970604, 0.8553546592489569, tensor(2.2478)]
[tensor(-0.5127), 0.5521111704970604, 0.8553546592489569, tensor(2.2478)]
[tensor(-0.5127), 0.5521111704970604, 0.8553546592489569, tensor(2.2478)]
early stopping at 9
[2023-01-18 18:13:19,238.238 dlc1n60lj7mhywta-master-0:1070 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 18:13:19,876.876 dlc1n60lj7mhywta-master-0:1138 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:13:19,876.876 dlc1n60lj7mhywta-master-0:1136 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:13:19,876.876 dlc1n60lj7mhywta-master-0:1137 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:13:19,877.877 dlc1n60lj7mhywta-master-0:1139 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:13:20,890.890 dlc1n60lj7mhywta-master-0:1139 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 18:13:21,877.877 dlc1n60lj7mhywta-master-0:1138 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 18:13:21,883.883 dlc1n60lj7mhywta-master-0:1137 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 18:13:21,890.890 dlc1n60lj7mhywta-master-0:1136 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:1136:1136 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:1138:1138 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1137:1137 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1139:1139 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5379), 0.5296632816675575, 0.847009735744089, tensor(2.1104)]
[tensor(-0.5294), 0.5296632816675575, 0.8630041724617524, tensor(2.1104)]
[tensor(-0.5006), 0.555318011758418, 0.8630041724617524, tensor(2.2760)]
[tensor(-0.5006), 0.555318011758418, 0.8630041724617524, tensor(2.2760)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5006), 0.555318011758418, 0.8630041724617524, tensor(2.2760)]
[tensor(-0.5006), 0.555318011758418, 0.8630041724617524, tensor(2.2760)]
[tensor(-0.5006), 0.555318011758418, 0.8630041724617524, tensor(2.2760)]
[tensor(-0.5006), 0.555318011758418, 0.8630041724617524, tensor(2.2760)]
early stopping at 8
[2023-01-18 18:31:00,020.020 dlc1n60lj7mhywta-master-0:1220 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 18:31:00,650.650 dlc1n60lj7mhywta-master-0:1287 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:31:00,653.653 dlc1n60lj7mhywta-master-0:1288 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:31:00,728.728 dlc1n60lj7mhywta-master-0:1286 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:31:00,738.738 dlc1n60lj7mhywta-master-0:1289 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:31:02,505.505 dlc1n60lj7mhywta-master-0:1287 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 18:31:02,564.564 dlc1n60lj7mhywta-master-0:1288 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 18:31:03,026.026 dlc1n60lj7mhywta-master-0:1289 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 18:31:03,030.030 dlc1n60lj7mhywta-master-0:1286 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:1286:1286 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:1287:1287 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1289:1289 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1288:1288 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5295), 0.538214858364511, 0.8477051460361613, tensor(2.1616)]
[tensor(-0.5295), 0.538214858364511, 0.8477051460361613, tensor(2.1616)]
[Wed Jan 18 18:36:32 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5115), 0.5398182789951897, 0.8525730180806675, tensor(2.1876)]
[tensor(-0.5115), 0.5398182789951897, 0.8525730180806675, tensor(2.1876)]
[tensor(-0.5115), 0.5398182789951897, 0.8623087621696801, tensor(2.1876)]
[2023-01-18 18:41:15,369.369 dlc1n60lj7mhywta-master-0:1363 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 18:41:16,004.004 dlc1n60lj7mhywta-master-0:1432 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:41:16,032.032 dlc1n60lj7mhywta-master-0:1431 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:41:16,082.082 dlc1n60lj7mhywta-master-0:1429 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:41:16,191.191 dlc1n60lj7mhywta-master-0:1430 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:41:17,396.396 dlc1n60lj7mhywta-master-0:1430 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 18:41:17,820.820 dlc1n60lj7mhywta-master-0:1432 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 18:41:17,876.876 dlc1n60lj7mhywta-master-0:1431 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 18:41:17,880.880 dlc1n60lj7mhywta-master-0:1429 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:1429:1429 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:1431:1431 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1432:1432 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1430:1430 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5239), 0.5339390700160342, 0.849095966620306, tensor(2.1458)]
[tensor(-0.5131), 0.5467664350614645, 0.8511821974965229, tensor(2.2208)]
[tensor(-0.5131), 0.5467664350614645, 0.8643949930458971, tensor(2.2208)]
[tensor(-0.5131), 0.5467664350614645, 0.8643949930458971, tensor(2.2208)]
[tensor(-0.5107), 0.5467664350614645, 0.8643949930458971, tensor(2.2208)]
[2023-01-18 18:51:18,700.700 dlc1n60lj7mhywta-master-0:1506 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 18:51:19,316.316 dlc1n60lj7mhywta-master-0:1574 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:51:19,316.316 dlc1n60lj7mhywta-master-0:1572 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:51:19,396.396 dlc1n60lj7mhywta-master-0:1573 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:51:19,408.408 dlc1n60lj7mhywta-master-0:1575 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 18:51:20,237.237 dlc1n60lj7mhywta-master-0:1574 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 18:51:20,759.759 dlc1n60lj7mhywta-master-0:1573 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 18:51:20,761.761 dlc1n60lj7mhywta-master-0:1575 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 18:51:20,763.763 dlc1n60lj7mhywta-master-0:1572 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:1572:1572 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:1574:1574 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1573:1573 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1575:1575 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5368), 0.5387493319080705, 0.8463143254520167, tensor(2.1569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5368), 0.5387493319080705, 0.8567454798331016, tensor(2.1569)]
[Wed Jan 18 18:56:59 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5368), 0.5387493319080705, 0.8567454798331016, tensor(2.1569)]
[tensor(-0.5090), 0.5510422234099412, 0.8616133518776078, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8616133518776078, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8616133518776078, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8616133518776078, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8616133518776078, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8671766342141863, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8671766342141863, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8671766342141863, tensor(2.2462)]
[Wed Jan 18 19:14:33 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5090), 0.5510422234099412, 0.8671766342141863, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8671766342141863, tensor(2.2462)]
[tensor(-0.5090), 0.5510422234099412, 0.8671766342141863, tensor(2.2462)]
early stopping at 14
[2023-01-18 19:19:15,139.139 dlc1n60lj7mhywta-master-0:1671 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 19:19:15,758.758 dlc1n60lj7mhywta-master-0:1738 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:15,759.759 dlc1n60lj7mhywta-master-0:1739 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:15,788.788 dlc1n60lj7mhywta-master-0:1740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:15,796.796 dlc1n60lj7mhywta-master-0:1737 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:19:16,807.807 dlc1n60lj7mhywta-master-0:1740 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 19:19:17,749.749 dlc1n60lj7mhywta-master-0:1738 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 19:19:17,754.754 dlc1n60lj7mhywta-master-0:1739 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 19:19:17,761.761 dlc1n60lj7mhywta-master-0:1737 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:1737:1737 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:1739:1739 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1738:1738 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1740:1740 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5225), 0.5467664350614645, 0.8567454798331016, tensor(2.2113)]
[tensor(-0.5211), 0.547300908605024, 0.8616133518776078, tensor(2.2155)]
[tensor(-0.5064), 0.547300908605024, 0.8616133518776078, tensor(2.2155)]
[tensor(-0.5064), 0.547300908605024, 0.8616133518776078, tensor(2.2262)]
[tensor(-0.5064), 0.547300908605024, 0.8616133518776078, tensor(2.2262)]
[tensor(-0.5064), 0.547300908605024, 0.8616133518776078, tensor(2.2262)]
[tensor(-0.5064), 0.547300908605024, 0.8616133518776078, tensor(2.2262)]
[tensor(-0.5064), 0.547300908605024, 0.8616133518776078, tensor(2.2262)]
[tensor(-0.5064), 0.547300908605024, 0.8616133518776078, tensor(2.2262)]
early stopping at 9
[2023-01-18 19:37:03,940.940 dlc1n60lj7mhywta-master-0:1824 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 19:37:04,567.567 dlc1n60lj7mhywta-master-0:1892 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:37:04,567.567 dlc1n60lj7mhywta-master-0:1891 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:37:04,649.649 dlc1n60lj7mhywta-master-0:1893 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:37:04,663.663 dlc1n60lj7mhywta-master-0:1890 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:37:06,480.480 dlc1n60lj7mhywta-master-0:1892 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 19:37:06,480.480 dlc1n60lj7mhywta-master-0:1891 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 19:37:06,977.977 dlc1n60lj7mhywta-master-0:1893 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 19:37:06,980.980 dlc1n60lj7mhywta-master-0:1890 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:1890:1890 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:1892:1892 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1891:1891 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:1893:1893 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5206), 0.538214858364511, 0.8581363004172462, tensor(2.1705)]
[tensor(-0.5206), 0.538214858364511, 0.8581363004172462, tensor(2.1705)]
[tensor(-0.5057), 0.5424906467129877, 0.8581363004172462, tensor(2.2067)]
[tensor(-0.5057), 0.5456974879743453, 0.8630041724617524, tensor(2.2125)]
[Wed Jan 18 19:45:39 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5057), 0.5483698556921432, 0.8630041724617524, tensor(2.2257)]
[2023-01-18 19:47:35,318.318 dlc1n60lj7mhywta-master-0:1966 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 19:47:35,942.942 dlc1n60lj7mhywta-master-0:2035 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:47:35,969.969 dlc1n60lj7mhywta-master-0:2032 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:47:36,020.020 dlc1n60lj7mhywta-master-0:2033 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:47:36,027.027 dlc1n60lj7mhywta-master-0:2034 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:47:37,355.355 dlc1n60lj7mhywta-master-0:2034 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 19:47:37,356.356 dlc1n60lj7mhywta-master-0:2033 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 19:47:37,811.811 dlc1n60lj7mhywta-master-0:2035 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 19:47:37,821.821 dlc1n60lj7mhywta-master-0:2032 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:2032:2032 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:2035:2035 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2033:2033 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2034:2034 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5181), 0.5307322287546766, 0.8546592489568846, tensor(2.1355)]
[tensor(-0.5181), 0.535542490646713, 0.8574408901251739, tensor(2.1555)]
[tensor(-0.5132), 0.5387493319080705, 0.8574408901251739, tensor(2.1806)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5132), 0.5387493319080705, 0.8588317107093185, tensor(2.1806)]
[tensor(-0.5128), 0.5387493319080705, 0.8588317107093185, tensor(2.1806)]
[2023-01-18 19:58:10,676.676 dlc1n60lj7mhywta-master-0:2110 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 19:58:11,292.292 dlc1n60lj7mhywta-master-0:2178 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:58:11,292.292 dlc1n60lj7mhywta-master-0:2177 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:58:11,293.293 dlc1n60lj7mhywta-master-0:2179 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:58:11,304.304 dlc1n60lj7mhywta-master-0:2176 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 19:58:12,371.371 dlc1n60lj7mhywta-master-0:2179 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 19:58:12,380.380 dlc1n60lj7mhywta-master-0:2177 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 19:58:12,380.380 dlc1n60lj7mhywta-master-0:2178 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 19:58:12,388.388 dlc1n60lj7mhywta-master-0:2176 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:2176:2176 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:2177:2177 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2178:2178 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2179:2179 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5261), 0.5387493319080705, 0.8546592489568846, tensor(2.1676)]
[tensor(-0.5071), 0.5462319615179049, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.5462319615179049, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.5462319615179049, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.5462319615179049, 0.8616133518776078, tensor(2.2240)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[Wed Jan 18 20:09:41 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5071), 0.5462319615179049, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.547300908605024, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.547300908605024, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.547300908605024, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.547300908605024, 0.8616133518776078, tensor(2.2240)]
[tensor(-0.5071), 0.549973276322822, 0.8616133518776078, tensor(2.2260)]
[Wed Jan 18 20:21:41 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5071), 0.549973276322822, 0.8616133518776078, tensor(2.2260)]
[Wed Jan 18 20:22:56 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5071), 0.549973276322822, 0.8616133518776078, tensor(2.2260)]
[tensor(-0.5071), 0.549973276322822, 0.8616133518776078, tensor(2.2260)]
[tensor(-0.5071), 0.549973276322822, 0.8616133518776078, tensor(2.2260)]
[tensor(-0.5071), 0.549973276322822, 0.8616133518776078, tensor(2.2260)]
early stopping at 16
[2023-01-18 20:30:37,339.339 dlc1n60lj7mhywta-master-0:2283 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 20:30:37,952.952 dlc1n60lj7mhywta-master-0:2350 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 20:30:37,952.952 dlc1n60lj7mhywta-master-0:2351 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 20:30:37,952.952 dlc1n60lj7mhywta-master-0:2352 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 20:30:37,988.988 dlc1n60lj7mhywta-master-0:2349 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 20:30:39,255.255 dlc1n60lj7mhywta-master-0:2351 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 20:30:39,256.256 dlc1n60lj7mhywta-master-0:2350 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 20:30:40,237.237 dlc1n60lj7mhywta-master-0:2352 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 20:30:40,246.246 dlc1n60lj7mhywta-master-0:2349 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:2349:2349 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:2352:2352 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2351:2351 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2350:2350 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5443), 0.5259219668626403, 0.8497913769123783, tensor(2.0853)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5345), 0.532870122928915, 0.8532684283727399, tensor(2.1299)]
[tensor(-0.5345), 0.532870122928915, 0.8532684283727399, tensor(2.1299)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5156), 0.5505077498663816, 0.8539638386648123, tensor(2.2369)]
[tensor(-0.5156), 0.5505077498663816, 0.8539638386648123, tensor(2.2369)]
[tensor(-0.5119), 0.5505077498663816, 0.8553546592489569, tensor(2.2369)]
[tensor(-0.5119), 0.5505077498663816, 0.8553546592489569, tensor(2.2369)]
[tensor(-0.5119), 0.5505077498663816, 0.8567454798331016, tensor(2.2369)]
[tensor(-0.5119), 0.5505077498663816, 0.8574408901251739, tensor(2.2369)]
[tensor(-0.5119), 0.5505077498663816, 0.8574408901251739, tensor(2.2369)]
[tensor(-0.5119), 0.5505077498663816, 0.8574408901251739, tensor(2.2369)]
[tensor(-0.5119), 0.5505077498663816, 0.8574408901251739, tensor(2.2369)]
[tensor(-0.5115), 0.5521111704970604, 0.8574408901251739, tensor(2.2491)]
[tensor(-0.5115), 0.5521111704970604, 0.8574408901251739, tensor(2.2491)]
[tensor(-0.5115), 0.5521111704970604, 0.8595271210013908, tensor(2.2491)]
[tensor(-0.5115), 0.5521111704970604, 0.8595271210013908, tensor(2.2491)]
[tensor(-0.5115), 0.5521111704970604, 0.8595271210013908, tensor(2.2491)]
[tensor(-0.5115), 0.5521111704970604, 0.8595271210013908, tensor(2.2491)]
[tensor(-0.5115), 0.5521111704970604, 0.8595271210013908, tensor(2.2491)]
[tensor(-0.5115), 0.5521111704970604, 0.8595271210013908, tensor(2.2491)]
early stopping at 20
[2023-01-18 21:10:58,459.459 dlc1n60lj7mhywta-master-0:2463 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 21:10:59,073.073 dlc1n60lj7mhywta-master-0:2531 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:10:59,073.073 dlc1n60lj7mhywta-master-0:2530 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:10:59,074.074 dlc1n60lj7mhywta-master-0:2532 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:10:59,120.120 dlc1n60lj7mhywta-master-0:2529 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:11:01,028.028 dlc1n60lj7mhywta-master-0:2532 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 21:11:01,042.042 dlc1n60lj7mhywta-master-0:2531 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 21:11:01,060.060 dlc1n60lj7mhywta-master-0:2530 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 21:11:01,070.070 dlc1n60lj7mhywta-master-0:2529 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:2529:2529 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:2531:2531 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2530:2530 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2532:2532 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5366), 0.5339390700160342, 0.8504867872044506, tensor(2.1331)]
[tensor(-0.5159), 0.5456974879743453, 0.8595271210013908, tensor(2.2126)]
[tensor(-0.5051), 0.5456974879743453, 0.8595271210013908, tensor(2.2126)]
[tensor(-0.5051), 0.549973276322822, 0.8609179415855355, tensor(2.2404)]
[tensor(-0.4994), 0.555318011758418, 0.8650904033379694, tensor(2.2772)]
[2023-01-18 21:22:01,845.845 dlc1n60lj7mhywta-master-0:2606 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 21:22:02,462.462 dlc1n60lj7mhywta-master-0:2675 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:22:02,479.479 dlc1n60lj7mhywta-master-0:2672 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:22:02,543.543 dlc1n60lj7mhywta-master-0:2673 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:22:02,551.551 dlc1n60lj7mhywta-master-0:2674 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:22:03,885.885 dlc1n60lj7mhywta-master-0:2674 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 21:22:03,888.888 dlc1n60lj7mhywta-master-0:2673 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 21:22:04,375.375 dlc1n60lj7mhywta-master-0:2675 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 21:22:04,378.378 dlc1n60lj7mhywta-master-0:2672 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:2672:2672 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:2675:2675 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2674:2674 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2673:2673 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5288), 0.5430251202565473, 0.8442280945757997, tensor(2.1863)]
[tensor(-0.5191), 0.5478353821485836, 0.8546592489568846, tensor(2.2201)]
[tensor(-0.5191), 0.5478353821485836, 0.8546592489568846, tensor(2.2201)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5191), 0.5478353821485836, 0.8567454798331016, tensor(2.2201)]
[tensor(-0.5175), 0.5478353821485836, 0.8630041724617524, tensor(2.2201)]
[2023-01-18 21:33:21,250.250 dlc1n60lj7mhywta-master-0:2750 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 21:33:21,873.873 dlc1n60lj7mhywta-master-0:2819 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:33:21,887.887 dlc1n60lj7mhywta-master-0:2816 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:33:21,928.928 dlc1n60lj7mhywta-master-0:2818 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:33:22,058.058 dlc1n60lj7mhywta-master-0:2817 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:33:23,178.178 dlc1n60lj7mhywta-master-0:2818 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 21:33:23,301.301 dlc1n60lj7mhywta-master-0:2817 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 21:33:23,838.838 dlc1n60lj7mhywta-master-0:2819 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 21:33:23,842.842 dlc1n60lj7mhywta-master-0:2816 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:2816:2816 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:2819:2819 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2817:2817 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2818:2818 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5315), 0.538214858364511, 0.8539638386648123, tensor(2.1595)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5202), 0.5414216996258685, 0.8616133518776078, tensor(2.1869)]
[tensor(-0.5202), 0.5435595938001069, 0.8657858136300417, tensor(2.1953)]
[tensor(-0.5082), 0.555318011758418, 0.8657858136300417, tensor(2.2684)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5011), 0.5574559059326564, 0.8657858136300417, tensor(2.2862)]
[tensor(-0.5011), 0.5574559059326564, 0.8657858136300417, tensor(2.2862)]
[tensor(-0.5011), 0.5574559059326564, 0.8657858136300417, tensor(2.2862)]
[tensor(-0.5011), 0.5574559059326564, 0.8657858136300417, tensor(2.2862)]
[tensor(-0.5011), 0.5574559059326564, 0.8657858136300417, tensor(2.2862)]
[tensor(-0.5011), 0.5574559059326564, 0.8657858136300417, tensor(2.2862)]
early stopping at 10
[2023-01-18 21:53:25,182.182 dlc1n60lj7mhywta-master-0:2907 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 21:53:25,805.805 dlc1n60lj7mhywta-master-0:2973 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:53:25,858.858 dlc1n60lj7mhywta-master-0:2976 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:53:25,862.862 dlc1n60lj7mhywta-master-0:2974 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:53:25,946.946 dlc1n60lj7mhywta-master-0:2975 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 21:53:27,154.154 dlc1n60lj7mhywta-master-0:2974 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 21:53:27,216.216 dlc1n60lj7mhywta-master-0:2975 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 21:53:27,684.684 dlc1n60lj7mhywta-master-0:2976 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 21:53:27,686.686 dlc1n60lj7mhywta-master-0:2973 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:2973:2973 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:2975:2975 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2976:2976 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:2974:2974 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5323), 0.5344735435595938, 0.8525730180806675, tensor(2.1401)]
[tensor(-0.5219), 0.5424906467129877, 0.8532684283727399, tensor(2.1905)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5039), 0.5489043292357029, 0.8567454798331016, tensor(2.2406)]
[tensor(-0.5039), 0.5489043292357029, 0.8581363004172462, tensor(2.2406)]
[tensor(-0.5039), 0.5489043292357029, 0.8581363004172462, tensor(2.2406)]
[tensor(-0.5039), 0.5489043292357029, 0.8581363004172462, tensor(2.2406)]
[tensor(-0.5039), 0.5489043292357029, 0.8581363004172462, tensor(2.2406)]
[tensor(-0.5039), 0.5489043292357029, 0.8581363004172462, tensor(2.2406)]
[tensor(-0.5039), 0.5489043292357029, 0.8581363004172462, tensor(2.2406)]
[tensor(-0.5039), 0.5489043292357029, 0.8581363004172462, tensor(2.2406)]
early stopping at 10
[2023-01-18 22:15:21,235.235 dlc1n60lj7mhywta-master-0:3066 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 22:15:21,846.846 dlc1n60lj7mhywta-master-0:3134 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:15:21,846.846 dlc1n60lj7mhywta-master-0:3135 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:15:21,846.846 dlc1n60lj7mhywta-master-0:3133 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:15:21,872.872 dlc1n60lj7mhywta-master-0:3132 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:15:23,927.927 dlc1n60lj7mhywta-master-0:3134 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 22:15:23,952.952 dlc1n60lj7mhywta-master-0:3135 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 22:15:23,959.959 dlc1n60lj7mhywta-master-0:3133 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 22:15:23,964.964 dlc1n60lj7mhywta-master-0:3132 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:3132:3132 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:3133:3133 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3134:3134 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3135:3135 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5941), 0.48102618920363444, 0.8477051460361613, tensor(1.8111)]
[tensor(-0.5158), 0.5403527525387494, 0.8623087621696801, tensor(2.1860)]
[tensor(-0.5158), 0.5403527525387494, 0.8623087621696801, tensor(2.1860)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5158), 0.547300908605024, 0.8643949930458971, tensor(2.2143)]
[tensor(-0.5158), 0.547300908605024, 0.8643949930458971, tensor(2.2143)]
[2023-01-18 22:25:28,557.557 dlc1n60lj7mhywta-master-0:3207 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 22:25:29,176.176 dlc1n60lj7mhywta-master-0:3275 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:25:29,177.177 dlc1n60lj7mhywta-master-0:3274 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:25:29,257.257 dlc1n60lj7mhywta-master-0:3273 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:25:29,268.268 dlc1n60lj7mhywta-master-0:3276 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:25:30,615.615 dlc1n60lj7mhywta-master-0:3276 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 22:25:31,093.093 dlc1n60lj7mhywta-master-0:3275 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 22:25:31,094.094 dlc1n60lj7mhywta-master-0:3274 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 22:25:31,100.100 dlc1n60lj7mhywta-master-0:3273 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:3273:3273 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:3274:3274 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3275:3275 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3276:3276 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5152), 0.5456974879743453, 0.8595271210013908, tensor(2.2132)]
[tensor(-0.5152), 0.5456974879743453, 0.8602225312934632, tensor(2.2132)]
[tensor(-0.5109), 0.5483698556921432, 0.866481223922114, tensor(2.2310)]
[tensor(-0.5109), 0.5483698556921432, 0.866481223922114, tensor(2.2310)]
[tensor(-0.5106), 0.5483698556921432, 0.866481223922114, tensor(2.2310)]
[2023-01-18 22:35:38,924.924 dlc1n60lj7mhywta-master-0:3350 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 22:35:39,534.534 dlc1n60lj7mhywta-master-0:3419 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:35:39,534.534 dlc1n60lj7mhywta-master-0:3417 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:35:39,534.534 dlc1n60lj7mhywta-master-0:3418 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:35:39,558.558 dlc1n60lj7mhywta-master-0:3416 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:35:41,473.473 dlc1n60lj7mhywta-master-0:3418 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 22:35:41,480.480 dlc1n60lj7mhywta-master-0:3417 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 22:35:41,482.482 dlc1n60lj7mhywta-master-0:3419 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 22:35:41,487.487 dlc1n60lj7mhywta-master-0:3416 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:3416:3416 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:3417:3417 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3418:3418 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3419:3419 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5219), 0.5376803848209514, 0.8518776077885952, tensor(2.1665)]
[tensor(-0.5219), 0.5376803848209514, 0.8567454798331016, tensor(2.1665)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5219), 0.5376803848209514, 0.8650904033379694, tensor(2.1665)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5183), 0.5376803848209514, 0.8657858136300417, tensor(2.1701)]
[tensor(-0.5157), 0.5387493319080705, 0.866481223922114, tensor(2.1781)]
[tensor(-0.5157), 0.5414216996258685, 0.866481223922114, tensor(2.1860)]
[tensor(-0.5157), 0.5414216996258685, 0.866481223922114, tensor(2.1860)]
[tensor(-0.5157), 0.5414216996258685, 0.866481223922114, tensor(2.1860)]
[tensor(-0.5157), 0.5414216996258685, 0.866481223922114, tensor(2.1860)]
[tensor(-0.5157), 0.5414216996258685, 0.866481223922114, tensor(2.1860)]
[tensor(-0.5157), 0.5414216996258685, 0.866481223922114, tensor(2.1860)]
early stopping at 11
[2023-01-18 22:57:57,036.036 dlc1n60lj7mhywta-master-0:3508 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 22:57:57,648.648 dlc1n60lj7mhywta-master-0:3577 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:57:57,648.648 dlc1n60lj7mhywta-master-0:3575 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:57:57,648.648 dlc1n60lj7mhywta-master-0:3576 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:57:57,678.678 dlc1n60lj7mhywta-master-0:3574 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 22:57:59,696.696 dlc1n60lj7mhywta-master-0:3575 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 22:57:59,724.724 dlc1n60lj7mhywta-master-0:3576 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 22:57:59,726.726 dlc1n60lj7mhywta-master-0:3577 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 22:57:59,730.730 dlc1n60lj7mhywta-master-0:3574 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:3574:3574 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:3576:3576 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3577:3577 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3575:3575 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5247), 0.5398182789951897, 0.8581363004172462, tensor(2.1744)]
[tensor(-0.5199), 0.5430251202565473, 0.8657858136300417, tensor(2.1952)]
[tensor(-0.5068), 0.5430251202565473, 0.8657858136300417, tensor(2.2030)]
[tensor(-0.5068), 0.5430251202565473, 0.8657858136300417, tensor(2.2030)]
[tensor(-0.5068), 0.5430251202565473, 0.8657858136300417, tensor(2.2030)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5068), 0.5430251202565473, 0.8657858136300417, tensor(2.2030)]
[tensor(-0.5068), 0.5430251202565473, 0.8657858136300417, tensor(2.2030)]
[tensor(-0.5068), 0.5446285408872261, 0.8657858136300417, tensor(2.2034)]
[tensor(-0.5068), 0.5446285408872261, 0.8657858136300417, tensor(2.2034)]
[tensor(-0.5068), 0.5446285408872261, 0.8657858136300417, tensor(2.2034)]
[tensor(-0.5068), 0.5446285408872261, 0.8657858136300417, tensor(2.2034)]
[tensor(-0.5068), 0.5446285408872261, 0.8657858136300417, tensor(2.2034)]
[tensor(-0.5068), 0.5446285408872261, 0.8657858136300417, tensor(2.2034)]
early stopping at 13
[2023-01-18 23:23:51,306.306 dlc1n60lj7mhywta-master-0:3673 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 23:23:51,950.950 dlc1n60lj7mhywta-master-0:3741 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:23:52,020.020 dlc1n60lj7mhywta-master-0:3742 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:23:52,029.029 dlc1n60lj7mhywta-master-0:3739 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:23:52,037.037 dlc1n60lj7mhywta-master-0:3740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:23:53,345.345 dlc1n60lj7mhywta-master-0:3740 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 23:23:53,956.956 dlc1n60lj7mhywta-master-0:3742 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 23:23:53,957.957 dlc1n60lj7mhywta-master-0:3741 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 23:23:53,966.966 dlc1n60lj7mhywta-master-0:3739 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:3739:3739 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:3742:3742 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3740:3740 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3741:3741 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5195), 0.547300908605024, 0.8525730180806675, tensor(2.2170)]
[tensor(-0.5195), 0.547300908605024, 0.8630041724617524, tensor(2.2170)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5195), 0.547300908605024, 0.8699582753824756, tensor(2.2170)]
[tensor(-0.5165), 0.547300908605024, 0.8713490959666204, tensor(2.2170)]
[tensor(-0.4977), 0.5510422234099412, 0.8713490959666204, tensor(2.2575)]
[2023-01-18 23:34:12,657.657 dlc1n60lj7mhywta-master-0:3815 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 23:34:13,274.274 dlc1n60lj7mhywta-master-0:3883 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:34:13,280.280 dlc1n60lj7mhywta-master-0:3882 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:34:13,282.282 dlc1n60lj7mhywta-master-0:3881 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:34:13,351.351 dlc1n60lj7mhywta-master-0:3884 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:34:14,256.256 dlc1n60lj7mhywta-master-0:3883 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 23:34:14,305.305 dlc1n60lj7mhywta-master-0:3882 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 23:34:14,430.430 dlc1n60lj7mhywta-master-0:3884 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 23:34:14,436.436 dlc1n60lj7mhywta-master-0:3881 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:3881:3881 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:3882:3882 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3883:3883 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:3884:3884 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5301), 0.5307322287546766, 0.8588317107093185, tensor(2.1235)]
[tensor(-0.5301), 0.5350080171031534, 0.8588317107093185, tensor(2.1338)]
[tensor(-0.5074), 0.5430251202565473, 0.8671766342141863, tensor(2.2077)]
[tensor(-0.5074), 0.5430251202565473, 0.8678720445062587, tensor(2.2077)]
[tensor(-0.5018), 0.5595938001068947, 0.868567454798331, tensor(2.2962)]
[2023-01-18 23:44:20,034.034 dlc1n60lj7mhywta-master-0:3957 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-18 23:44:20,672.672 dlc1n60lj7mhywta-master-0:4025 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:44:20,672.672 dlc1n60lj7mhywta-master-0:4026 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:44:20,755.755 dlc1n60lj7mhywta-master-0:4024 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:44:20,764.764 dlc1n60lj7mhywta-master-0:4023 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-18 23:44:22,071.071 dlc1n60lj7mhywta-master-0:4024 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-18 23:44:22,610.610 dlc1n60lj7mhywta-master-0:4025 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-18 23:44:22,611.611 dlc1n60lj7mhywta-master-0:4026 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-18 23:44:22,616.616 dlc1n60lj7mhywta-master-0:4023 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:4023:4023 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:4025:4025 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4026:4026 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4024:4024 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5269), 0.5446285408872261, 0.8414464534075105, tensor(2.1962)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5248), 0.5446285408872261, 0.8602225312934632, tensor(2.1962)]
[tensor(-0.5164), 0.5467664350614645, 0.8643949930458971, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8643949930458971, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8643949930458971, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8643949930458971, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8643949930458971, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8671766342141863, tensor(2.2174)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5104), 0.5467664350614645, 0.8671766342141863, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8671766342141863, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8671766342141863, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8671766342141863, tensor(2.2174)]
[tensor(-0.5104), 0.5467664350614645, 0.8671766342141863, tensor(2.2174)]
early stopping at 13
[2023-01-19 00:10:35,320.320 dlc1n60lj7mhywta-master-0:4121 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 00:10:35,952.952 dlc1n60lj7mhywta-master-0:4187 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:10:35,952.952 dlc1n60lj7mhywta-master-0:4190 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:10:35,955.955 dlc1n60lj7mhywta-master-0:4189 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:10:35,956.956 dlc1n60lj7mhywta-master-0:4188 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:10:37,945.945 dlc1n60lj7mhywta-master-0:4190 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 00:10:37,952.952 dlc1n60lj7mhywta-master-0:4188 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 00:10:37,955.955 dlc1n60lj7mhywta-master-0:4189 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 00:10:37,958.958 dlc1n60lj7mhywta-master-0:4187 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:4187:4187 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:4189:4189 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4188:4188 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4190:4190 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5603), 0.5232495991448424, 0.8525730180806675, tensor(2.0559)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5356), 0.5440940673436665, 0.8567454798331016, tensor(2.1849)]
[tensor(-0.5235), 0.5440940673436665, 0.8581363004172462, tensor(2.1849)]
[tensor(-0.5235), 0.5440940673436665, 0.8595271210013908, tensor(2.1849)]
[tensor(-0.5217), 0.5456974879743453, 0.8595271210013908, tensor(2.2067)]
[tensor(-0.5217), 0.5456974879743453, 0.8595271210013908, tensor(2.2067)]
[tensor(-0.5217), 0.5456974879743453, 0.8595271210013908, tensor(2.2067)]
[tensor(-0.5217), 0.5456974879743453, 0.8609179415855355, tensor(2.2067)]
[tensor(-0.5217), 0.5456974879743453, 0.8609179415855355, tensor(2.2067)]
[tensor(-0.5217), 0.5462319615179049, 0.8650904033379694, tensor(2.2067)]
[tensor(-0.5217), 0.5462319615179049, 0.8650904033379694, tensor(2.2067)]
[tensor(-0.5217), 0.5462319615179049, 0.8650904033379694, tensor(2.2067)]
[tensor(-0.5217), 0.5462319615179049, 0.8650904033379694, tensor(2.2067)]
[tensor(-0.5217), 0.5462319615179049, 0.8650904033379694, tensor(2.2067)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5217), 0.5462319615179049, 0.8650904033379694, tensor(2.2067)]
early stopping at 15
[2023-01-19 00:41:18,858.858 dlc1n60lj7mhywta-master-0:4288 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 00:41:19,468.468 dlc1n60lj7mhywta-master-0:4356 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:41:19,470.470 dlc1n60lj7mhywta-master-0:4354 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:41:19,553.553 dlc1n60lj7mhywta-master-0:4355 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:41:19,559.559 dlc1n60lj7mhywta-master-0:4357 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:41:20,681.681 dlc1n60lj7mhywta-master-0:4357 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 00:41:20,684.684 dlc1n60lj7mhywta-master-0:4355 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 00:41:21,370.370 dlc1n60lj7mhywta-master-0:4356 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 00:41:21,376.376 dlc1n60lj7mhywta-master-0:4354 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:4354:4354 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:4357:4357 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4355:4355 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4356:4356 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5577), 0.5248530197755211, 0.8511821974965229, tensor(2.0666)]
[tensor(-0.5577), 0.5387493319080705, 0.8518776077885952, tensor(2.1236)]
[tensor(-0.5488), 0.5392838054516301, 0.8525730180806675, tensor(2.1476)]
[tensor(-0.5178), 0.5489043292357029, 0.8532684283727399, tensor(2.2268)]
[tensor(-0.5171), 0.5489043292357029, 0.8532684283727399, tensor(2.2268)]
[2023-01-19 00:51:23,183.183 dlc1n60lj7mhywta-master-0:4429 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 00:51:23,804.804 dlc1n60lj7mhywta-master-0:4497 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:51:23,805.805 dlc1n60lj7mhywta-master-0:4496 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:51:23,805.805 dlc1n60lj7mhywta-master-0:4498 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:51:23,806.806 dlc1n60lj7mhywta-master-0:4495 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 00:51:24,744.744 dlc1n60lj7mhywta-master-0:4496 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 00:51:25,739.739 dlc1n60lj7mhywta-master-0:4498 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 00:51:25,741.741 dlc1n60lj7mhywta-master-0:4497 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 00:51:25,745.745 dlc1n60lj7mhywta-master-0:4495 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:4495:4495 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:4498:4498 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4497:4497 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4496:4496 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5355), 0.5323356493853554, 0.8484005563282336, tensor(2.1262)]
[tensor(-0.5355), 0.5323356493853554, 0.8484005563282336, tensor(2.1262)]
[tensor(-0.5176), 0.5451630144307856, 0.8525730180806675, tensor(2.2082)]
[tensor(-0.5176), 0.5558524853019775, 0.8525730180806675, tensor(2.2435)]
[tensor(-0.5176), 0.5558524853019775, 0.8553546592489569, tensor(2.2435)]
[2023-01-19 01:02:14,580.580 dlc1n60lj7mhywta-master-0:4572 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 01:02:15,192.192 dlc1n60lj7mhywta-master-0:4639 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:02:15,192.192 dlc1n60lj7mhywta-master-0:4641 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:02:15,192.192 dlc1n60lj7mhywta-master-0:4640 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:02:15,204.204 dlc1n60lj7mhywta-master-0:4638 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:02:16,139.139 dlc1n60lj7mhywta-master-0:4639 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 01:02:17,125.125 dlc1n60lj7mhywta-master-0:4641 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 01:02:17,134.134 dlc1n60lj7mhywta-master-0:4640 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 01:02:17,138.138 dlc1n60lj7mhywta-master-0:4638 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:4638:4638 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:4639:4639 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4641:4641 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4640:4640 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5876), 0.5002672367717798, 0.8428372739916551, tensor(1.9137)]
[tensor(-0.5439), 0.5259219668626403, 0.8504867872044506, tensor(2.0858)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5394), 0.5366114377338321, 0.8574408901251739, tensor(2.1437)]
[tensor(-0.5136), 0.5462319615179049, 0.8574408901251739, tensor(2.2175)]
[tensor(-0.5136), 0.5462319615179049, 0.8574408901251739, tensor(2.2175)]
[tensor(-0.5136), 0.5462319615179049, 0.8574408901251739, tensor(2.2175)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5136), 0.5462319615179049, 0.8623087621696801, tensor(2.2175)]
[tensor(-0.5136), 0.5462319615179049, 0.8623087621696801, tensor(2.2175)]
[tensor(-0.5136), 0.5462319615179049, 0.8623087621696801, tensor(2.2175)]
[tensor(-0.5136), 0.5462319615179049, 0.8623087621696801, tensor(2.2175)]
[tensor(-0.5136), 0.5462319615179049, 0.8623087621696801, tensor(2.2175)]
[tensor(-0.5136), 0.5462319615179049, 0.8623087621696801, tensor(2.2175)]
early stopping at 12
[2023-01-19 01:25:58,706.706 dlc1n60lj7mhywta-master-0:4732 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 01:25:59,322.322 dlc1n60lj7mhywta-master-0:4799 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:25:59,323.323 dlc1n60lj7mhywta-master-0:4798 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:25:59,402.402 dlc1n60lj7mhywta-master-0:4801 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:25:59,409.409 dlc1n60lj7mhywta-master-0:4800 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:26:00,686.686 dlc1n60lj7mhywta-master-0:4800 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 01:26:00,687.687 dlc1n60lj7mhywta-master-0:4801 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 01:26:01,173.173 dlc1n60lj7mhywta-master-0:4799 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 01:26:01,177.177 dlc1n60lj7mhywta-master-0:4798 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:4798:4798 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:4800:4800 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4799:4799 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4801:4801 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5411), 0.5344735435595938, 0.849095966620306, tensor(2.1312)]
[tensor(-0.5372), 0.5344735435595938, 0.8553546592489569, tensor(2.1312)]
[tensor(-0.5193), 0.5424906467129877, 0.8553546592489569, tensor(2.1931)]
[tensor(-0.5193), 0.5446285408872261, 0.8553546592489569, tensor(2.2033)]
[tensor(-0.5185), 0.5446285408872261, 0.8553546592489569, tensor(2.2033)]
[tensor(-0.5185), 0.5446285408872261, 0.8553546592489569, tensor(2.2033)]
[tensor(-0.5185), 0.5446285408872261, 0.8553546592489569, tensor(2.2033)]
[tensor(-0.5185), 0.5446285408872261, 0.8553546592489569, tensor(2.2033)]
[tensor(-0.5185), 0.5446285408872261, 0.8553546592489569, tensor(2.2033)]
[tensor(-0.5185), 0.5446285408872261, 0.8553546592489569, tensor(2.2033)]
early stopping at 10
[2023-01-19 01:47:51,754.754 dlc1n60lj7mhywta-master-0:4890 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 01:47:52,373.373 dlc1n60lj7mhywta-master-0:4959 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:47:52,373.373 dlc1n60lj7mhywta-master-0:4958 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:47:52,376.376 dlc1n60lj7mhywta-master-0:4957 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:47:52,377.377 dlc1n60lj7mhywta-master-0:4956 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:47:53,318.318 dlc1n60lj7mhywta-master-0:4958 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 01:47:54,309.309 dlc1n60lj7mhywta-master-0:4959 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 01:47:54,316.316 dlc1n60lj7mhywta-master-0:4957 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 01:47:54,320.320 dlc1n60lj7mhywta-master-0:4956 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:4956:4956 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:4957:4957 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4958:4958 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:4959:4959 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5342), 0.5339390700160342, 0.8463143254520167, tensor(2.1355)]
[tensor(-0.5087), 0.5628006413682523, 0.8567454798331016, tensor(2.3053)]
[Thu Jan 19 01:53:08 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5009), 0.5628006413682523, 0.8567454798331016, tensor(2.3053)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5009), 0.5628006413682523, 0.8567454798331016, tensor(2.3053)]
[tensor(-0.5009), 0.5628006413682523, 0.8609179415855355, tensor(2.3053)]
[2023-01-19 01:57:50,111.111 dlc1n60lj7mhywta-master-0:5031 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 01:57:50,726.726 dlc1n60lj7mhywta-master-0:5097 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:57:50,726.726 dlc1n60lj7mhywta-master-0:5100 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:57:50,727.727 dlc1n60lj7mhywta-master-0:5098 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:57:50,727.727 dlc1n60lj7mhywta-master-0:5099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 01:57:51,658.658 dlc1n60lj7mhywta-master-0:5098 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 01:57:51,662.662 dlc1n60lj7mhywta-master-0:5100 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 01:57:51,664.664 dlc1n60lj7mhywta-master-0:5099 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 01:57:51,665.665 dlc1n60lj7mhywta-master-0:5097 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:5097:5097 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:5100:5100 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5098:5098 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5099:5099 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5176), 0.5392838054516301, 0.8581363004172462, tensor(2.1789)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5017), 0.5558524853019775, 0.8636995827538247, tensor(2.2775)]
[tensor(-0.5017), 0.5558524853019775, 0.8636995827538247, tensor(2.2775)]
[tensor(-0.5017), 0.5558524853019775, 0.8636995827538247, tensor(2.2775)]
[tensor(-0.5017), 0.5558524853019775, 0.8650904033379694, tensor(2.2775)]
[2023-01-19 02:07:58,424.424 dlc1n60lj7mhywta-master-0:5174 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 02:07:59,037.037 dlc1n60lj7mhywta-master-0:5240 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:07:59,044.044 dlc1n60lj7mhywta-master-0:5243 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:07:59,119.119 dlc1n60lj7mhywta-master-0:5241 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:07:59,124.124 dlc1n60lj7mhywta-master-0:5242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:07:59,898.898 dlc1n60lj7mhywta-master-0:5243 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 02:08:00,387.387 dlc1n60lj7mhywta-master-0:5241 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 02:08:00,388.388 dlc1n60lj7mhywta-master-0:5242 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 02:08:00,396.396 dlc1n60lj7mhywta-master-0:5240 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:5240:5240 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:5243:5243 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5241:5241 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5242:5242 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5340), 0.5387493319080705, 0.8574408901251739, tensor(2.1597)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5270), 0.5387493319080705, 0.8574408901251739, tensor(2.1597)]
[Thu Jan 19 02:13:26 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5182), 0.5387493319080705, 0.8574408901251739, tensor(2.1622)]
[tensor(-0.5096), 0.5398182789951897, 0.8574408901251739, tensor(2.1894)]
[tensor(-0.5096), 0.5398182789951897, 0.8595271210013908, tensor(2.1894)]
[tensor(-0.5096), 0.5430251202565473, 0.8602225312934632, tensor(2.1909)]
[tensor(-0.5096), 0.5483698556921432, 0.8643949930458971, tensor(2.2277)]
[tensor(-0.5096), 0.5483698556921432, 0.8643949930458971, tensor(2.2277)]
[tensor(-0.5096), 0.5483698556921432, 0.8650904033379694, tensor(2.2277)]
[tensor(-0.5096), 0.5483698556921432, 0.8650904033379694, tensor(2.2277)]
[tensor(-0.5096), 0.5483698556921432, 0.8650904033379694, tensor(2.2277)]
[Thu Jan 19 02:30:59 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5096), 0.5483698556921432, 0.8650904033379694, tensor(2.2277)]
[tensor(-0.5096), 0.5483698556921432, 0.8650904033379694, tensor(2.2277)]
[tensor(-0.5096), 0.5483698556921432, 0.8650904033379694, tensor(2.2277)]
early stopping at 14
[2023-01-19 02:35:39,797.797 dlc1n60lj7mhywta-master-0:5339 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 02:35:40,408.408 dlc1n60lj7mhywta-master-0:5408 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:35:40,410.410 dlc1n60lj7mhywta-master-0:5406 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:35:40,493.493 dlc1n60lj7mhywta-master-0:5405 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:35:40,500.500 dlc1n60lj7mhywta-master-0:5407 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 02:35:42,273.273 dlc1n60lj7mhywta-master-0:5408 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 02:35:42,274.274 dlc1n60lj7mhywta-master-0:5406 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 02:35:42,785.785 dlc1n60lj7mhywta-master-0:5407 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 02:35:42,787.787 dlc1n60lj7mhywta-master-0:5405 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:5405:5405 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:5407:5407 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5408:5408 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5406:5406 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5264), 0.5398182789951897, 0.8442280945757997, tensor(2.1727)]
[tensor(-0.5175), 0.5451630144307856, 0.8567454798331016, tensor(2.2083)]
[tensor(-0.5123), 0.5451630144307856, 0.8574408901251739, tensor(2.2083)]
[tensor(-0.5123), 0.5451630144307856, 0.8574408901251739, tensor(2.2083)]
[tensor(-0.5123), 0.5451630144307856, 0.8650904033379694, tensor(2.2083)]
[tensor(-0.5123), 0.547300908605024, 0.8650904033379694, tensor(2.2232)]
[tensor(-0.5123), 0.5494388027792624, 0.8650904033379694, tensor(2.2347)]
[tensor(-0.5123), 0.5494388027792624, 0.8650904033379694, tensor(2.2347)]
[tensor(-0.5123), 0.5494388027792624, 0.8650904033379694, tensor(2.2347)]
[tensor(-0.5123), 0.5494388027792624, 0.8650904033379694, tensor(2.2347)]
[tensor(-0.5123), 0.5494388027792624, 0.8650904033379694, tensor(2.2347)]
[tensor(-0.5123), 0.5494388027792624, 0.8650904033379694, tensor(2.2347)]
early stopping at 12
[2023-01-19 03:00:00,958.958 dlc1n60lj7mhywta-master-0:5501 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 03:00:01,575.575 dlc1n60lj7mhywta-master-0:5569 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:00:01,575.575 dlc1n60lj7mhywta-master-0:5570 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:00:01,575.575 dlc1n60lj7mhywta-master-0:5568 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:00:01,575.575 dlc1n60lj7mhywta-master-0:5567 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:00:03,593.593 dlc1n60lj7mhywta-master-0:5569 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 03:00:03,595.595 dlc1n60lj7mhywta-master-0:5570 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 03:00:03,597.597 dlc1n60lj7mhywta-master-0:5568 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 03:00:03,606.606 dlc1n60lj7mhywta-master-0:5567 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:5567:5567 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:5570:5570 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5568:5568 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5569:5569 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5335), 0.5296632816675575, 0.8525730180806675, tensor(2.1148)]
[tensor(-0.5195), 0.538214858364511, 0.8588317107093185, tensor(2.1716)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5195), 0.538214858364511, 0.8588317107093185, tensor(2.1716)]
[tensor(-0.5195), 0.538214858364511, 0.868567454798331, tensor(2.1716)]
[Thu Jan 19 03:08:21 2023] [cudaHostAllocator] allocates 3.42 GiB
[tensor(-0.5024), 0.5569214323890967, 0.8720445062586927, tensor(2.2822)]
[2023-01-19 03:10:19,363.363 dlc1n60lj7mhywta-master-0:5643 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 03:10:19,973.973 dlc1n60lj7mhywta-master-0:5711 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:10:19,974.974 dlc1n60lj7mhywta-master-0:5712 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:10:20,050.050 dlc1n60lj7mhywta-master-0:5710 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:10:20,065.065 dlc1n60lj7mhywta-master-0:5709 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:10:21,346.346 dlc1n60lj7mhywta-master-0:5710 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 03:10:21,917.917 dlc1n60lj7mhywta-master-0:5711 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 03:10:21,937.937 dlc1n60lj7mhywta-master-0:5712 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 03:10:21,940.940 dlc1n60lj7mhywta-master-0:5709 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 5 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:5709:5709 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:5711:5711 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5710:5710 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5712:5712 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5757), 0.4997327632282202, 0.8602225312934632, tensor(1.9230)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5234), 0.5259219668626403, 0.8643949930458971, tensor(2.1062)]
[tensor(-0.5232), 0.5259219668626403, 0.8643949930458971, tensor(2.1062)]
[tensor(-0.5232), 0.5259219668626403, 0.8643949930458971, tensor(2.1062)]
[tensor(-0.5179), 0.5275253874933191, 0.8643949930458971, tensor(2.1197)]
[2023-01-19 03:20:34,724.724 dlc1n60lj7mhywta-master-0:5786 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 03:20:35,365.365 dlc1n60lj7mhywta-master-0:5854 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:20:35,365.365 dlc1n60lj7mhywta-master-0:5852 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:20:35,366.366 dlc1n60lj7mhywta-master-0:5855 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:20:35,385.385 dlc1n60lj7mhywta-master-0:5853 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:20:36,351.351 dlc1n60lj7mhywta-master-0:5854 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 03:20:36,355.355 dlc1n60lj7mhywta-master-0:5855 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 03:20:36,399.399 dlc1n60lj7mhywta-master-0:5853 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 03:20:36,403.403 dlc1n60lj7mhywta-master-0:5852 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:5852:5852 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:5854:5854 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5855:5855 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:5853:5853 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5358), 0.5323356493853554, 0.8442280945757997, tensor(2.1259)]
[tensor(-0.5152), 0.5435595938001069, 0.8567454798331016, tensor(2.2026)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5152), 0.5435595938001069, 0.8567454798331016, tensor(2.2026)]
[tensor(-0.5059), 0.5435595938001069, 0.8623087621696801, tensor(2.2092)]
[Thu Jan 19 03:29:00 2023] [cudaHostAllocator] allocates 3.42 GiB
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5059), 0.5435595938001069, 0.8623087621696801, tensor(2.2092)]
[tensor(-0.5059), 0.5435595938001069, 0.8623087621696801, tensor(2.2092)]
[tensor(-0.5059), 0.5435595938001069, 0.8623087621696801, tensor(2.2092)]
[tensor(-0.5059), 0.5435595938001069, 0.8623087621696801, tensor(2.2092)]
[tensor(-0.5059), 0.5435595938001069, 0.8623087621696801, tensor(2.2092)]
early stopping at 9
[2023-01-19 03:39:01,516.516 dlc1n60lj7mhywta-master-0:5940 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 03:39:02,133.133 dlc1n60lj7mhywta-master-0:6006 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:39:02,133.133 dlc1n60lj7mhywta-master-0:6007 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:39:02,139.139 dlc1n60lj7mhywta-master-0:6008 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:39:02,142.142 dlc1n60lj7mhywta-master-0:6009 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 03:39:04,040.040 dlc1n60lj7mhywta-master-0:6007 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 03:39:04,131.131 dlc1n60lj7mhywta-master-0:6008 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 03:39:04,138.138 dlc1n60lj7mhywta-master-0:6009 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 03:39:04,142.142 dlc1n60lj7mhywta-master-0:6006 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:6006:6006 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:6007:6007 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6008:6008 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6009:6009 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5664), 0.5066809192944949, 0.8435326842837274, tensor(1.9670)]
[tensor(-0.5274), 0.5323356493853554, 0.8553546592489569, tensor(2.1343)]
[tensor(-0.5274), 0.5323356493853554, 0.8553546592489569, tensor(2.1343)]
[tensor(-0.5274), 0.5323356493853554, 0.8553546592489569, tensor(2.1343)]
[tensor(-0.5274), 0.532870122928915, 0.8553546592489569, tensor(2.1343)]
[tensor(-0.5274), 0.5339390700160342, 0.8560500695410292, tensor(2.1343)]
[tensor(-0.5274), 0.535542490646713, 0.8643949930458971, tensor(2.1343)]
[tensor(-0.5274), 0.535542490646713, 0.8643949930458971, tensor(2.1343)]
[tensor(-0.5274), 0.535542490646713, 0.8643949930458971, tensor(2.1343)]
[tensor(-0.5274), 0.535542490646713, 0.8643949930458971, tensor(2.1343)]
[Thu Jan 19 04:00:06 2023] [cudaHostAllocator] allocates 1.95 GiB
[tensor(-0.5274), 0.535542490646713, 0.8643949930458971, tensor(2.1343)]
[tensor(-0.5274), 0.5366114377338321, 0.8643949930458971, tensor(2.1465)]
[tensor(-0.5274), 0.5366114377338321, 0.8643949930458971, tensor(2.1465)]
[tensor(-0.5274), 0.5366114377338321, 0.8643949930458971, tensor(2.1465)]
[tensor(-0.5274), 0.5366114377338321, 0.8643949930458971, tensor(2.1465)]
[tensor(-0.5274), 0.5366114377338321, 0.8643949930458971, tensor(2.1465)]
[tensor(-0.5274), 0.5366114377338321, 0.8643949930458971, tensor(2.1465)]
early stopping at 17
[2023-01-19 04:12:53,315.315 dlc1n60lj7mhywta-master-0:6116 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 04:12:53,947.947 dlc1n60lj7mhywta-master-0:6184 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:12:53,965.965 dlc1n60lj7mhywta-master-0:6182 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:12:54,026.026 dlc1n60lj7mhywta-master-0:6185 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:12:54,031.031 dlc1n60lj7mhywta-master-0:6183 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:12:55,327.327 dlc1n60lj7mhywta-master-0:6183 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 04:12:55,332.332 dlc1n60lj7mhywta-master-0:6185 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 04:12:55,783.783 dlc1n60lj7mhywta-master-0:6184 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 04:12:55,792.792 dlc1n60lj7mhywta-master-0:6182 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:6182:6182 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:6185:6185 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6184:6184 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6183:6183 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5242), 0.5467664350614645, 0.8456189151599444, tensor(2.2096)]
[tensor(-0.5203), 0.5467664350614645, 0.8456189151599444, tensor(2.2096)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5198), 0.5467664350614645, 0.8588317107093185, tensor(2.2096)]
[tensor(-0.5198), 0.5467664350614645, 0.8623087621696801, tensor(2.2096)]
[tensor(-0.5133), 0.5467664350614645, 0.8657858136300417, tensor(2.2096)]
[2023-01-19 04:22:57,635.635 dlc1n60lj7mhywta-master-0:6259 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 04:22:58,249.249 dlc1n60lj7mhywta-master-0:6327 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:22:58,250.250 dlc1n60lj7mhywta-master-0:6328 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:22:58,250.250 dlc1n60lj7mhywta-master-0:6326 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:22:58,307.307 dlc1n60lj7mhywta-master-0:6325 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:23:00,165.165 dlc1n60lj7mhywta-master-0:6326 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 04:23:00,168.168 dlc1n60lj7mhywta-master-0:6328 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 04:23:00,170.170 dlc1n60lj7mhywta-master-0:6327 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 04:23:00,180.180 dlc1n60lj7mhywta-master-0:6325 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:6325:6325 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:6326:6326 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6327:6327 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6328:6328 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5695), 0.48850881881346875, 0.8518776077885952, tensor(1.8731)]
[tensor(-0.5319), 0.5334045964724746, 0.8518776077885952, tensor(2.1351)]
[tensor(-0.5184), 0.5467664350614645, 0.8560500695410292, tensor(2.2155)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5184), 0.5467664350614645, 0.8560500695410292, tensor(2.2155)]
[tensor(-0.5145), 0.547300908605024, 0.8581363004172462, tensor(2.2220)]
[2023-01-19 04:33:13,979.979 dlc1n60lj7mhywta-master-0:6400 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 04:33:14,593.593 dlc1n60lj7mhywta-master-0:6468 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:33:14,593.593 dlc1n60lj7mhywta-master-0:6469 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:33:14,593.593 dlc1n60lj7mhywta-master-0:6467 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:33:14,595.595 dlc1n60lj7mhywta-master-0:6466 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:33:16,591.591 dlc1n60lj7mhywta-master-0:6468 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 04:33:16,612.612 dlc1n60lj7mhywta-master-0:6467 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 04:33:16,628.628 dlc1n60lj7mhywta-master-0:6469 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 04:33:16,637.637 dlc1n60lj7mhywta-master-0:6466 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:6466:6466 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:6467:6467 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6469:6469 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6468:6468 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5541), 0.5248530197755211, 0.8463143254520167, tensor(2.0702)]
[tensor(-0.5246), 0.5371459112773918, 0.8574408901251739, tensor(2.1611)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5200), 0.5446285408872261, 0.8623087621696801, tensor(2.2032)]
[tensor(-0.5105), 0.5510422234099412, 0.8636995827538247, tensor(2.2447)]
[tensor(-0.5105), 0.5510422234099412, 0.8636995827538247, tensor(2.2447)]
[tensor(-0.5105), 0.5510422234099412, 0.8636995827538247, tensor(2.2447)]
[tensor(-0.5105), 0.5510422234099412, 0.8636995827538247, tensor(2.2447)]
[tensor(-0.5105), 0.5510422234099412, 0.8636995827538247, tensor(2.2447)]
[tensor(-0.5105), 0.5510422234099412, 0.8636995827538247, tensor(2.2447)]
early stopping at 9
[2023-01-19 04:51:04,849.849 dlc1n60lj7mhywta-master-0:6552 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 04:51:05,455.455 dlc1n60lj7mhywta-master-0:6620 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:51:05,455.455 dlc1n60lj7mhywta-master-0:6621 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:51:05,455.455 dlc1n60lj7mhywta-master-0:6619 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:51:05,456.456 dlc1n60lj7mhywta-master-0:6618 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 04:51:07,411.411 dlc1n60lj7mhywta-master-0:6619 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 04:51:07,435.435 dlc1n60lj7mhywta-master-0:6620 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 04:51:07,446.446 dlc1n60lj7mhywta-master-0:6621 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 04:51:07,451.451 dlc1n60lj7mhywta-master-0:6618 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:6618:6618 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:6620:6620 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6619:6619 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6621:6621 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5398), 0.5227151256012827, 0.8560500695410292, tensor(2.0738)]
[tensor(-0.5204), 0.5456974879743453, 0.8581363004172462, tensor(2.2081)]
[tensor(-0.5065), 0.5456974879743453, 0.8581363004172462, tensor(2.2139)]
[tensor(-0.5065), 0.5456974879743453, 0.8581363004172462, tensor(2.2139)]
[tensor(-0.5065), 0.5456974879743453, 0.8609179415855355, tensor(2.2139)]
[tensor(-0.5065), 0.5456974879743453, 0.8609179415855355, tensor(2.2139)]
[tensor(-0.5065), 0.5489043292357029, 0.8609179415855355, tensor(2.2297)]
[tensor(-0.5065), 0.5489043292357029, 0.8609179415855355, tensor(2.2297)]
[tensor(-0.5065), 0.5489043292357029, 0.8609179415855355, tensor(2.2297)]
[tensor(-0.5065), 0.5489043292357029, 0.8609179415855355, tensor(2.2297)]
[tensor(-0.5065), 0.5489043292357029, 0.8609179415855355, tensor(2.2297)]
[tensor(-0.5065), 0.5489043292357029, 0.8609179415855355, tensor(2.2297)]
early stopping at 12
[2023-01-19 05:14:54,975.975 dlc1n60lj7mhywta-master-0:6714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 05:14:55,591.591 dlc1n60lj7mhywta-master-0:6783 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:14:55,591.591 dlc1n60lj7mhywta-master-0:6780 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:14:55,672.672 dlc1n60lj7mhywta-master-0:6782 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:14:55,688.688 dlc1n60lj7mhywta-master-0:6781 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:14:56,923.923 dlc1n60lj7mhywta-master-0:6782 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 05:14:56,924.924 dlc1n60lj7mhywta-master-0:6781 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 05:14:57,453.453 dlc1n60lj7mhywta-master-0:6783 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 05:14:57,460.460 dlc1n60lj7mhywta-master-0:6780 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:6780:6780 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:6782:6782 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6781:6781 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6783:6783 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5494), 0.5307322287546766, 0.8289290681502086, tensor(2.1043)]
[tensor(-0.5131), 0.55264564404062, 0.8623087621696801, tensor(2.2502)]
[tensor(-0.5131), 0.55264564404062, 0.8623087621696801, tensor(2.2502)]
[tensor(-0.5079), 0.55264564404062, 0.8623087621696801, tensor(2.2502)]
[tensor(-0.5061), 0.55264564404062, 0.8623087621696801, tensor(2.2502)]
[2023-01-19 05:24:57,431.431 dlc1n60lj7mhywta-master-0:6855 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 05:24:58,040.040 dlc1n60lj7mhywta-master-0:6924 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:24:58,040.040 dlc1n60lj7mhywta-master-0:6922 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:24:58,111.111 dlc1n60lj7mhywta-master-0:6923 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:24:58,162.162 dlc1n60lj7mhywta-master-0:6921 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:24:59,904.904 dlc1n60lj7mhywta-master-0:6922 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 05:24:59,904.904 dlc1n60lj7mhywta-master-0:6924 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 05:25:00,367.367 dlc1n60lj7mhywta-master-0:6923 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 05:25:00,370.370 dlc1n60lj7mhywta-master-0:6921 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 5 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:6921:6921 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:6923:6923 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6924:6924 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:6922:6922 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5138), 0.5424906467129877, 0.8616133518776078, tensor(2.1987)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5138), 0.5462319615179049, 0.8643949930458971, tensor(2.2119)]
[tensor(-0.5117), 0.5521111704970604, 0.8643949930458971, tensor(2.2489)]
[tensor(-0.5117), 0.5521111704970604, 0.8643949930458971, tensor(2.2489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.5117), 0.5521111704970604, 0.8643949930458971, tensor(2.2489)]
[2023-01-19 05:35:14,777.777 dlc1n60lj7mhywta-master-0:6998 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 05:35:15,396.396 dlc1n60lj7mhywta-master-0:7065 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:35:15,479.479 dlc1n60lj7mhywta-master-0:7067 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:35:15,480.480 dlc1n60lj7mhywta-master-0:7064 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:35:15,493.493 dlc1n60lj7mhywta-master-0:7066 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:35:16,799.799 dlc1n60lj7mhywta-master-0:7067 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 05:35:16,841.841 dlc1n60lj7mhywta-master-0:7066 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 05:35:17,207.207 dlc1n60lj7mhywta-master-0:7065 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 05:35:17,216.216 dlc1n60lj7mhywta-master-0:7064 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
downstreamv2 mosei
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:7064:7064 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:7065:7065 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:7066:7066 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:7067:7067 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[tensor(-0.5194), 0.5430251202565473, 0.8497913769123783, tensor(2.1957)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.5194), 0.5430251202565473, 0.8553546592489569, tensor(2.1957)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5188), 0.5430251202565473, 0.8581363004172462, tensor(2.1957)]
[tensor(-0.5080), 0.5451630144307856, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5080), 0.5462319615179049, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5080), 0.5462319615179049, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5080), 0.5462319615179049, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5080), 0.5462319615179049, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5080), 0.5462319615179049, 0.866481223922114, tensor(2.2179)]
[tensor(-0.5080), 0.5462319615179049, 0.866481223922114, tensor(2.2179)]
early stopping at 10
[2023-01-19 05:55:18,731.731 dlc1n60lj7mhywta-master-0:7154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-01-19 05:55:19,341.341 dlc1n60lj7mhywta-master-0:7223 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:55:19,342.342 dlc1n60lj7mhywta-master-0:7222 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:55:19,342.342 dlc1n60lj7mhywta-master-0:7221 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:55:19,343.343 dlc1n60lj7mhywta-master-0:7220 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
[2023-01-19 05:55:21,313.313 dlc1n60lj7mhywta-master-0:7223 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 3
[2023-01-19 05:55:21,325.325 dlc1n60lj7mhywta-master-0:7221 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 1
[2023-01-19 05:55:21,342.342 dlc1n60lj7mhywta-master-0:7222 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 2
[2023-01-19 05:55:21,346.346 dlc1n60lj7mhywta-master-0:7220 INFO distributed_c10d.py:195] Added key: store_based_barrier_key:1 to store for rank: 0
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mosei last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
fused layers 1
fused layers 1
fused layers 1
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /root/data/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei
downstreamv2 mosei

dlc1n60lj7mhywta-master-0:7220:7220 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
NCCL version 2.8.3+cuda10.1

dlc1n60lj7mhywta-master-0:7223:7223 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:7222:7222 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dlc1n60lj7mhywta-master-0:7221:7221 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.5197), 0.5467664350614645, 0.8636995827538247, tensor(2.2141)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.5178), 0.5467664350614645, 0.8657858136300417, tensor(2.2141)]
[tensor(-0.5040), 0.5467664350614645, 0.8657858136300417, tensor(2.2141)]
[tensor(-0.5040), 0.5467664350614645, 0.8657858136300417, tensor(2.2141)]
[tensor(-0.5040), 0.5483698556921432, 0.8657858136300417, tensor(2.2248)]
[tensor(-0.5040), 0.5483698556921432, 0.8657858136300417, tensor(2.2248)]
[tensor(-0.5040), 0.5483698556921432, 0.8657858136300417, tensor(2.2248)]
[tensor(-0.5040), 0.5483698556921432, 0.8657858136300417, tensor(2.2248)]
[tensor(-0.5040), 0.5483698556921432, 0.8657858136300417, tensor(2.2248)]
[tensor(-0.5040), 0.5483698556921432, 0.8657858136300417, tensor(2.2248)]
early stopping at 10
