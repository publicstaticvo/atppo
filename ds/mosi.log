[2023-01-15 13:44:56,328.328 dsw44922-6f76bf568-tbjcv:60785 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9856), 0.44269662921348313, tensor(0.2279)]
[tensor(-1.3027), 0.6404494382022472, tensor(1.8996)]
[tensor(-1.0814), 0.6966292134831461, tensor(2.4018)]
[tensor(-1.0814), 0.6966292134831461, tensor(2.4018)]
[tensor(-1.0814), 0.6966292134831461, tensor(2.4018)]
[tensor(-1.0814), 0.6966292134831461, tensor(2.4018)]
[tensor(-1.0814), 0.701123595505618, tensor(2.4018)]
[tensor(-1.0814), 0.7056179775280899, tensor(2.4018)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0814), 0.7056179775280899, tensor(2.4018)]
[tensor(-1.0814), 0.7056179775280899, tensor(2.4018)]
[2023-01-15 13:50:40,533.533 dsw44922-6f76bf568-tbjcv:60820 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5972), 0.5573033707865168, tensor(1.1893)]
[tensor(-1.1788), 0.6404494382022472, tensor(2.0234)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1126), 0.6808988764044944, tensor(2.2919)]
[tensor(-1.1126), 0.6808988764044944, tensor(2.2919)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1126), 0.6876404494382022, tensor(2.2919)]
[tensor(-1.1126), 0.6876404494382022, tensor(2.2919)]
[tensor(-1.1126), 0.6876404494382022, tensor(2.2919)]
[tensor(-1.1126), 0.6943820224719102, tensor(2.2919)]
[tensor(-1.1126), 0.701123595505618, tensor(2.2919)]
[tensor(-1.1126), 0.7033707865168539, tensor(2.2919)]
[2023-01-15 13:56:11,034.034 dsw44922-6f76bf568-tbjcv:60856 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8078), 0.48089887640449436, tensor(0.5967)]
[tensor(-1.5851), 0.5640449438202247, tensor(1.2351)]
[tensor(-1.2498), 0.6539325842696629, tensor(2.0199)]
[tensor(-1.2158), 0.6719101123595506, tensor(2.1437)]
[tensor(-1.1006), 0.6943820224719102, tensor(2.3713)]
[tensor(-1.0994), 0.698876404494382, tensor(2.3950)]
[tensor(-1.0994), 0.7078651685393258, tensor(2.4067)]
[tensor(-1.0994), 0.7078651685393258, tensor(2.4067)]
[tensor(-1.0994), 0.7078651685393258, tensor(2.4067)]
[tensor(-1.0994), 0.7078651685393258, tensor(2.4067)]
[tensor(-1.0994), 0.7078651685393258, tensor(2.4067)]
[tensor(-1.0994), 0.7078651685393258, tensor(2.4067)]
[tensor(-1.0994), 0.7101123595505618, tensor(2.4067)]
[tensor(-1.0994), 0.7101123595505618, tensor(2.4067)]
[tensor(-1.0994), 0.7101123595505618, tensor(2.4067)]
[tensor(-1.0994), 0.7101123595505618, tensor(2.4067)]
[tensor(-1.0994), 0.7123595505617978, tensor(2.4067)]
[tensor(-1.0994), 0.7191011235955056, tensor(2.4067)]
[tensor(-1.0994), 0.7191011235955056, tensor(2.4067)]
[tensor(-1.0994), 0.7191011235955056, tensor(2.4067)]
[tensor(-1.0994), 0.7191011235955056, tensor(2.4067)]
[tensor(-1.0994), 0.7191011235955056, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
[tensor(-1.0994), 0.7213483146067415, tensor(2.4067)]
early stopping at 33
[2023-01-15 14:13:55,611.611 dsw44922-6f76bf568-tbjcv:60909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.8758), 0.4651685393258427, tensor(0.4501)]
[tensor(-1.2399), 0.6651685393258427, tensor(2.0859)]
[tensor(-1.0228), 0.6921348314606741, tensor(2.4378)]
[tensor(-1.0228), 0.701123595505618, tensor(2.4378)]
[tensor(-1.0228), 0.7056179775280899, tensor(2.4380)]
[tensor(-1.0228), 0.7213483146067415, tensor(2.4962)]
[tensor(-1.0228), 0.7213483146067415, tensor(2.4962)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0228), 0.7213483146067415, tensor(2.4962)]
[tensor(-1.0228), 0.7213483146067415, tensor(2.4962)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0228), 0.7213483146067415, tensor(2.4962)]
[tensor(-1.0228), 0.7213483146067415, tensor(2.4962)]
[tensor(-1.0228), 0.7235955056179775, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
[tensor(-1.0228), 0.7348314606741573, tensor(2.4962)]
early stopping at 23
[2023-01-15 14:26:19,047.047 dsw44922-6f76bf568-tbjcv:60955 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9585), 0.27074235807860264, 0.8287037037037037, tensor(0.3952)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9585), 0.27074235807860264, 0.8287037037037037, tensor(0.3952)]
[tensor(-0.8020), 0.39737991266375544, 0.8703703703703703, tensor(1.1849)]
[tensor(-0.7573), 0.40611353711790393, 0.8703703703703703, tensor(1.2733)]
[tensor(-0.7573), 0.40611353711790393, 0.8703703703703703, tensor(1.2733)]
[2023-01-15 14:29:02,590.590 dsw44922-6f76bf568-tbjcv:60986 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1488), 0.2183406113537118, 0.6712962962962963, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9497), 0.35807860262008734, 0.75, tensor(0.8407)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.7895), 0.3799126637554585, 0.8657407407407407, tensor(1.1101)]
[tensor(-0.7329), 0.43231441048034935, 0.8796296296296297, tensor(1.4287)]
[tensor(-0.7026), 0.43231441048034935, 0.8796296296296297, tensor(1.4287)]
[2023-01-15 14:31:46,008.008 dsw44922-6f76bf568-tbjcv:61017 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.8756), 0.31004366812227074, 0.8564814814814815, tensor(0.6746)]
[tensor(-0.8549), 0.31004366812227074, 0.8564814814814815, tensor(0.6746)]
[tensor(-0.8222), 0.4192139737991266, 0.8564814814814815, tensor(1.2738)]
[tensor(-0.8174), 0.4192139737991266, 0.8564814814814815, tensor(1.2738)]
[tensor(-0.7591), 0.4192139737991266, 0.8564814814814815, tensor(1.2933)]
[tensor(-0.7591), 0.43231441048034935, 0.8564814814814815, tensor(1.4019)]
[tensor(-0.7591), 0.43231441048034935, 0.8564814814814815, tensor(1.4019)]
[tensor(-0.7463), 0.43231441048034935, 0.8564814814814815, tensor(1.4019)]
[tensor(-0.7228), 0.43231441048034935, 0.8657407407407407, tensor(1.4019)]
[tensor(-0.7228), 0.43231441048034935, 0.8703703703703703, tensor(1.4019)]
[tensor(-0.7228), 0.43231441048034935, 0.8703703703703703, tensor(1.4019)]
[tensor(-0.7110), 0.43231441048034935, 0.8703703703703703, tensor(1.4287)]
[tensor(-0.7110), 0.43231441048034935, 0.8796296296296297, tensor(1.4287)]
[tensor(-0.7110), 0.43231441048034935, 0.8796296296296297, tensor(1.4287)]
[tensor(-0.7029), 0.43231441048034935, 0.8796296296296297, tensor(1.4287)]
[tensor(-0.7029), 0.43231441048034935, 0.8796296296296297, tensor(1.4287)]
[tensor(-0.7029), 0.43231441048034935, 0.8796296296296297, tensor(1.4287)]
[tensor(-0.7029), 0.43231441048034935, 0.8842592592592593, tensor(1.4287)]
[tensor(-0.7005), 0.43231441048034935, 0.8842592592592593, tensor(1.4393)]
[tensor(-0.6977), 0.4366812227074236, 0.8842592592592593, tensor(1.4857)]
[tensor(-0.6977), 0.4366812227074236, 0.8935185185185185, tensor(1.4857)]
[tensor(-0.6977), 0.44541484716157204, 0.8935185185185185, tensor(1.5261)]
[tensor(-0.6977), 0.44541484716157204, 0.8935185185185185, tensor(1.5261)]
[tensor(-0.6789), 0.44541484716157204, 0.8935185185185185, tensor(1.5261)]
[tensor(-0.6751), 0.44541484716157204, 0.8935185185185185, tensor(1.5261)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.44541484716157204, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6719), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6687), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6687), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6687), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6687), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[tensor(-0.6687), 0.4497816593886463, 0.8935185185185185, tensor(1.5552)]
[2023-01-15 14:58:13,890.890 dsw44922-6f76bf568-tbjcv:61084 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0756), 0.3056768558951965, 0.7592592592592593, tensor(0.4528)]
[tensor(-0.8726), 0.388646288209607, 0.7870370370370371, tensor(1.0706)]
[tensor(-0.7937), 0.39737991266375544, 0.8425925925925926, tensor(1.1932)]
[tensor(-0.7446), 0.4017467248908297, 0.8425925925925926, tensor(1.2642)]
[tensor(-0.6958), 0.4497816593886463, 0.8564814814814815, tensor(1.5531)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.6958), 0.4497816593886463, 0.8564814814814815, tensor(1.5531)]
[tensor(-0.6958), 0.4497816593886463, 0.8611111111111112, tensor(1.5531)]
[tensor(-0.6958), 0.4497816593886463, 0.8611111111111112, tensor(1.5531)]
[tensor(-0.6958), 0.4497816593886463, 0.8611111111111112, tensor(1.5531)]
[tensor(-0.6958), 0.4497816593886463, 0.8611111111111112, tensor(1.5531)]
[tensor(-0.6958), 0.4672489082969432, 0.8657407407407407, tensor(1.6263)]
[tensor(-0.6958), 0.4672489082969432, 0.8657407407407407, tensor(1.6263)]
[tensor(-0.6958), 0.4672489082969432, 0.8657407407407407, tensor(1.6263)]
[tensor(-0.6834), 0.4672489082969432, 0.875, tensor(1.6263)]
[tensor(-0.6834), 0.4672489082969432, 0.875, tensor(1.6263)]
[tensor(-0.6834), 0.47161572052401746, 0.875, tensor(1.6419)]
[tensor(-0.6834), 0.47161572052401746, 0.875, tensor(1.6419)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
[tensor(-0.6834), 0.4890829694323144, 0.875, tensor(1.7490)]
early stopping at 32
[2023-01-15 15:15:02,402.402 dsw44922-6f76bf568-tbjcv:61136 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9062), 0.45393258426966293, tensor(0.3634)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3206), 0.6426966292134831, tensor(1.8929)]
[tensor(-1.1844), 0.6831460674157304, tensor(2.2313)]
[tensor(-1.1577), 0.6831460674157304, tensor(2.2580)]
[tensor(-1.1577), 0.6898876404494382, tensor(2.2614)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1577), 0.6898876404494382, tensor(2.2614)]
[tensor(-1.1577), 0.7033707865168539, tensor(2.2614)]
[tensor(-1.1577), 0.7033707865168539, tensor(2.2614)]
[tensor(-1.1577), 0.7146067415730337, tensor(2.3022)]
[tensor(-1.1577), 0.7146067415730337, tensor(2.3022)]
[2023-01-15 15:20:39,655.655 dsw44922-6f76bf568-tbjcv:61171 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7884), 0.46292134831460674, tensor(0.5262)]
[tensor(-1.2589), 0.6539325842696629, tensor(2.0108)]
[tensor(-1.2514), 0.6584269662921348, tensor(2.0407)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2514), 0.6584269662921348, tensor(2.0407)]
[tensor(-1.2514), 0.6741573033707865, tensor(2.1036)]
[tensor(-1.2514), 0.6741573033707865, tensor(2.1036)]
[tensor(-1.2514), 0.6741573033707865, tensor(2.1036)]
[tensor(-1.2514), 0.6786516853932584, tensor(2.1036)]
[tensor(-1.2514), 0.6786516853932584, tensor(2.1036)]
[tensor(-1.2514), 0.6786516853932584, tensor(2.1036)]
[2023-01-15 15:26:08,160.160 dsw44922-6f76bf568-tbjcv:61207 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6878), 0.4786516853932584, tensor(0.7055)]
[tensor(-1.4587), 0.6, tensor(1.5413)]
[tensor(-1.2549), 0.6382022471910113, tensor(1.9361)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2549), 0.6561797752808989, tensor(2.0035)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2096), 0.6719101123595506, tensor(2.1500)]
[tensor(-1.2096), 0.6764044943820224, tensor(2.1500)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2096), 0.6808988764044944, tensor(2.1500)]
[tensor(-1.2096), 0.6808988764044944, tensor(2.1500)]
[tensor(-1.2096), 0.6853932584269663, tensor(2.1500)]
[tensor(-1.2096), 0.6966292134831461, tensor(2.1500)]
[tensor(-1.2096), 0.6966292134831461, tensor(2.1500)]
[tensor(-1.2096), 0.698876404494382, tensor(2.1500)]
[tensor(-1.2096), 0.698876404494382, tensor(2.1500)]
[tensor(-1.2096), 0.698876404494382, tensor(2.1500)]
[tensor(-1.2096), 0.698876404494382, tensor(2.1500)]
[tensor(-1.2096), 0.698876404494382, tensor(2.1500)]
[tensor(-1.2096), 0.698876404494382, tensor(2.1500)]
[tensor(-1.2096), 0.698876404494382, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
[tensor(-1.2096), 0.7123595505617978, tensor(2.1500)]
early stopping at 42
[2023-01-15 15:49:02,885.885 dsw44922-6f76bf568-tbjcv:61270 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9552), 0.451685393258427, tensor(0.3032)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4259), 0.6112359550561798, tensor(1.6303)]
[tensor(-1.2311), 0.6584269662921348, tensor(2.0611)]
[tensor(-1.2311), 0.6719101123595506, tensor(2.0611)]
[tensor(-1.2311), 0.6719101123595506, tensor(2.1107)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2311), 0.6719101123595506, tensor(2.1107)]
[tensor(-1.2311), 0.6876404494382022, tensor(2.1107)]
[tensor(-1.2311), 0.6876404494382022, tensor(2.1107)]
[tensor(-1.2311), 0.6876404494382022, tensor(2.1107)]
[tensor(-1.2311), 0.6876404494382022, tensor(2.1107)]
[tensor(-1.2311), 0.6876404494382022, tensor(2.1107)]
[tensor(-1.2311), 0.6876404494382022, tensor(2.1107)]
[tensor(-1.2311), 0.6876404494382022, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
[tensor(-1.2311), 0.6898876404494382, tensor(2.1107)]
early stopping at 30
[2023-01-15 16:05:01,976.976 dsw44922-6f76bf568-tbjcv:61322 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.1778), 0.3231441048034934, 0.5740740740740741, tensor(0.4379)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1778), 0.3231441048034934, 0.6990740740740741, tensor(0.4379)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1778), 0.3231441048034934, 0.6990740740740741, tensor(0.4379)]
[tensor(-1.1778), 0.3231441048034934, 0.6990740740740741, tensor(0.4379)]
[tensor(-0.9943), 0.3231441048034934, 0.7916666666666666, tensor(0.6214)]
[2023-01-15 16:07:48,921.921 dsw44922-6f76bf568-tbjcv:61353 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9484), 0.34934497816593885, 0.8148148148148148, tensor(0.7984)]
[tensor(-0.8400), 0.3930131004366812, 0.8472222222222222, tensor(1.1251)]
[tensor(-0.8400), 0.3930131004366812, 0.8472222222222222, tensor(1.1251)]
[tensor(-0.8106), 0.3930131004366812, 0.8564814814814815, tensor(1.1544)]
[tensor(-0.8106), 0.4017467248908297, 0.8564814814814815, tensor(1.1962)]
[2023-01-15 16:10:39,733.733 dsw44922-6f76bf568-tbjcv:61384 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-0.9231), 0.34934497816593885, 0.8379629629629629, tensor(0.8236)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.8526), 0.35807860262008734, 0.8379629629629629, tensor(0.9378)]
[tensor(-0.7915), 0.4192139737991266, 0.8379629629629629, tensor(1.3046)]
[tensor(-0.7719), 0.4192139737991266, 0.8379629629629629, tensor(1.3046)]
[tensor(-0.7714), 0.4366812227074236, 0.8379629629629629, tensor(1.4120)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7620), 0.44541484716157204, 0.8379629629629629, tensor(1.4651)]
[tensor(-0.7620), 0.44541484716157204, 0.8379629629629629, tensor(1.4651)]
[tensor(-0.7420), 0.44541484716157204, 0.8379629629629629, tensor(1.4651)]
[tensor(-0.6928), 0.45414847161572053, 0.8472222222222222, tensor(1.5780)]
[tensor(-0.6906), 0.45414847161572053, 0.8472222222222222, tensor(1.5801)]
[tensor(-0.6906), 0.47161572052401746, 0.8472222222222222, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8472222222222222, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8472222222222222, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8472222222222222, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8472222222222222, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8518518518518519, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8518518518518519, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8518518518518519, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8518518518518519, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8518518518518519, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8518518518518519, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8611111111111112, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8611111111111112, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8611111111111112, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8611111111111112, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8611111111111112, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8611111111111112, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8611111111111112, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6906), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6432)]
[tensor(-0.6881), 0.47161572052401746, 0.8657407407407407, tensor(1.6569)]
[tensor(-0.6881), 0.49344978165938863, 0.8657407407407407, tensor(1.7761)]
[tensor(-0.6881), 0.49344978165938863, 0.8657407407407407, tensor(1.7761)]
[tensor(-0.6881), 0.49344978165938863, 0.8657407407407407, tensor(1.7761)]
[tensor(-0.6836), 0.49344978165938863, 0.8657407407407407, tensor(1.7761)]
[tensor(-0.6683), 0.49344978165938863, 0.8657407407407407, tensor(1.7761)]
[tensor(-0.6683), 0.49344978165938863, 0.8657407407407407, tensor(1.7761)]
[tensor(-0.6683), 0.49344978165938863, 0.8657407407407407, tensor(1.7761)]
[2023-01-15 16:37:05,560.560 dsw44922-6f76bf568-tbjcv:61451 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.3883), 0.21397379912663755, 0.6898148148148148, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3353), 0.2445414847161572, 0.6898148148148148, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0510), 0.34497816593886466, 0.7777777777777778, tensor(0.6739)]
[tensor(-0.9845), 0.34497816593886466, 0.7870370370370371, tensor(0.7186)]
[tensor(-0.9503), 0.34497816593886466, 0.7962962962962963, tensor(0.7310)]
[tensor(-0.9192), 0.34497816593886466, 0.8148148148148148, tensor(0.7838)]
[tensor(-0.8610), 0.4104803493449782, 0.8148148148148148, tensor(1.1914)]
[tensor(-0.8610), 0.4104803493449782, 0.8148148148148148, tensor(1.1914)]
[tensor(-0.8610), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8410), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8410), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8218), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8218), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8218), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8198), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8152), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8027), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8027), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8027), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8027), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8027), 0.4104803493449782, 0.8518518518518519, tensor(1.1914)]
[tensor(-0.8027), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2550)]
[tensor(-0.8020), 0.4148471615720524, 0.8564814814814815, tensor(1.2699)]
[tensor(-0.7934), 0.4148471615720524, 0.8564814814814815, tensor(1.2699)]
[tensor(-0.7934), 0.4148471615720524, 0.8564814814814815, tensor(1.2699)]
[tensor(-0.7934), 0.4148471615720524, 0.8564814814814815, tensor(1.2699)]
[tensor(-0.7934), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7934), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7838), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7838), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[tensor(-0.7815), 0.4192139737991266, 0.8564814814814815, tensor(1.3001)]
[2023-01-15 17:03:24,671.671 dsw44922-6f76bf568-tbjcv:61518 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9634), 0.44719101123595506, tensor(0.2726)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3594), 0.6224719101123596, tensor(1.7530)]
[tensor(-1.2162), 0.651685393258427, tensor(2.0422)]
[tensor(-1.1390), 0.6898876404494382, tensor(2.3104)]
[tensor(-1.1390), 0.6943820224719102, tensor(2.3104)]
[tensor(-1.1390), 0.7101123595505618, tensor(2.3354)]
[tensor(-1.1390), 0.7101123595505618, tensor(2.3354)]
[tensor(-1.1390), 0.7101123595505618, tensor(2.3354)]
[tensor(-1.1390), 0.7101123595505618, tensor(2.3354)]
[tensor(-1.1390), 0.7101123595505618, tensor(2.3354)]
[2023-01-15 17:09:09,387.387 dsw44922-6f76bf568-tbjcv:61553 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7349), 0.4898876404494382, tensor(0.7145)]
[tensor(-1.2175), 0.6426966292134831, tensor(1.9960)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1634), 0.6741573033707865, tensor(2.2074)]
[tensor(-1.1634), 0.6786516853932584, tensor(2.2074)]
[tensor(-1.1634), 0.6786516853932584, tensor(2.2074)]
[tensor(-1.1634), 0.6808988764044944, tensor(2.2074)]
[tensor(-1.1634), 0.6808988764044944, tensor(2.2074)]
[tensor(-1.1634), 0.6831460674157304, tensor(2.2074)]
[tensor(-1.1634), 0.6876404494382022, tensor(2.2074)]
[tensor(-1.1634), 0.6921348314606741, tensor(2.2074)]
[2023-01-15 17:14:41,716.716 dsw44922-6f76bf568-tbjcv:61588 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8002), 0.4584269662921348, tensor(0.4920)]
[tensor(-1.4938), 0.604494382022472, tensor(1.5287)]
[tensor(-1.2278), 0.6561797752808989, tensor(2.0531)]
[tensor(-1.2179), 0.6696629213483146, tensor(2.1304)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2064), 0.6898876404494382, tensor(2.2430)]
[tensor(-1.2064), 0.6898876404494382, tensor(2.2430)]
[tensor(-1.2064), 0.6898876404494382, tensor(2.2430)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2064), 0.6898876404494382, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
[tensor(-1.2064), 0.6943820224719102, tensor(2.2430)]
early stopping at 19
[2023-01-15 17:25:08,662.662 dsw44922-6f76bf568-tbjcv:61631 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1390), 0.42247191011235957, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3627), 0.6157303370786517, tensor(1.7159)]
[tensor(-1.1812), 0.6584269662921348, tensor(2.1109)]
[tensor(-1.1812), 0.7078651685393258, tensor(2.3383)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
[tensor(-1.1770), 0.7078651685393258, tensor(2.3399)]
early stopping at 15
[2023-01-15 17:33:15,994.994 dsw44922-6f76bf568-tbjcv:61670 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-0.9200), 0.3406113537117904, 0.8333333333333334, tensor(0.7831)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.8291), 0.37554585152838427, 0.8564814814814815, tensor(1.0486)]
[tensor(-0.8291), 0.37554585152838427, 0.8564814814814815, tensor(1.0486)]
[tensor(-0.8148), 0.3799126637554585, 0.8564814814814815, tensor(1.0847)]
[tensor(-0.7389), 0.4148471615720524, 0.8657407407407407, tensor(1.3354)]
[2023-01-15 17:36:04,346.346 dsw44922-6f76bf568-tbjcv:61701 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4194), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-0.9238), 0.3406113537117904, 0.8240740740740741, tensor(0.7792)]
[tensor(-0.8652), 0.35807860262008734, 0.8333333333333334, tensor(0.9252)]
[tensor(-0.8652), 0.35807860262008734, 0.8333333333333334, tensor(0.9252)]
[tensor(-0.7638), 0.4279475982532751, 0.8611111111111112, tensor(1.3759)]
[2023-01-15 17:38:48,668.668 dsw44922-6f76bf568-tbjcv:61733 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8590), 0.33624454148471616, 0.8240740740740741, tensor(0.8222)]
[tensor(-0.8121), 0.37117903930131, 0.8287037037037037, tensor(1.0438)]
[tensor(-0.8121), 0.388646288209607, 0.8287037037037037, tensor(1.1120)]
[tensor(-0.7299), 0.4279475982532751, 0.8657407407407407, tensor(1.4098)]
[tensor(-0.6983), 0.4279475982532751, 0.8703703703703703, tensor(1.4414)]
[tensor(-0.6949), 0.4279475982532751, 0.875, tensor(1.4449)]
[tensor(-0.6911), 0.4279475982532751, 0.8842592592592593, tensor(1.4449)]
[tensor(-0.6530), 0.43231441048034935, 0.8842592592592593, tensor(1.5086)]
[tensor(-0.6530), 0.43231441048034935, 0.8842592592592593, tensor(1.5086)]
[tensor(-0.6530), 0.43231441048034935, 0.8842592592592593, tensor(1.5086)]
[tensor(-0.6530), 0.4410480349344978, 0.8842592592592593, tensor(1.5374)]
[tensor(-0.6530), 0.4410480349344978, 0.8842592592592593, tensor(1.5374)]
[tensor(-0.6530), 0.4410480349344978, 0.8842592592592593, tensor(1.5374)]
[tensor(-0.6530), 0.4410480349344978, 0.8842592592592593, tensor(1.5374)]
[tensor(-0.6530), 0.4410480349344978, 0.8842592592592593, tensor(1.5374)]
[tensor(-0.6530), 0.4585152838427948, 0.8842592592592593, tensor(1.6227)]
[tensor(-0.6530), 0.4585152838427948, 0.8842592592592593, tensor(1.6256)]
[tensor(-0.6530), 0.4585152838427948, 0.8842592592592593, tensor(1.6256)]
[tensor(-0.6530), 0.4585152838427948, 0.8842592592592593, tensor(1.6256)]
[tensor(-0.6530), 0.4585152838427948, 0.8842592592592593, tensor(1.6256)]
[tensor(-0.6530), 0.4585152838427948, 0.8842592592592593, tensor(1.6256)]
[tensor(-0.6530), 0.4672489082969432, 0.8842592592592593, tensor(1.6623)]
[tensor(-0.6530), 0.4672489082969432, 0.8842592592592593, tensor(1.6623)]
[tensor(-0.6530), 0.4672489082969432, 0.8842592592592593, tensor(1.6623)]
[tensor(-0.6530), 0.4672489082969432, 0.8842592592592593, tensor(1.6623)]
[tensor(-0.6530), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6530), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6530), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4672489082969432, 0.8842592592592593, tensor(1.6815)]
[tensor(-0.6482), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6482), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6482), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6390), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6390), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6390), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6312), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6312), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6312), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6312), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6312), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6310), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[tensor(-0.6310), 0.4890829694323144, 0.8842592592592593, tensor(1.7949)]
[2023-01-15 18:05:13,062.062 dsw44922-6f76bf568-tbjcv:61799 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0337), 0.314410480349345, 0.8055555555555556, tensor(0.5383)]
[tensor(-0.8756), 0.37554585152838427, 0.8240740740740741, tensor(1.0021)]
[tensor(-0.8201), 0.4148471615720524, 0.8425925925925926, tensor(1.2542)]
[tensor(-0.7855), 0.4148471615720524, 0.8657407407407407, tensor(1.2542)]
[tensor(-0.7838), 0.4148471615720524, 0.8657407407407407, tensor(1.2542)]
[tensor(-0.7650), 0.4148471615720524, 0.8703703703703703, tensor(1.3092)]
[tensor(-0.7201), 0.43231441048034935, 0.8796296296296297, tensor(1.4415)]
[tensor(-0.7201), 0.43231441048034935, 0.8796296296296297, tensor(1.4415)]
[tensor(-0.7201), 0.43231441048034935, 0.8796296296296297, tensor(1.4415)]
[tensor(-0.7201), 0.43231441048034935, 0.8796296296296297, tensor(1.4415)]
[tensor(-0.7201), 0.43231441048034935, 0.8796296296296297, tensor(1.4415)]
[tensor(-0.7186), 0.43231441048034935, 0.8796296296296297, tensor(1.4415)]
[tensor(-0.7186), 0.43231441048034935, 0.8888888888888888, tensor(1.4415)]
[tensor(-0.7186), 0.43231441048034935, 0.8888888888888888, tensor(1.4415)]
[tensor(-0.7186), 0.43231441048034935, 0.8888888888888888, tensor(1.4415)]
[tensor(-0.7186), 0.43231441048034935, 0.8888888888888888, tensor(1.4426)]
[tensor(-0.7186), 0.43231441048034935, 0.8888888888888888, tensor(1.4426)]
[tensor(-0.6956), 0.45414847161572053, 0.8888888888888888, tensor(1.5752)]
[tensor(-0.6956), 0.45414847161572053, 0.8888888888888888, tensor(1.5752)]
[tensor(-0.6956), 0.4672489082969432, 0.8888888888888888, tensor(1.6329)]
[tensor(-0.6956), 0.4672489082969432, 0.8888888888888888, tensor(1.6329)]
[tensor(-0.6827), 0.4672489082969432, 0.8888888888888888, tensor(1.6329)]
[tensor(-0.6756), 0.4672489082969432, 0.8888888888888888, tensor(1.6329)]
[tensor(-0.6756), 0.47161572052401746, 0.8888888888888888, tensor(1.6763)]
[tensor(-0.6732), 0.47161572052401746, 0.8888888888888888, tensor(1.6849)]
[tensor(-0.6536), 0.47161572052401746, 0.8888888888888888, tensor(1.7045)]
[tensor(-0.6501), 0.47161572052401746, 0.8888888888888888, tensor(1.7045)]
[tensor(-0.6501), 0.47161572052401746, 0.8888888888888888, tensor(1.7045)]
[tensor(-0.6501), 0.48034934497816595, 0.8888888888888888, tensor(1.7466)]
[tensor(-0.6501), 0.48034934497816595, 0.8888888888888888, tensor(1.7466)]
[tensor(-0.6501), 0.48034934497816595, 0.8888888888888888, tensor(1.7466)]
[tensor(-0.6501), 0.48034934497816595, 0.8888888888888888, tensor(1.7466)]
[tensor(-0.6501), 0.48034934497816595, 0.8888888888888888, tensor(1.7466)]
[tensor(-0.6501), 0.4847161572052402, 0.8888888888888888, tensor(1.7685)]
[tensor(-0.6471), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6459), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6443), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6443), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6443), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[tensor(-0.6443), 0.49344978165938863, 0.8888888888888888, tensor(1.8202)]
[2023-01-15 18:31:23,322.322 dsw44922-6f76bf568-tbjcv:61866 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9611), 0.4606741573033708, tensor(0.3423)]
[tensor(-1.3055), 0.6337078651685393, tensor(1.8630)]
[tensor(-1.1616), 0.6786516853932584, tensor(2.2317)]
[tensor(-1.1144), 0.6853932584269663, tensor(2.3126)]
[tensor(-1.1144), 0.6853932584269663, tensor(2.3126)]
[tensor(-1.1144), 0.6876404494382022, tensor(2.3126)]
[tensor(-1.1144), 0.7078651685393258, tensor(2.3493)]
[tensor(-1.1144), 0.7078651685393258, tensor(2.3493)]
[tensor(-1.1144), 0.7146067415730337, tensor(2.3493)]
[tensor(-1.1144), 0.7146067415730337, tensor(2.3493)]
[2023-01-15 18:37:01,573.573 dsw44922-6f76bf568-tbjcv:61901 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6900), 0.4966292134831461, tensor(0.7931)]
[tensor(-1.1580), 0.6808988764044944, tensor(2.2465)]
[tensor(-1.1286), 0.6831460674157304, tensor(2.2871)]
[tensor(-1.1286), 0.6943820224719102, tensor(2.2871)]
[tensor(-1.1286), 0.6943820224719102, tensor(2.2871)]
[tensor(-1.1286), 0.6943820224719102, tensor(2.2871)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1286), 0.6943820224719102, tensor(2.2871)]
[tensor(-1.1286), 0.6966292134831461, tensor(2.2871)]
[tensor(-1.1286), 0.6966292134831461, tensor(2.2871)]
[tensor(-1.1286), 0.7213483146067415, tensor(2.3328)]
[2023-01-15 18:42:51,803.803 dsw44922-6f76bf568-tbjcv:61997 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8112), 0.4853932584269663, tensor(0.6158)]
[tensor(-1.5543), 0.5662921348314607, tensor(1.2771)]
[tensor(-1.2316), 0.6561797752808989, tensor(2.0493)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2316), 0.6561797752808989, tensor(2.0493)]
[tensor(-1.1321), 0.6966292134831461, tensor(2.3510)]
[tensor(-1.1321), 0.698876404494382, tensor(2.3583)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1321), 0.701123595505618, tensor(2.3583)]
[tensor(-1.1321), 0.701123595505618, tensor(2.3583)]
[tensor(-1.1321), 0.7056179775280899, tensor(2.3583)]
[tensor(-1.1321), 0.7056179775280899, tensor(2.3583)]
[tensor(-1.1321), 0.7056179775280899, tensor(2.3583)]
[tensor(-1.1321), 0.7056179775280899, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
[tensor(-1.1321), 0.7078651685393258, tensor(2.3583)]
early stopping at 23
[2023-01-15 18:55:34,467.467 dsw44922-6f76bf568-tbjcv:62142 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.9730), 0.4404494382022472, tensor(0.2292)]
[tensor(-1.3362), 0.6157303370786517, tensor(1.7425)]
[tensor(-1.1632), 0.6629213483146067, tensor(2.1514)]
[tensor(-1.1632), 0.6629213483146067, tensor(2.1514)]
[tensor(-1.1632), 0.6898876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.6898876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.6898876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.6898876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.6898876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.698876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.698876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.698876404494382, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
[tensor(-1.1632), 0.701123595505618, tensor(2.2854)]
early stopping at 23
[2023-01-15 19:07:54,688.688 dsw44922-6f76bf568-tbjcv:62187 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1392), 0.2838427947598253, 0.7546296296296297, tensor(0.2800)]
[tensor(-0.8881), 0.3537117903930131, 0.8425925925925926, tensor(0.8805)]
[tensor(-0.8881), 0.37117903930131, 0.8611111111111112, tensor(0.9394)]
[tensor(-0.8321), 0.37117903930131, 0.8703703703703703, tensor(0.9802)]
[tensor(-0.6855), 0.43231441048034935, 0.8842592592592593, tensor(1.4761)]
[2023-01-15 19:10:38,188.188 dsw44922-6f76bf568-tbjcv:62218 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9119), 0.31004366812227074, 0.8333333333333334, tensor(0.6383)]
[tensor(-0.8852), 0.3537117903930131, 0.8333333333333334, tensor(0.8834)]
[tensor(-0.7545), 0.388646288209607, 0.8842592592592593, tensor(1.1888)]
[tensor(-0.6559), 0.43231441048034935, 0.8842592592592593, tensor(1.5057)]
[tensor(-0.6559), 0.43231441048034935, 0.8842592592592593, tensor(1.5057)]
[2023-01-15 19:13:27,399.399 dsw44922-6f76bf568-tbjcv:62250 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9820), 0.34934497816593885, 0.8148148148148148, tensor(0.7647)]
[tensor(-0.8798), 0.37117903930131, 0.8287037037037037, tensor(0.9761)]
[tensor(-0.8785), 0.37117903930131, 0.8333333333333334, tensor(0.9761)]
[tensor(-0.8322), 0.37117903930131, 0.8472222222222222, tensor(1.0019)]
[tensor(-0.7578), 0.42358078602620086, 0.8472222222222222, tensor(1.3601)]
[tensor(-0.7514), 0.42358078602620086, 0.8472222222222222, tensor(1.3601)]
[tensor(-0.7384), 0.43231441048034935, 0.8472222222222222, tensor(1.4232)]
[tensor(-0.7384), 0.43231441048034935, 0.8472222222222222, tensor(1.4232)]
[tensor(-0.7173), 0.43231441048034935, 0.8657407407407407, tensor(1.4232)]
[tensor(-0.7149), 0.4497816593886463, 0.8657407407407407, tensor(1.5340)]
[tensor(-0.7149), 0.4497816593886463, 0.8657407407407407, tensor(1.5340)]
[tensor(-0.7149), 0.4497816593886463, 0.8657407407407407, tensor(1.5340)]
[tensor(-0.7108), 0.4497816593886463, 0.875, tensor(1.5340)]
[tensor(-0.7102), 0.4497816593886463, 0.875, tensor(1.5340)]
[tensor(-0.7011), 0.4497816593886463, 0.875, tensor(1.5340)]
[tensor(-0.6966), 0.4497816593886463, 0.8796296296296297, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.4497816593886463, 0.8842592592592593, tensor(1.5340)]
[tensor(-0.6939), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6939), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6939), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6939), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6939), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6939), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5595)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5699)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5709)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5709)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5709)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5709)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5709)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5709)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5709)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[tensor(-0.6935), 0.45414847161572053, 0.8842592592592593, tensor(1.5759)]
[2023-01-15 19:39:54,289.289 dsw44922-6f76bf568-tbjcv:62316 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9980), 0.3056768558951965, 0.8055555555555556, tensor(0.5304)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9200), 0.31877729257641924, 0.8240740740740741, tensor(0.6739)]
[tensor(-0.8008), 0.40611353711790393, 0.8240740740740741, tensor(1.2297)]
[tensor(-0.7814), 0.4104803493449782, 0.8240740740740741, tensor(1.2710)]
[tensor(-0.7737), 0.4104803493449782, 0.8240740740740741, tensor(1.2710)]
[tensor(-0.7737), 0.4148471615720524, 0.8333333333333334, tensor(1.2710)]
[tensor(-0.7737), 0.4148471615720524, 0.8333333333333334, tensor(1.2710)]
[tensor(-0.7517), 0.4192139737991266, 0.8333333333333334, tensor(1.3443)]
[tensor(-0.7517), 0.4192139737991266, 0.8472222222222222, tensor(1.3443)]
[tensor(-0.7409), 0.4410480349344978, 0.8472222222222222, tensor(1.4643)]
[tensor(-0.7409), 0.4410480349344978, 0.8472222222222222, tensor(1.4643)]
[tensor(-0.7266), 0.4410480349344978, 0.8472222222222222, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
[tensor(-0.7226), 0.4410480349344978, 0.8518518518518519, tensor(1.4643)]
early stopping at 23
[2023-01-15 19:52:01,384.384 dsw44922-6f76bf568-tbjcv:62361 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9303), 0.42696629213483145, tensor(0.2046)]
[tensor(-1.3170), 0.6224719101123596, tensor(1.7953)]
[tensor(-1.1900), 0.6561797752808989, tensor(2.0909)]
[tensor(-1.1035), 0.6876404494382022, tensor(2.3347)]
[tensor(-1.1035), 0.698876404494382, tensor(2.3393)]
[tensor(-1.1035), 0.698876404494382, tensor(2.3393)]
[tensor(-1.1035), 0.7033707865168539, tensor(2.3393)]
[tensor(-1.1035), 0.7033707865168539, tensor(2.3393)]
[tensor(-1.1035), 0.7101123595505618, tensor(2.3393)]
[tensor(-1.1035), 0.7101123595505618, tensor(2.3393)]
[2023-01-15 19:57:40,070.070 dsw44922-6f76bf568-tbjcv:62397 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6295), 0.5258426966292135, tensor(0.9997)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2061), 0.6629213483146067, tensor(2.1085)]
[tensor(-1.1589), 0.6741573033707865, tensor(2.2119)]
[tensor(-1.1589), 0.6921348314606741, tensor(2.2781)]
[tensor(-1.1501), 0.7191011235955056, tensor(2.4454)]
[tensor(-1.1501), 0.7191011235955056, tensor(2.4454)]
[tensor(-1.1501), 0.7191011235955056, tensor(2.4454)]
[tensor(-1.1501), 0.7191011235955056, tensor(2.4454)]
[tensor(-1.1501), 0.7191011235955056, tensor(2.4454)]
[tensor(-1.1501), 0.7191011235955056, tensor(2.4454)]
early stopping at 10
[2023-01-15 20:03:12,084.084 dsw44922-6f76bf568-tbjcv:62432 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8282), 0.451685393258427, tensor(0.4302)]
[tensor(-1.5546), 0.5887640449438202, tensor(1.3893)]
[tensor(-1.2282), 0.6584269662921348, tensor(2.0640)]
[tensor(-1.2282), 0.6584269662921348, tensor(2.0640)]
[tensor(-1.2241), 0.6741573033707865, tensor(2.1467)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2212), 0.6786516853932584, tensor(2.1720)]
[tensor(-1.2212), 0.6786516853932584, tensor(2.1720)]
[tensor(-1.2212), 0.6898876404494382, tensor(2.1720)]
[tensor(-1.2212), 0.6943820224719102, tensor(2.1720)]
[tensor(-1.2212), 0.6943820224719102, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.6966292134831461, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
[tensor(-1.2212), 0.7056179775280899, tensor(2.1720)]
early stopping at 34
[2023-01-15 20:21:53,084.084 dsw44922-6f76bf568-tbjcv:62487 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7998), 0.4584269662921348, tensor(0.4923)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2713), 0.6426966292134831, tensor(1.9422)]
[tensor(-1.1298), 0.6831460674157304, tensor(2.2859)]
[tensor(-1.1298), 0.6831460674157304, tensor(2.2859)]
[tensor(-1.1298), 0.6831460674157304, tensor(2.2859)]
[tensor(-1.1298), 0.6876404494382022, tensor(2.2859)]
[tensor(-1.1298), 0.6966292134831461, tensor(2.2859)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1298), 0.6966292134831461, tensor(2.2859)]
[tensor(-1.1298), 0.6966292134831461, tensor(2.2859)]
[tensor(-1.1298), 0.701123595505618, tensor(2.2859)]
[tensor(-1.1298), 0.701123595505618, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7056179775280899, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7123595505617978, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
[tensor(-1.1298), 0.7191011235955056, tensor(2.2859)]
early stopping at 45
[2023-01-15 20:46:10,491.491 dsw44922-6f76bf568-tbjcv:62551 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0900), 0.2183406113537118, 0.7685185185185185, tensor(0.0017)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0177), 0.25327510917030566, 0.8240740740740741, tensor(0.2487)]
[tensor(-1.0177), 0.3537117903930131, 0.8240740740740741, tensor(0.7222)]
[tensor(-0.8268), 0.35807860262008734, 0.8425925925925926, tensor(0.9636)]
[tensor(-0.7268), 0.40611353711790393, 0.8425925925925926, tensor(1.3037)]
[2023-01-15 20:48:59,457.457 dsw44922-6f76bf568-tbjcv:62582 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8640), 0.37554585152838427, 0.8240740740740741, tensor(1.0137)]
[tensor(-0.7724), 0.4104803493449782, 0.8287037037037037, tensor(1.2800)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7724), 0.4104803493449782, 0.8287037037037037, tensor(1.2800)]
[tensor(-0.7724), 0.4104803493449782, 0.8287037037037037, tensor(1.2800)]
[tensor(-0.7153), 0.44541484716157204, 0.875, tensor(1.5118)]
[2023-01-15 20:51:46,956.956 dsw44922-6f76bf568-tbjcv:62613 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.0376), 0.3056768558951965, 0.8148148148148148, tensor(0.4908)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.8272), 0.35807860262008734, 0.8564814814814815, tensor(0.9632)]
[tensor(-0.7763), 0.35807860262008734, 0.8657407407407407, tensor(0.9923)]
[tensor(-0.7763), 0.388646288209607, 0.8657407407407407, tensor(1.1537)]
[tensor(-0.7653), 0.4017467248908297, 0.8703703703703703, tensor(1.2435)]
[tensor(-0.7585), 0.4017467248908297, 0.8703703703703703, tensor(1.2435)]
[tensor(-0.7529), 0.40611353711790393, 0.8703703703703703, tensor(1.2777)]
[tensor(-0.7529), 0.40611353711790393, 0.8703703703703703, tensor(1.2777)]
[tensor(-0.7057), 0.40611353711790393, 0.8703703703703703, tensor(1.2777)]
[tensor(-0.7042), 0.40611353711790393, 0.8703703703703703, tensor(1.2827)]
[tensor(-0.7042), 0.40611353711790393, 0.8703703703703703, tensor(1.3096)]
[tensor(-0.6964), 0.4148471615720524, 0.8703703703703703, tensor(1.3778)]
[tensor(-0.6964), 0.42358078602620086, 0.8703703703703703, tensor(1.4181)]
[tensor(-0.6950), 0.42358078602620086, 0.875, tensor(1.4181)]
[tensor(-0.6950), 0.43231441048034935, 0.875, tensor(1.4657)]
[tensor(-0.6950), 0.43231441048034935, 0.875, tensor(1.4657)]
[tensor(-0.6950), 0.43231441048034935, 0.8796296296296297, tensor(1.4657)]
[tensor(-0.6894), 0.4366812227074236, 0.8796296296296297, tensor(1.4940)]
[tensor(-0.6778), 0.4366812227074236, 0.8796296296296297, tensor(1.5056)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5186)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5186)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5186)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5186)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5186)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5188)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5188)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5188)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5188)]
[tensor(-0.6778), 0.4410480349344978, 0.8796296296296297, tensor(1.5188)]
[tensor(-0.6778), 0.44541484716157204, 0.8796296296296297, tensor(1.5468)]
[tensor(-0.6778), 0.4497816593886463, 0.8842592592592593, tensor(1.5631)]
[tensor(-0.6778), 0.4497816593886463, 0.8842592592592593, tensor(1.5631)]
[tensor(-0.6778), 0.4497816593886463, 0.8842592592592593, tensor(1.5631)]
[tensor(-0.6778), 0.4497816593886463, 0.8842592592592593, tensor(1.5631)]
[tensor(-0.6778), 0.45414847161572053, 0.8842592592592593, tensor(1.5769)]
[tensor(-0.6778), 0.45414847161572053, 0.8842592592592593, tensor(1.5769)]
[tensor(-0.6778), 0.45414847161572053, 0.8842592592592593, tensor(1.5769)]
[tensor(-0.6778), 0.45414847161572053, 0.8842592592592593, tensor(1.5769)]
[tensor(-0.6778), 0.45414847161572053, 0.8842592592592593, tensor(1.5803)]
[tensor(-0.6778), 0.45414847161572053, 0.8842592592592593, tensor(1.5803)]
[tensor(-0.6778), 0.4672489082969432, 0.8842592592592593, tensor(1.6544)]
[tensor(-0.6778), 0.4672489082969432, 0.8842592592592593, tensor(1.6544)]
[tensor(-0.6778), 0.4672489082969432, 0.8842592592592593, tensor(1.6544)]
[tensor(-0.6778), 0.47161572052401746, 0.8842592592592593, tensor(1.6801)]
[tensor(-0.6778), 0.47161572052401746, 0.8842592592592593, tensor(1.6801)]
[tensor(-0.6778), 0.47161572052401746, 0.8842592592592593, tensor(1.6801)]
[tensor(-0.6778), 0.47161572052401746, 0.8842592592592593, tensor(1.6801)]
[tensor(-0.6778), 0.47161572052401746, 0.8842592592592593, tensor(1.6801)]
[tensor(-0.6778), 0.47161572052401746, 0.8842592592592593, tensor(1.6801)]
[tensor(-0.6778), 0.47161572052401746, 0.8842592592592593, tensor(1.6801)]
[2023-01-15 21:18:13,084.084 dsw44922-6f76bf568-tbjcv:62680 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1639), 0.2183406113537118, 0.7685185185185185, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9750), 0.3318777292576419, 0.8148148148148148, tensor(0.6844)]
[tensor(-0.9407), 0.37117903930131, 0.8194444444444444, tensor(0.9152)]
[tensor(-0.8454), 0.388646288209607, 0.8472222222222222, tensor(1.0978)]
[tensor(-0.8378), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.8378), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.8378), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.8132), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7929), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7854), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7854), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7854), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7854), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7854), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7854), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8472222222222222, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8518518518518519, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8518518518518519, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8518518518518519, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8518518518518519, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7665), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7640), 0.4497816593886463, 0.8564814814814815, tensor(1.4111)]
[tensor(-0.7502), 0.4497816593886463, 0.8564814814814815, tensor(1.4332)]
[tensor(-0.7502), 0.4497816593886463, 0.8564814814814815, tensor(1.4882)]
[tensor(-0.7502), 0.4497816593886463, 0.8564814814814815, tensor(1.4882)]
[tensor(-0.7502), 0.4497816593886463, 0.8564814814814815, tensor(1.4882)]
[tensor(-0.7502), 0.4497816593886463, 0.8564814814814815, tensor(1.4913)]
[tensor(-0.7321), 0.4497816593886463, 0.8611111111111112, tensor(1.4949)]
[tensor(-0.7321), 0.4497816593886463, 0.8611111111111112, tensor(1.4949)]
[tensor(-0.7321), 0.4497816593886463, 0.8611111111111112, tensor(1.4949)]
[tensor(-0.7321), 0.4497816593886463, 0.8703703703703703, tensor(1.4949)]
[tensor(-0.7321), 0.4497816593886463, 0.8703703703703703, tensor(1.4949)]
[tensor(-0.7321), 0.4497816593886463, 0.8703703703703703, tensor(1.4949)]
[tensor(-0.7321), 0.45414847161572053, 0.8703703703703703, tensor(1.5084)]
[tensor(-0.7321), 0.45414847161572053, 0.8703703703703703, tensor(1.5084)]
[tensor(-0.7321), 0.45414847161572053, 0.8703703703703703, tensor(1.5084)]
[tensor(-0.7321), 0.45414847161572053, 0.8703703703703703, tensor(1.5084)]
[tensor(-0.7321), 0.45414847161572053, 0.8703703703703703, tensor(1.5084)]
[tensor(-0.7321), 0.45414847161572053, 0.8703703703703703, tensor(1.5084)]
[tensor(-0.7321), 0.462882096069869, 0.8703703703703703, tensor(1.5687)]
[tensor(-0.7321), 0.462882096069869, 0.8703703703703703, tensor(1.5687)]
[tensor(-0.7321), 0.462882096069869, 0.8703703703703703, tensor(1.5687)]
[2023-01-15 21:44:28,690.690 dsw44922-6f76bf568-tbjcv:62747 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8975), 0.449438202247191, tensor(0.3497)]
[tensor(-1.2920), 0.6629213483146067, tensor(2.0227)]
[tensor(-1.1825), 0.6674157303370787, tensor(2.1546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0978), 0.6921348314606741, tensor(2.3629)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0978), 0.6921348314606741, tensor(2.3629)]
[tensor(-1.0978), 0.6921348314606741, tensor(2.3629)]
[tensor(-1.0978), 0.698876404494382, tensor(2.3629)]
[tensor(-1.0978), 0.7056179775280899, tensor(2.3629)]
[tensor(-1.0978), 0.7056179775280899, tensor(2.3629)]
[tensor(-1.0978), 0.7101123595505618, tensor(2.3629)]
[2023-01-15 21:50:07,658.658 dsw44922-6f76bf568-tbjcv:62787 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7492), 0.49887640449438203, tensor(0.7452)]
[tensor(-1.2358), 0.6449438202247191, tensor(1.9889)]
[tensor(-1.1287), 0.6808988764044944, tensor(2.2758)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1287), 0.6808988764044944, tensor(2.2758)]
[tensor(-1.1287), 0.7033707865168539, tensor(2.3669)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1287), 0.7033707865168539, tensor(2.3669)]
[tensor(-1.1287), 0.7078651685393258, tensor(2.3669)]
[tensor(-1.1287), 0.7078651685393258, tensor(2.3669)]
[tensor(-1.1287), 0.7146067415730337, tensor(2.3669)]
[tensor(-1.1287), 0.7146067415730337, tensor(2.3669)]
[2023-01-15 21:55:50,964.964 dsw44922-6f76bf568-tbjcv:62832 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9273), 0.44269662921348313, tensor(0.2862)]
[tensor(-1.6430), 0.5528089887640449, tensor(1.1211)]
[tensor(-1.4430), 0.5910112359550562, tensor(1.5120)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4079), 0.6314606741573033, tensor(1.7494)]
[tensor(-1.2896), 0.6719101123595506, tensor(2.0699)]
[tensor(-1.2896), 0.6719101123595506, tensor(2.0699)]
[tensor(-1.2896), 0.6741573033707865, tensor(2.0699)]
[tensor(-1.2896), 0.6786516853932584, tensor(2.0699)]
[tensor(-1.2896), 0.6943820224719102, tensor(2.1446)]
[tensor(-1.2896), 0.6943820224719102, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
[tensor(-1.2896), 0.698876404494382, tensor(2.1446)]
early stopping at 21
[2023-01-15 22:07:23,621.621 dsw44922-6f76bf568-tbjcv:62877 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8913), 0.44269662921348313, tensor(0.3222)]
[tensor(-1.2303), 0.6561797752808989, tensor(2.0506)]
[tensor(-1.1612), 0.6674157303370787, tensor(2.1758)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1612), 0.6674157303370787, tensor(2.1758)]
[tensor(-1.1612), 0.6764044943820224, tensor(2.1758)]
[tensor(-1.1612), 0.6764044943820224, tensor(2.1758)]
[tensor(-1.1612), 0.6921348314606741, tensor(2.1758)]
[tensor(-1.1612), 0.6921348314606741, tensor(2.1758)]
[tensor(-1.1612), 0.6921348314606741, tensor(2.1758)]
[tensor(-1.1612), 0.6943820224719102, tensor(2.1758)]
[tensor(-1.1612), 0.6943820224719102, tensor(2.1758)]
[tensor(-1.1612), 0.6943820224719102, tensor(2.1758)]
[tensor(-1.1612), 0.6943820224719102, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
[tensor(-1.1612), 0.701123595505618, tensor(2.1758)]
early stopping at 24
[2023-01-15 22:20:37,302.302 dsw44922-6f76bf568-tbjcv:62923 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9610), 0.30131004366812225, 0.8055555555555556, tensor(0.5456)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9176), 0.3624454148471616, 0.8425925925925926, tensor(0.8946)]
[tensor(-0.8969), 0.3624454148471616, 0.8425925925925926, tensor(0.8946)]
[tensor(-0.8191), 0.37554585152838427, 0.8472222222222222, tensor(1.0586)]
[tensor(-0.7657), 0.39737991266375544, 0.8518518518518519, tensor(1.2212)]
[2023-01-15 22:23:45,412.412 dsw44922-6f76bf568-tbjcv:62955 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0091), 0.3406113537117904, 0.8240740740740741, tensor(0.6940)]
[tensor(-0.8249), 0.38427947598253276, 0.8425925925925926, tensor(1.0965)]
[tensor(-0.8249), 0.38427947598253276, 0.8425925925925926, tensor(1.0965)]
[tensor(-0.7654), 0.42358078602620086, 0.8472222222222222, tensor(1.3525)]
[tensor(-0.7203), 0.4672489082969432, 0.8611111111111112, tensor(1.6160)]
[2023-01-15 22:26:33,473.473 dsw44922-6f76bf568-tbjcv:62986 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9101), 0.3624454148471616, 0.8194444444444444, tensor(0.9021)]
[tensor(-0.8261), 0.36681222707423583, 0.8425925925925926, tensor(1.0079)]
[tensor(-0.8261), 0.4104803493449782, 0.8425925925925926, tensor(1.1756)]
[tensor(-0.7790), 0.4104803493449782, 0.8796296296296297, tensor(1.1756)]
[tensor(-0.7311), 0.4672489082969432, 0.8796296296296297, tensor(1.6052)]
[tensor(-0.7222), 0.4672489082969432, 0.8796296296296297, tensor(1.6052)]
[tensor(-0.7222), 0.4672489082969432, 0.8796296296296297, tensor(1.6052)]
[tensor(-0.7222), 0.4672489082969432, 0.8796296296296297, tensor(1.6052)]
[tensor(-0.6926), 0.4672489082969432, 0.8796296296296297, tensor(1.6218)]
[tensor(-0.6926), 0.4672489082969432, 0.8796296296296297, tensor(1.6218)]
[tensor(-0.6926), 0.4672489082969432, 0.8796296296296297, tensor(1.6218)]
[tensor(-0.6926), 0.4672489082969432, 0.8796296296296297, tensor(1.6218)]
[tensor(-0.6926), 0.4672489082969432, 0.8796296296296297, tensor(1.6218)]
[tensor(-0.6926), 0.4672489082969432, 0.8796296296296297, tensor(1.6218)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
[tensor(-0.6906), 0.4672489082969432, 0.8796296296296297, tensor(1.6456)]
early stopping at 25
[2023-01-15 22:39:55,498.498 dsw44922-6f76bf568-tbjcv:63033 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8263), 0.4104803493449782, 0.8472222222222222, tensor(1.2261)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.8263), 0.4104803493449782, 0.8472222222222222, tensor(1.2261)]
[tensor(-0.8124), 0.42358078602620086, 0.8472222222222222, tensor(1.3055)]
[tensor(-0.8124), 0.42358078602620086, 0.8472222222222222, tensor(1.3055)]
[tensor(-0.7650), 0.43231441048034935, 0.8564814814814815, tensor(1.3965)]
[tensor(-0.7568), 0.43231441048034935, 0.8564814814814815, tensor(1.3965)]
[tensor(-0.7308), 0.43231441048034935, 0.8564814814814815, tensor(1.3965)]
[tensor(-0.7077), 0.43231441048034935, 0.8657407407407407, tensor(1.3965)]
[tensor(-0.6955), 0.43231441048034935, 0.8657407407407407, tensor(1.3965)]
[tensor(-0.6927), 0.4410480349344978, 0.8657407407407407, tensor(1.5125)]
[tensor(-0.6927), 0.4410480349344978, 0.8657407407407407, tensor(1.5125)]
[tensor(-0.6798), 0.4410480349344978, 0.8657407407407407, tensor(1.5125)]
[tensor(-0.6649), 0.4585152838427948, 0.8657407407407407, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.8657407407407407, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.8657407407407407, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.8657407407407407, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.8657407407407407, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.8657407407407407, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.875, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.875, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.875, tensor(1.6277)]
[tensor(-0.6649), 0.4585152838427948, 0.875, tensor(1.6277)]
[tensor(-0.6649), 0.462882096069869, 0.875, tensor(1.6386)]
[tensor(-0.6649), 0.462882096069869, 0.875, tensor(1.6386)]
[tensor(-0.6649), 0.462882096069869, 0.8796296296296297, tensor(1.6391)]
[tensor(-0.6649), 0.462882096069869, 0.8796296296296297, tensor(1.6391)]
[tensor(-0.6649), 0.462882096069869, 0.8796296296296297, tensor(1.6391)]
[tensor(-0.6649), 0.462882096069869, 0.8796296296296297, tensor(1.6391)]
[tensor(-0.6649), 0.462882096069869, 0.8796296296296297, tensor(1.6391)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4672489082969432, 0.8796296296296297, tensor(1.6640)]
[tensor(-0.6649), 0.4759825327510917, 0.8796296296296297, tensor(1.6988)]
[tensor(-0.6649), 0.4759825327510917, 0.8796296296296297, tensor(1.6988)]
[tensor(-0.6649), 0.4759825327510917, 0.8796296296296297, tensor(1.6988)]
[tensor(-0.6649), 0.4847161572052402, 0.8796296296296297, tensor(1.7389)]
[tensor(-0.6649), 0.4847161572052402, 0.8796296296296297, tensor(1.7389)]
[tensor(-0.6649), 0.4847161572052402, 0.8796296296296297, tensor(1.7389)]
[tensor(-0.6649), 0.4847161572052402, 0.8796296296296297, tensor(1.7389)]
[tensor(-0.6649), 0.4847161572052402, 0.8796296296296297, tensor(1.7389)]
[tensor(-0.6649), 0.4847161572052402, 0.8796296296296297, tensor(1.7389)]
[2023-01-15 23:06:10,919.919 dsw44922-6f76bf568-tbjcv:63100 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8356), 0.12359550561797752, 0.0]
[tensor(-2.8250), 0.12808988764044943, 0.0]
[tensor(-2.8178), 0.1393258426966292, 0.0]
early stopping at 3
[2023-01-15 23:08:08,812.812 dsw44922-6f76bf568-tbjcv:63130 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8368), 0.12808988764044943, 0.0]
[tensor(-2.8255), 0.12808988764044943, 0.0]
[tensor(-2.8207), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:09:53,565.565 dsw44922-6f76bf568-tbjcv:63159 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8298), 0.12808988764044943, 0.0]
[tensor(-2.8271), 0.12808988764044943, 0.0]
[tensor(-2.8236), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:11:37,867.867 dsw44922-6f76bf568-tbjcv:63189 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8416), 0.12808988764044943, 0.0]
[tensor(-2.8348), 0.12808988764044943, 0.0]
[tensor(-2.8295), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:13:23,426.426 dsw44922-6f76bf568-tbjcv:63219 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4737), 0.1703056768558952, 0.5740740740740741, 0.0]
[tensor(-1.4478), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4478), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:15:09,068.068 dsw44922-6f76bf568-tbjcv:63248 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4575), 0.1703056768558952, 0.5740740740740741, 0.0]
[tensor(-1.4484), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4484), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:16:53,411.411 dsw44922-6f76bf568-tbjcv:63278 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4470), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4462), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4462), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:18:35,254.254 dsw44922-6f76bf568-tbjcv:63307 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4463), 0.21397379912663755, 0.5740740740740741, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4463), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4463), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:20:18,652.652 dsw44922-6f76bf568-tbjcv:63337 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8350), 0.12134831460674157, 0.0]
[tensor(-2.8249), 0.12808988764044943, 0.0]
[tensor(-2.8179), 0.13707865168539327, 0.0]
early stopping at 3
[2023-01-15 23:22:15,776.776 dsw44922-6f76bf568-tbjcv:63367 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8348), 0.12808988764044943, 0.0]
[tensor(-2.8245), 0.12808988764044943, 0.0]
[tensor(-2.8200), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:24:08,652.652 dsw44922-6f76bf568-tbjcv:63397 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8301), 0.12808988764044943, 0.0]
[tensor(-2.8271), 0.12808988764044943, 0.0]
[tensor(-2.8241), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:25:59,181.181 dsw44922-6f76bf568-tbjcv:63426 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8402), 0.12808988764044943, 0.0]
[tensor(-2.8333), 0.12808988764044943, 0.0]
[tensor(-2.8294), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:27:50,645.645 dsw44922-6f76bf568-tbjcv:63456 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4644), 0.1703056768558952, 0.5740740740740741, 0.0]
[tensor(-1.4462), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4462), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:29:50,536.536 dsw44922-6f76bf568-tbjcv:63486 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4606), 0.1703056768558952, 0.5740740740740741, 0.0]
[tensor(-1.4484), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4484), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:31:33,784.784 dsw44922-6f76bf568-tbjcv:63516 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4478), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4468), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4468), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:33:16,545.545 dsw44922-6f76bf568-tbjcv:63545 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4466), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4466), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4466), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:35:00,724.724 dsw44922-6f76bf568-tbjcv:63575 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8346), 0.11685393258426967, 0.0]
[tensor(-2.8242), 0.12808988764044943, 0.0]
[tensor(-2.8170), 0.1393258426966292, 0.0]
early stopping at 3
[2023-01-15 23:36:57,976.976 dsw44922-6f76bf568-tbjcv:63605 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8339), 0.12808988764044943, 0.0]
[tensor(-2.8234), 0.12808988764044943, 0.0]
[tensor(-2.8184), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:38:41,111.111 dsw44922-6f76bf568-tbjcv:63634 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8295), 0.12808988764044943, 0.0]
[tensor(-2.8264), 0.12808988764044943, 0.0]
[tensor(-2.8231), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:40:27,594.594 dsw44922-6f76bf568-tbjcv:63664 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8394), 0.12808988764044943, 0.0]
[tensor(-2.8326), 0.12808988764044943, 0.0]
[tensor(-2.8282), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-15 23:42:21,252.252 dsw44922-6f76bf568-tbjcv:63694 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4629), 0.1703056768558952, 0.5740740740740741, 0.0]
[tensor(-1.4477), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4477), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:44:03,098.098 dsw44922-6f76bf568-tbjcv:63723 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4592), 0.1703056768558952, 0.5740740740740741, 0.0]
[tensor(-1.4469), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4469), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:45:50,216.216 dsw44922-6f76bf568-tbjcv:63753 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4509), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4471), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4471), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:47:32,177.177 dsw44922-6f76bf568-tbjcv:63783 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4476), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4476), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4476), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-15 23:49:16,455.455 dsw44922-6f76bf568-tbjcv:63812 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0312), 0.41123595505617977, tensor(0.0250)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5356), 0.5662921348314607, tensor(1.2959)]
[tensor(-1.3023), 0.6202247191011236, tensor(1.7988)]
[tensor(-1.1802), 0.6719101123595506, tensor(2.1793)]
[tensor(-1.1671), 0.6943820224719102, tensor(2.3048)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1265), 0.7101123595505618, tensor(2.4241)]
[tensor(-1.1265), 0.7146067415730337, tensor(2.4241)]
[tensor(-1.1265), 0.7146067415730337, tensor(2.4241)]
[tensor(-1.1265), 0.7280898876404495, tensor(2.4280)]
[tensor(-1.1265), 0.7280898876404495, tensor(2.4280)]
[2023-01-15 23:55:19,971.971 dsw44922-6f76bf568-tbjcv:63849 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8619), 0.47415730337078654, tensor(0.5089)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.4394), 0.6067415730337079, tensor(1.5943)]
[tensor(-1.2309), 0.6404494382022472, tensor(1.9713)]
[tensor(-1.2309), 0.6719101123595506, tensor(2.1115)]
[tensor(-1.1863), 0.6831460674157304, tensor(2.2295)]
[tensor(-1.1863), 0.6966292134831461, tensor(2.2423)]
[tensor(-1.1863), 0.6966292134831461, tensor(2.2423)]
[tensor(-1.1863), 0.6966292134831461, tensor(2.2423)]
[tensor(-1.1863), 0.7078651685393258, tensor(2.2423)]
[tensor(-1.1863), 0.7258426966292135, tensor(2.3219)]
[2023-01-16 00:01:05,728.728 dsw44922-6f76bf568-tbjcv:63884 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0598), 0.4202247191011236, tensor(0.0413)]
[tensor(-1.7485), 0.48764044943820223, tensor(0.6897)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5297), 0.5752808988764045, tensor(1.3467)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2905), 0.6089887640449438, tensor(1.7544)]
[tensor(-1.1622), 0.7033707865168539, tensor(2.3547)]
[tensor(-1.1313), 0.7033707865168539, tensor(2.3547)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
[tensor(-1.1313), 0.7168539325842697, tensor(2.4348)]
early stopping at 17
[2023-01-16 00:10:57,428.428 dsw44922-6f76bf568-tbjcv:63926 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3123), 0.33707865168539325, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4505), 0.5865168539325842, tensor(1.4821)]
[tensor(-1.2836), 0.6359550561797753, tensor(1.8961)]
[tensor(-1.2343), 0.6561797752808989, tensor(2.0466)]
[tensor(-1.1984), 0.6764044943820224, tensor(2.1836)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1984), 0.6831460674157304, tensor(2.1836)]
[tensor(-1.1984), 0.6831460674157304, tensor(2.1836)]
[tensor(-1.1984), 0.6898876404494382, tensor(2.1836)]
[tensor(-1.1984), 0.6898876404494382, tensor(2.1836)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1984), 0.6898876404494382, tensor(2.1836)]
[tensor(-1.1984), 0.6898876404494382, tensor(2.1836)]
[tensor(-1.1984), 0.6898876404494382, tensor(2.1836)]
[tensor(-1.1984), 0.6898876404494382, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.6966292134831461, tensor(2.1836)]
[tensor(-1.1984), 0.698876404494382, tensor(2.1836)]
[tensor(-1.1984), 0.7056179775280899, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
[tensor(-1.1984), 0.7168539325842697, tensor(2.1836)]
early stopping at 37
[2023-01-16 00:31:52,720.720 dsw44922-6f76bf568-tbjcv:63984 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.5354), 0.21397379912663755, 0.42592592592592593, 0.0]
[tensor(-1.4626), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4479), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 00:33:39,932.932 dsw44922-6f76bf568-tbjcv:64014 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4901), 0.1703056768558952, 0.5740740740740741, 0.0]
[tensor(-1.2798), 0.24017467248908297, 0.6851851851851852, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.1617), 0.24017467248908297, 0.7777777777777778, tensor(0.0174)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.1617), 0.3231441048034934, 0.7777777777777778, tensor(0.4260)]
[tensor(-1.1534), 0.3231441048034934, 0.7777777777777778, tensor(0.4260)]
[2023-01-16 00:36:38,793.793 dsw44922-6f76bf568-tbjcv:64045 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.3717), 0.2576419213973799, 0.5740740740740741, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1693), 0.2576419213973799, 0.7407407407407407, tensor(0.0316)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7407407407407407, tensor(0.2048)]
[tensor(-1.1693), 0.3056768558951965, 0.7453703703703703, tensor(0.3365)]
[tensor(-1.1599), 0.3056768558951965, 0.7453703703703703, tensor(0.3365)]
[tensor(-1.1599), 0.3056768558951965, 0.7453703703703703, tensor(0.3365)]
[tensor(-1.0415), 0.31004366812227074, 0.7824074074074074, tensor(0.5087)]
[tensor(-1.0415), 0.31877729257641924, 0.7824074074074074, tensor(0.5515)]
[tensor(-1.0415), 0.31877729257641924, 0.8101851851851852, tensor(0.5515)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0415), 0.31877729257641924, 0.8101851851851852, tensor(0.5515)]
[tensor(-0.9324), 0.34934497816593885, 0.8101851851851852, tensor(0.8144)]
[tensor(-0.9317), 0.34934497816593885, 0.8148148148148148, tensor(0.8144)]
[tensor(-0.8976), 0.35807860262008734, 0.8194444444444444, tensor(0.8928)]
[tensor(-0.8976), 0.35807860262008734, 0.8194444444444444, tensor(0.8928)]
[tensor(-0.8843), 0.35807860262008734, 0.8425925925925926, tensor(0.8928)]
[tensor(-0.8843), 0.3624454148471616, 0.8425925925925926, tensor(0.9188)]
[tensor(-0.8621), 0.37554585152838427, 0.8425925925925926, tensor(1.0156)]
[tensor(-0.8585), 0.37554585152838427, 0.8425925925925926, tensor(1.0156)]
[tensor(-0.8518), 0.40611353711790393, 0.8425925925925926, tensor(1.1787)]
[tensor(-0.8518), 0.40611353711790393, 0.8425925925925926, tensor(1.1787)]
[tensor(-0.8437), 0.4104803493449782, 0.8425925925925926, tensor(1.2087)]
[tensor(-0.8311), 0.4104803493449782, 0.8425925925925926, tensor(1.2213)]
[tensor(-0.8275), 0.4104803493449782, 0.8425925925925926, tensor(1.2213)]
[tensor(-0.8275), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8229), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8099), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8099), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8099), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8099), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8099), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8089), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8037), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.8037), 0.42358078602620086, 0.8425925925925926, tensor(1.2852)]
[tensor(-0.7944), 0.42358078602620086, 0.8472222222222222, tensor(1.2852)]
[tensor(-0.7944), 0.42358078602620086, 0.8472222222222222, tensor(1.2852)]
[tensor(-0.7944), 0.42358078602620086, 0.8472222222222222, tensor(1.2852)]
[tensor(-0.7919), 0.42358078602620086, 0.8472222222222222, tensor(1.2852)]
[tensor(-0.7841), 0.42358078602620086, 0.8472222222222222, tensor(1.2852)]
[tensor(-0.7841), 0.42358078602620086, 0.8472222222222222, tensor(1.2852)]
[tensor(-0.7773), 0.42358078602620086, 0.8564814814814815, tensor(1.2852)]
[tensor(-0.7773), 0.42358078602620086, 0.8564814814814815, tensor(1.2852)]
[2023-01-16 01:04:14,613.613 dsw44922-6f76bf568-tbjcv:64114 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.3710), 0.21397379912663755, 0.7685185185185185, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2017), 0.2925764192139738, 0.7685185185185185, tensor(0.2612)]
[tensor(-0.9968), 0.3537117903930131, 0.8009259259259259, tensor(0.7718)]
[tensor(-0.9546), 0.3537117903930131, 0.8009259259259259, tensor(0.7718)]
[tensor(-0.9546), 0.3537117903930131, 0.8009259259259259, tensor(0.7718)]
[tensor(-0.8315), 0.3537117903930131, 0.8287037037037037, tensor(0.9153)]
[tensor(-0.7879), 0.3537117903930131, 0.8379629629629629, tensor(0.9588)]
[tensor(-0.7614), 0.43231441048034935, 0.8518518518518519, tensor(1.4002)]
[tensor(-0.7570), 0.43231441048034935, 0.8611111111111112, tensor(1.4002)]
[tensor(-0.7570), 0.43231441048034935, 0.8611111111111112, tensor(1.4002)]
[tensor(-0.7298), 0.43231441048034935, 0.8611111111111112, tensor(1.4002)]
[tensor(-0.7298), 0.43231441048034935, 0.8611111111111112, tensor(1.4002)]
[tensor(-0.7298), 0.43231441048034935, 0.8611111111111112, tensor(1.4002)]
[tensor(-0.7192), 0.43231441048034935, 0.8611111111111112, tensor(1.4002)]
[tensor(-0.7192), 0.43231441048034935, 0.8611111111111112, tensor(1.4002)]
[tensor(-0.7192), 0.43231441048034935, 0.8657407407407407, tensor(1.4002)]
[tensor(-0.7192), 0.43231441048034935, 0.8657407407407407, tensor(1.4002)]
[tensor(-0.7152), 0.43231441048034935, 0.8657407407407407, tensor(1.4002)]
[tensor(-0.7152), 0.43231441048034935, 0.875, tensor(1.4002)]
[tensor(-0.7152), 0.43231441048034935, 0.875, tensor(1.4002)]
[tensor(-0.7152), 0.43231441048034935, 0.875, tensor(1.4002)]
[tensor(-0.7152), 0.43231441048034935, 0.875, tensor(1.4002)]
[tensor(-0.7152), 0.43231441048034935, 0.875, tensor(1.4002)]
[tensor(-0.7152), 0.47161572052401746, 0.875, tensor(1.6203)]
[tensor(-0.7152), 0.47161572052401746, 0.875, tensor(1.6203)]
[tensor(-0.7152), 0.47161572052401746, 0.875, tensor(1.6203)]
[tensor(-0.7152), 0.47161572052401746, 0.875, tensor(1.6203)]
[tensor(-0.7152), 0.47161572052401746, 0.875, tensor(1.6203)]
[tensor(-0.7152), 0.47161572052401746, 0.875, tensor(1.6203)]
[tensor(-0.7152), 0.47161572052401746, 0.875, tensor(1.6203)]
[tensor(-0.7024), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.7024), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.7024), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6943), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6902), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6902), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6634), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6203)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6750)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6750)]
[tensor(-0.6508), 0.47161572052401746, 0.8842592592592593, tensor(1.6750)]
[tensor(-0.6508), 0.4759825327510917, 0.8842592592592593, tensor(1.7077)]
[2023-01-16 01:31:50,198.198 dsw44922-6f76bf568-tbjcv:64182 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9548), 0.43595505617977526, tensor(0.2250)]
[tensor(-1.3667), 0.6067415730337079, tensor(1.6670)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2053), 0.6831460674157304, tensor(2.2104)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1986), 0.6831460674157304, tensor(2.2171)]
[tensor(-1.1986), 0.6831460674157304, tensor(2.2171)]
[tensor(-1.1957), 0.701123595505618, tensor(2.3099)]
[tensor(-1.1957), 0.701123595505618, tensor(2.3099)]
[tensor(-1.1957), 0.701123595505618, tensor(2.3099)]
[tensor(-1.1957), 0.701123595505618, tensor(2.3099)]
[tensor(-1.1957), 0.7101123595505618, tensor(2.3099)]
[2023-01-16 01:37:44,542.542 dsw44922-6f76bf568-tbjcv:64218 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8918), 0.44269662921348313, tensor(0.3217)]
[tensor(-1.2852), 0.6426966292134831, tensor(1.9283)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1942), 0.6629213483146067, tensor(2.1204)]
[tensor(-1.1762), 0.6808988764044944, tensor(2.2283)]
[tensor(-1.1188), 0.6966292134831461, tensor(2.3643)]
[tensor(-1.1188), 0.6966292134831461, tensor(2.3643)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1188), 0.698876404494382, tensor(2.3643)]
[tensor(-1.1188), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.1188), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.1188), 0.7101123595505618, tensor(2.3643)]
[2023-01-16 01:43:37,989.989 dsw44922-6f76bf568-tbjcv:64254 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8500), 0.4337078651685393, tensor(0.3185)]
[tensor(-1.5168), 0.5550561797752809, tensor(1.2585)]
[tensor(-1.2429), 0.6426966292134831, tensor(1.9706)]
[tensor(-1.2429), 0.6539325842696629, tensor(1.9860)]
[tensor(-1.1083), 0.6853932584269663, tensor(2.3186)]
[tensor(-1.1083), 0.6921348314606741, tensor(2.3465)]
[tensor(-1.1083), 0.6921348314606741, tensor(2.3465)]
[tensor(-1.1083), 0.698876404494382, tensor(2.3465)]
[tensor(-1.1083), 0.7101123595505618, tensor(2.3465)]
[tensor(-1.1083), 0.7101123595505618, tensor(2.3564)]
[tensor(-1.1083), 0.7101123595505618, tensor(2.3564)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1083), 0.7146067415730337, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
[tensor(-1.1083), 0.7213483146067415, tensor(2.3564)]
early stopping at 23
[2023-01-16 01:56:45,743.743 dsw44922-6f76bf568-tbjcv:64300 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9571), 0.40224719101123596, tensor(0.0542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4451), 0.604494382022472, tensor(1.5774)]
[tensor(-1.1936), 0.6584269662921348, tensor(2.0985)]
[tensor(-1.1936), 0.6584269662921348, tensor(2.0985)]
[tensor(-1.1831), 0.6831460674157304, tensor(2.2326)]
[tensor(-1.1831), 0.6898876404494382, tensor(2.2326)]
[tensor(-1.1831), 0.698876404494382, tensor(2.2326)]
[tensor(-1.1831), 0.698876404494382, tensor(2.2326)]
[tensor(-1.1831), 0.698876404494382, tensor(2.2326)]
[tensor(-1.1831), 0.7033707865168539, tensor(2.2326)]
[tensor(-1.1831), 0.7101123595505618, tensor(2.2326)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1831), 0.7123595505617978, tensor(2.2326)]
[tensor(-1.1831), 0.7123595505617978, tensor(2.2326)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
[tensor(-1.1831), 0.7146067415730337, tensor(2.2326)]
early stopping at 24
[2023-01-16 02:10:25,952.952 dsw44922-6f76bf568-tbjcv:64349 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.5457), 0.21397379912663755, 0.42592592592592593, 0.0]
[tensor(-1.4610), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4478), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 02:12:19,479.479 dsw44922-6f76bf568-tbjcv:64398 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4383), 0.21397379912663755, 0.5740740740740741, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0948), 0.3406113537117904, 0.7870370370370371, tensor(0.6083)]
[tensor(-1.0948), 0.3406113537117904, 0.7870370370370371, tensor(0.6083)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0948), 0.3406113537117904, 0.7870370370370371, tensor(0.6083)]
[tensor(-0.9926), 0.3406113537117904, 0.7870370370370371, tensor(0.6083)]
[2023-01-16 02:15:17,241.241 dsw44922-6f76bf568-tbjcv:64429 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4463), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4458), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4450), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 02:17:06,960.960 dsw44922-6f76bf568-tbjcv:64459 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4738), 0.21397379912663755, 0.42592592592592593, 0.0]
[tensor(-1.4461), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4461), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 02:18:54,493.493 dsw44922-6f76bf568-tbjcv:64489 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8104), 0.48089887640449436, tensor(0.5941)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3152), 0.6382022471910113, tensor(1.8758)]
[tensor(-1.1762), 0.6651685393258427, tensor(2.1496)]
[tensor(-1.1717), 0.698876404494382, tensor(2.3227)]
[tensor(-1.1717), 0.698876404494382, tensor(2.3227)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1717), 0.698876404494382, tensor(2.3227)]
[tensor(-1.1717), 0.7033707865168539, tensor(2.3227)]
[tensor(-1.1717), 0.7235955056179775, tensor(2.3913)]
[tensor(-1.1717), 0.7235955056179775, tensor(2.3913)]
[tensor(-1.1717), 0.7235955056179775, tensor(2.3913)]
[2023-01-16 02:24:54,717.717 dsw44922-6f76bf568-tbjcv:64525 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8159), 0.48089887640449436, tensor(0.5886)]
[tensor(-1.3269), 0.6157303370786517, tensor(1.7518)]
[tensor(-1.2605), 0.6292134831460674, tensor(1.8856)]
[tensor(-1.2104), 0.6674157303370787, tensor(2.1267)]
[tensor(-1.1408), 0.7056179775280899, tensor(2.3873)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1408), 0.7056179775280899, tensor(2.3873)]
[tensor(-1.1408), 0.7056179775280899, tensor(2.3873)]
[tensor(-1.1408), 0.7056179775280899, tensor(2.3873)]
[tensor(-1.1408), 0.7056179775280899, tensor(2.3873)]
[tensor(-1.1408), 0.7101123595505618, tensor(2.3873)]
[2023-01-16 02:30:40,356.356 dsw44922-6f76bf568-tbjcv:64560 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.4905), 0.5752808988764045, tensor(1.3859)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3267), 0.6179775280898876, tensor(1.7632)]
[tensor(-1.1429), 0.6629213483146067, tensor(2.1718)]
[tensor(-1.1429), 0.6853932584269663, tensor(2.2002)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1429), 0.6966292134831461, tensor(2.3039)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7056179775280899, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
[tensor(-1.1429), 0.7213483146067415, tensor(2.3623)]
early stopping at 24
[2023-01-16 02:44:24,283.283 dsw44922-6f76bf568-tbjcv:64608 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8231), 0.48314606741573035, tensor(0.5926)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3763), 0.597752808988764, tensor(1.6125)]
[tensor(-1.1770), 0.6808988764044944, tensor(2.2275)]
[tensor(-1.1770), 0.6808988764044944, tensor(2.2275)]
[tensor(-1.1463), 0.6876404494382022, tensor(2.2919)]
[tensor(-1.1463), 0.7033707865168539, tensor(2.2919)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
[tensor(-1.1463), 0.7191011235955056, tensor(2.4166)]
early stopping at 20
[2023-01-16 02:55:51,547.547 dsw44922-6f76bf568-tbjcv:64652 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.5281), 0.21397379912663755, 0.42592592592592593, 0.0]
[tensor(-1.4636), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4503), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 02:57:38,864.864 dsw44922-6f76bf568-tbjcv:64682 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2589), 0.3231441048034934, 0.7083333333333334, tensor(0.3569)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0656), 0.3231441048034934, 0.7916666666666666, tensor(0.4628)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0645), 0.3318777292576419, 0.7962962962962963, tensor(0.5949)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0645), 0.3318777292576419, 0.7962962962962963, tensor(0.5949)]
[tensor(-0.9543), 0.34934497816593885, 0.8287037037037037, tensor(0.7925)]
[2023-01-16 03:00:42,277.277 dsw44922-6f76bf568-tbjcv:64713 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4634), 0.16593886462882096, 0.5740740740740741, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2466), 0.21397379912663755, 0.7175925925925926, 0.0]
[tensor(-1.0000), 0.29694323144104806, 0.7824074074074074, tensor(0.4847)]
[tensor(-1.0000), 0.29694323144104806, 0.7824074074074074, tensor(0.4847)]
[tensor(-0.9053), 0.35807860262008734, 0.8148148148148148, tensor(0.8851)]
[tensor(-0.8871), 0.3624454148471616, 0.8194444444444444, tensor(0.9251)]
[tensor(-0.8106), 0.43231441048034935, 0.8194444444444444, tensor(1.3510)]
[tensor(-0.8106), 0.43231441048034935, 0.8194444444444444, tensor(1.3510)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7387), 0.4410480349344978, 0.8518518518518519, tensor(1.4666)]
[tensor(-0.7387), 0.44541484716157204, 0.8518518518518519, tensor(1.4822)]
[tensor(-0.7387), 0.44541484716157204, 0.8518518518518519, tensor(1.4822)]
[tensor(-0.7387), 0.44541484716157204, 0.8518518518518519, tensor(1.4822)]
[tensor(-0.7127), 0.47161572052401746, 0.8657407407407407, tensor(1.6454)]
[tensor(-0.6999), 0.47161572052401746, 0.8657407407407407, tensor(1.6454)]
[tensor(-0.6999), 0.47161572052401746, 0.8657407407407407, tensor(1.6454)]
[tensor(-0.6996), 0.47161572052401746, 0.8657407407407407, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
[tensor(-0.6841), 0.47161572052401746, 0.875, tensor(1.6454)]
early stopping at 27
[2023-01-16 03:15:55,801.801 dsw44922-6f76bf568-tbjcv:64763 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4348), 0.21397379912663755, 0.42592592592592593, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4348), 0.21397379912663755, 0.42592592592592593, 0.0]
[tensor(-1.4348), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 03:17:43,198.198 dsw44922-6f76bf568-tbjcv:64793 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9023), 0.44269662921348313, tensor(0.3112)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3455), 0.6134831460674157, tensor(1.7220)]
[tensor(-1.1708), 0.6764044943820224, tensor(2.2112)]
[tensor(-1.1708), 0.6764044943820224, tensor(2.2112)]
[tensor(-1.1708), 0.6764044943820224, tensor(2.2112)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1708), 0.6943820224719102, tensor(2.2865)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.3099)]
[tensor(-1.1708), 0.7168539325842697, tensor(2.3796)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1708), 0.7168539325842697, tensor(2.3796)]
[tensor(-1.1708), 0.7213483146067415, tensor(2.3796)]
[2023-01-16 03:23:39,239.239 dsw44922-6f76bf568-tbjcv:64829 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.8489), 0.46741573033707867, tensor(0.4881)]
[tensor(-1.3675), 0.6202247191011236, tensor(1.7336)]
[tensor(-1.2042), 0.6719101123595506, tensor(2.1553)]
[tensor(-1.2042), 0.6719101123595506, tensor(2.1553)]
[tensor(-1.1943), 0.6943820224719102, tensor(2.2776)]
[tensor(-1.1943), 0.7101123595505618, tensor(2.3213)]
[tensor(-1.1943), 0.7101123595505618, tensor(2.3213)]
[tensor(-1.1943), 0.7101123595505618, tensor(2.3213)]
[tensor(-1.1943), 0.7191011235955056, tensor(2.3213)]
[tensor(-1.1943), 0.7191011235955056, tensor(2.3213)]
[2023-01-16 03:29:24,193.193 dsw44922-6f76bf568-tbjcv:64865 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6380), 0.5123595505617977, tensor(0.9238)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4366), 0.5820224719101124, tensor(1.4735)]
[tensor(-1.2634), 0.6337078651685393, tensor(1.9051)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2376), 0.6606741573033708, tensor(2.0657)]
[tensor(-1.2376), 0.6651685393258427, tensor(2.0821)]
[tensor(-1.2376), 0.6719101123595506, tensor(2.1131)]
[tensor(-1.2376), 0.6719101123595506, tensor(2.1131)]
[tensor(-1.2376), 0.6853932584269663, tensor(2.1131)]
[tensor(-1.2376), 0.6943820224719102, tensor(2.1320)]
[tensor(-1.2376), 0.6943820224719102, tensor(2.1587)]
[tensor(-1.2376), 0.6943820224719102, tensor(2.1587)]
[tensor(-1.2376), 0.6943820224719102, tensor(2.1587)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.701123595505618, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.2376), 0.7078651685393258, tensor(2.1607)]
early stopping at 32
[2023-01-16 03:47:37,813.813 dsw44922-6f76bf568-tbjcv:64919 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8586), 0.4786516853932584, tensor(0.5347)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4135), 0.5820224719101124, tensor(1.4966)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1766), 0.6674157303370787, tensor(2.1605)]
[tensor(-1.1766), 0.6674157303370787, tensor(2.1605)]
[tensor(-1.1766), 0.6674157303370787, tensor(2.1605)]
[tensor(-1.1766), 0.6876404494382022, tensor(2.1942)]
[tensor(-1.1766), 0.6876404494382022, tensor(2.1942)]
[tensor(-1.1766), 0.6898876404494382, tensor(2.1942)]
[tensor(-1.1766), 0.6898876404494382, tensor(2.1942)]
[tensor(-1.1766), 0.7056179775280899, tensor(2.1962)]
[tensor(-1.1766), 0.7056179775280899, tensor(2.1962)]
[tensor(-1.1766), 0.7056179775280899, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7101123595505618, tensor(2.1962)]
[tensor(-1.1766), 0.7146067415730337, tensor(2.1962)]
[tensor(-1.1766), 0.7146067415730337, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7168539325842697, tensor(2.1962)]
[tensor(-1.1766), 0.7191011235955056, tensor(2.1962)]
[tensor(-1.1766), 0.7191011235955056, tensor(2.1962)]
[tensor(-1.1766), 0.7191011235955056, tensor(2.1962)]
[tensor(-1.1766), 0.7191011235955056, tensor(2.1962)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1766), 0.7191011235955056, tensor(2.1962)]
[tensor(-1.1766), 0.7191011235955056, tensor(2.1962)]
[tensor(-1.1766), 0.7235955056179775, tensor(2.1962)]
[tensor(-1.1766), 0.7235955056179775, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
[tensor(-1.1766), 0.7280898876404495, tensor(2.1962)]
early stopping at 49
[2023-01-16 04:15:32,249.249 dsw44922-6f76bf568-tbjcv:64988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.5486), 0.21397379912663755, 0.42592592592592593, 0.0]
[tensor(-1.4641), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4507), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 04:17:19,639.639 dsw44922-6f76bf568-tbjcv:65018 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3143), 0.2314410480349345, 0.5740740740740741, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3143), 0.2314410480349345, 0.5740740740740741, 0.0]
[tensor(-1.3143), 0.2314410480349345, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 04:19:06,962.962 dsw44922-6f76bf568-tbjcv:65047 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0891), 0.2794759825327511, 0.7685185185185185, tensor(0.3083)]
[tensor(-0.9180), 0.3406113537117904, 0.8379629629629629, tensor(0.7850)]
[tensor(-0.8882), 0.3799126637554585, 0.8518518518518519, tensor(1.0114)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8723), 0.3799126637554585, 0.8564814814814815, tensor(1.0114)]
[tensor(-0.8438), 0.3799126637554585, 0.8564814814814815, tensor(1.0121)]
[tensor(-0.8124), 0.39737991266375544, 0.8564814814814815, tensor(1.1745)]
[tensor(-0.7961), 0.40611353711790393, 0.8564814814814815, tensor(1.2345)]
[tensor(-0.7517), 0.4366812227074236, 0.8703703703703703, tensor(1.4317)]
[tensor(-0.6963), 0.4366812227074236, 0.8703703703703703, tensor(1.4652)]
[tensor(-0.6963), 0.4366812227074236, 0.8703703703703703, tensor(1.4855)]
[tensor(-0.6842), 0.4366812227074236, 0.875, tensor(1.4855)]
[tensor(-0.6842), 0.4366812227074236, 0.875, tensor(1.4855)]
[tensor(-0.6842), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6842), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6842), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6752), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6736), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6736), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6736), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6736), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6736), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6667), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6667), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6659), 0.4672489082969432, 0.875, tensor(1.6347)]
[tensor(-0.6659), 0.4672489082969432, 0.8796296296296297, tensor(1.6347)]
[tensor(-0.6659), 0.4672489082969432, 0.8796296296296297, tensor(1.6347)]
[tensor(-0.6659), 0.4672489082969432, 0.8796296296296297, tensor(1.6347)]
[tensor(-0.6659), 0.4672489082969432, 0.8796296296296297, tensor(1.6347)]
[tensor(-0.6623), 0.4672489082969432, 0.8796296296296297, tensor(1.6347)]
[tensor(-0.6468), 0.4672489082969432, 0.8796296296296297, tensor(1.6895)]
[tensor(-0.6468), 0.4672489082969432, 0.8888888888888888, tensor(1.6895)]
[tensor(-0.6468), 0.4672489082969432, 0.8888888888888888, tensor(1.6895)]
[tensor(-0.6468), 0.4890829694323144, 0.8888888888888888, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8888888888888888, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8888888888888888, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8888888888888888, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8888888888888888, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
[tensor(-0.6468), 0.4890829694323144, 0.8935185185185185, tensor(1.7943)]
early stopping at 49
[2023-01-16 04:46:23,896.896 dsw44922-6f76bf568-tbjcv:65115 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4698), 0.21397379912663755, 0.42592592592592593, 0.0]
[tensor(-1.4408), 0.21397379912663755, 0.5740740740740741, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0713), 0.30131004366812225, 0.7916666666666666, tensor(0.4352)]
[tensor(-1.0713), 0.31877729257641924, 0.7916666666666666, tensor(0.4352)]
[tensor(-1.0713), 0.31877729257641924, 0.7916666666666666, tensor(0.4352)]
[tensor(-1.0713), 0.34497816593886466, 0.7916666666666666, tensor(0.5670)]
[tensor(-1.0078), 0.3537117903930131, 0.7962962962962963, tensor(0.7608)]
[tensor(-0.8560), 0.3537117903930131, 0.8333333333333334, tensor(0.8252)]
[tensor(-0.8560), 0.35807860262008734, 0.8333333333333334, tensor(0.8574)]
[tensor(-0.8560), 0.35807860262008734, 0.8333333333333334, tensor(0.9332)]
[tensor(-0.8560), 0.35807860262008734, 0.8472222222222222, tensor(0.9332)]
[tensor(-0.8231), 0.4192139737991266, 0.8472222222222222, tensor(1.2729)]
[tensor(-0.8231), 0.4192139737991266, 0.8472222222222222, tensor(1.2729)]
[tensor(-0.7999), 0.4192139737991266, 0.8518518518518519, tensor(1.2729)]
[tensor(-0.7999), 0.4192139737991266, 0.8518518518518519, tensor(1.2729)]
[tensor(-0.7868), 0.42358078602620086, 0.8518518518518519, tensor(1.3311)]
[tensor(-0.7868), 0.42358078602620086, 0.8611111111111112, tensor(1.3311)]
[tensor(-0.7845), 0.42358078602620086, 0.8611111111111112, tensor(1.3311)]
[tensor(-0.7789), 0.4366812227074236, 0.8611111111111112, tensor(1.4045)]
[tensor(-0.7750), 0.4366812227074236, 0.8611111111111112, tensor(1.4045)]
[tensor(-0.7750), 0.4366812227074236, 0.8611111111111112, tensor(1.4045)]
[tensor(-0.7725), 0.4366812227074236, 0.8611111111111112, tensor(1.4045)]
[tensor(-0.7627), 0.4366812227074236, 0.8611111111111112, tensor(1.4045)]
[tensor(-0.7627), 0.4410480349344978, 0.8611111111111112, tensor(1.4414)]
[tensor(-0.7558), 0.4410480349344978, 0.8611111111111112, tensor(1.4414)]
[tensor(-0.7429), 0.4497816593886463, 0.8657407407407407, tensor(1.5060)]
[tensor(-0.7166), 0.4585152838427948, 0.8703703703703703, tensor(1.5760)]
[tensor(-0.7166), 0.4585152838427948, 0.8703703703703703, tensor(1.5760)]
[tensor(-0.7166), 0.4585152838427948, 0.8703703703703703, tensor(1.5760)]
[tensor(-0.7166), 0.4585152838427948, 0.8703703703703703, tensor(1.5760)]
[tensor(-0.7166), 0.4585152838427948, 0.8796296296296297, tensor(1.5760)]
[tensor(-0.7166), 0.4585152838427948, 0.8796296296296297, tensor(1.5760)]
[tensor(-0.7166), 0.4585152838427948, 0.8796296296296297, tensor(1.5760)]
[tensor(-0.7166), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.7166), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.7166), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.7166), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.7166), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.7079), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.6939), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.6939), 0.4672489082969432, 0.8796296296296297, tensor(1.5906)]
[tensor(-0.6939), 0.4672489082969432, 0.8796296296296297, tensor(1.5980)]
[tensor(-0.6939), 0.4672489082969432, 0.8796296296296297, tensor(1.5980)]
[tensor(-0.6939), 0.4759825327510917, 0.8796296296296297, tensor(1.6780)]
[tensor(-0.6939), 0.4759825327510917, 0.8796296296296297, tensor(1.6780)]
[tensor(-0.6939), 0.4759825327510917, 0.8796296296296297, tensor(1.6799)]
[tensor(-0.6939), 0.4759825327510917, 0.8796296296296297, tensor(1.6799)]
[tensor(-0.6939), 0.4759825327510917, 0.8796296296296297, tensor(1.6799)]
[tensor(-0.6939), 0.4759825327510917, 0.8796296296296297, tensor(1.6799)]
[tensor(-0.6939), 0.4759825327510917, 0.8796296296296297, tensor(1.6799)]
[2023-01-16 05:13:52,710.710 dsw44922-6f76bf568-tbjcv:65183 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2514), 0.36404494382022473, 0.0]
[tensor(-1.4547), 0.5685393258426966, tensor(1.3880)]
[tensor(-1.2720), 0.651685393258427, tensor(1.9865)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2540), 0.6629213483146067, tensor(2.0606)]
[tensor(-1.2540), 0.6629213483146067, tensor(2.0606)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2540), 0.6629213483146067, tensor(2.0606)]
[tensor(-1.2540), 0.6651685393258427, tensor(2.0606)]
[tensor(-1.2540), 0.6719101123595506, tensor(2.0606)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2540), 0.6808988764044944, tensor(2.0606)]
[tensor(-1.2540), 0.6808988764044944, tensor(2.0606)]
[2023-01-16 05:19:34,368.368 dsw44922-6f76bf568-tbjcv:65219 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8661), 0.4853932584269663, tensor(0.5609)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3010), 0.6157303370786517, tensor(1.7777)]
[tensor(-1.2139), 0.6606741573033708, tensor(2.0895)]
[tensor(-1.2139), 0.6606741573033708, tensor(2.0895)]
[tensor(-1.2139), 0.6786516853932584, tensor(2.1477)]
[tensor(-1.2139), 0.6786516853932584, tensor(2.1477)]
[tensor(-1.2139), 0.6853932584269663, tensor(2.1477)]
[tensor(-1.2139), 0.6898876404494382, tensor(2.1477)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2139), 0.6943820224719102, tensor(2.1477)]
[tensor(-1.2139), 0.6943820224719102, tensor(2.1477)]
[2023-01-16 05:25:05,696.696 dsw44922-6f76bf568-tbjcv:65254 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0905), 0.44269662921348313, tensor(0.1230)]
[tensor(-1.7424), 0.5168539325842697, tensor(0.8419)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3786), 0.6134831460674157, tensor(1.6888)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2819), 0.6584269662921348, tensor(2.0103)]
[tensor(-1.2010), 0.6719101123595506, tensor(2.1585)]
[tensor(-1.1955), 0.6741573033707865, tensor(2.1753)]
[tensor(-1.1955), 0.6741573033707865, tensor(2.1753)]
[tensor(-1.1955), 0.6741573033707865, tensor(2.1753)]
[tensor(-1.1955), 0.6741573033707865, tensor(2.1753)]
[tensor(-1.1955), 0.6943820224719102, tensor(2.1753)]
[tensor(-1.1955), 0.6943820224719102, tensor(2.1753)]
[tensor(-1.1955), 0.6943820224719102, tensor(2.1753)]
[tensor(-1.1955), 0.6943820224719102, tensor(2.1753)]
[tensor(-1.1955), 0.6943820224719102, tensor(2.1753)]
[tensor(-1.1955), 0.6966292134831461, tensor(2.1753)]
[tensor(-1.1955), 0.6966292134831461, tensor(2.1753)]
[tensor(-1.1955), 0.6966292134831461, tensor(2.1753)]
[tensor(-1.1955), 0.6966292134831461, tensor(2.1753)]
[tensor(-1.1955), 0.698876404494382, tensor(2.1753)]
[tensor(-1.1955), 0.698876404494382, tensor(2.1753)]
[tensor(-1.1955), 0.698876404494382, tensor(2.1753)]
[tensor(-1.1955), 0.698876404494382, tensor(2.1753)]
[tensor(-1.1955), 0.698876404494382, tensor(2.1753)]
[tensor(-1.1955), 0.698876404494382, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
[tensor(-1.1955), 0.7056179775280899, tensor(2.1753)]
early stopping at 35
[2023-01-16 05:44:00,177.177 dsw44922-6f76bf568-tbjcv:65309 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8347), 0.12808988764044943, 0.0]
[tensor(-2.8336), 0.12808988764044943, 0.0]
[tensor(-2.8291), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-16 05:45:44,326.326 dsw44922-6f76bf568-tbjcv:65339 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9924), 0.3318777292576419, 0.8101851851851852, tensor(0.6670)]
[tensor(-0.9175), 0.37117903930131, 0.8333333333333334, tensor(0.9384)]
[tensor(-0.8058), 0.40611353711790393, 0.8472222222222222, tensor(1.2248)]
[tensor(-0.8055), 0.40611353711790393, 0.8472222222222222, tensor(1.2248)]
[tensor(-0.7672), 0.4104803493449782, 0.8518518518518519, tensor(1.2852)]
[2023-01-16 05:48:31,738.738 dsw44922-6f76bf568-tbjcv:65370 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0154), 0.31877729257641924, 0.7824074074074074, tensor(0.5785)]
[tensor(-0.8154), 0.4104803493449782, 0.8472222222222222, tensor(1.2370)]
[tensor(-0.8154), 0.4104803493449782, 0.8472222222222222, tensor(1.2370)]
[tensor(-0.7742), 0.4104803493449782, 0.8611111111111112, tensor(1.2370)]
[tensor(-0.7361), 0.44541484716157204, 0.8611111111111112, tensor(1.4910)]
[2023-01-16 05:51:15,407.407 dsw44922-6f76bf568-tbjcv:65401 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9676), 0.3406113537117904, 0.8194444444444444, tensor(0.7355)]
[tensor(-0.9285), 0.3406113537117904, 0.8194444444444444, tensor(0.7355)]
[tensor(-0.9285), 0.3406113537117904, 0.8194444444444444, tensor(0.7355)]
[tensor(-0.8384), 0.39737991266375544, 0.8194444444444444, tensor(1.1485)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7509), 0.4279475982532751, 0.8518518518518519, tensor(1.3888)]
[tensor(-0.7348), 0.45414847161572053, 0.8564814814814815, tensor(1.5359)]
[tensor(-0.6958), 0.4585152838427948, 0.8564814814814815, tensor(1.5968)]
[tensor(-0.6958), 0.4585152838427948, 0.8564814814814815, tensor(1.5968)]
[tensor(-0.6825), 0.4759825327510917, 0.875, tensor(1.6974)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.6974)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.6974)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.6974)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.7014)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.7014)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.7014)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.7062)]
[tensor(-0.6699), 0.4759825327510917, 0.875, tensor(1.7062)]
[tensor(-0.6662), 0.4759825327510917, 0.875, tensor(1.7062)]
[tensor(-0.6539), 0.4759825327510917, 0.8796296296296297, tensor(1.7062)]
[tensor(-0.6539), 0.4759825327510917, 0.8842592592592593, tensor(1.7062)]
[tensor(-0.6539), 0.48034934497816595, 0.8842592592592593, tensor(1.7477)]
[tensor(-0.6539), 0.48034934497816595, 0.8842592592592593, tensor(1.7477)]
[tensor(-0.6470), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6470), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6404), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6404), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6404), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6404), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6395), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6395), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6395), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6343), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6343), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6343), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6343), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6343), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6343), 0.5021834061135371, 0.8842592592592593, tensor(1.8639)]
[tensor(-0.6343), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
[tensor(-0.6325), 0.5065502183406113, 0.8842592592592593, tensor(1.8958)]
early stopping at 49
[2023-01-16 06:17:26,697.697 dsw44922-6f76bf568-tbjcv:65468 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9558), 0.34934497816593885, 0.8009259259259259, tensor(0.7909)]
[tensor(-0.9558), 0.34934497816593885, 0.8009259259259259, tensor(0.7909)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.8848), 0.3930131004366812, 0.8194444444444444, tensor(1.0802)]
[tensor(-0.8112), 0.4148471615720524, 0.8240740740740741, tensor(1.2630)]
[tensor(-0.7499), 0.4366812227074236, 0.8287037037037037, tensor(1.4335)]
[tensor(-0.7499), 0.4366812227074236, 0.8287037037037037, tensor(1.4335)]
[tensor(-0.7499), 0.4366812227074236, 0.8333333333333334, tensor(1.4335)]
[tensor(-0.7499), 0.4497816593886463, 0.8425925925925926, tensor(1.4842)]
[tensor(-0.7499), 0.4585152838427948, 0.8564814814814815, tensor(1.5426)]
[tensor(-0.7328), 0.4585152838427948, 0.8564814814814815, tensor(1.5426)]
[tensor(-0.7287), 0.462882096069869, 0.8564814814814815, tensor(1.5857)]
[tensor(-0.7244), 0.462882096069869, 0.875, tensor(1.5857)]
[tensor(-0.7244), 0.462882096069869, 0.875, tensor(1.5857)]
[tensor(-0.7034), 0.4759825327510917, 0.875, tensor(1.6765)]
[tensor(-0.7034), 0.4759825327510917, 0.875, tensor(1.6765)]
[tensor(-0.7034), 0.4759825327510917, 0.875, tensor(1.6765)]
[tensor(-0.7034), 0.4847161572052402, 0.875, tensor(1.7043)]
[tensor(-0.7034), 0.4847161572052402, 0.875, tensor(1.7043)]
[tensor(-0.6935), 0.4847161572052402, 0.875, tensor(1.7083)]
[tensor(-0.6935), 0.4890829694323144, 0.875, tensor(1.7420)]
[tensor(-0.6935), 0.4890829694323144, 0.875, tensor(1.7420)]
[tensor(-0.6935), 0.4890829694323144, 0.875, tensor(1.7420)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
[tensor(-0.6935), 0.5283842794759825, 0.875, tensor(1.9352)]
early stopping at 33
[2023-01-16 06:35:11,847.847 dsw44922-6f76bf568-tbjcv:65521 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3256), 0.32808988764044944, 0.0]
[tensor(-1.5207), 0.5595505617977528, tensor(1.2770)]
[tensor(-1.2226), 0.6808988764044944, tensor(2.1819)]
[tensor(-1.1917), 0.6808988764044944, tensor(2.1819)]
[tensor(-1.1917), 0.6808988764044944, tensor(2.1819)]
[tensor(-1.1917), 0.6808988764044944, tensor(2.1819)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1917), 0.6853932584269663, tensor(2.1819)]
[tensor(-1.1917), 0.6876404494382022, tensor(2.1819)]
[tensor(-1.1917), 0.6876404494382022, tensor(2.1819)]
[tensor(-1.1917), 0.6943820224719102, tensor(2.1819)]
[2023-01-16 06:41:04,818.818 dsw44922-6f76bf568-tbjcv:65557 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.9517), 0.4651685393258427, tensor(0.3741)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2739), 0.6359550561797753, tensor(1.9059)]
[tensor(-1.1970), 0.6741573033707865, tensor(2.1738)]
[tensor(-1.1970), 0.6741573033707865, tensor(2.1738)]
[tensor(-1.1948), 0.6966292134831461, tensor(2.2884)]
[tensor(-1.1948), 0.6966292134831461, tensor(2.2884)]
[tensor(-1.1948), 0.6966292134831461, tensor(2.2884)]
[tensor(-1.1948), 0.6966292134831461, tensor(2.2884)]
[tensor(-1.1948), 0.701123595505618, tensor(2.2884)]
[tensor(-1.1948), 0.7168539325842697, tensor(2.2884)]
[2023-01-16 06:46:30,977.977 dsw44922-6f76bf568-tbjcv:65592 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0980), 0.451685393258427, tensor(0.1604)]
[tensor(-1.7628), 0.5101123595505618, tensor(0.7877)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3586), 0.6269662921348315, tensor(1.7763)]
[tensor(-1.3586), 0.6539325842696629, tensor(1.8855)]
[tensor(-1.1577), 0.6831460674157304, tensor(2.2580)]
[tensor(-1.1550), 0.6898876404494382, tensor(2.2944)]
[tensor(-1.1550), 0.6966292134831461, tensor(2.2944)]
[tensor(-1.1550), 0.6966292134831461, tensor(2.2944)]
[tensor(-1.1550), 0.6966292134831461, tensor(2.2944)]
[tensor(-1.1550), 0.7033707865168539, tensor(2.2944)]
[tensor(-1.1550), 0.7078651685393258, tensor(2.2944)]
[tensor(-1.1550), 0.7078651685393258, tensor(2.2944)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1550), 0.7078651685393258, tensor(2.2944)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1550), 0.7101123595505618, tensor(2.2944)]
[tensor(-1.1550), 0.7191011235955056, tensor(2.2944)]
[tensor(-1.1550), 0.7191011235955056, tensor(2.2944)]
[tensor(-1.1550), 0.7213483146067415, tensor(2.2944)]
[tensor(-1.1550), 0.7213483146067415, tensor(2.2944)]
[tensor(-1.1550), 0.7213483146067415, tensor(2.2944)]
[tensor(-1.1550), 0.7213483146067415, tensor(2.2944)]
[tensor(-1.1550), 0.7213483146067415, tensor(2.2944)]
[tensor(-1.1550), 0.7213483146067415, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
[tensor(-1.1550), 0.7235955056179775, tensor(2.2944)]
early stopping at 33
[2023-01-16 07:04:24,938.938 dsw44922-6f76bf568-tbjcv:65646 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8101), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7111), 0.4853932584269663, tensor(0.7158)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3225), 0.5955056179775281, tensor(1.6550)]
[tensor(-1.1560), 0.6808988764044944, tensor(2.2485)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.701123595505618, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
[tensor(-1.1025), 0.7033707865168539, tensor(2.4031)]
early stopping at 29
[2023-01-16 07:19:53,875.875 dsw44922-6f76bf568-tbjcv:65696 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9501), 0.314410480349345, 0.8333333333333334, tensor(0.6220)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9501), 0.3537117903930131, 0.8333333333333334, tensor(0.7938)]
[tensor(-0.8965), 0.35807860262008734, 0.8333333333333334, tensor(0.8939)]
[tensor(-0.8810), 0.39737991266375544, 0.8333333333333334, tensor(1.1059)]
[tensor(-0.8427), 0.39737991266375544, 0.8518518518518519, tensor(1.1059)]
[2023-01-16 07:22:40,658.658 dsw44922-6f76bf568-tbjcv:65727 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3038), 0.27510917030567683, 0.5740740740740741, tensor(0.0717)]
[tensor(-0.9922), 0.3406113537117904, 0.7962962962962963, tensor(0.7108)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.8243), 0.38427947598253276, 0.8472222222222222, tensor(1.0971)]
[tensor(-0.8103), 0.38427947598253276, 0.8564814814814815, tensor(1.0971)]
[tensor(-0.7162), 0.4410480349344978, 0.8564814814814815, tensor(1.4890)]
[2023-01-16 07:25:23,712.712 dsw44922-6f76bf568-tbjcv:65759 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0396), 0.29694323144104806, 0.8472222222222222, tensor(0.4451)]
[tensor(-0.9408), 0.3056768558951965, 0.8472222222222222, tensor(0.5876)]
[tensor(-0.9408), 0.3056768558951965, 0.8472222222222222, tensor(0.5876)]
[tensor(-0.7479), 0.4104803493449782, 0.8518518518518519, tensor(1.3045)]
[tensor(-0.7479), 0.4148471615720524, 0.8518518518518519, tensor(1.3260)]
[tensor(-0.7142), 0.4148471615720524, 0.8518518518518519, tensor(1.3382)]
[tensor(-0.6698), 0.42358078602620086, 0.8981481481481481, tensor(1.4481)]
[tensor(-0.6698), 0.42358078602620086, 0.8981481481481481, tensor(1.4481)]
[tensor(-0.6698), 0.4410480349344978, 0.8981481481481481, tensor(1.5116)]
[tensor(-0.6688), 0.4410480349344978, 0.8981481481481481, tensor(1.5116)]
[tensor(-0.6688), 0.4410480349344978, 0.8981481481481481, tensor(1.5116)]
[tensor(-0.6589), 0.4410480349344978, 0.8981481481481481, tensor(1.5246)]
[tensor(-0.6477), 0.4410480349344978, 0.8981481481481481, tensor(1.5246)]
[tensor(-0.6477), 0.4410480349344978, 0.8981481481481481, tensor(1.5246)]
[tensor(-0.6477), 0.45414847161572053, 0.8981481481481481, tensor(1.6016)]
[tensor(-0.6477), 0.45414847161572053, 0.8981481481481481, tensor(1.6016)]
[tensor(-0.6477), 0.462882096069869, 0.8981481481481481, tensor(1.6550)]
[tensor(-0.6477), 0.462882096069869, 0.9074074074074074, tensor(1.6550)]
[tensor(-0.6477), 0.462882096069869, 0.9074074074074074, tensor(1.6550)]
[tensor(-0.6477), 0.462882096069869, 0.9074074074074074, tensor(1.6550)]
[tensor(-0.6477), 0.462882096069869, 0.9074074074074074, tensor(1.6550)]
[tensor(-0.6477), 0.462882096069869, 0.9074074074074074, tensor(1.6550)]
[tensor(-0.6477), 0.4759825327510917, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6477), 0.4759825327510917, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6450), 0.4759825327510917, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6450), 0.4759825327510917, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6450), 0.4759825327510917, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6450), 0.4759825327510917, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6446), 0.4759825327510917, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6446), 0.48034934497816595, 0.9074074074074074, tensor(1.7161)]
[tensor(-0.6446), 0.4847161572052402, 0.9074074074074074, tensor(1.7546)]
[tensor(-0.6446), 0.4847161572052402, 0.9074074074074074, tensor(1.7546)]
[tensor(-0.6446), 0.4890829694323144, 0.9074074074074074, tensor(1.7872)]
[tensor(-0.6446), 0.4890829694323144, 0.9074074074074074, tensor(1.7872)]
[tensor(-0.6446), 0.4890829694323144, 0.9074074074074074, tensor(1.7872)]
[tensor(-0.6446), 0.4890829694323144, 0.9074074074074074, tensor(1.7872)]
[tensor(-0.6446), 0.4890829694323144, 0.9074074074074074, tensor(1.7872)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[tensor(-0.6446), 0.4978165938864629, 0.9074074074074074, tensor(1.8247)]
[2023-01-16 07:51:48,150.150 dsw44922-6f76bf568-tbjcv:65825 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0806), 0.2794759825327511, 0.7777777777777778, tensor(0.3168)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9300), 0.36681222707423583, 0.7777777777777778, tensor(0.9040)]
[tensor(-0.9078), 0.3930131004366812, 0.8101851851851852, tensor(1.0572)]
[tensor(-0.8452), 0.3930131004366812, 0.8472222222222222, tensor(1.0572)]
[tensor(-0.7488), 0.4192139737991266, 0.8564814814814815, tensor(1.3473)]
[tensor(-0.7448), 0.43231441048034935, 0.8611111111111112, tensor(1.4168)]
[tensor(-0.6910), 0.4759825327510917, 0.8611111111111112, tensor(1.6889)]
[tensor(-0.6910), 0.4759825327510917, 0.8611111111111112, tensor(1.6889)]
[tensor(-0.6910), 0.4759825327510917, 0.8611111111111112, tensor(1.6889)]
[tensor(-0.6513), 0.4759825327510917, 0.8703703703703703, tensor(1.6889)]
[tensor(-0.6513), 0.4759825327510917, 0.8703703703703703, tensor(1.6889)]
[tensor(-0.6513), 0.4847161572052402, 0.8703703703703703, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.8703703703703703, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
[tensor(-0.6513), 0.4847161572052402, 0.875, tensor(1.7550)]
early stopping at 33
[2023-01-16 08:09:09,113.113 dsw44922-6f76bf568-tbjcv:65878 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2312), 0.3415730337078652, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4817), 0.5775280898876405, tensor(1.4059)]
[tensor(-1.2303), 0.6292134831460674, tensor(1.9157)]
[tensor(-1.1870), 0.6584269662921348, tensor(2.1051)]
[tensor(-1.1870), 0.6674157303370787, tensor(2.1051)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1870), 0.6696629213483146, tensor(2.1051)]
[tensor(-1.1870), 0.6764044943820224, tensor(2.1265)]
[tensor(-1.1870), 0.6764044943820224, tensor(2.1265)]
[tensor(-1.1870), 0.6853932584269663, tensor(2.1265)]
[tensor(-1.1870), 0.6853932584269663, tensor(2.1265)]
[2023-01-16 08:14:47,143.143 dsw44922-6f76bf568-tbjcv:65913 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8650), 0.46741573033707867, tensor(0.4721)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2739), 0.6089887640449438, tensor(1.7710)]
[tensor(-1.1899), 0.6561797752808989, tensor(2.0910)]
[tensor(-1.1899), 0.6741573033707865, tensor(2.1650)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1899), 0.6741573033707865, tensor(2.1650)]
[tensor(-1.1899), 0.6831460674157304, tensor(2.1650)]
[tensor(-1.1899), 0.6831460674157304, tensor(2.1650)]
[tensor(-1.1899), 0.6831460674157304, tensor(2.1650)]
[tensor(-1.1899), 0.6831460674157304, tensor(2.1650)]
[tensor(-1.1899), 0.6876404494382022, tensor(2.1650)]
[2023-01-16 08:20:17,252.252 dsw44922-6f76bf568-tbjcv:65949 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9747), 0.4337078651685393, tensor(0.1938)]
[tensor(-1.6930), 0.5191011235955056, tensor(0.9025)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3491), 0.6179775280898876, tensor(1.7407)]
[tensor(-1.3421), 0.6337078651685393, tensor(1.8265)]
[tensor(-1.1965), 0.6674157303370787, tensor(2.1406)]
[tensor(-1.1965), 0.6674157303370787, tensor(2.1406)]
[tensor(-1.1965), 0.6719101123595506, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6786516853932584, tensor(2.1406)]
[tensor(-1.1965), 0.6831460674157304, tensor(2.1406)]
[tensor(-1.1965), 0.6831460674157304, tensor(2.1406)]
[tensor(-1.1965), 0.6831460674157304, tensor(2.1406)]
[tensor(-1.1965), 0.6831460674157304, tensor(2.1406)]
[tensor(-1.1965), 0.6831460674157304, tensor(2.1406)]
[tensor(-1.1965), 0.6831460674157304, tensor(2.1406)]
[tensor(-1.1965), 0.6831460674157304, tensor(2.1406)]
[tensor(-1.1965), 0.6853932584269663, tensor(2.1406)]
[tensor(-1.1965), 0.6853932584269663, tensor(2.1406)]
[tensor(-1.1965), 0.6853932584269663, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
[tensor(-1.1965), 0.6876404494382022, tensor(2.1406)]
early stopping at 38
[2023-01-16 08:40:45,812.812 dsw44922-6f76bf568-tbjcv:66006 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1965), 0.41123595505617977, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4677), 0.5775280898876405, tensor(1.4200)]
[tensor(-1.1898), 0.6584269662921348, tensor(2.1023)]
[tensor(-1.1898), 0.6651685393258427, tensor(2.1023)]
[tensor(-1.1738), 0.6764044943820224, tensor(2.2083)]
[tensor(-1.1738), 0.6808988764044944, tensor(2.2083)]
[tensor(-1.1738), 0.6921348314606741, tensor(2.2367)]
[tensor(-1.1738), 0.6966292134831461, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
[tensor(-1.1738), 0.698876404494382, tensor(2.2367)]
early stopping at 27
[2023-01-16 08:55:09,899.899 dsw44922-6f76bf568-tbjcv:66055 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9070), 0.314410480349345, 0.8379629629629629, tensor(0.6651)]
[tensor(-0.9070), 0.34934497816593885, 0.8379629629629629, tensor(0.7223)]
[tensor(-0.8491), 0.40611353711790393, 0.8379629629629629, tensor(1.1815)]
[tensor(-0.8460), 0.40611353711790393, 0.8564814814814815, tensor(1.1815)]
[tensor(-0.7184), 0.40611353711790393, 0.875, tensor(1.1815)]
[2023-01-16 08:57:53,605.605 dsw44922-6f76bf568-tbjcv:66086 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1522), 0.23580786026200873, 0.7453703703703703, tensor(0.0268)]
[tensor(-0.8525), 0.31877729257641924, 0.8379629629629629, tensor(0.7414)]
[tensor(-0.7860), 0.388646288209607, 0.8518518518518519, tensor(1.1572)]
[tensor(-0.7860), 0.4017467248908297, 0.8564814814814815, tensor(1.2147)]
[tensor(-0.7860), 0.4104803493449782, 0.8564814814814815, tensor(1.2425)]
[2023-01-16 09:00:41,491.491 dsw44922-6f76bf568-tbjcv:66117 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9826), 0.34497816593886466, 0.8240740740740741, tensor(0.7423)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8934), 0.34497816593886466, 0.8240740740740741, tensor(0.7879)]
[tensor(-0.8660), 0.37554585152838427, 0.8240740740740741, tensor(1.0117)]
[tensor(-0.7894), 0.4279475982532751, 0.8333333333333334, tensor(1.3503)]
[tensor(-0.7557), 0.4279475982532751, 0.8564814814814815, tensor(1.3503)]
[tensor(-0.7329), 0.4279475982532751, 0.8611111111111112, tensor(1.3850)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7329), 0.4279475982532751, 0.8611111111111112, tensor(1.3850)]
[tensor(-0.7329), 0.4279475982532751, 0.8611111111111112, tensor(1.3850)]
[tensor(-0.6962), 0.45414847161572053, 0.8657407407407407, tensor(1.5745)]
[tensor(-0.6887), 0.45414847161572053, 0.8703703703703703, tensor(1.5745)]
[tensor(-0.6887), 0.45414847161572053, 0.8703703703703703, tensor(1.5745)]
[tensor(-0.6887), 0.4672489082969432, 0.8703703703703703, tensor(1.6288)]
[tensor(-0.6828), 0.4672489082969432, 0.8703703703703703, tensor(1.6535)]
[tensor(-0.6674), 0.4672489082969432, 0.8796296296296297, tensor(1.6535)]
[tensor(-0.6674), 0.47161572052401746, 0.8796296296296297, tensor(1.6755)]
[tensor(-0.6674), 0.47161572052401746, 0.8796296296296297, tensor(1.6755)]
[tensor(-0.6674), 0.47161572052401746, 0.8796296296296297, tensor(1.6755)]
[tensor(-0.6647), 0.47161572052401746, 0.8796296296296297, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
[tensor(-0.6611), 0.47161572052401746, 0.8935185185185185, tensor(1.6755)]
early stopping at 29
[2023-01-16 09:15:55,567.567 dsw44922-6f76bf568-tbjcv:66167 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9371), 0.314410480349345, 0.8472222222222222, tensor(0.6349)]
[tensor(-0.8681), 0.40611353711790393, 0.8472222222222222, tensor(1.1624)]
[tensor(-0.7735), 0.4148471615720524, 0.8703703703703703, tensor(1.3008)]
[tensor(-0.7173), 0.4585152838427948, 0.875, tensor(1.5753)]
[tensor(-0.7173), 0.4585152838427948, 0.875, tensor(1.5753)]
[tensor(-0.7173), 0.4585152838427948, 0.875, tensor(1.5753)]
[tensor(-0.6855), 0.4585152838427948, 0.875, tensor(1.5753)]
[tensor(-0.6855), 0.4585152838427948, 0.8796296296296297, tensor(1.5753)]
[tensor(-0.6855), 0.4585152838427948, 0.8796296296296297, tensor(1.5753)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.462882096069869, 0.8796296296296297, tensor(1.6485)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
[tensor(-0.6659), 0.4759825327510917, 0.8796296296296297, tensor(1.6870)]
early stopping at 37
[2023-01-16 09:35:29,420.420 dsw44922-6f76bf568-tbjcv:66293 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1538), 0.4067415730337079, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4998), 0.5640449438202247, tensor(1.3204)]
[tensor(-1.3609), 0.6382022471910113, tensor(1.8301)]
[tensor(-1.2663), 0.6449438202247191, tensor(1.9584)]
[tensor(-1.2663), 0.6561797752808989, tensor(1.9694)]
[tensor(-1.2642), 0.6921348314606741, tensor(2.1964)]
[tensor(-1.2642), 0.6921348314606741, tensor(2.1964)]
[tensor(-1.2642), 0.6921348314606741, tensor(2.1964)]
[tensor(-1.2642), 0.6921348314606741, tensor(2.1964)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2642), 0.6921348314606741, tensor(2.1964)]
[2023-01-16 09:41:10,695.695 dsw44922-6f76bf568-tbjcv:66328 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8578), 0.45393258426966293, tensor(0.4118)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2396), 0.6539325842696629, tensor(2.0300)]
[tensor(-1.2396), 0.6539325842696629, tensor(2.0300)]
[tensor(-1.2396), 0.6539325842696629, tensor(2.0300)]
[tensor(-1.2041), 0.6876404494382022, tensor(2.2341)]
[tensor(-1.2041), 0.6921348314606741, tensor(2.2341)]
[tensor(-1.2041), 0.6921348314606741, tensor(2.2341)]
[tensor(-1.2041), 0.6921348314606741, tensor(2.2341)]
[tensor(-1.2041), 0.6966292134831461, tensor(2.2341)]
[tensor(-1.2041), 0.6966292134831461, tensor(2.2341)]
[2023-01-16 09:47:00,253.253 dsw44922-6f76bf568-tbjcv:66364 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0042), 0.449438202247191, tensor(0.2430)]
[tensor(-1.6881), 0.5573033707865168, tensor(1.0985)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3484), 0.6134831460674157, tensor(1.7190)]
[tensor(-1.3484), 0.6134831460674157, tensor(1.7190)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2662), 0.6494382022471911, tensor(1.9810)]
[tensor(-1.2418), 0.6539325842696629, tensor(2.0278)]
[tensor(-1.2418), 0.6651685393258427, tensor(2.0278)]
[tensor(-1.2418), 0.6651685393258427, tensor(2.0278)]
[tensor(-1.2418), 0.6651685393258427, tensor(2.0278)]
[tensor(-1.2418), 0.6674157303370787, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6696629213483146, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
[tensor(-1.2418), 0.6719101123595506, tensor(2.0278)]
early stopping at 46
[2023-01-16 10:11:56,697.697 dsw44922-6f76bf568-tbjcv:66428 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0658), 0.41123595505617977, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3529), 0.6314606741573033, tensor(1.8044)]
[tensor(-1.2011), 0.6449438202247191, tensor(2.0236)]
[tensor(-1.2011), 0.651685393258427, tensor(2.0236)]
[tensor(-1.2011), 0.6606741573033708, tensor(2.0713)]
[tensor(-1.2011), 0.6651685393258427, tensor(2.0713)]
[tensor(-1.2011), 0.6696629213483146, tensor(2.0763)]
[tensor(-1.2011), 0.6696629213483146, tensor(2.0763)]
[tensor(-1.2011), 0.6696629213483146, tensor(2.0763)]
[tensor(-1.2011), 0.6696629213483146, tensor(2.0763)]
[tensor(-1.2011), 0.6696629213483146, tensor(2.0763)]
[tensor(-1.2011), 0.6696629213483146, tensor(2.0763)]
[tensor(-1.2011), 0.6831460674157304, tensor(2.0763)]
[tensor(-1.2011), 0.6831460674157304, tensor(2.0763)]
[tensor(-1.2011), 0.6831460674157304, tensor(2.0763)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2011), 0.6831460674157304, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
[tensor(-1.2011), 0.6898876404494382, tensor(2.0763)]
early stopping at 27
[2023-01-16 10:26:27,847.847 dsw44922-6f76bf568-tbjcv:66477 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9293), 0.35807860262008734, 0.8055555555555556, tensor(0.8611)]
[tensor(-0.9293), 0.35807860262008734, 0.8055555555555556, tensor(0.8611)]
[tensor(-0.9100), 0.3624454148471616, 0.8148148148148148, tensor(0.9022)]
[tensor(-0.7837), 0.39737991266375544, 0.8564814814814815, tensor(1.2032)]
[tensor(-0.7803), 0.39737991266375544, 0.8564814814814815, tensor(1.2032)]
[2023-01-16 10:29:14,151.151 dsw44922-6f76bf568-tbjcv:66508 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4399), 0.2838427947598253, 0.6481481481481481, 0.0]
[tensor(-1.0320), 0.31877729257641924, 0.8148148148148148, tensor(0.5619)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0320), 0.31877729257641924, 0.8148148148148148, tensor(0.5619)]
[tensor(-0.9835), 0.33624454148471616, 0.8240740740740741, tensor(0.6977)]
[tensor(-0.8865), 0.3799126637554585, 0.8240740740740741, tensor(1.0131)]
[2023-01-16 10:32:01,285.285 dsw44922-6f76bf568-tbjcv:66539 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-0.9786), 0.33624454148471616, 0.7824074074074074, tensor(0.7026)]
[tensor(-0.9058), 0.34934497816593885, 0.8055555555555556, tensor(0.8410)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.8562), 0.388646288209607, 0.8287037037037037, tensor(1.0870)]
[tensor(-0.8084), 0.42358078602620086, 0.8379629629629629, tensor(1.3095)]
[tensor(-0.7512), 0.42358078602620086, 0.8611111111111112, tensor(1.3230)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.7512), 0.42358078602620086, 0.8611111111111112, tensor(1.3617)]
[tensor(-0.7512), 0.42358078602620086, 0.8611111111111112, tensor(1.3617)]
[tensor(-0.7512), 0.4366812227074236, 0.8611111111111112, tensor(1.4212)]
[tensor(-0.7512), 0.4366812227074236, 0.8611111111111112, tensor(1.4212)]
[tensor(-0.7512), 0.4366812227074236, 0.8611111111111112, tensor(1.4212)]
[tensor(-0.7512), 0.4366812227074236, 0.8611111111111112, tensor(1.4212)]
[tensor(-0.7512), 0.4366812227074236, 0.8611111111111112, tensor(1.4212)]
[tensor(-0.7512), 0.4366812227074236, 0.8611111111111112, tensor(1.4212)]
[tensor(-0.7512), 0.4366812227074236, 0.8611111111111112, tensor(1.4212)]
[tensor(-0.7507), 0.44541484716157204, 0.8611111111111112, tensor(1.4763)]
[tensor(-0.7387), 0.45414847161572053, 0.8611111111111112, tensor(1.5321)]
[tensor(-0.7375), 0.45414847161572053, 0.8611111111111112, tensor(1.5321)]
[tensor(-0.7178), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7178), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7178), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7178), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7178), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7178), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7178), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7144), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7057), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7057), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7057), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.7057), 0.45414847161572053, 0.8703703703703703, tensor(1.5529)]
[tensor(-0.6968), 0.4759825327510917, 0.8703703703703703, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8703703703703703, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8703703703703703, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8703703703703703, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8703703703703703, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8703703703703703, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8703703703703703, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6918), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6825), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6825), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6825), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6825), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6804), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6804), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6804), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6804), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6804), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6804), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[tensor(-0.6804), 0.4759825327510917, 0.8796296296296297, tensor(1.6831)]
[2023-01-16 10:58:35,011.011 dsw44922-6f76bf568-tbjcv:66606 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-1.4344), 0.21397379912663755, 0.5740740740740741, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4344), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.4344), 0.21397379912663755, 0.5740740740740741, 0.0]
early stopping at 3
[2023-01-16 11:00:20,407.407 dsw44922-6f76bf568-tbjcv:66636 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0440), 0.43595505617977526, tensor(0.1358)]
[tensor(-1.3500), 0.6224719101123596, tensor(1.7624)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1115), 0.6966292134831461, tensor(2.3716)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1115), 0.6966292134831461, tensor(2.3716)]
[tensor(-1.1115), 0.7033707865168539, tensor(2.3716)]
[tensor(-1.1115), 0.7033707865168539, tensor(2.3716)]
[tensor(-1.1115), 0.7033707865168539, tensor(2.3716)]
[tensor(-1.1115), 0.7033707865168539, tensor(2.3716)]
[tensor(-1.1115), 0.7056179775280899, tensor(2.3716)]
[tensor(-1.1115), 0.7056179775280899, tensor(2.3716)]
[2023-01-16 11:06:05,428.428 dsw44922-6f76bf568-tbjcv:66671 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8477), 0.4606741573033708, tensor(0.4556)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2395), 0.6561797752808989, tensor(2.0414)]
[tensor(-1.0936), 0.6921348314606741, tensor(2.3671)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0936), 0.6921348314606741, tensor(2.3671)]
[tensor(-1.0936), 0.7078651685393258, tensor(2.3921)]
[tensor(-1.0936), 0.7078651685393258, tensor(2.3921)]
[tensor(-1.0936), 0.7078651685393258, tensor(2.3921)]
[tensor(-1.0936), 0.7191011235955056, tensor(2.3921)]
[tensor(-1.0936), 0.7191011235955056, tensor(2.3921)]
[tensor(-1.0936), 0.7191011235955056, tensor(2.3921)]
[2023-01-16 11:11:29,294.294 dsw44922-6f76bf568-tbjcv:66707 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8094), 0.45617977528089887, tensor(0.4715)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5674), 0.5752808988764045, tensor(1.3091)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2696), 0.6539325842696629, tensor(2.0001)]
[tensor(-1.1900), 0.6741573033707865, tensor(2.1808)]
[tensor(-1.1390), 0.6921348314606741, tensor(2.3217)]
[tensor(-1.1390), 0.6943820224719102, tensor(2.3325)]
[tensor(-1.1390), 0.6966292134831461, tensor(2.3325)]
[tensor(-1.1390), 0.701123595505618, tensor(2.3325)]
[tensor(-1.1390), 0.7078651685393258, tensor(2.3325)]
[tensor(-1.1390), 0.7078651685393258, tensor(2.3325)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1390), 0.7078651685393258, tensor(2.3325)]
[tensor(-1.1390), 0.7146067415730337, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
[tensor(-1.1390), 0.7213483146067415, tensor(2.3325)]
early stopping at 23
[2023-01-16 11:24:11,160.160 dsw44922-6f76bf568-tbjcv:66753 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0853), 0.41123595505617977, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2292), 0.647191011235955, tensor(2.0068)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1017), 0.6674157303370787, tensor(2.2354)]
[tensor(-1.1017), 0.6764044943820224, tensor(2.2354)]
[tensor(-1.1017), 0.6876404494382022, tensor(2.3227)]
[tensor(-1.1017), 0.6876404494382022, tensor(2.3227)]
[tensor(-1.1017), 0.7033707865168539, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.3227)]
early stopping at 18
[2023-01-16 11:34:15,206.206 dsw44922-6f76bf568-tbjcv:66795 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
[tensor(-0.9154), 0.3318777292576419, 0.8425925925925926, tensor(0.7440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.8287), 0.34934497816593885, 0.8518518518518519, tensor(0.9181)]
[tensor(-0.8287), 0.34934497816593885, 0.8518518518518519, tensor(0.9181)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8287), 0.37117903930131, 0.8518518518518519, tensor(0.9549)]
[tensor(-0.7508), 0.37117903930131, 0.8657407407407407, tensor(1.0833)]
[2023-01-16 11:37:05,059.059 dsw44922-6f76bf568-tbjcv:66826 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3429), 0.21397379912663755, 0.5740740740740741, 0.0]
[tensor(-1.0565), 0.2925764192139738, 0.7962962962962963, tensor(0.4064)]
[tensor(-0.9308), 0.3318777292576419, 0.8148148148148148, tensor(0.7286)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.8716), 0.34497816593886466, 0.8472222222222222, tensor(0.8533)]
[tensor(-0.8494), 0.35807860262008734, 0.8472222222222222, tensor(0.9410)]
[2023-01-16 11:39:53,717.717 dsw44922-6f76bf568-tbjcv:66857 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9261), 0.3406113537117904, 0.8287037037037037, tensor(0.7769)]
[tensor(-0.8783), 0.3406113537117904, 0.8333333333333334, tensor(0.8248)]
[tensor(-0.8783), 0.36681222707423583, 0.8379629629629629, tensor(0.9164)]
[tensor(-0.7450), 0.45414847161572053, 0.8564814814814815, tensor(1.5257)]
[tensor(-0.7241), 0.45414847161572053, 0.8564814814814815, tensor(1.5257)]
[tensor(-0.7241), 0.45414847161572053, 0.8564814814814815, tensor(1.5257)]
[tensor(-0.7241), 0.45414847161572053, 0.8564814814814815, tensor(1.5257)]
[tensor(-0.7241), 0.45414847161572053, 0.8657407407407407, tensor(1.5257)]
[tensor(-0.7112), 0.45414847161572053, 0.8657407407407407, tensor(1.5257)]
[tensor(-0.7071), 0.45414847161572053, 0.8657407407407407, tensor(1.5257)]
[tensor(-0.7071), 0.45414847161572053, 0.8657407407407407, tensor(1.5257)]
[tensor(-0.7071), 0.45414847161572053, 0.8657407407407407, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.8657407407407407, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.8657407407407407, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.8703703703703703, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.8703703703703703, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.8703703703703703, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.8703703703703703, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.8703703703703703, tensor(1.5257)]
[tensor(-0.7058), 0.45414847161572053, 0.875, tensor(1.5257)]
[tensor(-0.6981), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6981), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6981), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6981), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6981), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6981), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6981), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6969), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6969), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6969), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6890), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6880), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6880), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6878), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6878), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6841), 0.45414847161572053, 0.875, tensor(1.5508)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
[tensor(-0.6841), 0.462882096069869, 0.875, tensor(1.6230)]
early stopping at 47
[2023-01-16 12:04:29,402.402 dsw44922-6f76bf568-tbjcv:66934 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1987), 0.26200873362445415, 0.7546296296296297, tensor(0.1114)]
[tensor(-0.9519), 0.3406113537117904, 0.8333333333333334, tensor(0.7511)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8726), 0.39737991266375544, 0.8333333333333334, tensor(1.1143)]
[tensor(-0.8132), 0.4148471615720524, 0.8425925925925926, tensor(1.2610)]
[tensor(-0.7834), 0.44541484716157204, 0.8425925925925926, tensor(1.4437)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7729), 0.44541484716157204, 0.8425925925925926, tensor(1.4437)]
[tensor(-0.7201), 0.44541484716157204, 0.8564814814814815, tensor(1.4633)]
[tensor(-0.7201), 0.4497816593886463, 0.8564814814814815, tensor(1.4997)]
[tensor(-0.7201), 0.4497816593886463, 0.8564814814814815, tensor(1.4997)]
[tensor(-0.7201), 0.4497816593886463, 0.8611111111111112, tensor(1.4997)]
[tensor(-0.7201), 0.45414847161572053, 0.8611111111111112, tensor(1.5349)]
[tensor(-0.7130), 0.45414847161572053, 0.8611111111111112, tensor(1.5577)]
[tensor(-0.7130), 0.45414847161572053, 0.8611111111111112, tensor(1.5577)]
[tensor(-0.7058), 0.45414847161572053, 0.8611111111111112, tensor(1.5649)]
[tensor(-0.7016), 0.4759825327510917, 0.8611111111111112, tensor(1.6783)]
[tensor(-0.7016), 0.4759825327510917, 0.8611111111111112, tensor(1.6783)]
[tensor(-0.6965), 0.4759825327510917, 0.8611111111111112, tensor(1.6783)]
[tensor(-0.6838), 0.4759825327510917, 0.8611111111111112, tensor(1.6783)]
[tensor(-0.6834), 0.4759825327510917, 0.875, tensor(1.6783)]
[tensor(-0.6806), 0.4759825327510917, 0.875, tensor(1.6783)]
[tensor(-0.6783), 0.4759825327510917, 0.875, tensor(1.6783)]
[tensor(-0.6783), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6783), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6783), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
[tensor(-0.6765), 0.5021834061135371, 0.875, tensor(1.8283)]
early stopping at 43
[2023-01-16 12:26:57,016.016 dsw44922-6f76bf568-tbjcv:66995 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0722), 0.44269662921348313, tensor(0.1413)]
[tensor(-1.4307), 0.5797752808988764, tensor(1.4681)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1571), 0.6853932584269663, tensor(2.2698)]
[tensor(-1.1077), 0.701123595505618, tensor(2.3980)]
[tensor(-1.1077), 0.7033707865168539, tensor(2.3980)]
[tensor(-1.1077), 0.7033707865168539, tensor(2.3980)]
[tensor(-1.1077), 0.7033707865168539, tensor(2.3980)]
[tensor(-1.1077), 0.7123595505617978, tensor(2.3980)]
[tensor(-1.1077), 0.7235955056179775, tensor(2.4168)]
[tensor(-1.1077), 0.7235955056179775, tensor(2.4168)]
[2023-01-16 12:32:37,860.860 dsw44922-6f76bf568-tbjcv:67030 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8087), 0.4786516853932584, tensor(0.5845)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3023), 0.6089887640449438, tensor(1.7426)]
[tensor(-1.1714), 0.6674157303370787, tensor(2.1656)]
[tensor(-1.1714), 0.6674157303370787, tensor(2.1656)]
[tensor(-1.1714), 0.6741573033707865, tensor(2.1656)]
[tensor(-1.1714), 0.6898876404494382, tensor(2.2130)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1714), 0.6898876404494382, tensor(2.2130)]
[tensor(-1.1714), 0.7033707865168539, tensor(2.2283)]
[tensor(-1.1714), 0.7123595505617978, tensor(2.2750)]
[tensor(-1.1714), 0.7123595505617978, tensor(2.2750)]
[2023-01-16 12:38:07,756.756 dsw44922-6f76bf568-tbjcv:67065 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8285), 0.4404494382022472, tensor(0.3737)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5504), 0.5797752808988764, tensor(1.3484)]
[tensor(-1.2577), 0.6382022471910113, tensor(1.9333)]
[tensor(-1.2126), 0.6719101123595506, tensor(2.1470)]
[tensor(-1.1434), 0.6898876404494382, tensor(2.3061)]
[tensor(-1.1370), 0.6966292134831461, tensor(2.3461)]
[tensor(-1.1370), 0.6966292134831461, tensor(2.3461)]
[tensor(-1.1370), 0.6966292134831461, tensor(2.3461)]
[tensor(-1.1370), 0.698876404494382, tensor(2.3461)]
[tensor(-1.1370), 0.7078651685393258, tensor(2.3461)]
[tensor(-1.1370), 0.7078651685393258, tensor(2.3461)]
[tensor(-1.1370), 0.7078651685393258, tensor(2.3461)]
[tensor(-1.1370), 0.7078651685393258, tensor(2.3461)]
[tensor(-1.1370), 0.7078651685393258, tensor(2.3461)]
[tensor(-1.1370), 0.7101123595505618, tensor(2.3461)]
[tensor(-1.1370), 0.7101123595505618, tensor(2.3461)]
[tensor(-1.1370), 0.7101123595505618, tensor(2.3461)]
[tensor(-1.1370), 0.7101123595505618, tensor(2.3461)]
[tensor(-1.1370), 0.7101123595505618, tensor(2.3461)]
[tensor(-1.1370), 0.7101123595505618, tensor(2.3461)]
[tensor(-1.1370), 0.7101123595505618, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7123595505617978, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
[tensor(-1.1370), 0.7191011235955056, tensor(2.3461)]
early stopping at 49
[2023-01-16 13:04:34,514.514 dsw44922-6f76bf568-tbjcv:67134 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0417), 0.40898876404494383, tensor(0.0032)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4109), 0.6247191011235955, tensor(1.7127)]
[tensor(-1.1961), 0.6629213483146067, tensor(2.1185)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1961), 0.6629213483146067, tensor(2.1185)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1961), 0.6786516853932584, tensor(2.1185)]
[tensor(-1.1961), 0.6853932584269663, tensor(2.1475)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.6943820224719102, tensor(2.1705)]
[tensor(-1.1961), 0.698876404494382, tensor(2.1705)]
[tensor(-1.1961), 0.698876404494382, tensor(2.1705)]
[tensor(-1.1961), 0.701123595505618, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
[tensor(-1.1961), 0.7123595505617978, tensor(2.1705)]
early stopping at 33
[2023-01-16 13:22:19,053.053 dsw44922-6f76bf568-tbjcv:67188 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9758), 0.2838427947598253, 0.8333333333333334, tensor(0.4435)]
[tensor(-0.9758), 0.28820960698689957, 0.8333333333333334, tensor(0.4435)]
[tensor(-0.9758), 0.314410480349345, 0.8333333333333334, tensor(0.4435)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8253), 0.35807860262008734, 0.8518518518518519, tensor(0.9651)]
[tensor(-0.7894), 0.42358078602620086, 0.8518518518518519, tensor(1.3285)]
[2023-01-16 13:25:07,984.984 dsw44922-6f76bf568-tbjcv:67219 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6550), 0.25327510917030566, 0.6342592592592593, 0.0]
[tensor(-1.0218), 0.32751091703056767, 0.7962962962962963, tensor(0.6158)]
[tensor(-1.0218), 0.32751091703056767, 0.7962962962962963, tensor(0.6158)]
[tensor(-0.9423), 0.39737991266375544, 0.7962962962962963, tensor(1.0446)]
[tensor(-0.8161), 0.39737991266375544, 0.8472222222222222, tensor(1.1490)]
[2023-01-16 13:27:54,423.423 dsw44922-6f76bf568-tbjcv:67250 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9376), 0.3231441048034934, 0.8333333333333334, tensor(0.6781)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.8539), 0.34497816593886466, 0.8518518518518519, tensor(0.8710)]
[tensor(-0.8539), 0.40611353711790393, 0.8518518518518519, tensor(1.1597)]
[tensor(-0.7857), 0.40611353711790393, 0.8518518518518519, tensor(1.2448)]
[tensor(-0.7614), 0.42358078602620086, 0.8611111111111112, tensor(1.3565)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.7400), 0.4279475982532751, 0.8611111111111112, tensor(1.3998)]
[tensor(-0.7395), 0.4279475982532751, 0.8657407407407407, tensor(1.3998)]
[tensor(-0.7395), 0.4279475982532751, 0.8657407407407407, tensor(1.3998)]
[tensor(-0.7120), 0.45414847161572053, 0.8657407407407407, tensor(1.5588)]
[tensor(-0.7030), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.7030), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.7030), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.7030), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.7030), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.7030), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.6961), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.6919), 0.4585152838427948, 0.8657407407407407, tensor(1.5896)]
[tensor(-0.6919), 0.4585152838427948, 0.8703703703703703, tensor(1.5896)]
[tensor(-0.6891), 0.4585152838427948, 0.8703703703703703, tensor(1.5896)]
[tensor(-0.6891), 0.4585152838427948, 0.8703703703703703, tensor(1.5896)]
[tensor(-0.6891), 0.4585152838427948, 0.8703703703703703, tensor(1.5896)]
[tensor(-0.6891), 0.4585152838427948, 0.8703703703703703, tensor(1.5896)]
[tensor(-0.6891), 0.4585152838427948, 0.8703703703703703, tensor(1.5896)]
[tensor(-0.6891), 0.4672489082969432, 0.8703703703703703, tensor(1.6335)]
[tensor(-0.6891), 0.4672489082969432, 0.8703703703703703, tensor(1.6335)]
[tensor(-0.6891), 0.4672489082969432, 0.8703703703703703, tensor(1.6335)]
[tensor(-0.6891), 0.4672489082969432, 0.8703703703703703, tensor(1.6335)]
[tensor(-0.6871), 0.4672489082969432, 0.8703703703703703, tensor(1.6335)]
[tensor(-0.6820), 0.4672489082969432, 0.875, tensor(1.6335)]
[tensor(-0.6820), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6820), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6820), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6820), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6796), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6647), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4672489082969432, 0.875, tensor(1.6502)]
[tensor(-0.6584), 0.4759825327510917, 0.875, tensor(1.6827)]
[tensor(-0.6584), 0.4759825327510917, 0.875, tensor(1.6952)]
[tensor(-0.6584), 0.4759825327510917, 0.875, tensor(1.6952)]
[tensor(-0.6584), 0.4759825327510917, 0.875, tensor(1.6952)]
[tensor(-0.6584), 0.4759825327510917, 0.875, tensor(1.6952)]
[tensor(-0.6584), 0.4759825327510917, 0.875, tensor(1.6952)]
[2023-01-16 13:54:22,144.144 dsw44922-6f76bf568-tbjcv:67334 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mosi
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0029), 0.2925764192139738, 0.7870370370370371, tensor(0.4600)]
[tensor(-1.0029), 0.3406113537117904, 0.8009259259259259, tensor(0.6957)]
[tensor(-0.8322), 0.4017467248908297, 0.8240740740740741, tensor(1.1766)]
[tensor(-0.7965), 0.4017467248908297, 0.8518518518518519, tensor(1.1766)]
[tensor(-0.7092), 0.45414847161572053, 0.8657407407407407, tensor(1.5615)]
[tensor(-0.7092), 0.45414847161572053, 0.8657407407407407, tensor(1.5615)]
[tensor(-0.7067), 0.45414847161572053, 0.8657407407407407, tensor(1.5615)]
[tensor(-0.7067), 0.45414847161572053, 0.8657407407407407, tensor(1.5615)]
[tensor(-0.7031), 0.45414847161572053, 0.8657407407407407, tensor(1.5615)]
[tensor(-0.6865), 0.4759825327510917, 0.8657407407407407, tensor(1.6934)]
[tensor(-0.6865), 0.4759825327510917, 0.8657407407407407, tensor(1.6934)]
[tensor(-0.6864), 0.4759825327510917, 0.8657407407407407, tensor(1.6934)]
[tensor(-0.6864), 0.4759825327510917, 0.8657407407407407, tensor(1.6934)]
[tensor(-0.6799), 0.4847161572052402, 0.8657407407407407, tensor(1.7437)]
[tensor(-0.6799), 0.4847161572052402, 0.8657407407407407, tensor(1.7437)]
[tensor(-0.6679), 0.4847161572052402, 0.8657407407407407, tensor(1.7437)]
[tensor(-0.6601), 0.5021834061135371, 0.8703703703703703, tensor(1.8508)]
[tensor(-0.6525), 0.5021834061135371, 0.8703703703703703, tensor(1.8585)]
[tensor(-0.6525), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6525), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6525), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6525), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6525), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6525), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6505), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6505), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6468), 0.5021834061135371, 0.8796296296296297, tensor(1.8585)]
[tensor(-0.6458), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[tensor(-0.6411), 0.5021834061135371, 0.8796296296296297, tensor(1.8652)]
[2023-01-16 14:20:34,135.135 dsw44922-6f76bf568-tbjcv:67400 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:20:37,534.534 dsw44922-6f76bf568-tbjcv:67415 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:20:41,082.082 dsw44922-6f76bf568-tbjcv:67430 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:20:44,449.449 dsw44922-6f76bf568-tbjcv:67445 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:20:47,776.776 dsw44922-6f76bf568-tbjcv:67460 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:20:51,089.089 dsw44922-6f76bf568-tbjcv:67476 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 5 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:20:54,619.619 dsw44922-6f76bf568-tbjcv:67491 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 2 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:20:57,916.916 dsw44922-6f76bf568-tbjcv:67506 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 24 epochs 50 lr 2.0e-05 gradacc 1 task mosi last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-16 14:21:01,324.324 dsw44922-6f76bf568-tbjcv:67521 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1443), 0.5143953934740882, tensor(1.4276)]
[tensor(-1.0560), 0.5834932821497121, tensor(1.8615)]
[tensor(-1.0316), 0.5873320537428023, tensor(1.9050)]
[tensor(-1.0316), 0.5950095969289827, tensor(1.9050)]
[tensor(-1.0316), 0.5950095969289827, tensor(1.9050)]
[2023-01-16 14:34:25,506.506 dsw44922-6f76bf568-tbjcv:67568 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1554), 0.5623800383877159, tensor(1.6565)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1133), 0.5623800383877159, tensor(1.6565)]
[tensor(-1.0342), 0.5969289827255279, tensor(1.9504)]
[tensor(-1.0342), 0.5969289827255279, tensor(1.9504)]
[tensor(-1.0342), 0.5969289827255279, tensor(1.9504)]
[2023-01-16 14:47:46,641.641 dsw44922-6f76bf568-tbjcv:67615 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9821), 0.5719769673704415, tensor(1.8778)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9821), 0.5719769673704415, tensor(1.8778)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1080)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1080)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1080)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1080)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1080)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9821), 0.6199616122840691, tensor(2.1080)]
early stopping at 8
[2023-01-16 15:09:12,543.543 dsw44922-6f76bf568-tbjcv:67674 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.3_4gpu-80 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.3_4gpu-80 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6981), 0.2802303262955854, 0.0]
[tensor(-1.6717), 0.2802303262955854, 0.0]
[tensor(-1.6669), 0.2802303262955854, 0.0]
early stopping at 3
[2023-01-16 15:17:26,242.242 dsw44922-6f76bf568-tbjcv:67714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'response_selection_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0080), 0.5930902111324377, tensor(1.9575)]
[tensor(-1.0080), 0.5930902111324377, tensor(1.9575)]
[tensor(-1.0009), 0.5930902111324377, tensor(1.9575)]
[tensor(-1.0009), 0.6065259117082533, tensor(1.9763)]
[tensor(-1.0009), 0.6084452975047985, tensor(1.9763)]
[2023-01-16 15:30:42,827.827 dsw44922-6f76bf568-tbjcv:67760 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9917), 0.6142034548944337, tensor(2.0794)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9917), 0.6142034548944337, tensor(2.0794)]
[tensor(-0.9917), 0.6142034548944337, tensor(2.0794)]
[tensor(-0.9917), 0.6142034548944337, tensor(2.0794)]
[tensor(-0.9917), 0.6142034548944337, tensor(2.0794)]
[2023-01-16 15:43:58,965.965 dsw44922-6f76bf568-tbjcv:67807 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9817), 0.5930902111324377, tensor(1.9838)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9817), 0.5930902111324377, tensor(1.9838)]
[tensor(-0.9817), 0.5930902111324377, tensor(1.9838)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9817), 0.5930902111324377, tensor(1.9838)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9817), 0.5930902111324377, tensor(1.9838)]
[tensor(-0.9817), 0.6007677543186181, tensor(1.9838)]
[tensor(-0.9817), 0.6026871401151631, tensor(1.9838)]
[tensor(-0.9817), 0.6026871401151631, tensor(1.9838)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9817), 0.6026871401151631, tensor(1.9838)]
[tensor(-0.9817), 0.6026871401151631, tensor(1.9838)]
[tensor(-0.9817), 0.6026871401151631, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
[tensor(-0.9817), 0.6180422264875239, tensor(1.9838)]
early stopping at 24
[2023-01-16 16:47:21,364.364 dsw44922-6f76bf568-tbjcv:67930 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-10 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-10 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6883), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5735), 0.32437619961612285, tensor(0.0484)]
[tensor(-1.2437), 0.47024952015355087, tensor(1.1076)]
[tensor(-1.1713), 0.4894433781190019, tensor(1.2759)]
[tensor(-1.0556), 0.5662188099808061, tensor(1.7755)]
[tensor(-1.0486), 0.5719769673704415, tensor(1.8113)]
[tensor(-1.0486), 0.5777351247600768, tensor(1.8113)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0486), 0.5834932821497121, tensor(1.8113)]
[tensor(-1.0486), 0.5834932821497121, tensor(1.8113)]
[tensor(-1.0486), 0.5834932821497121, tensor(1.8113)]
[tensor(-1.0486), 0.5834932821497121, tensor(1.8113)]
[tensor(-1.0486), 0.5834932821497121, tensor(1.8113)]
[tensor(-1.0486), 0.5834932821497121, tensor(1.8113)]
early stopping at 13
[2023-01-16 17:21:34,069.069 dsw44922-6f76bf568-tbjcv:68008 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0164), 0.581573896353167, tensor(1.8915)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9840), 0.5950095969289827, tensor(1.9911)]
[tensor(-0.9840), 0.6026871401151631, tensor(1.9911)]
[tensor(-0.9840), 0.6026871401151631, tensor(1.9911)]
[tensor(-0.9840), 0.6026871401151631, tensor(1.9911)]
[2023-01-16 17:34:57,981.981 dsw44922-6f76bf568-tbjcv:68054 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0782), 0.5777351247600768, tensor(1.8105)]
[tensor(-1.0349), 0.5777351247600768, tensor(1.8442)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0349), 0.5777351247600768, tensor(1.8442)]
[tensor(-1.0349), 0.5777351247600768, tensor(1.8442)]
[tensor(-1.0349), 0.5777351247600768, tensor(1.8442)]
[2023-01-16 17:48:24,008.008 dsw44922-6f76bf568-tbjcv:68102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0045), 0.564299424184261, tensor(1.8170)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0045), 0.581573896353167, tensor(1.8835)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9837), 0.6103646833013435, tensor(2.0681)]
[tensor(-0.9837), 0.6103646833013435, tensor(2.0681)]
[tensor(-0.9837), 0.6103646833013435, tensor(2.0681)]
[tensor(-0.9837), 0.6602687140115163, tensor(2.2479)]
[tensor(-0.9837), 0.6602687140115163, tensor(2.2479)]
[tensor(-0.9837), 0.6602687140115163, tensor(2.2479)]
[tensor(-0.9837), 0.6602687140115163, tensor(2.2479)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9837), 0.6602687140115163, tensor(2.2479)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9837), 0.6602687140115163, tensor(2.2479)]
early stopping at 11
[2023-01-16 18:17:29,391.391 dsw44922-6f76bf568-tbjcv:68173 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-20 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-20 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0981), 0.5374280230326296, tensor(1.5890)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0275), 0.5796545105566219, tensor(1.8708)]
[tensor(-1.0275), 0.5796545105566219, tensor(1.8708)]
[tensor(-1.0275), 0.5796545105566219, tensor(1.8708)]
[tensor(-1.0275), 0.5796545105566219, tensor(1.8708)]
[tensor(-1.0275), 0.5796545105566219, tensor(1.8708)]
[tensor(-1.0275), 0.5796545105566219, tensor(1.8708)]
early stopping at 7
[2023-01-16 18:36:03,622.622 dsw44922-6f76bf568-tbjcv:68227 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['response_selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'response_selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0907), 0.5527831094049904, tensor(1.6732)]
[tensor(-1.0066), 0.5873320537428023, tensor(1.9300)]
[tensor(-1.0066), 0.5873320537428023, tensor(1.9300)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0066), 0.6007677543186181, tensor(1.9300)]
[tensor(-1.0066), 0.6007677543186181, tensor(1.9300)]
[2023-01-16 18:49:42,448.448 dsw44922-6f76bf568-tbjcv:68275 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0130), 0.5988483685220729, tensor(1.9812)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0130), 0.5988483685220729, tensor(1.9812)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.0130), 0.5988483685220729, tensor(1.9812)]
[tensor(-1.0130), 0.5988483685220729, tensor(1.9812)]
[tensor(-1.0130), 0.6007677543186181, tensor(1.9812)]
[2023-01-16 19:03:01,403.403 dsw44922-6f76bf568-tbjcv:68322 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0379), 0.564299424184261, tensor(1.7836)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0379), 0.5700575815738963, tensor(1.8084)]
[tensor(-1.0347), 0.5950095969289827, tensor(1.9404)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0347), 0.6026871401151631, tensor(1.9499)]
[tensor(-1.0347), 0.6103646833013435, tensor(1.9499)]
[tensor(-1.0347), 0.6257197696737045, tensor(1.9499)]
[tensor(-1.0347), 0.6257197696737045, tensor(1.9499)]
[tensor(-1.0347), 0.6257197696737045, tensor(1.9499)]
[tensor(-1.0347), 0.6257197696737045, tensor(1.9499)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0347), 0.6257197696737045, tensor(1.9499)]
[tensor(-1.0347), 0.6257197696737045, tensor(1.9499)]
early stopping at 11
[2023-01-16 19:32:09,949.949 dsw44922-6f76bf568-tbjcv:68393 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-30 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-30 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6964), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6770), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.4979), 0.33397312859884837, tensor(0.1719)]
[tensor(-1.3716), 0.33397312859884837, tensor(0.2694)]
[tensor(-1.2611), 0.39923224568138194, tensor(0.7350)]
[tensor(-1.2167), 0.45489443378119004, tensor(1.0578)]
[tensor(-1.1391), 0.4971209213051823, tensor(1.3465)]
[tensor(-1.1391), 0.4971209213051823, tensor(1.3465)]
[tensor(-1.1315), 0.54510556621881, tensor(1.5941)]
[tensor(-1.1315), 0.54510556621881, tensor(1.5941)]
[tensor(-1.1315), 0.54510556621881, tensor(1.5941)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1315), 0.54510556621881, tensor(1.5941)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.1315), 0.54510556621881, tensor(1.5941)]
[tensor(-1.1315), 0.54510556621881, tensor(1.5941)]
early stopping at 14
[2023-01-16 20:09:19,366.366 dsw44922-6f76bf568-tbjcv:68476 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0772), 0.5623800383877159, tensor(1.7347)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0243), 0.5796545105566219, tensor(1.8740)]
[tensor(-1.0172), 0.5873320537428023, tensor(1.9195)]
[tensor(-1.0172), 0.5969289827255279, tensor(1.9395)]
[tensor(-1.0172), 0.5969289827255279, tensor(1.9395)]
[2023-01-16 20:22:58,824.824 dsw44922-6f76bf568-tbjcv:68522 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0526), 0.5719769673704415, tensor(1.8072)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0209), 0.5758157389635317, tensor(1.8582)]
[tensor(-1.0209), 0.5854126679462572, tensor(1.8582)]
[tensor(-1.0209), 0.5854126679462572, tensor(1.8582)]
[tensor(-1.0209), 0.6065259117082533, tensor(1.8582)]
[2023-01-16 20:36:31,413.413 dsw44922-6f76bf568-tbjcv:68570 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0063), 0.5738963531669866, tensor(1.8632)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0063), 0.581573896353167, tensor(1.8745)]
[tensor(-0.9683), 0.6142034548944337, tensor(2.1028)]
[tensor(-0.9683), 0.6142034548944337, tensor(2.1028)]
[tensor(-0.9683), 0.6142034548944337, tensor(2.1028)]
[tensor(-0.9683), 0.6353166986564299, tensor(2.1414)]
[tensor(-0.9683), 0.6353166986564299, tensor(2.1414)]
[tensor(-0.9683), 0.6353166986564299, tensor(2.1414)]
[tensor(-0.9683), 0.6353166986564299, tensor(2.1414)]
[tensor(-0.9683), 0.6353166986564299, tensor(2.1414)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9683), 0.6353166986564299, tensor(2.1414)]
early stopping at 11
[2023-01-16 21:05:45,114.114 dsw44922-6f76bf568-tbjcv:68641 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-40 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-40 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3937), 0.3742802303262956, tensor(0.4777)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2232), 0.4510556621880998, tensor(1.0321)]
[tensor(-1.1084), 0.5028790786948176, tensor(1.4060)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0852), 0.5585412667946257, tensor(1.7075)]
[tensor(-1.0348), 0.5758157389635317, tensor(1.8443)]
[tensor(-1.0348), 0.5758157389635317, tensor(1.8443)]
[tensor(-1.0348), 0.5758157389635317, tensor(1.8443)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6026871401151631, tensor(1.9809)]
[tensor(-1.0325), 0.6046065259117083, tensor(1.9809)]
[tensor(-1.0325), 0.6046065259117083, tensor(1.9809)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0325), 0.6046065259117083, tensor(1.9809)]
[tensor(-1.0325), 0.6046065259117083, tensor(1.9809)]
[tensor(-1.0325), 0.6046065259117083, tensor(1.9809)]
[tensor(-1.0325), 0.6046065259117083, tensor(1.9809)]
early stopping at 24
[2023-01-16 22:09:23,885.885 dsw44922-6f76bf568-tbjcv:68764 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0553), 0.5681381957773513, tensor(1.7853)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9897), 0.6007677543186181, tensor(2.0141)]
[tensor(-0.9897), 0.6142034548944337, tensor(2.0539)]
[tensor(-0.9897), 0.6142034548944337, tensor(2.0539)]
[tensor(-0.9897), 0.6142034548944337, tensor(2.0539)]
[2023-01-16 22:23:00,182.182 dsw44922-6f76bf568-tbjcv:68811 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0550), 0.5681381957773513, tensor(1.7857)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0395), 0.5681381957773513, tensor(1.7857)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0395), 0.5681381957773513, tensor(1.7857)]
[tensor(-1.0395), 0.5834932821497121, tensor(1.7857)]
[tensor(-1.0395), 0.5892514395393474, tensor(1.7857)]
[2023-01-16 22:36:40,740.740 dsw44922-6f76bf568-tbjcv:68858 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'response_selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0352), 0.5604606525911708, tensor(1.7671)]
[tensor(-1.0352), 0.5604606525911708, tensor(1.7671)]
[tensor(-0.9893), 0.6046065259117083, tensor(2.0337)]
[tensor(-0.9893), 0.6046065259117083, tensor(2.0337)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9893), 0.6046065259117083, tensor(2.0337)]
[tensor(-0.9893), 0.6238003838771593, tensor(2.0337)]
[tensor(-0.9893), 0.6238003838771593, tensor(2.0337)]
[tensor(-0.9893), 0.6238003838771593, tensor(2.0337)]
[tensor(-0.9893), 0.6238003838771593, tensor(2.0337)]
[tensor(-0.9893), 0.6238003838771593, tensor(2.0337)]
[tensor(-0.9893), 0.6238003838771593, tensor(2.0337)]
early stopping at 11
[2023-01-16 23:05:41,414.414 dsw44922-6f76bf568-tbjcv:68929 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.2_4gpu-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.2_4gpu-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6990), 0.2802303262955854, 0.0]
[tensor(-1.6703), 0.2802303262955854, 0.0]
[tensor(-1.6552), 0.2802303262955854, 0.0]
early stopping at 3
[2023-01-16 23:13:41,409.409 dsw44922-6f76bf568-tbjcv:68968 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6901), 0.2783109404990403, 0.0]
[tensor(-1.6688), 0.28598848368522073, 0.0]
[tensor(-1.6688), 0.28598848368522073, 0.0]
early stopping at 3
[2023-01-16 23:21:47,395.395 dsw44922-6f76bf568-tbjcv:69007 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'response_selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6865), 0.2840690978886756, 0.0]
[tensor(-1.6703), 0.2840690978886756, 0.0]
[tensor(-1.6703), 0.2840690978886756, 0.0]
early stopping at 3
[2023-01-16 23:29:56,221.221 dsw44922-6f76bf568-tbjcv:69046 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6679), 0.272552783109405, 0.0]
[tensor(-1.6679), 0.272552783109405, 0.0]
[tensor(-1.6679), 0.272552783109405, 0.0]
early stopping at 3
[2023-01-16 23:38:13,788.788 dsw44922-6f76bf568-tbjcv:69086 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6938), 0.2821497120921305, 0.0]
[tensor(-1.6715), 0.2821497120921305, 0.0]
[tensor(-1.6715), 0.2821497120921305, 0.0]
early stopping at 3
[2023-01-16 23:46:17,177.177 dsw44922-6f76bf568-tbjcv:69125 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['response_selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6907), 0.2763915547024952, 0.0]
[tensor(-1.6700), 0.2802303262955854, 0.0]
[tensor(-1.6700), 0.2840690978886756, 0.0]
early stopping at 3
[2023-01-16 23:54:28,553.553 dsw44922-6f76bf568-tbjcv:69164 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'response_selection_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6837), 0.2802303262955854, 0.0]
[tensor(-1.6724), 0.2802303262955854, 0.0]
[tensor(-1.6724), 0.2802303262955854, 0.0]
early stopping at 3
[2023-01-17 00:02:34,013.013 dsw44922-6f76bf568-tbjcv:69203 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6679), 0.272552783109405, 0.0]
[tensor(-1.6679), 0.272552783109405, 0.0]
[tensor(-1.6679), 0.272552783109405, 0.0]
early stopping at 3
[2023-01-17 00:10:40,740.740 dsw44922-6f76bf568-tbjcv:69242 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'response_selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.7033), 0.29366602687140114, 0.0]
[tensor(-1.6764), 0.29366602687140114, 0.0]
[tensor(-1.6754), 0.29366602687140114, 0.0]
early stopping at 3
[2023-01-17 00:18:43,841.841 dsw44922-6f76bf568-tbjcv:69281 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6910), 0.2783109404990403, 0.0]
[tensor(-1.6703), 0.28598848368522073, 0.0]
[tensor(-1.6703), 0.28598848368522073, 0.0]
early stopping at 3
[2023-01-17 00:26:48,507.507 dsw44922-6f76bf568-tbjcv:69321 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6829), 0.2802303262955854, 0.0]
[tensor(-1.6715), 0.2802303262955854, 0.0]
[tensor(-1.6715), 0.2802303262955854, 0.0]
early stopping at 3
[2023-01-17 00:34:48,184.184 dsw44922-6f76bf568-tbjcv:69360 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6678), 0.272552783109405, 0.0]
[tensor(-1.6678), 0.272552783109405, 0.0]
[tensor(-1.6678), 0.272552783109405, 0.0]
early stopping at 3
[2023-01-17 00:42:53,935.935 dsw44922-6f76bf568-tbjcv:69399 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6990), 0.28790786948176583, 0.0]
[tensor(-1.6739), 0.28790786948176583, 0.0]
[tensor(-1.6721), 0.28790786948176583, 0.0]
early stopping at 3
[2023-01-17 00:50:54,255.255 dsw44922-6f76bf568-tbjcv:69438 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6647), 0.2706333973128599, 0.0]
[tensor(-1.6637), 0.2706333973128599, 0.0]
[tensor(-1.6637), 0.2706333973128599, 0.0]
early stopping at 3
[2023-01-17 00:59:48,021.021 dsw44922-6f76bf568-tbjcv:69478 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0695), 0.5374280230326296, tensor(1.6177)]
[tensor(-1.0236), 0.581573896353167, tensor(1.8843)]
[tensor(-1.0236), 0.5834932821497121, tensor(1.8843)]
[tensor(-1.0236), 0.5834932821497121, tensor(1.8843)]
[tensor(-1.0236), 0.5834932821497121, tensor(1.8843)]
[2023-01-17 01:14:34,194.194 dsw44922-6f76bf568-tbjcv:69527 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'response_selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6742), 0.2629558541266795, 0.0]
[tensor(-1.6688), 0.2629558541266795, 0.0]
[tensor(-1.6688), 0.2667946257197697, 0.0]
early stopping at 3
[2023-01-17 01:23:33,953.953 dsw44922-6f76bf568-tbjcv:69568 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'response_selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'response_selection_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1746), 0.4932821497120921, tensor(1.2918)]
[tensor(-1.1015), 0.5911708253358925, tensor(1.8544)]
[tensor(-1.0277), 0.5930902111324377, tensor(1.9377)]
[tensor(-1.0277), 0.5930902111324377, tensor(1.9377)]
[tensor(-1.0277), 0.5930902111324377, tensor(1.9377)]
[tensor(-1.0277), 0.5930902111324377, tensor(1.9377)]
[tensor(-1.0277), 0.5930902111324377, tensor(1.9377)]
[tensor(-1.0277), 0.5930902111324377, tensor(1.9377)]
early stopping at 8
[2023-01-17 01:46:57,881.881 dsw44922-6f76bf568-tbjcv:69629 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'response_selection_head.bias', 'mlm_head.layer_norm.weight', 'response_selection_head.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6672), 0.28598848368522073, 0.0]
[tensor(-1.6642), 0.28598848368522073, 0.0]
[tensor(-1.6642), 0.28598848368522073, 0.0]
early stopping at 3
[2023-01-17 01:55:56,349.349 dsw44922-6f76bf568-tbjcv:69670 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1007), 0.4875239923224568, tensor(1.3369)]
[tensor(-1.0738), 0.5662188099808061, tensor(1.7573)]
[tensor(-0.9480), 0.6295585412667947, tensor(2.1998)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9480), 0.6295585412667947, tensor(2.1998)]
[tensor(-0.9480), 0.6295585412667947, tensor(2.1998)]
[2023-01-17 02:10:55,585.585 dsw44922-6f76bf568-tbjcv:69720 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'response_selection_head.weight', 'mlm_head.dense.weight', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6764), 0.2706333973128599, 0.0]
[tensor(-1.6683), 0.2706333973128599, 0.0]
[tensor(-1.6683), 0.2706333973128599, 0.0]
early stopping at 3
[2023-01-17 02:19:48,840.840 dsw44922-6f76bf568-tbjcv:69760 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6704), 0.2802303262955854, 0.0]
[tensor(-1.6620), 0.2802303262955854, 0.0]
[tensor(-1.6620), 0.2802303262955854, 0.0]
early stopping at 3
[2023-01-17 02:28:45,687.687 dsw44922-6f76bf568-tbjcv:69800 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.bias', 'response_selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'response_selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6679), 0.2744721689059501, 0.0]
[tensor(-1.6633), 0.2783109404990403, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6633), 0.2783109404990403, 0.0]
early stopping at 3
[2023-01-17 02:37:43,231.231 dsw44922-6f76bf568-tbjcv:69841 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'response_selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5759), 0.31669865642994244, tensor(0.0076)]
[tensor(-1.5759), 0.31669865642994244, tensor(0.0076)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.4200), 0.3685220729366603, tensor(0.4226)]
[tensor(-1.2459), 0.44529750479846447, tensor(0.9806)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.2459), 0.44529750479846447, tensor(0.9806)]
[2023-01-17 02:52:29,571.571 dsw44922-6f76bf568-tbjcv:69890 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'response_selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'response_selection_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6703), 0.28790786948176583, 0.0]
[tensor(-1.6668), 0.28790786948176583, 0.0]
[tensor(-1.6668), 0.28790786948176583, 0.0]
early stopping at 3
[2023-01-17 03:01:31,064.064 dsw44922-6f76bf568-tbjcv:69931 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'response_selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6695), 0.2802303262955854, 0.0]
[tensor(-1.6627), 0.2802303262955854, 0.0]
[tensor(-1.6627), 0.2802303262955854, 0.0]
early stopping at 3
[2023-01-17 03:10:28,471.471 dsw44922-6f76bf568-tbjcv:69971 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'response_selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'response_selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1937), 0.4932821497120921, tensor(1.2727)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0290), 0.5566218809980806, tensor(1.7542)]
[tensor(-1.0290), 0.5566218809980806, tensor(1.7542)]
[tensor(-1.0290), 0.5873320537428023, tensor(1.8748)]
[tensor(-1.0290), 0.5873320537428023, tensor(1.8748)]
[2023-01-17 03:25:09,212.212 dsw44922-6f76bf568-tbjcv:70020 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'response_selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6664), 0.2629558541266795, 0.0]
[tensor(-1.6664), 0.28790786948176583, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.5962), 0.3704414587332054, tensor(0.2560)]
[tensor(-1.5962), 0.3704414587332054, tensor(0.2560)]
[tensor(-1.5962), 0.3704414587332054, tensor(0.2560)]
[2023-01-17 03:40:02,058.058 dsw44922-6f76bf568-tbjcv:70069 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'response_selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'response_selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2143), 0.4779270633397313, tensor(1.1753)]
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
[tensor(-1.0501), 0.5758157389635317, tensor(1.8290)]
early stopping at 9
[2023-01-17 04:06:37,379.379 dsw44922-6f76bf568-tbjcv:70136 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1.5-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fusion layers 4
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.1.5-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'response_selection_head.bias', 'response_selection_head.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6672), 0.28598848368522073, 0.0]
[tensor(-1.6629), 0.28598848368522073, 0.0]
[tensor(-1.6629), 0.28598848368522073, 0.0]
early stopping at 3
[2023-01-17 04:15:29,628.628 dsw44922-6f76bf568-tbjcv:70177 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0877), 0.525911708253359, tensor(1.5419)]
[tensor(-0.9599), 0.5911708253358925, tensor(1.9959)]
[tensor(-0.9542), 0.6199616122840691, tensor(2.1456)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9542), 0.6199616122840691, tensor(2.1456)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9542), 0.6199616122840691, tensor(2.1456)]
[2023-01-17 04:28:49,238.238 dsw44922-6f76bf568-tbjcv:70224 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1049), 0.5796545105566219, tensor(1.7934)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0401), 0.5796545105566219, tensor(1.8102)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0315), 0.5854126679462572, tensor(1.8955)]
[tensor(-1.0315), 0.6065259117082533, tensor(1.9464)]
[tensor(-1.0315), 0.6065259117082533, tensor(1.9464)]
[2023-01-17 04:42:15,872.872 dsw44922-6f76bf568-tbjcv:70271 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0043), 0.5604606525911708, tensor(1.7980)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0043), 0.5604606525911708, tensor(1.7980)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9373), 0.6161228406909789, tensor(2.1433)]
[tensor(-0.9373), 0.6161228406909789, tensor(2.1433)]
[tensor(-0.9373), 0.6218809980806143, tensor(2.1433)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9373), 0.6583493282149712, tensor(2.2518)]
[tensor(-0.9373), 0.6583493282149712, tensor(2.2518)]
[tensor(-0.9373), 0.6583493282149712, tensor(2.2518)]
[tensor(-0.9373), 0.6583493282149712, tensor(2.2518)]
[tensor(-0.9373), 0.6583493282149712, tensor(2.2518)]
[tensor(-0.9373), 0.6583493282149712, tensor(2.2518)]
early stopping at 11
[2023-01-17 05:11:11,535.535 dsw44922-6f76bf568-tbjcv:70341 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4471), 0.33205374280230326, tensor(0.2131)]
[tensor(-1.2780), 0.4510556621880998, tensor(0.9773)]
[tensor(-1.1208), 0.4952015355086372, tensor(1.3552)]
[tensor(-1.0397), 0.5623800383877159, tensor(1.7722)]
[tensor(-1.0397), 0.5623800383877159, tensor(1.7722)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0396), 0.5796545105566219, tensor(1.8587)]
[tensor(-1.0396), 0.581573896353167, tensor(1.8587)]
[tensor(-1.0396), 0.581573896353167, tensor(1.8587)]
[tensor(-1.0396), 0.581573896353167, tensor(1.8587)]
[tensor(-1.0396), 0.6007677543186181, tensor(1.9235)]
[tensor(-1.0396), 0.6007677543186181, tensor(1.9235)]
[tensor(-1.0396), 0.6007677543186181, tensor(1.9235)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0396), 0.6007677543186181, tensor(1.9235)]
[tensor(-1.0396), 0.6007677543186181, tensor(1.9235)]
[tensor(-1.0396), 0.6007677543186181, tensor(1.9235)]
early stopping at 15
[2023-01-17 05:50:50,540.540 dsw44922-6f76bf568-tbjcv:70428 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9895), 0.5738963531669866, tensor(1.8800)]
[tensor(-0.9861), 0.6007677543186181, tensor(2.0177)]
[tensor(-0.9838), 0.6122840690978887, tensor(2.0777)]
[tensor(-0.9838), 0.6122840690978887, tensor(2.0777)]
[tensor(-0.9838), 0.6276391554702495, tensor(2.0777)]
[2023-01-17 06:04:12,490.490 dsw44922-6f76bf568-tbjcv:70475 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0248), 0.5950095969289827, tensor(1.9503)]
[tensor(-1.0049), 0.5950095969289827, tensor(1.9503)]
[tensor(-0.9927), 0.6142034548944337, tensor(2.0783)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9927), 0.6314779270633397, tensor(2.0783)]
[tensor(-0.9927), 0.6314779270633397, tensor(2.0783)]
[2023-01-17 06:17:28,306.306 dsw44922-6f76bf568-tbjcv:70522 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-0.9602), 0.6007677543186181, tensor(2.0436)]
[tensor(-0.9602), 0.6007677543186181, tensor(2.0436)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9602), 0.6007677543186181, tensor(2.0436)]
[tensor(-0.9602), 0.6007677543186181, tensor(2.0436)]
[tensor(-0.9602), 0.6218809980806143, tensor(2.0436)]
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
[tensor(-0.9602), 0.6333973128598849, tensor(2.0559)]
early stopping at 15
[2023-01-17 06:57:16,072.072 dsw44922-6f76bf568-tbjcv:70608 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2251), 0.5067178502879078, tensor(1.3085)]
[tensor(-1.1394), 0.5777351247600768, tensor(1.7493)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.0722), 0.5777351247600768, tensor(1.7493)]
[tensor(-1.0722), 0.5777351247600768, tensor(1.7493)]
[tensor(-1.0722), 0.5777351247600768, tensor(1.7493)]
[tensor(-1.0722), 0.5777351247600768, tensor(1.7493)]
[tensor(-1.0722), 0.5777351247600768, tensor(1.7493)]
[tensor(-1.0722), 0.5892514395393474, tensor(1.7493)]
[tensor(-1.0722), 0.5892514395393474, tensor(1.7493)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.0722), 0.5892514395393474, tensor(1.7493)]
[tensor(-1.0722), 0.5892514395393474, tensor(1.7493)]
[tensor(-1.0722), 0.5892514395393474, tensor(1.7493)]
[tensor(-1.0722), 0.5892514395393474, tensor(1.7493)]
early stopping at 13
[2023-01-17 07:31:46,170.170 dsw44922-6f76bf568-tbjcv:70687 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0853), 0.5297504798464492, tensor(1.5635)]
[tensor(-0.9442), 0.6142034548944337, tensor(2.1268)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9442), 0.6142034548944337, tensor(2.1268)]
[tensor(-0.9442), 0.6218809980806143, tensor(2.1268)]
[tensor(-0.9442), 0.6218809980806143, tensor(2.1268)]
[2023-01-17 07:44:59,618.618 dsw44922-6f76bf568-tbjcv:70734 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0690), 0.5834932821497121, tensor(1.8485)]
[tensor(-1.0029), 0.5969289827255279, tensor(1.9817)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0029), 0.5969289827255279, tensor(1.9817)]
[tensor(-1.0029), 0.6199616122840691, tensor(2.0213)]
[tensor(-1.0029), 0.6199616122840691, tensor(2.0213)]
[2023-01-17 07:58:19,973.973 dsw44922-6f76bf568-tbjcv:70781 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9834), 0.5950095969289827, tensor(1.9917)]
[tensor(-0.9834), 0.5950095969289827, tensor(1.9917)]
[tensor(-0.9834), 0.6103646833013435, tensor(2.0583)]
[tensor(-0.9834), 0.6103646833013435, tensor(2.0583)]
[tensor(-0.9834), 0.6103646833013435, tensor(2.0583)]
[tensor(-0.9834), 0.6391554702495201, tensor(2.0849)]
[tensor(-0.9834), 0.6391554702495201, tensor(2.0849)]
[tensor(-0.9834), 0.6391554702495201, tensor(2.0849)]
[tensor(-0.9834), 0.6391554702495201, tensor(2.0849)]
[tensor(-0.9834), 0.6391554702495201, tensor(2.0849)]
[tensor(-0.9834), 0.6391554702495201, tensor(2.0849)]
early stopping at 11
[2023-01-17 08:27:32,709.709 dsw44922-6f76bf568-tbjcv:70852 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6925), 0.2802303262955854, 0.0]
[tensor(-1.6701), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5145), 0.28790786948176583, 0.0]
early stopping at 3
[2023-01-17 08:35:32,575.575 dsw44922-6f76bf568-tbjcv:70891 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0308), 0.5950095969289827, tensor(1.9443)]
[tensor(-1.0308), 0.5950095969289827, tensor(1.9443)]
[tensor(-1.0204), 0.6065259117082533, tensor(2.0122)]
[tensor(-1.0114), 0.6276391554702495, tensor(2.1268)]
[tensor(-1.0114), 0.6276391554702495, tensor(2.1268)]
[2023-01-17 08:48:50,168.168 dsw44922-6f76bf568-tbjcv:70938 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0809), 0.5796545105566219, tensor(1.8174)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9643), 0.5930902111324377, tensor(2.0012)]
[tensor(-0.9643), 0.5930902111324377, tensor(2.0012)]
[tensor(-0.9643), 0.6084452975047985, tensor(2.0012)]
[tensor(-0.9643), 0.6295585412667947, tensor(2.0012)]
[2023-01-17 09:02:11,259.259 dsw44922-6f76bf568-tbjcv:70985 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0135), 0.5758157389635317, tensor(1.8656)]
[tensor(-1.0135), 0.5758157389635317, tensor(1.8656)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9763), 0.6007677543186181, tensor(2.0275)]
[tensor(-0.9763), 0.6007677543186181, tensor(2.0275)]
[tensor(-0.9763), 0.6007677543186181, tensor(2.0275)]
[tensor(-0.9763), 0.6218809980806143, tensor(2.0275)]
[tensor(-0.9763), 0.6218809980806143, tensor(2.0275)]
[tensor(-0.9763), 0.6218809980806143, tensor(2.0275)]
[tensor(-0.9763), 0.6218809980806143, tensor(2.0275)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9763), 0.6238003838771593, tensor(2.0275)]
[tensor(-0.9763), 0.6238003838771593, tensor(2.0275)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9763), 0.6429942418426103, tensor(2.0275)]
[tensor(-0.9763), 0.6429942418426103, tensor(2.0275)]
[tensor(-0.9763), 0.6429942418426103, tensor(2.0275)]
[tensor(-0.9763), 0.6429942418426103, tensor(2.0275)]
[tensor(-0.9763), 0.6429942418426103, tensor(2.0275)]
[tensor(-0.9763), 0.6449136276391555, tensor(2.0275)]
[tensor(-0.9763), 0.6449136276391555, tensor(2.0275)]
[tensor(-0.9763), 0.6449136276391555, tensor(2.0275)]
[tensor(-0.9763), 0.6449136276391555, tensor(2.0275)]
[tensor(-0.9763), 0.6449136276391555, tensor(2.0275)]
[tensor(-0.9763), 0.6449136276391555, tensor(2.0275)]
early stopping at 22
[2023-01-17 10:00:14,602.602 dsw44922-6f76bf568-tbjcv:71099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1418), 0.5431861804222649, tensor(1.5741)]
[tensor(-1.0263), 0.6007677543186181, tensor(1.9776)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0202), 0.6007677543186181, tensor(1.9776)]
[tensor(-1.0202), 0.6007677543186181, tensor(1.9776)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0202), 0.6007677543186181, tensor(1.9776)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0202), 0.6007677543186181, tensor(1.9776)]
[tensor(-1.0202), 0.6007677543186181, tensor(1.9776)]
[tensor(-1.0202), 0.6065259117082533, tensor(1.9776)]
[tensor(-1.0202), 0.6065259117082533, tensor(1.9776)]
[tensor(-1.0202), 0.6103646833013435, tensor(1.9776)]
[tensor(-1.0202), 0.6103646833013435, tensor(1.9776)]
[tensor(-1.0202), 0.6122840690978887, tensor(1.9776)]
[tensor(-1.0202), 0.6122840690978887, tensor(1.9776)]
[tensor(-1.0202), 0.6122840690978887, tensor(1.9776)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0202), 0.6122840690978887, tensor(1.9776)]
[tensor(-1.0202), 0.6122840690978887, tensor(1.9776)]
[tensor(-1.0202), 0.6122840690978887, tensor(1.9776)]
early stopping at 17
[2023-01-17 10:45:02,368.368 dsw44922-6f76bf568-tbjcv:71193 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0116), 0.5950095969289827, tensor(1.9634)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0116), 0.5950095969289827, tensor(1.9634)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1177)]
[tensor(-0.9821), 0.6199616122840691, tensor(2.1177)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9821), 0.6199616122840691, tensor(2.1177)]
[2023-01-17 10:58:27,699.699 dsw44922-6f76bf568-tbjcv:71240 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0781), 0.5662188099808061, tensor(1.7530)]
[tensor(-1.0041), 0.5662188099808061, tensor(1.7983)]
[tensor(-1.0041), 0.5662188099808061, tensor(1.7983)]
[tensor(-1.0041), 0.5758157389635317, tensor(1.7983)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0041), 0.5758157389635317, tensor(1.7983)]
[2023-01-17 11:11:48,240.240 dsw44922-6f76bf568-tbjcv:71287 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0049), 0.6026871401151631, tensor(2.0085)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0049), 0.6026871401151631, tensor(2.0085)]
[tensor(-0.9915), 0.6084452975047985, tensor(2.0508)]
[tensor(-0.9915), 0.6084452975047985, tensor(2.0508)]
[tensor(-0.9915), 0.6084452975047985, tensor(2.0508)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
[tensor(-0.9915), 0.6314779270633397, tensor(2.0659)]
early stopping at 11
[2023-01-17 11:41:11,160.160 dsw44922-6f76bf568-tbjcv:71358 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0626), 0.5470249520153551, tensor(1.6726)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.5969289827255279, tensor(1.9855)]
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9991), 0.6122840690978887, tensor(1.9855)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9991), 0.6161228406909789, tensor(1.9855)]
[tensor(-0.9991), 0.6161228406909789, tensor(1.9855)]
[tensor(-0.9991), 0.6161228406909789, tensor(1.9855)]
[tensor(-0.9991), 0.6257197696737045, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
[tensor(-0.9991), 0.6295585412667947, tensor(1.9855)]
early stopping at 19
[2023-01-17 12:32:27,631.631 dsw44922-6f76bf568-tbjcv:71462 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0230), 0.5662188099808061, tensor(1.8081)]
[tensor(-0.9654), 0.6026871401151631, tensor(2.0481)]
[tensor(-0.9540), 0.6218809980806143, tensor(2.1554)]
[tensor(-0.9540), 0.6218809980806143, tensor(2.1554)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9540), 0.6218809980806143, tensor(2.1554)]
[2023-01-17 12:46:07,629.629 dsw44922-6f76bf568-tbjcv:71509 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0797), 0.5777351247600768, tensor(1.8090)]
[tensor(-1.0657), 0.5777351247600768, tensor(1.8090)]
[tensor(-1.0222), 0.5834932821497121, tensor(1.8953)]
[tensor(-1.0222), 0.6161228406909789, tensor(1.9663)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0222), 0.6161228406909789, tensor(1.9663)]
[2023-01-17 12:59:50,794.794 dsw44922-6f76bf568-tbjcv:71559 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9755), 0.5950095969289827, tensor(1.9995)]
[tensor(-0.9755), 0.5950095969289827, tensor(1.9995)]
[tensor(-0.9640), 0.6161228406909789, tensor(2.1166)]
[tensor(-0.9640), 0.6161228406909789, tensor(2.1166)]
[tensor(-0.9640), 0.6161228406909789, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
[tensor(-0.9640), 0.6314779270633397, tensor(2.1166)]
early stopping at 11
[2023-01-17 13:29:20,964.964 dsw44922-6f76bf568-tbjcv:71632 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1066), 0.54510556621881, tensor(1.6189)]
[tensor(-1.0304), 0.5930902111324377, tensor(1.9351)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0302), 0.5930902111324377, tensor(1.9351)]
[tensor(-1.0302), 0.5930902111324377, tensor(1.9351)]
[tensor(-1.0302), 0.5930902111324377, tensor(1.9351)]
[tensor(-1.0302), 0.6103646833013435, tensor(1.9498)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0302), 0.6103646833013435, tensor(1.9498)]
[tensor(-1.0302), 0.6103646833013435, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
[tensor(-1.0302), 0.6238003838771593, tensor(1.9498)]
early stopping at 14
[2023-01-17 14:06:54,251.251 dsw44922-6f76bf568-tbjcv:71714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-17 14:07:00,432.432 dsw44922-6f76bf568-tbjcv:71731 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 16 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-17 14:07:06,623.623 dsw44922-6f76bf568-tbjcv:71747 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-17 14:07:12,731.731 dsw44922-6f76bf568-tbjcv:71763 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.1-100 datasize 960 batchsize 16 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
/home/pai/lib/python3.6/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.
  utils.DeprecatedIn35,
Traceback (most recent call last):
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1679, in from_pretrained
    user_agent=user_agent,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 290, in cached_path
    local_files_only=local_files_only,
  File "/home/pai/lib/python3.6/site-packages/transformers/utils/hub.py", line 546, in get_from_cache
    "Connection error, and we cannot find the requested files in the cached path."
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 118, in <module>
    model = DownstreamModel(args.model, config, label_num, turn_embeddings=turn_embeddings).to(args.device)
  File "/mnt/workspace/mtt/model.py", line 59, in __init__
    self.model = ATModel.from_pretrained(ckpt_path, config=config)
  File "/home/pai/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1753, in from_pretrained
    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached "
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /mnt/ewwe/yts/saved_models/v4.1-100 is not the path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2023-01-17 14:07:18,811.811 dsw44922-6f76bf568-tbjcv:71780 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0300), 0.5527831094049904, tensor(1.7340)]
[tensor(-1.0149), 0.5604606525911708, tensor(1.7874)]
[tensor(-1.0149), 0.6026871401151631, tensor(1.9934)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0149), 0.6314779270633397, tensor(2.1238)]
[tensor(-1.0149), 0.6314779270633397, tensor(2.1238)]
[2023-01-17 14:28:49,684.684 dsw44922-6f76bf568-tbjcv:71839 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0870), 0.5470249520153551, tensor(1.6481)]
[tensor(-1.0590), 0.5547024952015355, tensor(1.7146)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0464), 0.5796545105566219, tensor(1.8519)]
[tensor(-1.0464), 0.5930902111324377, tensor(1.8519)]
[tensor(-1.0464), 0.5930902111324377, tensor(1.8519)]
[2023-01-17 14:50:17,568.568 dsw44922-6f76bf568-tbjcv:71898 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0112), 0.5700575815738963, tensor(1.8390)]
[tensor(-0.9558), 0.6142034548944337, tensor(2.1152)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9558), 0.6142034548944337, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9558), 0.6180422264875239, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6199616122840691, tensor(2.1152)]
[tensor(-0.9558), 0.6238003838771593, tensor(2.1152)]
[tensor(-0.9558), 0.6238003838771593, tensor(2.1152)]
[tensor(-0.9558), 0.6238003838771593, tensor(2.1152)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
[tensor(-0.9558), 0.6314779270633397, tensor(2.1152)]
early stopping at 21
[2023-01-17 16:20:16,816.816 dsw44922-6f76bf568-tbjcv:72060 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6690), 0.2802303262955854, 0.0]
[tensor(-1.6690), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6690), 0.30902111324376197, 0.0]
early stopping at 3
[2023-01-17 16:33:11,335.335 dsw44922-6f76bf568-tbjcv:72106 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9839), 0.5604606525911708, tensor(1.8184)]
[tensor(-0.9839), 0.6122840690978887, tensor(2.0774)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9839), 0.6122840690978887, tensor(2.0774)]
[tensor(-0.9839), 0.6295585412667947, tensor(2.0774)]
[tensor(-0.9839), 0.6295585412667947, tensor(2.0774)]
[2023-01-17 16:54:46,544.544 dsw44922-6f76bf568-tbjcv:72165 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6975), 0.29942418426103645, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.4609), 0.345489443378119, tensor(0.2665)]
[tensor(-1.2425), 0.43378119001919385, tensor(0.9264)]
[tensor(-1.1825), 0.508637236084453, tensor(1.3607)]
[tensor(-1.1379), 0.508637236084453, tensor(1.3607)]
[2023-01-17 17:16:10,951.951 dsw44922-6f76bf568-tbjcv:72225 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
[tensor(-0.9451), 0.6065259117082533, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
[tensor(-0.9451), 0.6276391554702495, tensor(2.0875)]
early stopping at 12
[2023-01-17 18:07:24,885.885 dsw44922-6f76bf568-tbjcv:72329 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
[tensor(-1.6742), 0.2802303262955854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6742), 0.2821497120921305, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4389), 0.33589251439539347, tensor(0.2406)]
[tensor(-1.2091), 0.4875239923224568, tensor(1.2285)]
[tensor(-1.1795), 0.5374280230326296, tensor(1.5076)]
[tensor(-1.1333), 0.5374280230326296, tensor(1.5076)]
[tensor(-1.0792), 0.5489443378119002, tensor(1.6655)]
[tensor(-1.0792), 0.5738963531669866, tensor(1.7566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0792), 0.5796545105566219, tensor(1.7566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0792), 0.5854126679462572, tensor(1.8131)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
[tensor(-1.0508), 0.6122840690978887, tensor(2.0106)]
early stopping at 16
[2023-01-17 19:15:07,633.633 dsw44922-6f76bf568-tbjcv:72457 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0098), 0.581573896353167, tensor(1.8981)]
[tensor(-0.9832), 0.6084452975047985, tensor(2.0590)]
[tensor(-0.9832), 0.6084452975047985, tensor(2.0590)]
[tensor(-0.9832), 0.6084452975047985, tensor(2.0590)]
[tensor(-0.9832), 0.6257197696737045, tensor(2.0590)]
[2023-01-17 19:36:32,000.000 dsw44922-6f76bf568-tbjcv:72516 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0351), 0.5796545105566219, tensor(1.8632)]
[tensor(-1.0011), 0.5796545105566219, tensor(1.8632)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9984), 0.5892514395393474, tensor(1.9479)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9984), 0.5950095969289827, tensor(1.9479)]
[tensor(-0.9984), 0.5950095969289827, tensor(1.9479)]
[2023-01-17 19:57:45,871.871 dsw44922-6f76bf568-tbjcv:72576 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0159), 0.5508637236084453, tensor(1.7384)]
[tensor(-0.9660), 0.5854126679462572, tensor(1.9610)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9660), 0.5911708253358925, tensor(1.9610)]
[tensor(-0.9660), 0.5911708253358925, tensor(1.9610)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9660), 0.5911708253358925, tensor(1.9610)]
[tensor(-0.9660), 0.6065259117082533, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
[tensor(-0.9660), 0.6257197696737045, tensor(1.9610)]
early stopping at 12
[2023-01-17 20:48:34,782.782 dsw44922-6f76bf568-tbjcv:72679 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
[tensor(-0.9885), 0.6122840690978887, tensor(2.0729)]
early stopping at 6
[2023-01-17 21:14:27,727.727 dsw44922-6f76bf568-tbjcv:72738 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0140), 0.5892514395393474, tensor(1.9322)]
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9498), 0.6180422264875239, tensor(2.1404)]
[2023-01-17 21:35:48,234.234 dsw44922-6f76bf568-tbjcv:72796 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0635), 0.5355086372360844, tensor(1.6140)]
[tensor(-1.0632), 0.5489443378119002, tensor(1.6815)]
[tensor(-1.0632), 0.6026871401151631, tensor(1.9489)]
[tensor(-1.0632), 0.6026871401151631, tensor(1.9489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0632), 0.6084452975047985, tensor(1.9489)]
[2023-01-17 21:57:08,682.682 dsw44922-6f76bf568-tbjcv:72852 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-0.9933), 0.6084452975047985, tensor(2.0489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-0.9812), 0.6084452975047985, tensor(2.0489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9812), 0.6103646833013435, tensor(2.0537)]
[tensor(-0.9812), 0.6199616122840691, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
[tensor(-0.9812), 0.6238003838771593, tensor(2.0537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9812), 0.6257197696737045, tensor(2.0537)]
[tensor(-0.9812), 0.6257197696737045, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
[tensor(-0.9812), 0.6429942418426103, tensor(2.0537)]
early stopping at 22
[2023-01-17 23:30:34,049.049 dsw44922-6f76bf568-tbjcv:73009 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6702), 0.2802303262955854, 0.0]
[tensor(-1.4585), 0.3857965451055662, tensor(0.4705)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3074), 0.39923224568138194, tensor(0.6888)]
[tensor(-1.1165), 0.5316698656429942, tensor(1.5419)]
[tensor(-1.1165), 0.5316698656429942, tensor(1.5419)]
[tensor(-1.1165), 0.5316698656429942, tensor(1.5419)]
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1165), 0.5470249520153551, tensor(1.5446)]
[tensor(-1.1165), 0.5585412667946257, tensor(1.5887)]
[tensor(-1.1165), 0.5585412667946257, tensor(1.5887)]
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
[tensor(-1.1165), 0.5662188099808061, tensor(1.6440)]
[tensor(-1.1165), 0.5719769673704415, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5738963531669866, tensor(1.6440)]
[tensor(-1.1165), 0.5873320537428023, tensor(1.6440)]
[tensor(-1.1165), 0.5873320537428023, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.5911708253358925, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
[tensor(-1.1165), 0.6103646833013435, tensor(1.6440)]
early stopping at 37
[2023-01-18 02:08:07,494.494 dsw44922-6f76bf568-tbjcv:73251 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9915), 0.5758157389635317, tensor(1.8876)]
[tensor(-0.9915), 0.5758157389635317, tensor(1.8876)]
[tensor(-0.9915), 0.5892514395393474, tensor(1.8876)]
[tensor(-0.9915), 0.5911708253358925, tensor(1.8876)]
[tensor(-0.9915), 0.5950095969289827, tensor(1.8876)]
[2023-01-18 02:29:22,558.558 dsw44922-6f76bf568-tbjcv:73304 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0015), 0.6007677543186181, tensor(2.0024)]
[tensor(-1.0015), 0.6007677543186181, tensor(2.0024)]
[tensor(-1.0015), 0.6122840690978887, tensor(2.0330)]
[tensor(-1.0015), 0.6295585412667947, tensor(2.0745)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0015), 0.6295585412667947, tensor(2.0745)]
[2023-01-18 02:50:42,860.860 dsw44922-6f76bf568-tbjcv:73361 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0460), 0.5738963531669866, tensor(1.8235)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0284), 0.5738963531669866, tensor(1.8235)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.5969289827255279, tensor(2.0059)]
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
[tensor(-0.9787), 0.6161228406909789, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6180422264875239, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
[tensor(-0.9787), 0.6218809980806143, tensor(2.0059)]
early stopping at 23
[2023-01-18 04:27:31,816.816 dsw44922-6f76bf568-tbjcv:73517 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2543), 0.43953934740882916, tensor(0.9434)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1462), 0.4817658349328215, tensor(1.2627)]
[tensor(-1.1462), 0.5143953934740882, tensor(1.4165)]
[tensor(-1.1081), 0.5508637236084453, tensor(1.6462)]
[tensor(-1.1081), 0.5508637236084453, tensor(1.6462)]
[tensor(-1.1081), 0.5604606525911708, tensor(1.6462)]
[tensor(-1.1081), 0.5662188099808061, tensor(1.7148)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.5854126679462572, tensor(1.8116)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
[tensor(-1.1081), 0.6026871401151631, tensor(1.8405)]
early stopping at 17
[2023-01-18 05:39:02,694.694 dsw44922-6f76bf568-tbjcv:73640 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0674), 0.5431861804222649, tensor(1.6486)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9961), 0.5892514395393474, tensor(1.9502)]
[tensor(-0.9961), 0.5950095969289827, tensor(1.9502)]
[tensor(-0.9961), 0.6218809980806143, tensor(2.0729)]
[tensor(-0.9961), 0.6218809980806143, tensor(2.0729)]
[2023-01-18 06:00:14,855.855 dsw44922-6f76bf568-tbjcv:73696 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 12 epochs 5 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1079), 0.4990403071017274, tensor(1.3873)]
[tensor(-1.0372), 0.564299424184261, tensor(1.7843)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0352), 0.5854126679462572, tensor(1.8919)]
[tensor(-1.0352), 0.6142034548944337, tensor(1.9760)]
[tensor(-1.0352), 0.6199616122840691, tensor(1.9760)]
[2023-01-18 06:21:31,448.448 dsw44922-6f76bf568-tbjcv:73753 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 2 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0041), 0.581573896353167, tensor(1.9038)]
[tensor(-1.0041), 0.581573896353167, tensor(1.9038)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0041), 0.581573896353167, tensor(1.9038)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0041), 0.6065259117082533, tensor(2.0188)]
[tensor(-1.0041), 0.6065259117082533, tensor(2.0188)]
[tensor(-1.0041), 0.6065259117082533, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0041), 0.6142034548944337, tensor(2.0188)]
[tensor(-1.0041), 0.6142034548944337, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0041), 0.6142034548944337, tensor(2.0188)]
[tensor(-1.0041), 0.6199616122840691, tensor(2.0188)]
[tensor(-1.0041), 0.6238003838771593, tensor(2.0188)]
[tensor(-1.0041), 0.6238003838771593, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6257197696737045, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0041), 0.6333973128598849, tensor(2.0188)]
early stopping at 23
[2023-01-18 07:58:39,333.333 dsw44922-6f76bf568-tbjcv:73914 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 12 epochs 50 lr 2.0e-05 gradacc 1 task iemocap last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode eleventurn
has_audio_cls True multi audio True v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0254), 0.5854126679462572, tensor(1.9017)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5854126679462572, tensor(1.9192)]
[tensor(-0.9791), 0.5988483685220729, tensor(1.9192)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
[tensor(-0.9791), 0.6429942418426103, tensor(2.0642)]
early stopping at 13
