[2023-01-18 20:10:28,446.446 dsw44977-5b9b48888d-729dj:91303 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4994), 0.29213483146067415, 0.0]
[tensor(-1.5807), 0.5662921348314607, tensor(1.2508)]
[tensor(-1.2736), 0.6404494382022472, tensor(1.9287)]
[tensor(-1.1397), 0.6808988764044944, tensor(2.2647)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0515), 0.7123595505617978, tensor(2.5103)]
[tensor(-1.0515), 0.7123595505617978, tensor(2.5103)]
[tensor(-1.0515), 0.7123595505617978, tensor(2.5103)]
[tensor(-1.0515), 0.7123595505617978, tensor(2.5103)]
[tensor(-1.0515), 0.7123595505617978, tensor(2.5103)]
[tensor(-1.0515), 0.7123595505617978, tensor(2.5103)]
early stopping at 10
[2023-01-18 20:17:15,830.830 dsw44977-5b9b48888d-729dj:92494 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9489), 0.449438202247191, tensor(0.2983)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3005), 0.6337078651685393, tensor(1.8681)]
[tensor(-1.1613), 0.6719101123595506, tensor(2.1982)]
[tensor(-1.1271), 0.6853932584269663, tensor(2.2999)]
[tensor(-1.1271), 0.6853932584269663, tensor(2.2999)]
[tensor(-1.1271), 0.6853932584269663, tensor(2.2999)]
[tensor(-1.1271), 0.6966292134831461, tensor(2.3305)]
[tensor(-1.1271), 0.6966292134831461, tensor(2.3305)]
[tensor(-1.1271), 0.6966292134831461, tensor(2.3305)]
[tensor(-1.1271), 0.6966292134831461, tensor(2.3305)]
[2023-01-18 20:23:57,777.777 dsw44977-5b9b48888d-729dj:93666 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2067), 0.4449438202247191, tensor(0.0180)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7440), 0.5213483146067416, tensor(0.8628)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2224), 0.6404494382022472, tensor(1.9798)]
[tensor(-1.1561), 0.6696629213483146, tensor(2.1923)]
[tensor(-1.1093), 0.6764044943820224, tensor(2.2728)]
[tensor(-1.0778), 0.6808988764044944, tensor(2.3266)]
[tensor(-1.0778), 0.6808988764044944, tensor(2.3266)]
[tensor(-1.0778), 0.6831460674157304, tensor(2.3266)]
[tensor(-1.0778), 0.6898876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.6898876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.6898876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.6898876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.6966292134831461, tensor(2.3460)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.698876404494382, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
[tensor(-1.0778), 0.701123595505618, tensor(2.3460)]
early stopping at 32
[2023-01-18 20:44:58,645.645 dsw44977-5b9b48888d-729dj:97305 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.2781), 0.32808988764044944, 0.0]
[tensor(-1.4002), 0.6382022471910113, tensor(1.7908)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1055), 0.6831460674157304, tensor(2.3102)]
[tensor(-1.0755), 0.6876404494382022, tensor(2.3627)]
[tensor(-1.0755), 0.6921348314606741, tensor(2.3627)]
[tensor(-1.0755), 0.6921348314606741, tensor(2.3627)]
[tensor(-1.0755), 0.6921348314606741, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.6966292134831461, tensor(2.3627)]
[tensor(-1.0755), 0.698876404494382, tensor(2.3627)]
[tensor(-1.0755), 0.698876404494382, tensor(2.3627)]
[tensor(-1.0755), 0.698876404494382, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
[tensor(-1.0755), 0.7078651685393258, tensor(2.3627)]
early stopping at 48
[2023-01-18 21:16:05,686.686 dsw44977-5b9b48888d-729dj:102688 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3156), 0.36629213483146067, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5521), 0.5842696629213483, tensor(1.3692)]
[tensor(-1.2501), 0.6404494382022472, tensor(1.9522)]
[tensor(-1.1978), 0.6606741573033708, tensor(2.1056)]
[tensor(-1.1231), 0.6876404494382022, tensor(2.3151)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1231), 0.6876404494382022, tensor(2.3151)]
[tensor(-1.1231), 0.6898876404494382, tensor(2.3218)]
[tensor(-1.1163), 0.6966292134831461, tensor(2.3668)]
[tensor(-1.1163), 0.6966292134831461, tensor(2.3668)]
[tensor(-1.1163), 0.6966292134831461, tensor(2.3668)]
[2023-01-18 21:22:42,381.381 dsw44977-5b9b48888d-729dj:103847 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.0924), 0.451685393258427, tensor(0.1660)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3919), 0.6067415730337079, tensor(1.6418)]
[tensor(-1.2110), 0.6629213483146067, tensor(2.1036)]
[tensor(-1.1242), 0.6764044943820224, tensor(2.2578)]
[tensor(-1.0988), 0.6831460674157304, tensor(2.3169)]
[tensor(-1.0848), 0.698876404494382, tensor(2.4096)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0848), 0.698876404494382, tensor(2.4096)]
[tensor(-1.0848), 0.698876404494382, tensor(2.4096)]
[tensor(-1.0848), 0.698876404494382, tensor(2.4096)]
[tensor(-1.0848), 0.698876404494382, tensor(2.4096)]
[2023-01-18 21:29:17,639.639 dsw44977-5b9b48888d-729dj:105001 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2492), 0.43595505617977526, 0.0]
[tensor(-1.8065), 0.49887640449438203, tensor(0.6879)]
[tensor(-1.2104), 0.6674157303370787, tensor(2.1267)]
[tensor(-1.1449), 0.6786516853932584, tensor(2.2484)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1190), 0.6898876404494382, tensor(2.3304)]
[tensor(-1.1038), 0.6898876404494382, tensor(2.3456)]
[tensor(-1.0884), 0.6898876404494382, tensor(2.3498)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
[tensor(-1.0738), 0.7101123595505618, tensor(2.4767)]
early stopping at 18
[2023-01-18 21:41:09,978.978 dsw44977-5b9b48888d-729dj:107065 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-2.1904), 0.39325842696629215, 0.0]
[tensor(-1.3679), 0.6426966292134831, tensor(1.8456)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1268), 0.6719101123595506, tensor(2.2328)]
[tensor(-1.0928), 0.6898876404494382, tensor(2.3567)]
[tensor(-1.0828), 0.6898876404494382, tensor(2.3567)]
[tensor(-1.0828), 0.6921348314606741, tensor(2.3703)]
[tensor(-1.0828), 0.6943820224719102, tensor(2.3703)]
[tensor(-1.0828), 0.6943820224719102, tensor(2.3703)]
[tensor(-1.0828), 0.7056179775280899, tensor(2.3714)]
[tensor(-1.0828), 0.7056179775280899, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.3714)]
early stopping at 21
[2023-01-18 21:54:49,555.555 dsw44977-5b9b48888d-729dj:109438 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3358), 0.31685393258426964, 0.0]
[tensor(-1.4296), 0.6224719101123596, tensor(1.6828)]
[tensor(-1.2032), 0.6561797752808989, tensor(2.0777)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1213), 0.6943820224719102, tensor(2.3506)]
[tensor(-1.1187), 0.6943820224719102, tensor(2.3532)]
[tensor(-1.1187), 0.7033707865168539, tensor(2.3810)]
[tensor(-1.1187), 0.7033707865168539, tensor(2.3810)]
[tensor(-1.1187), 0.7033707865168539, tensor(2.3810)]
[tensor(-1.1187), 0.7033707865168539, tensor(2.3810)]
[tensor(-1.1187), 0.7056179775280899, tensor(2.3810)]
[2023-01-18 22:01:40,375.375 dsw44977-5b9b48888d-729dj:110633 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8569), 0.49213483146067416, tensor(0.6038)]
[tensor(-1.2965), 0.6382022471910113, tensor(1.8945)]
[tensor(-1.1394), 0.6651685393258427, tensor(2.1865)]
[tensor(-1.1205), 0.6786516853932584, tensor(2.2728)]
[tensor(-1.1200), 0.6966292134831461, tensor(2.3632)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1200), 0.7078651685393258, tensor(2.3943)]
[tensor(-1.1200), 0.7078651685393258, tensor(2.3943)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1200), 0.7078651685393258, tensor(2.3943)]
[tensor(-1.1200), 0.7078651685393258, tensor(2.3943)]
[tensor(-1.1200), 0.7078651685393258, tensor(2.3943)]
[2023-01-18 22:08:21,540.540 dsw44977-5b9b48888d-729dj:111805 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1801), 0.45617977528089887, tensor(0.1008)]
[tensor(-1.6812), 0.5550561797752809, tensor(1.0941)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1666), 0.6696629213483146, tensor(2.1817)]
[tensor(-1.1164), 0.6831460674157304, tensor(2.2994)]
[tensor(-1.0952), 0.698876404494382, tensor(2.3992)]
[tensor(-1.0696), 0.701123595505618, tensor(2.4360)]
[tensor(-1.0696), 0.701123595505618, tensor(2.4360)]
[tensor(-1.0696), 0.7101123595505618, tensor(2.4360)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
[tensor(-1.0696), 0.7146067415730337, tensor(2.4506)]
early stopping at 19
[2023-01-18 22:20:56,721.721 dsw44977-5b9b48888d-729dj:113996 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1399), 0.449438202247191, tensor(0.1073)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3513), 0.6314606741573033, tensor(1.8060)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1294), 0.6494382022471911, tensor(2.1178)]
[tensor(-1.1294), 0.6808988764044944, tensor(2.2684)]
[tensor(-1.1294), 0.6921348314606741, tensor(2.3213)]
[tensor(-1.1208), 0.6921348314606741, tensor(2.3213)]
[tensor(-1.1208), 0.6921348314606741, tensor(2.3213)]
[tensor(-1.1208), 0.6921348314606741, tensor(2.3213)]
[tensor(-1.1208), 0.6966292134831461, tensor(2.3420)]
[tensor(-1.1208), 0.6966292134831461, tensor(2.3420)]
[tensor(-1.1208), 0.698876404494382, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
[tensor(-1.1208), 0.7191011235955056, tensor(2.3420)]
early stopping at 22
[2023-01-18 22:35:25,314.314 dsw44977-5b9b48888d-729dj:116507 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1389), 0.4247191011235955, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3871), 0.6247191011235955, tensor(1.7365)]
[tensor(-1.1934), 0.6696629213483146, tensor(2.1549)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1250), 0.6696629213483146, tensor(2.2008)]
[tensor(-1.1094), 0.6831460674157304, tensor(2.3063)]
[tensor(-1.1094), 0.6831460674157304, tensor(2.3063)]
[tensor(-1.1094), 0.6831460674157304, tensor(2.3063)]
[tensor(-1.1094), 0.6921348314606741, tensor(2.3063)]
[tensor(-1.1094), 0.6921348314606741, tensor(2.3063)]
[tensor(-1.1094), 0.6921348314606741, tensor(2.3063)]
[2023-01-18 22:42:04,333.333 dsw44977-5b9b48888d-729dj:117672 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7535), 0.49887640449438203, tensor(0.7408)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2562), 0.647191011235955, tensor(1.9797)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1465), 0.6651685393258427, tensor(2.1794)]
[tensor(-1.0653), 0.6831460674157304, tensor(2.3504)]
[tensor(-1.0653), 0.6943820224719102, tensor(2.3742)]
[tensor(-1.0653), 0.7101123595505618, tensor(2.4342)]
[tensor(-1.0653), 0.7101123595505618, tensor(2.4342)]
[tensor(-1.0653), 0.7101123595505618, tensor(2.4342)]
[tensor(-1.0653), 0.7101123595505618, tensor(2.4342)]
[tensor(-1.0653), 0.7101123595505618, tensor(2.4342)]
[2023-01-18 22:48:42,631.631 dsw44977-5b9b48888d-729dj:118838 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9686), 0.4696629213483146, tensor(0.3797)]
[tensor(-1.5871), 0.5685393258426966, tensor(1.2556)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1877), 0.6651685393258427, tensor(2.1381)]
[tensor(-1.1269), 0.6898876404494382, tensor(2.3225)]
[tensor(-1.1269), 0.6921348314606741, tensor(2.3290)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1117), 0.6921348314606741, tensor(2.3490)]
[tensor(-1.1117), 0.6921348314606741, tensor(2.3490)]
[tensor(-1.1117), 0.6943820224719102, tensor(2.3490)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
[tensor(-1.1117), 0.7101123595505618, tensor(2.3924)]
early stopping at 21
[2023-01-18 23:02:29,272.272 dsw44977-5b9b48888d-729dj:121228 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0030), 0.4404494382022472, tensor(0.1992)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3391), 0.6269662921348315, tensor(1.7958)]
[tensor(-1.1512), 0.6606741573033708, tensor(2.1522)]
[tensor(-1.1108), 0.6629213483146067, tensor(2.2038)]
[tensor(-1.1108), 0.6921348314606741, tensor(2.3432)]
[tensor(-1.1082), 0.6921348314606741, tensor(2.3432)]
[tensor(-1.1082), 0.6921348314606741, tensor(2.3432)]
[tensor(-1.1082), 0.698876404494382, tensor(2.3432)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3616)]
early stopping at 19
[2023-01-18 23:14:52,356.356 dsw44977-5b9b48888d-729dj:123383 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3293), 0.3348314606741573, 0.0]
[tensor(-1.4188), 0.6202247191011236, tensor(1.6823)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2165), 0.6741573033707865, tensor(2.1543)]
[tensor(-1.1338), 0.6921348314606741, tensor(2.3269)]
[tensor(-1.1085), 0.701123595505618, tensor(2.3972)]
[tensor(-1.1085), 0.7101123595505618, tensor(2.4380)]
[tensor(-1.0922), 0.7101123595505618, tensor(2.4380)]
[tensor(-1.0922), 0.7101123595505618, tensor(2.4380)]
[tensor(-1.0922), 0.7101123595505618, tensor(2.4380)]
[tensor(-1.0922), 0.7101123595505618, tensor(2.4380)]
[2023-01-18 23:21:41,244.244 dsw44977-5b9b48888d-729dj:124573 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9517), 0.47415730337078654, tensor(0.4191)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2832), 0.6404494382022472, tensor(1.9190)]
[tensor(-1.1648), 0.6629213483146067, tensor(2.1498)]
[tensor(-1.1148), 0.6921348314606741, tensor(2.3459)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0996), 0.6921348314606741, tensor(2.3611)]
[tensor(-1.0996), 0.6921348314606741, tensor(2.3611)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0996), 0.6943820224719102, tensor(2.3611)]
[tensor(-1.0996), 0.6943820224719102, tensor(2.3611)]
[tensor(-1.0996), 0.6943820224719102, tensor(2.3611)]
[tensor(-1.0996), 0.6943820224719102, tensor(2.3611)]
[2023-01-18 23:28:19,395.395 dsw44977-5b9b48888d-729dj:125738 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1995), 0.44269662921348313, tensor(0.0140)]
[tensor(-1.7320), 0.5483146067415731, tensor(1.0095)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2379), 0.6561797752808989, tensor(2.0430)]
[tensor(-1.1631), 0.6808988764044944, tensor(2.2414)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0949), 0.6808988764044944, tensor(2.2983)]
[tensor(-1.0577), 0.6943820224719102, tensor(2.4143)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
[tensor(-1.0577), 0.7033707865168539, tensor(2.4460)]
early stopping at 17
[2023-01-18 23:39:28,827.827 dsw44977-5b9b48888d-729dj:127682 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2497), 0.4044943820224719, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4264), 0.6202247191011236, tensor(1.6747)]
[tensor(-1.1615), 0.6786516853932584, tensor(2.2318)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1580), 0.6831460674157304, tensor(2.2577)]
[tensor(-1.1226), 0.6943820224719102, tensor(2.3493)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
[tensor(-1.1226), 0.6966292134831461, tensor(2.3498)]
early stopping at 20
[2023-01-18 23:52:36,372.372 dsw44977-5b9b48888d-729dj:129964 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2174), 0.33707865168539325, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4239), 0.6314606741573033, tensor(1.7334)]
[tensor(-1.1882), 0.6764044943820224, tensor(2.1939)]
[tensor(-1.1697), 0.6764044943820224, tensor(2.2011)]
[tensor(-1.0995), 0.6921348314606741, tensor(2.3612)]
[tensor(-1.0995), 0.6921348314606741, tensor(2.3612)]
[tensor(-1.0957), 0.701123595505618, tensor(2.4099)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4324)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4324)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4324)]
[2023-01-18 23:59:11,531.531 dsw44977-5b9b48888d-729dj:131117 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7843), 0.5101123595505618, tensor(0.7662)]
[tensor(-1.2565), 0.6426966292134831, tensor(1.9570)]
[tensor(-1.1434), 0.6741573033707865, tensor(2.2274)]
[tensor(-1.0798), 0.6921348314606741, tensor(2.3809)]
[tensor(-1.0798), 0.6943820224719102, tensor(2.3809)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0798), 0.7056179775280899, tensor(2.4082)]
[tensor(-1.0798), 0.7056179775280899, tensor(2.4082)]
[tensor(-1.0798), 0.7078651685393258, tensor(2.4082)]
[tensor(-1.0798), 0.7078651685393258, tensor(2.4082)]
[tensor(-1.0798), 0.7078651685393258, tensor(2.4082)]
[2023-01-19 00:05:46,414.414 dsw44977-5b9b48888d-729dj:132270 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0179), 0.4764044943820225, tensor(0.3641)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6220), 0.5640449438202247, tensor(1.1982)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1827), 0.6674157303370787, tensor(2.1544)]
[tensor(-1.1421), 0.6853932584269663, tensor(2.2849)]
[tensor(-1.1176), 0.6853932584269663, tensor(2.3093)]
[tensor(-1.0799), 0.698876404494382, tensor(2.4144)]
[tensor(-1.0799), 0.698876404494382, tensor(2.4144)]
[tensor(-1.0799), 0.698876404494382, tensor(2.4144)]
[tensor(-1.0799), 0.698876404494382, tensor(2.4144)]
[tensor(-1.0799), 0.698876404494382, tensor(2.4144)]
[tensor(-1.0799), 0.698876404494382, tensor(2.4144)]
[tensor(-1.0799), 0.7033707865168539, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7056179775280899, tensor(2.4144)]
[tensor(-1.0799), 0.7078651685393258, tensor(2.4144)]
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
[tensor(-1.0799), 0.7123595505617978, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
[tensor(-1.0799), 0.7146067415730337, tensor(2.4144)]
early stopping at 40
[2023-01-19 00:32:00,714.714 dsw44977-5b9b48888d-729dj:136809 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.0434), 0.449438202247191, tensor(0.2038)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3217), 0.6337078651685393, tensor(1.8469)]
[tensor(-1.1391), 0.6808988764044944, tensor(2.2654)]
[tensor(-1.1242), 0.6808988764044944, tensor(2.2654)]
[tensor(-1.1242), 0.6808988764044944, tensor(2.2654)]
[tensor(-1.1242), 0.6921348314606741, tensor(2.3311)]
[tensor(-1.1242), 0.698876404494382, tensor(2.3311)]
[tensor(-1.1242), 0.698876404494382, tensor(2.3311)]
[tensor(-1.1242), 0.698876404494382, tensor(2.3311)]
[tensor(-1.1242), 0.698876404494382, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3311)]
early stopping at 21
[2023-01-19 00:45:39,179.179 dsw44977-5b9b48888d-729dj:139175 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3927), 0.33707865168539325, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4919), 0.6157303370786517, tensor(1.5867)]
[tensor(-1.2655), 0.6404494382022472, tensor(1.9368)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1750), 0.6876404494382022, tensor(2.2632)]
[tensor(-1.0995), 0.6966292134831461, tensor(2.3836)]
[tensor(-1.0995), 0.6966292134831461, tensor(2.3836)]
[tensor(-1.0843), 0.698876404494382, tensor(2.4100)]
[tensor(-1.0843), 0.7078651685393258, tensor(2.4261)]
[tensor(-1.0843), 0.7078651685393258, tensor(2.4261)]
[tensor(-1.0843), 0.7078651685393258, tensor(2.4261)]
[2023-01-19 00:52:23,785.785 dsw44977-5b9b48888d-729dj:140359 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8824), 0.5325842696629214, tensor(0.7805)]
[tensor(-1.2315), 0.6584269662921348, tensor(2.0607)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1110), 0.6876404494382022, tensor(2.3272)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1110), 0.6876404494382022, tensor(2.3272)]
[tensor(-1.1033), 0.6943820224719102, tensor(2.3686)]
[tensor(-1.1033), 0.6943820224719102, tensor(2.3686)]
[tensor(-1.1033), 0.6943820224719102, tensor(2.3686)]
[tensor(-1.1033), 0.6966292134831461, tensor(2.3686)]
[tensor(-1.1033), 0.6966292134831461, tensor(2.3686)]
[tensor(-1.1033), 0.7078651685393258, tensor(2.3686)]
[2023-01-19 00:59:02,816.816 dsw44977-5b9b48888d-729dj:141524 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2609), 0.451685393258427, 0.0]
[tensor(-1.7596), 0.5258426966292135, tensor(0.8696)]
[tensor(-1.2228), 0.6426966292134831, tensor(1.9906)]
[tensor(-1.1519), 0.6719101123595506, tensor(2.2076)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1216), 0.6741573033707865, tensor(2.2492)]
[tensor(-1.0514), 0.6921348314606741, tensor(2.4092)]
[tensor(-1.0514), 0.6921348314606741, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0514), 0.6966292134831461, tensor(2.4092)]
early stopping at 18
[2023-01-19 01:10:57,109.109 dsw44977-5b9b48888d-729dj:143594 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.2716), 0.3955056179775281, 0.0]
[tensor(-1.4062), 0.6224719101123596, tensor(1.7062)]
[tensor(-1.1586), 0.6719101123595506, tensor(2.2010)]
[tensor(-1.1371), 0.6876404494382022, tensor(2.3011)]
[tensor(-1.0895), 0.6966292134831461, tensor(2.3937)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0895), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
[tensor(-1.0861), 0.6966292134831461, tensor(2.3937)]
early stopping at 22
[2023-01-19 01:25:16,451.451 dsw44977-5b9b48888d-729dj:146081 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2432), 0.3550561797752809, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4540), 0.6247191011235955, tensor(1.6696)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2532), 0.647191011235955, tensor(1.9828)]
[tensor(-1.1794), 0.6696629213483146, tensor(2.1689)]
[tensor(-1.1145), 0.6853932584269663, tensor(2.3125)]
[tensor(-1.1145), 0.698876404494382, tensor(2.3656)]
[tensor(-1.1108), 0.698876404494382, tensor(2.3724)]
[tensor(-1.1108), 0.698876404494382, tensor(2.3724)]
[tensor(-1.1108), 0.7078651685393258, tensor(2.4058)]
[tensor(-1.1108), 0.7078651685393258, tensor(2.4058)]
[2023-01-19 01:31:55,183.183 dsw44977-5b9b48888d-729dj:147247 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8389), 0.49213483146067416, tensor(0.6218)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3042), 0.6337078651685393, tensor(1.8643)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1613), 0.6876404494382022, tensor(2.2769)]
[tensor(-1.0951), 0.6876404494382022, tensor(2.2769)]
[tensor(-1.0704), 0.6943820224719102, tensor(2.4015)]
[tensor(-1.0704), 0.6943820224719102, tensor(2.4015)]
[tensor(-1.0704), 0.6943820224719102, tensor(2.4015)]
[tensor(-1.0704), 0.7078651685393258, tensor(2.4015)]
[tensor(-1.0704), 0.7078651685393258, tensor(2.4015)]
[tensor(-1.0704), 0.7078651685393258, tensor(2.4015)]
[2023-01-19 01:38:29,232.232 dsw44977-5b9b48888d-729dj:148400 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1765), 0.451685393258427, tensor(0.0819)]
[tensor(-1.7309), 0.5258426966292135, tensor(0.8983)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2971), 0.6494382022471911, tensor(1.9501)]
[tensor(-1.1874), 0.6764044943820224, tensor(2.1946)]
[tensor(-1.1112), 0.6764044943820224, tensor(2.2596)]
[tensor(-1.0939), 0.6853932584269663, tensor(2.3330)]
[tensor(-1.0939), 0.6853932584269663, tensor(2.3330)]
[tensor(-1.0939), 0.6921348314606741, tensor(2.3330)]
[tensor(-1.0939), 0.6943820224719102, tensor(2.3330)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.3330)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.3330)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.3330)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.3330)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.3330)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0939), 0.7056179775280899, tensor(2.3330)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.3330)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.3330)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
[tensor(-1.0939), 0.7123595505617978, tensor(2.3330)]
early stopping at 42
[2023-01-19 02:05:54,305.305 dsw44977-5b9b48888d-729dj:153144 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-2.1517), 0.41123595505617977, 0.0]
[tensor(-1.3614), 0.6292134831460674, tensor(1.7846)]
[tensor(-1.1318), 0.6696629213483146, tensor(2.2165)]
[tensor(-1.0984), 0.6876404494382022, tensor(2.3398)]
[tensor(-1.0895), 0.6943820224719102, tensor(2.3824)]
[tensor(-1.0818), 0.6943820224719102, tensor(2.3901)]
[tensor(-1.0818), 0.698876404494382, tensor(2.3901)]
[tensor(-1.0818), 0.698876404494382, tensor(2.3901)]
[tensor(-1.0818), 0.698876404494382, tensor(2.3901)]
[tensor(-1.0818), 0.698876404494382, tensor(2.3901)]
[tensor(-1.0818), 0.698876404494382, tensor(2.3901)]
[tensor(-1.0818), 0.698876404494382, tensor(2.3901)]
[tensor(-1.0818), 0.698876404494382, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
[tensor(-1.0818), 0.7033707865168539, tensor(2.3901)]
early stopping at 26
[2023-01-19 02:22:50,722.722 dsw44977-5b9b48888d-729dj:156083 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4754), 0.27415730337078653, 0.0]
[tensor(-1.6731), 0.5146067415730337, tensor(0.8999)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2989), 0.6314606741573033, tensor(1.8584)]
[tensor(-1.1692), 0.647191011235955, tensor(2.0668)]
[tensor(-1.1692), 0.651685393258427, tensor(2.0817)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1692), 0.6741573033707865, tensor(2.1864)]
[tensor(-1.1692), 0.6741573033707865, tensor(2.1864)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1692), 0.6741573033707865, tensor(2.1864)]
[tensor(-1.1692), 0.6741573033707865, tensor(2.1864)]
[tensor(-1.1692), 0.6741573033707865, tensor(2.1864)]
[2023-01-19 02:29:30,824.824 dsw44977-5b9b48888d-729dj:157252 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0966), 0.40224719101123596, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.3689), 0.6134831460674157, tensor(1.6985)]
[tensor(-1.1762), 0.6629213483146067, tensor(2.1384)]
[tensor(-1.1112), 0.6808988764044944, tensor(2.2933)]
[tensor(-1.1112), 0.6808988764044944, tensor(2.2933)]
[tensor(-1.1112), 0.6898876404494382, tensor(2.2933)]
[tensor(-1.1112), 0.6898876404494382, tensor(2.2933)]
[tensor(-1.1112), 0.6898876404494382, tensor(2.2933)]
[tensor(-1.1112), 0.6898876404494382, tensor(2.2933)]
[tensor(-1.1112), 0.6898876404494382, tensor(2.2933)]
[2023-01-19 02:36:07,931.931 dsw44977-5b9b48888d-729dj:158408 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5005), 0.24269662921348314, 0.0]
[tensor(-2.1012), 0.4247191011235955, tensor(0.0224)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4134), 0.597752808988764, tensor(1.5753)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3119), 0.6202247191011236, tensor(1.7892)]
[tensor(-1.2544), 0.6269662921348315, tensor(1.8805)]
[tensor(-1.1673), 0.6494382022471911, tensor(2.0799)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1673), 0.6494382022471911, tensor(2.0799)]
[tensor(-1.1673), 0.6606741573033708, tensor(2.0841)]
[tensor(-1.1673), 0.6674157303370787, tensor(2.1288)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
[tensor(-1.1673), 0.6808988764044944, tensor(2.1317)]
early stopping at 20
[2023-01-19 02:49:19,451.451 dsw44977-5b9b48888d-729dj:160702 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.3404), 0.3797752808988764, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3813), 0.6426966292134831, tensor(1.8322)]
[tensor(-1.1496), 0.6786516853932584, tensor(2.2437)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1126), 0.6966292134831461, tensor(2.3705)]
[tensor(-1.1126), 0.6966292134831461, tensor(2.3705)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1126), 0.6966292134831461, tensor(2.3705)]
[tensor(-1.1126), 0.6966292134831461, tensor(2.3705)]
[tensor(-1.1126), 0.6966292134831461, tensor(2.3705)]
[tensor(-1.1126), 0.6966292134831461, tensor(2.3705)]
[tensor(-1.1126), 0.6966292134831461, tensor(2.3705)]
[tensor(-1.1126), 0.701123595505618, tensor(2.3705)]
[tensor(-1.1126), 0.701123595505618, tensor(2.3705)]
[tensor(-1.1126), 0.701123595505618, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
[tensor(-1.1126), 0.7056179775280899, tensor(2.3705)]
early stopping at 24
[2023-01-19 03:04:52,260.260 dsw44977-5b9b48888d-729dj:163407 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2060), 0.39775280898876403, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4299), 0.6089887640449438, tensor(1.6150)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2370), 0.6629213483146067, tensor(2.0776)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2026), 0.6629213483146067, tensor(2.0776)]
[tensor(-1.1140), 0.6876404494382022, tensor(2.3242)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1140), 0.6921348314606741, tensor(2.3242)]
[tensor(-1.1140), 0.6921348314606741, tensor(2.3242)]
[tensor(-1.1140), 0.6943820224719102, tensor(2.3242)]
[tensor(-1.1140), 0.6943820224719102, tensor(2.3242)]
[tensor(-1.1140), 0.6943820224719102, tensor(2.3242)]
[2023-01-19 03:11:28,786.786 dsw44977-5b9b48888d-729dj:164560 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8081), 0.5191011235955056, tensor(0.7874)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2837), 0.6224719101123596, tensor(1.8286)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1639), 0.6584269662921348, tensor(2.1282)]
[tensor(-1.1207), 0.6786516853932584, tensor(2.2726)]
[tensor(-1.0769), 0.6786516853932584, tensor(2.3163)]
[tensor(-1.0769), 0.6898876404494382, tensor(2.3346)]
[tensor(-1.0769), 0.6921348314606741, tensor(2.3346)]
[tensor(-1.0769), 0.6943820224719102, tensor(2.3346)]
[tensor(-1.0769), 0.6943820224719102, tensor(2.3346)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0769), 0.6943820224719102, tensor(2.3346)]
[2023-01-19 03:18:02,795.795 dsw44977-5b9b48888d-729dj:165714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0865), 0.45393258426966293, tensor(0.1832)]
[tensor(-1.6560), 0.5640449438202247, tensor(1.1642)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2529), 0.6382022471910113, tensor(1.9382)]
[tensor(-1.1788), 0.6808988764044944, tensor(2.2256)]
[tensor(-1.1369), 0.6943820224719102, tensor(2.3350)]
[tensor(-1.0859), 0.701123595505618, tensor(2.4197)]
[tensor(-1.0859), 0.701123595505618, tensor(2.4197)]
[tensor(-1.0859), 0.7033707865168539, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
[tensor(-1.0859), 0.7123595505617978, tensor(2.4197)]
early stopping at 33
[2023-01-19 03:39:40,967.967 dsw44977-5b9b48888d-729dj:169462 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9997), 0.45393258426966293, tensor(0.2700)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3401), 0.6112359550561798, tensor(1.7161)]
[tensor(-1.1357), 0.6651685393258427, tensor(2.1901)]
[tensor(-1.1195), 0.6898876404494382, tensor(2.3299)]
[tensor(-1.1195), 0.6898876404494382, tensor(2.3299)]
[tensor(-1.1195), 0.6898876404494382, tensor(2.3299)]
[tensor(-1.1195), 0.6898876404494382, tensor(2.3299)]
[tensor(-1.1195), 0.698876404494382, tensor(2.3527)]
[tensor(-1.1195), 0.698876404494382, tensor(2.3527)]
[tensor(-1.1195), 0.698876404494382, tensor(2.3527)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1195), 0.701123595505618, tensor(2.3527)]
[tensor(-1.1195), 0.701123595505618, tensor(2.3527)]
[tensor(-1.1195), 0.701123595505618, tensor(2.3527)]
[tensor(-1.1195), 0.701123595505618, tensor(2.3527)]
[tensor(-1.1195), 0.7078651685393258, tensor(2.3527)]
[tensor(-1.1195), 0.7078651685393258, tensor(2.3527)]
[tensor(-1.1195), 0.7078651685393258, tensor(2.3527)]
[tensor(-1.1195), 0.7078651685393258, tensor(2.3527)]
[tensor(-1.1195), 0.7078651685393258, tensor(2.3527)]
[tensor(-1.1195), 0.7123595505617978, tensor(2.3527)]
[tensor(-1.1195), 0.7123595505617978, tensor(2.3527)]
[tensor(-1.1195), 0.7123595505617978, tensor(2.3527)]
[tensor(-1.1195), 0.7146067415730337, tensor(2.3527)]
[tensor(-1.1195), 0.7146067415730337, tensor(2.3527)]
[tensor(-1.1195), 0.7168539325842697, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
[tensor(-1.1195), 0.7258426966292135, tensor(2.3527)]
early stopping at 36
[2023-01-19 04:03:06,124.124 dsw44977-5b9b48888d-729dj:173518 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4226), 0.29213483146067415, 0.0]
[tensor(-1.5742), 0.5573033707865168, tensor(1.2123)]
[tensor(-1.2397), 0.6606741573033708, tensor(2.0637)]
[tensor(-1.1422), 0.6741573033707865, tensor(2.2286)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1391), 0.6786516853932584, tensor(2.2542)]
[tensor(-1.1391), 0.6853932584269663, tensor(2.2542)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1391), 0.6898876404494382, tensor(2.2902)]
[tensor(-1.1391), 0.6898876404494382, tensor(2.2902)]
[tensor(-1.1391), 0.6898876404494382, tensor(2.2902)]
[tensor(-1.1391), 0.6921348314606741, tensor(2.2902)]
[2023-01-19 04:09:47,517.517 dsw44977-5b9b48888d-729dj:174689 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1312), 0.40898876404494383, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3699), 0.6269662921348315, tensor(1.7650)]
[tensor(-1.1742), 0.6853932584269663, tensor(2.2528)]
[tensor(-1.1038), 0.6898876404494382, tensor(2.3457)]
[tensor(-1.1038), 0.6898876404494382, tensor(2.3457)]
[tensor(-1.1038), 0.6898876404494382, tensor(2.3457)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1038), 0.7101123595505618, tensor(2.4245)]
[tensor(-1.1038), 0.7101123595505618, tensor(2.4245)]
[tensor(-1.1038), 0.7101123595505618, tensor(2.4245)]
[tensor(-1.1038), 0.7101123595505618, tensor(2.4245)]
[2023-01-19 04:16:25,956.956 dsw44977-5b9b48888d-729dj:175855 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.5404), 0.31235955056179776, 0.0]
[tensor(-2.0843), 0.41123595505617977, 0.0]
[tensor(-1.2510), 0.6426966292134831, tensor(1.9625)]
[tensor(-1.1896), 0.6606741573033708, tensor(2.1138)]
[tensor(-1.1619), 0.6853932584269663, tensor(2.2651)]
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
[tensor(-1.1455), 0.6943820224719102, tensor(2.3264)]
[tensor(-1.1455), 0.6966292134831461, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.701123595505618, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
[tensor(-1.1455), 0.7056179775280899, tensor(2.3264)]
early stopping at 33
[2023-01-19 04:38:03,883.883 dsw44977-5b9b48888d-729dj:179603 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3019), 0.3797752808988764, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4367), 0.604494382022472, tensor(1.5857)]
[tensor(-1.1605), 0.6674157303370787, tensor(2.1765)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1605), 0.6853932584269663, tensor(2.2593)]
[tensor(-1.1426), 0.6853932584269663, tensor(2.2732)]
[tensor(-1.1426), 0.6853932584269663, tensor(2.2732)]
[tensor(-1.1426), 0.6943820224719102, tensor(2.3020)]
[tensor(-1.1426), 0.6966292134831461, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7056179775280899, tensor(2.3020)]
[tensor(-1.1426), 0.7078651685393258, tensor(2.3020)]
[tensor(-1.1426), 0.7078651685393258, tensor(2.3020)]
[tensor(-1.1426), 0.7078651685393258, tensor(2.3020)]
[tensor(-1.1426), 0.7078651685393258, tensor(2.3020)]
[tensor(-1.1426), 0.7101123595505618, tensor(2.3020)]
[tensor(-1.1426), 0.7101123595505618, tensor(2.3020)]
[tensor(-1.1426), 0.7101123595505618, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
[tensor(-1.1426), 0.7123595505617978, tensor(2.3020)]
early stopping at 39
[2023-01-19 05:03:23,116.116 dsw44977-5b9b48888d-729dj:183985 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1423), 0.41797752808988764, 0.0]
[tensor(-1.3944), 0.6449438202247191, tensor(1.8303)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2491), 0.6584269662921348, tensor(2.0431)]
[tensor(-1.1855), 0.6674157303370787, tensor(2.1516)]
[tensor(-1.1346), 0.6898876404494382, tensor(2.3149)]
[tensor(-1.1346), 0.6898876404494382, tensor(2.3149)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1346), 0.6943820224719102, tensor(2.3149)]
[tensor(-1.1346), 0.7033707865168539, tensor(2.3346)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1346), 0.7101123595505618, tensor(2.3737)]
[tensor(-1.1346), 0.7101123595505618, tensor(2.3737)]
[2023-01-19 05:10:01,922.922 dsw44977-5b9b48888d-729dj:185150 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8382), 0.5348314606741573, tensor(0.8360)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2682), 0.6606741573033708, tensor(2.0352)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1244), 0.6898876404494382, tensor(2.3250)]
[tensor(-1.1086), 0.6898876404494382, tensor(2.3296)]
[tensor(-1.0579), 0.6921348314606741, tensor(2.4027)]
[tensor(-1.0579), 0.6921348314606741, tensor(2.4027)]
[tensor(-1.0579), 0.6921348314606741, tensor(2.4027)]
[tensor(-1.0579), 0.6921348314606741, tensor(2.4027)]
[tensor(-1.0579), 0.701123595505618, tensor(2.4027)]
[tensor(-1.0579), 0.701123595505618, tensor(2.4027)]
[2023-01-19 05:16:38,448.448 dsw44977-5b9b48888d-729dj:186304 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9276), 0.49887640449438203, tensor(0.5668)]
[tensor(-1.5376), 0.5955056179775281, tensor(1.4400)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1891), 0.6696629213483146, tensor(2.1593)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1595), 0.6696629213483146, tensor(2.1664)]
[tensor(-1.1398), 0.6696629213483146, tensor(2.2085)]
[tensor(-1.1140), 0.6943820224719102, tensor(2.3579)]
[tensor(-1.1140), 0.6943820224719102, tensor(2.3579)]
[tensor(-1.1140), 0.6943820224719102, tensor(2.3579)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
[tensor(-1.1140), 0.7056179775280899, tensor(2.3679)]
early stopping at 19
[2023-01-19 05:29:06,694.694 dsw44977-5b9b48888d-729dj:188477 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9514), 0.4966292134831461, tensor(0.5317)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3395), 0.6247191011235955, tensor(1.7841)]
[tensor(-1.1476), 0.6651685393258427, tensor(2.1782)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1463), 0.6853932584269663, tensor(2.2807)]
[tensor(-1.1463), 0.6876404494382022, tensor(2.2847)]
[tensor(-1.1463), 0.6876404494382022, tensor(2.2847)]
[tensor(-1.1463), 0.6876404494382022, tensor(2.2847)]
[tensor(-1.1463), 0.6921348314606741, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
[tensor(-1.1463), 0.698876404494382, tensor(2.2847)]
early stopping at 19
[2023-01-19 05:41:29,832.832 dsw44977-5b9b48888d-729dj:190632 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4128), 0.27415730337078653, 0.0]
[tensor(-1.5730), 0.5460674157303371, tensor(1.1573)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2742), 0.6561797752808989, tensor(2.0067)]
[tensor(-1.1774), 0.6584269662921348, tensor(2.1147)]
[tensor(-1.1697), 0.6719101123595506, tensor(2.1898)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1697), 0.6876404494382022, tensor(2.2656)]
[tensor(-1.1697), 0.6943820224719102, tensor(2.2929)]
[tensor(-1.1697), 0.6943820224719102, tensor(2.2929)]
[tensor(-1.1697), 0.6943820224719102, tensor(2.2929)]
[tensor(-1.1697), 0.6943820224719102, tensor(2.2929)]
[2023-01-19 05:48:10,081.081 dsw44977-5b9b48888d-729dj:191798 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1596), 0.39775280898876403, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4068), 0.5955056179775281, tensor(1.5707)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2201), 0.6674157303370787, tensor(2.1170)]
[tensor(-1.1358), 0.6674157303370787, tensor(2.2013)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1224), 0.6831460674157304, tensor(2.2934)]
[tensor(-1.1224), 0.6876404494382022, tensor(2.2934)]
[tensor(-1.1224), 0.6876404494382022, tensor(2.2934)]
[tensor(-1.1224), 0.7033707865168539, tensor(2.2934)]
[tensor(-1.1224), 0.7033707865168539, tensor(2.2934)]
[tensor(-1.1224), 0.7033707865168539, tensor(2.2934)]
[2023-01-19 05:54:51,282.282 dsw44977-5b9b48888d-729dj:192969 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4756), 0.3146067415730337, 0.0]
[tensor(-1.9786), 0.47415730337078654, tensor(0.3922)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2382), 0.6269662921348315, tensor(1.8967)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1915), 0.651685393258427, tensor(2.0669)]
[tensor(-1.1623), 0.6651685393258427, tensor(2.1635)]
[tensor(-1.1186), 0.6808988764044944, tensor(2.2859)]
[tensor(-1.1186), 0.6808988764044944, tensor(2.2859)]
[tensor(-1.1186), 0.6808988764044944, tensor(2.2859)]
[tensor(-1.1186), 0.6876404494382022, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.698876404494382, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7033707865168539, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
[tensor(-1.1186), 0.7078651685393258, tensor(2.2859)]
early stopping at 49
[2023-01-19 06:26:49,053.053 dsw44977-5b9b48888d-729dj:198498 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8400), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.6326), 0.23595505617977527, 0.0]
[tensor(-1.9386), 0.451685393258427, tensor(0.3198)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5739), 0.5438202247191011, tensor(1.1452)]
[tensor(-1.4324), 0.5955056179775281, tensor(1.5452)]
[tensor(-1.3329), 0.6022471910112359, tensor(1.6784)]
[tensor(-1.3221), 0.6202247191011236, tensor(1.7790)]
[tensor(-1.3221), 0.647191011235955, tensor(1.9073)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3221), 0.6584269662921348, tensor(1.9073)]
[tensor(-1.3221), 0.6584269662921348, tensor(1.9073)]
[tensor(-1.3221), 0.6584269662921348, tensor(1.9073)]
[tensor(-1.3221), 0.6584269662921348, tensor(1.9073)]
[tensor(-1.3221), 0.6584269662921348, tensor(1.9073)]
[tensor(-1.3221), 0.6584269662921348, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6674157303370787, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
[tensor(-1.3221), 0.6853932584269663, tensor(1.9073)]
early stopping at 37
[2023-01-19 06:50:45,131.131 dsw44977-5b9b48888d-729dj:202644 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0580), 0.44719101123595506, tensor(0.1779)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3795), 0.6247191011235955, tensor(1.7441)]
[tensor(-1.2046), 0.6719101123595506, tensor(2.1550)]
[tensor(-1.1488), 0.6853932584269663, tensor(2.2782)]
[tensor(-1.1004), 0.6966292134831461, tensor(2.3828)]
[tensor(-1.1004), 0.6966292134831461, tensor(2.3828)]
[tensor(-1.1004), 0.7078651685393258, tensor(2.4179)]
[tensor(-1.1004), 0.7078651685393258, tensor(2.4179)]
[tensor(-1.1004), 0.7078651685393258, tensor(2.4179)]
[tensor(-1.1004), 0.7078651685393258, tensor(2.4179)]
[2023-01-19 06:57:23,105.105 dsw44977-5b9b48888d-729dj:203804 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7359), 0.501123595505618, tensor(0.7697)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2651), 0.6494382022471911, tensor(1.9821)]
[tensor(-1.1411), 0.6808988764044944, tensor(2.2634)]
[tensor(-1.1243), 0.6808988764044944, tensor(2.2634)]
[tensor(-1.0887), 0.6808988764044944, tensor(2.3158)]
[tensor(-1.0887), 0.6808988764044944, tensor(2.3158)]
[tensor(-1.0887), 0.6808988764044944, tensor(2.3158)]
[tensor(-1.0887), 0.6808988764044944, tensor(2.3158)]
[tensor(-1.0887), 0.6808988764044944, tensor(2.3158)]
[tensor(-1.0887), 0.6808988764044944, tensor(2.3158)]
[2023-01-19 07:03:59,095.095 dsw44977-5b9b48888d-729dj:204963 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-2.0397), 0.4786516853932584, tensor(0.3535)]
[tensor(-1.6165), 0.5595505617977528, tensor(1.1812)]
[tensor(-1.1871), 0.6831460674157304, tensor(2.2286)]
[tensor(-1.1422), 0.6831460674157304, tensor(2.2286)]
[tensor(-1.0964), 0.6943820224719102, tensor(2.3755)]
[tensor(-1.0964), 0.6943820224719102, tensor(2.3755)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3755)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3755)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3755)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3755)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
[tensor(-1.0964), 0.7078651685393258, tensor(2.3755)]
early stopping at 22
[2023-01-19 07:18:22,733.733 dsw44977-5b9b48888d-729dj:207462 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.0407), 0.4314606741573034, tensor(0.1166)]
[tensor(-1.3457), 0.6561797752808989, tensor(1.9352)]
[tensor(-1.1585), 0.6741573033707865, tensor(2.2123)]
[tensor(-1.1450), 0.6808988764044944, tensor(2.2595)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0997), 0.6853932584269663, tensor(2.3273)]
[tensor(-1.0997), 0.6898876404494382, tensor(2.3273)]
[tensor(-1.0997), 0.6898876404494382, tensor(2.3273)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.701123595505618, tensor(2.3368)]
[tensor(-1.0997), 0.7056179775280899, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
[tensor(-1.0997), 0.7123595505617978, tensor(2.3368)]
early stopping at 29
[2023-01-19 07:37:12,566.566 dsw44977-5b9b48888d-729dj:210728 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3735), 0.33707865168539325, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5646), 0.5797752808988764, tensor(1.3342)]
[tensor(-1.2774), 0.647191011235955, tensor(1.9585)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2091), 0.647191011235955, tensor(2.0044)]
[tensor(-1.1827), 0.6674157303370787, tensor(2.1544)]
[tensor(-1.1827), 0.6741573033707865, tensor(2.1569)]
[tensor(-1.1768), 0.6853932584269663, tensor(2.2501)]
[tensor(-1.1768), 0.6853932584269663, tensor(2.2501)]
[tensor(-1.1768), 0.6966292134831461, tensor(2.2501)]
[tensor(-1.1768), 0.6966292134831461, tensor(2.2501)]
[2023-01-19 07:43:49,064.064 dsw44977-5b9b48888d-729dj:211881 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0890), 0.42696629213483145, tensor(0.0459)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4014), 0.6089887640449438, tensor(1.6435)]
[tensor(-1.2155), 0.6561797752808989, tensor(2.0654)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1663), 0.6719101123595506, tensor(2.1932)]
[tensor(-1.1401), 0.6719101123595506, tensor(2.2082)]
[tensor(-1.1401), 0.6741573033707865, tensor(2.2082)]
[tensor(-1.1401), 0.6786516853932584, tensor(2.2160)]
[tensor(-1.1401), 0.6966292134831461, tensor(2.2564)]
[tensor(-1.1401), 0.6966292134831461, tensor(2.2564)]
[tensor(-1.1401), 0.6966292134831461, tensor(2.2564)]
[2023-01-19 07:50:24,099.099 dsw44977-5b9b48888d-729dj:213041 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4548), 0.3775280898876405, 0.0]
[tensor(-1.9303), 0.4651685393258427, tensor(0.3955)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3015), 0.6157303370786517, tensor(1.7772)]
[tensor(-1.2247), 0.6382022471910113, tensor(1.9663)]
[tensor(-1.1997), 0.647191011235955, tensor(2.0362)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1836), 0.651685393258427, tensor(2.0748)]
[tensor(-1.1836), 0.6606741573033708, tensor(2.0949)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1836), 0.6674157303370787, tensor(2.0949)]
[tensor(-1.1836), 0.6674157303370787, tensor(2.0949)]
[tensor(-1.1836), 0.6741573033707865, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
[tensor(-1.1836), 0.6764044943820224, tensor(2.0949)]
early stopping at 21
[2023-01-19 08:04:09,868.868 dsw44977-5b9b48888d-729dj:215431 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3602), 0.3595505617977528, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4515), 0.5955056179775281, tensor(1.5260)]
[tensor(-1.1562), 0.6831460674157304, tensor(2.2596)]
[tensor(-1.1393), 0.6853932584269663, tensor(2.2877)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1393), 0.6853932584269663, tensor(2.2877)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1357), 0.6943820224719102, tensor(2.3362)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
[tensor(-1.1278), 0.7123595505617978, tensor(2.4340)]
early stopping at 17
[2023-01-19 08:15:15,508.508 dsw44977-5b9b48888d-729dj:217363 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1141), 0.4337078651685393, tensor(0.0545)]
[tensor(-1.3993), 0.6247191011235955, tensor(1.7243)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2120), 0.6561797752808989, tensor(2.0689)]
[tensor(-1.1413), 0.6853932584269663, tensor(2.2857)]
[tensor(-1.0901), 0.701123595505618, tensor(2.4156)]
[tensor(-1.0901), 0.701123595505618, tensor(2.4156)]
[tensor(-1.0901), 0.701123595505618, tensor(2.4156)]
[tensor(-1.0901), 0.701123595505618, tensor(2.4156)]
[tensor(-1.0901), 0.7056179775280899, tensor(2.4156)]
[tensor(-1.0901), 0.7056179775280899, tensor(2.4156)]
[2023-01-19 08:21:55,195.195 dsw44977-5b9b48888d-729dj:218529 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7783), 0.5056179775280899, tensor(0.7498)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2884), 0.6494382022471911, tensor(1.9588)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1671), 0.6651685393258427, tensor(2.1587)]
[tensor(-1.1671), 0.6674157303370787, tensor(2.1671)]
[tensor(-1.0996), 0.6898876404494382, tensor(2.3499)]
[tensor(-1.0996), 0.6898876404494382, tensor(2.3499)]
[tensor(-1.0996), 0.6898876404494382, tensor(2.3499)]
[tensor(-1.0996), 0.6898876404494382, tensor(2.3499)]
[tensor(-1.0996), 0.6966292134831461, tensor(2.3499)]
[tensor(-1.0996), 0.6966292134831461, tensor(2.3499)]
[2023-01-19 08:28:28,075.075 dsw44977-5b9b48888d-729dj:219676 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9236), 0.49213483146067416, tensor(0.5371)]
[tensor(-1.5483), 0.5910112359550562, tensor(1.4068)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1727), 0.6786516853932584, tensor(2.2205)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1413), 0.6786516853932584, tensor(2.2205)]
[tensor(-1.1142), 0.6966292134831461, tensor(2.3690)]
[tensor(-1.1142), 0.6966292134831461, tensor(2.3690)]
[tensor(-1.1142), 0.6966292134831461, tensor(2.3690)]
[tensor(-1.1142), 0.6966292134831461, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
[tensor(-1.1142), 0.698876404494382, tensor(2.3690)]
early stopping at 24
[2023-01-19 08:44:07,369.369 dsw44977-5b9b48888d-729dj:222393 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.0472), 0.42696629213483145, tensor(0.0877)]
[tensor(-1.3765), 0.6269662921348315, tensor(1.7583)]
[tensor(-1.1475), 0.6651685393258427, tensor(2.1783)]
[tensor(-1.1475), 0.6764044943820224, tensor(2.2083)]
[tensor(-1.0957), 0.6876404494382022, tensor(2.3425)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0957), 0.6898876404494382, tensor(2.3463)]
[tensor(-1.0957), 0.6898876404494382, tensor(2.3463)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4112)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4112)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4112)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4112)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4112)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4112)]
[tensor(-1.0957), 0.7078651685393258, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
[tensor(-1.0957), 0.7146067415730337, tensor(2.4112)]
early stopping at 25
[2023-01-19 09:00:27,809.809 dsw44977-5b9b48888d-729dj:225230 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1612), 0.4134831460674157, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4180), 0.6, tensor(1.5820)]
[tensor(-1.1749), 0.6853932584269663, tensor(2.2520)]
[tensor(-1.1148), 0.6853932584269663, tensor(2.2897)]
[tensor(-1.0940), 0.7033707865168539, tensor(2.4228)]
[tensor(-1.0918), 0.7078651685393258, tensor(2.4476)]
[tensor(-1.0918), 0.7078651685393258, tensor(2.4476)]
[tensor(-1.0918), 0.7078651685393258, tensor(2.4476)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4476)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4476)]
[2023-01-19 09:07:07,969.969 dsw44977-5b9b48888d-729dj:226401 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8744), 0.48314606741573035, tensor(0.5413)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.2786), 0.6584269662921348, tensor(2.0135)]
[tensor(-1.1274), 0.6853932584269663, tensor(2.2995)]
[tensor(-1.0823), 0.6966292134831461, tensor(2.4009)]
[tensor(-1.0823), 0.6966292134831461, tensor(2.4009)]
[tensor(-1.0823), 0.6966292134831461, tensor(2.4009)]
[tensor(-1.0823), 0.701123595505618, tensor(2.4009)]
[tensor(-1.0823), 0.701123595505618, tensor(2.4009)]
[tensor(-1.0823), 0.701123595505618, tensor(2.4009)]
[tensor(-1.0823), 0.7056179775280899, tensor(2.4009)]
[2023-01-19 09:13:43,143.143 dsw44977-5b9b48888d-729dj:227555 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1273), 0.43595505617977526, tensor(0.0525)]
[tensor(-1.6694), 0.550561797752809, tensor(1.0834)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1671), 0.6719101123595506, tensor(2.1925)]
[tensor(-1.1236), 0.6764044943820224, tensor(2.2584)]
[tensor(-1.1236), 0.6764044943820224, tensor(2.2584)]
[tensor(-1.1096), 0.6898876404494382, tensor(2.3398)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1096), 0.6966292134831461, tensor(2.3398)]
[tensor(-1.1096), 0.6966292134831461, tensor(2.3398)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1096), 0.6966292134831461, tensor(2.3398)]
[tensor(-1.1096), 0.6966292134831461, tensor(2.3398)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
[tensor(-1.1096), 0.7123595505617978, tensor(2.3597)]
early stopping at 21
[2023-01-19 09:27:23,393.393 dsw44977-5b9b48888d-729dj:229929 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1344), 0.42696629213483145, tensor(0.0004)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3782), 0.6494382022471911, tensor(1.8689)]
[tensor(-1.1354), 0.6719101123595506, tensor(2.2242)]
[tensor(-1.0973), 0.6898876404494382, tensor(2.3521)]
[tensor(-1.0973), 0.6898876404494382, tensor(2.3521)]
[tensor(-1.0973), 0.6898876404494382, tensor(2.3521)]
[tensor(-1.0973), 0.6898876404494382, tensor(2.3521)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0973), 0.701123595505618, tensor(2.3786)]
[tensor(-1.0973), 0.701123595505618, tensor(2.3786)]
[tensor(-1.0973), 0.7056179775280899, tensor(2.3786)]
[tensor(-1.0973), 0.7056179775280899, tensor(2.3786)]
[tensor(-1.0973), 0.7056179775280899, tensor(2.3786)]
[tensor(-1.0973), 0.7056179775280899, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
[tensor(-1.0973), 0.7123595505617978, tensor(2.3786)]
early stopping at 24
[2023-01-19 09:42:57,779.779 dsw44977-5b9b48888d-729dj:232633 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0973), 0.44719101123595506, tensor(0.1386)]
[tensor(-1.3858), 0.6292134831460674, tensor(1.7602)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2006), 0.6719101123595506, tensor(2.1589)]
[tensor(-1.1207), 0.6719101123595506, tensor(2.2277)]
[tensor(-1.0824), 0.6966292134831461, tensor(2.4008)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0824), 0.698876404494382, tensor(2.4008)]
[tensor(-1.0824), 0.698876404494382, tensor(2.4008)]
[tensor(-1.0824), 0.698876404494382, tensor(2.4008)]
[tensor(-1.0824), 0.7033707865168539, tensor(2.4008)]
[tensor(-1.0824), 0.7033707865168539, tensor(2.4008)]
[2023-01-19 09:49:35,323.323 dsw44977-5b9b48888d-729dj:233793 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6962), 0.5348314606741573, tensor(0.9780)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2272), 0.6674157303370787, tensor(2.1099)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1012), 0.6719101123595506, tensor(2.2584)]
[tensor(-1.1009), 0.6831460674157304, tensor(2.3149)]
[tensor(-1.0749), 0.7078651685393258, tensor(2.4644)]
[tensor(-1.0749), 0.7078651685393258, tensor(2.4644)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0749), 0.7078651685393258, tensor(2.4644)]
[tensor(-1.0749), 0.7078651685393258, tensor(2.4644)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.0749), 0.7078651685393258, tensor(2.4644)]
[tensor(-1.0749), 0.7078651685393258, tensor(2.4644)]
[2023-01-19 09:56:14,223.223 dsw44977-5b9b48888d-729dj:234958 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9521), 0.4853932584269663, tensor(0.4748)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5941), 0.5595505617977528, tensor(1.2036)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1940), 0.6629213483146067, tensor(2.1206)]
[tensor(-1.1193), 0.6876404494382022, tensor(2.3189)]
[tensor(-1.0869), 0.6966292134831461, tensor(2.3962)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
[tensor(-1.0499), 0.7056179775280899, tensor(2.4782)]
early stopping at 21
[2023-01-19 10:10:07,001.001 dsw44977-5b9b48888d-729dj:237373 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9615), 0.45617977528089887, tensor(0.3194)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3046), 0.6629213483146067, tensor(2.0100)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1146), 0.6898876404494382, tensor(2.3348)]
[tensor(-1.0816), 0.6898876404494382, tensor(2.3348)]
[tensor(-1.0758), 0.7056179775280899, tensor(2.4523)]
[tensor(-1.0758), 0.7056179775280899, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7101123595505618, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
[tensor(-1.0758), 0.7168539325842697, tensor(2.4523)]
early stopping at 27
[2023-01-19 10:27:41,899.899 dsw44977-5b9b48888d-729dj:240421 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1836), 0.4134831460674157, 0.0]
[tensor(-1.4113), 0.6157303370786517, tensor(1.6673)]
[tensor(-1.1956), 0.6764044943820224, tensor(2.1864)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1306), 0.6831460674157304, tensor(2.2851)]
[tensor(-1.1260), 0.6898876404494382, tensor(2.3235)]
[tensor(-1.1260), 0.6898876404494382, tensor(2.3235)]
[tensor(-1.1260), 0.6966292134831461, tensor(2.3500)]
[tensor(-1.1260), 0.698876404494382, tensor(2.3500)]
[tensor(-1.1260), 0.7056179775280899, tensor(2.3500)]
[tensor(-1.1260), 0.7056179775280899, tensor(2.3500)]
[2023-01-19 10:34:22,597.597 dsw44977-5b9b48888d-729dj:241593 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8552), 0.4898876404494382, tensor(0.5943)]
[tensor(-1.2729), 0.6359550561797753, tensor(1.9069)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1297), 0.6741573033707865, tensor(2.2410)]
[tensor(-1.0963), 0.701123595505618, tensor(2.4094)]
[tensor(-1.0963), 0.701123595505618, tensor(2.4094)]
[tensor(-1.0963), 0.701123595505618, tensor(2.4094)]
[tensor(-1.0963), 0.701123595505618, tensor(2.4094)]
[tensor(-1.0963), 0.701123595505618, tensor(2.4094)]
[tensor(-1.0963), 0.701123595505618, tensor(2.4094)]
[tensor(-1.0963), 0.701123595505618, tensor(2.4094)]
[2023-01-19 10:40:56,245.245 dsw44977-5b9b48888d-729dj:242740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1602), 0.42921348314606744, 0.0]
[tensor(-1.7102), 0.5460674157303371, tensor(1.0201)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2090), 0.647191011235955, tensor(2.0270)]
[tensor(-1.1473), 0.6741573033707865, tensor(2.2235)]
[tensor(-1.1473), 0.6764044943820224, tensor(2.2338)]
[tensor(-1.1138), 0.6853932584269663, tensor(2.3131)]
[tensor(-1.1138), 0.6898876404494382, tensor(2.3131)]
[tensor(-1.1138), 0.6943820224719102, tensor(2.3131)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7078651685393258, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
[tensor(-1.1138), 0.7101123595505618, tensor(2.3454)]
early stopping at 47
[2023-01-19 11:11:41,175.175 dsw44977-5b9b48888d-729dj:248055 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1248), 0.42921348314606744, tensor(0.0213)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3921), 0.6314606741573033, tensor(1.7652)]
[tensor(-1.1348), 0.6674157303370787, tensor(2.2022)]
[tensor(-1.1060), 0.6853932584269663, tensor(2.3210)]
[tensor(-1.1060), 0.6853932584269663, tensor(2.3210)]
[tensor(-1.1060), 0.6853932584269663, tensor(2.3210)]
[tensor(-1.1023), 0.6853932584269663, tensor(2.3210)]
[tensor(-1.1023), 0.6943820224719102, tensor(2.3295)]
[tensor(-1.1023), 0.6943820224719102, tensor(2.3295)]
[tensor(-1.1023), 0.6943820224719102, tensor(2.3295)]
[tensor(-1.1023), 0.698876404494382, tensor(2.3295)]
[tensor(-1.1023), 0.698876404494382, tensor(2.3295)]
[tensor(-1.1023), 0.698876404494382, tensor(2.3295)]
[tensor(-1.1023), 0.7056179775280899, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
[tensor(-1.1023), 0.7101123595505618, tensor(2.3295)]
early stopping at 25
[2023-01-19 11:28:01,041.041 dsw44977-5b9b48888d-729dj:250889 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0769), 0.43595505617977526, tensor(0.1028)]
[tensor(-1.3636), 0.6426966292134831, tensor(1.8499)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1930), 0.6764044943820224, tensor(2.1890)]
[tensor(-1.1217), 0.7056179775280899, tensor(2.4063)]
[tensor(-1.0996), 0.7056179775280899, tensor(2.4285)]
[tensor(-1.0996), 0.7146067415730337, tensor(2.4456)]
[tensor(-1.0996), 0.7146067415730337, tensor(2.4456)]
[tensor(-1.0996), 0.7146067415730337, tensor(2.4456)]
[tensor(-1.0996), 0.7146067415730337, tensor(2.4456)]
[tensor(-1.0996), 0.7146067415730337, tensor(2.4456)]
[2023-01-19 11:34:38,463.463 dsw44977-5b9b48888d-729dj:252051 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7390), 0.5280898876404494, tensor(0.9014)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2112), 0.6808988764044944, tensor(2.1933)]
[tensor(-1.1801), 0.6808988764044944, tensor(2.1933)]
[tensor(-1.0652), 0.6943820224719102, tensor(2.4067)]
[tensor(-1.0460), 0.7168539325842697, tensor(2.5383)]
[tensor(-1.0460), 0.7168539325842697, tensor(2.5383)]
[tensor(-1.0460), 0.7168539325842697, tensor(2.5383)]
[tensor(-1.0460), 0.7191011235955056, tensor(2.5383)]
[tensor(-1.0460), 0.7191011235955056, tensor(2.5383)]
[tensor(-1.0460), 0.7191011235955056, tensor(2.5383)]
[2023-01-19 11:41:08,605.605 dsw44977-5b9b48888d-729dj:253193 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9289), 0.48314606741573035, tensor(0.4869)]
[tensor(-1.5403), 0.5842696629213483, tensor(1.3810)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1584), 0.6808988764044944, tensor(2.2461)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0896), 0.6898876404494382, tensor(2.3598)]
[tensor(-1.0896), 0.6921348314606741, tensor(2.3598)]
[tensor(-1.0791), 0.6966292134831461, tensor(2.4040)]
[tensor(-1.0791), 0.701123595505618, tensor(2.4265)]
[tensor(-1.0791), 0.7078651685393258, tensor(2.4265)]
[tensor(-1.0791), 0.7078651685393258, tensor(2.4265)]
[tensor(-1.0791), 0.7078651685393258, tensor(2.4265)]
[tensor(-1.0791), 0.7123595505617978, tensor(2.4265)]
[tensor(-1.0791), 0.7123595505617978, tensor(2.4265)]
[tensor(-1.0791), 0.7123595505617978, tensor(2.4265)]
[tensor(-1.0791), 0.7123595505617978, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
[tensor(-1.0791), 0.7146067415730337, tensor(2.4265)]
early stopping at 26
[2023-01-19 11:58:14,641.641 dsw44977-5b9b48888d-729dj:256154 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9453), 0.48089887640449436, tensor(0.4592)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3191), 0.651685393258427, tensor(1.9393)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1386), 0.6808988764044944, tensor(2.2659)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1162), 0.6898876404494382, tensor(2.3333)]
[tensor(-1.1162), 0.6898876404494382, tensor(2.3333)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7146067415730337, tensor(2.4692)]
[tensor(-1.1038), 0.7168539325842697, tensor(2.4692)]
[tensor(-1.1038), 0.7168539325842697, tensor(2.4692)]
[tensor(-1.1038), 0.7168539325842697, tensor(2.4692)]
[tensor(-1.1038), 0.7168539325842697, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
[tensor(-1.1038), 0.7213483146067415, tensor(2.4692)]
early stopping at 32
[2023-01-19 12:19:04,098.098 dsw44977-5b9b48888d-729dj:259758 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3558), 0.3438202247191011, 0.0]
[tensor(-1.5948), 0.5213483146067416, tensor(1.0120)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2644), 0.6382022471910113, tensor(1.9266)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.2451)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1480), 0.698876404494382, tensor(2.3464)]
[tensor(-1.1408), 0.698876404494382, tensor(2.3464)]
[tensor(-1.1398), 0.7123595505617978, tensor(2.4220)]
[tensor(-1.1398), 0.7123595505617978, tensor(2.4220)]
[tensor(-1.1398), 0.7168539325842697, tensor(2.4220)]
[tensor(-1.1398), 0.7168539325842697, tensor(2.4220)]
[2023-01-19 12:25:41,523.523 dsw44977-5b9b48888d-729dj:260923 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0898), 0.4202247191011236, tensor(0.0113)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4010), 0.5887640449438202, tensor(1.5429)]
[tensor(-1.1898), 0.6584269662921348, tensor(2.1023)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1206), 0.7078651685393258, tensor(2.4187)]
[tensor(-1.0784), 0.7123595505617978, tensor(2.4834)]
[tensor(-1.0718), 0.7123595505617978, tensor(2.4834)]
[tensor(-1.0718), 0.7191011235955056, tensor(2.5012)]
[tensor(-1.0718), 0.7191011235955056, tensor(2.5012)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0718), 0.7191011235955056, tensor(2.5012)]
[tensor(-1.0718), 0.7213483146067415, tensor(2.5012)]
[2023-01-19 12:32:18,044.044 dsw44977-5b9b48888d-729dj:262076 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3328), 0.36404494382022473, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8958), 0.46741573033707867, tensor(0.4412)]
[tensor(-1.2556), 0.6359550561797753, tensor(1.9242)]
[tensor(-1.1833), 0.6719101123595506, tensor(2.1763)]
[tensor(-1.1806), 0.6741573033707865, tensor(2.1902)]
[tensor(-1.1136), 0.7033707865168539, tensor(2.4033)]
[tensor(-1.1136), 0.7033707865168539, tensor(2.4033)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1136), 0.7033707865168539, tensor(2.4033)]
[tensor(-1.1136), 0.7078651685393258, tensor(2.4098)]
[tensor(-1.1136), 0.7078651685393258, tensor(2.4098)]
[tensor(-1.1136), 0.7078651685393258, tensor(2.4098)]
[tensor(-1.1136), 0.7123595505617978, tensor(2.4098)]
[tensor(-1.1136), 0.7191011235955056, tensor(2.4098)]
[tensor(-1.1136), 0.7191011235955056, tensor(2.4098)]
[tensor(-1.1136), 0.7191011235955056, tensor(2.4098)]
[tensor(-1.1136), 0.7191011235955056, tensor(2.4098)]
[tensor(-1.1136), 0.7191011235955056, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
[tensor(-1.1136), 0.7213483146067415, tensor(2.4098)]
early stopping at 30
[2023-01-19 12:51:49,721.721 dsw44977-5b9b48888d-729dj:3620 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2014), 0.41797752808988764, 0.0]
[tensor(-1.4192), 0.6247191011235955, tensor(1.7044)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1730), 0.6719101123595506, tensor(2.1865)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1444), 0.6831460674157304, tensor(2.2713)]
[tensor(-1.1444), 0.6831460674157304, tensor(2.2713)]
[tensor(-1.1391), 0.6966292134831461, tensor(2.3441)]
[tensor(-1.1391), 0.6966292134831461, tensor(2.3441)]
[tensor(-1.1391), 0.7033707865168539, tensor(2.3441)]
[tensor(-1.1391), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.1391), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.1391), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.1391), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.1391), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7168539325842697, tensor(2.3813)]
[tensor(-1.1391), 0.7191011235955056, tensor(2.3813)]
[tensor(-1.1391), 0.7191011235955056, tensor(2.3813)]
[tensor(-1.1391), 0.7191011235955056, tensor(2.3813)]
[tensor(-1.1391), 0.7191011235955056, tensor(2.3813)]
[tensor(-1.1391), 0.7191011235955056, tensor(2.3813)]
[tensor(-1.1391), 0.7191011235955056, tensor(2.3813)]
[tensor(-1.1391), 0.7191011235955056, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
[tensor(-1.1391), 0.7213483146067415, tensor(2.3813)]
early stopping at 49
[2023-01-19 13:23:38,463.463 dsw44977-5b9b48888d-729dj:9124 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1140), 0.42696629213483145, tensor(0.0208)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4373), 0.6314606741573033, tensor(1.7200)]
[tensor(-1.1849), 0.6786516853932584, tensor(2.2084)]
[tensor(-1.1399), 0.6808988764044944, tensor(2.2646)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1181), 0.6966292134831461, tensor(2.3651)]
[tensor(-1.1035), 0.7146067415730337, tensor(2.4695)]
[tensor(-1.1035), 0.7146067415730337, tensor(2.4695)]
[tensor(-1.1035), 0.7146067415730337, tensor(2.4695)]
[tensor(-1.1021), 0.7168539325842697, tensor(2.4822)]
[tensor(-1.1021), 0.7168539325842697, tensor(2.4822)]
[2023-01-19 13:30:19,046.046 dsw44977-5b9b48888d-729dj:10295 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8662), 0.4898876404494382, tensor(0.5833)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2974), 0.647191011235955, tensor(1.9386)]
[tensor(-1.1759), 0.6808988764044944, tensor(2.2286)]
[tensor(-1.0567), 0.7078651685393258, tensor(2.4826)]
[tensor(-1.0567), 0.7168539325842697, tensor(2.5164)]
[tensor(-1.0567), 0.7191011235955056, tensor(2.5338)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0567), 0.7213483146067415, tensor(2.5338)]
[tensor(-1.0567), 0.7258426966292135, tensor(2.5338)]
[tensor(-1.0567), 0.7370786516853932, tensor(2.5338)]
[tensor(-1.0567), 0.7370786516853932, tensor(2.5338)]
[2023-01-19 13:36:59,251.251 dsw44977-5b9b48888d-729dj:11461 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0480), 0.48089887640449436, tensor(0.3565)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6758), 0.5393258426966292, tensor(1.0208)]
[tensor(-1.1909), 0.6606741573033708, tensor(2.1125)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1209), 0.6606741573033708, tensor(2.1825)]
[tensor(-1.1209), 0.6808988764044944, tensor(2.2755)]
[tensor(-1.0637), 0.6831460674157304, tensor(2.3520)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0637), 0.6966292134831461, tensor(2.3828)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3918)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
[tensor(-1.0637), 0.7033707865168539, tensor(2.3947)]
early stopping at 19
[2023-01-19 13:49:20,056.056 dsw44977-5b9b48888d-729dj:13615 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0176), 0.449438202247191, tensor(0.2296)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4193), 0.6202247191011236, tensor(1.6818)]
[tensor(-1.1825), 0.651685393258427, tensor(2.0759)]
[tensor(-1.1825), 0.6921348314606741, tensor(2.2730)]
[tensor(-1.1392), 0.7123595505617978, tensor(2.4226)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0923), 0.7123595505617978, tensor(2.4226)]
[tensor(-1.0923), 0.7123595505617978, tensor(2.4226)]
[tensor(-1.0923), 0.7123595505617978, tensor(2.4231)]
[tensor(-1.0923), 0.7213483146067415, tensor(2.4557)]
[tensor(-1.0923), 0.7213483146067415, tensor(2.4557)]
[tensor(-1.0923), 0.7213483146067415, tensor(2.4557)]
[tensor(-1.0923), 0.7213483146067415, tensor(2.4557)]
[tensor(-1.0923), 0.7213483146067415, tensor(2.4557)]
[tensor(-1.0923), 0.7235955056179775, tensor(2.4557)]
[tensor(-1.0923), 0.7280898876404495, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
[tensor(-1.0923), 0.7303370786516854, tensor(2.4557)]
early stopping at 26
[2023-01-19 14:06:15,349.349 dsw44977-5b9b48888d-729dj:16562 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4347), 0.27640449438202247, 0.0]
[tensor(-1.6821), 0.4853932584269663, tensor(0.7449)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2858), 0.6404494382022472, tensor(1.9165)]
[tensor(-1.2099), 0.6584269662921348, tensor(2.0822)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1656), 0.6764044943820224, tensor(2.2164)]
[tensor(-1.1370), 0.6764044943820224, tensor(2.2164)]
[tensor(-1.1370), 0.6921348314606741, tensor(2.3193)]
[tensor(-1.1370), 0.698876404494382, tensor(2.3193)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1370), 0.698876404494382, tensor(2.3193)]
[tensor(-1.1370), 0.698876404494382, tensor(2.3193)]
[2023-01-19 14:12:54,927.927 dsw44977-5b9b48888d-729dj:17728 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0360), 0.4202247191011236, tensor(0.0652)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4148), 0.5640449438202247, tensor(1.4054)]
[tensor(-1.2170), 0.6426966292134831, tensor(1.9965)]
[tensor(-1.1406), 0.6898876404494382, tensor(2.3088)]
[tensor(-1.1406), 0.6898876404494382, tensor(2.3088)]
[tensor(-1.1406), 0.6898876404494382, tensor(2.3088)]
[tensor(-1.1406), 0.6943820224719102, tensor(2.3288)]
[tensor(-1.1406), 0.701123595505618, tensor(2.3288)]
[tensor(-1.1406), 0.7056179775280899, tensor(2.3288)]
[tensor(-1.1406), 0.7056179775280899, tensor(2.3288)]
[2023-01-19 14:19:31,952.952 dsw44977-5b9b48888d-729dj:18887 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3209), 0.3707865168539326, 0.0]
[tensor(-1.8568), 0.47415730337078654, tensor(0.5140)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3043), 0.6337078651685393, tensor(1.8642)]
[tensor(-1.2308), 0.6584269662921348, tensor(2.0614)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2144), 0.6629213483146067, tensor(2.1002)]
[tensor(-1.1436), 0.6898876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.6898876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.6898876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.6898876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.6898876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.6898876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.6898876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.698876404494382, tensor(2.3059)]
[tensor(-1.1436), 0.7078651685393258, tensor(2.3059)]
[tensor(-1.1436), 0.7078651685393258, tensor(2.3059)]
[tensor(-1.1436), 0.7078651685393258, tensor(2.3059)]
[tensor(-1.1436), 0.7078651685393258, tensor(2.3059)]
[tensor(-1.1436), 0.7078651685393258, tensor(2.3059)]
[tensor(-1.1436), 0.7078651685393258, tensor(2.3059)]
[tensor(-1.1436), 0.7123595505617978, tensor(2.3059)]
[tensor(-1.1436), 0.7123595505617978, tensor(2.3059)]
[tensor(-1.1436), 0.7123595505617978, tensor(2.3059)]
[tensor(-1.1436), 0.7123595505617978, tensor(2.3059)]
[tensor(-1.1436), 0.7123595505617978, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7168539325842697, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
[tensor(-1.1436), 0.7235955056179775, tensor(2.3059)]
early stopping at 44
[2023-01-19 14:48:14,958.958 dsw44977-5b9b48888d-729dj:23867 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2287), 0.3685393258426966, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4278), 0.597752808988764, tensor(1.5610)]
[tensor(-1.2002), 0.6696629213483146, tensor(2.1481)]
[tensor(-1.1549), 0.6719101123595506, tensor(2.2046)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1549), 0.6853932584269663, tensor(2.2502)]
[tensor(-1.1549), 0.6853932584269663, tensor(2.2502)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1549), 0.6853932584269663, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.6876404494382022, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
[tensor(-1.1549), 0.7078651685393258, tensor(2.2502)]
early stopping at 28
[2023-01-19 15:06:24,852.852 dsw44977-5b9b48888d-729dj:27018 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1237), 0.43820224719101125, tensor(0.0673)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3961), 0.6359550561797753, tensor(1.7837)]
[tensor(-1.1914), 0.6764044943820224, tensor(2.1907)]
[tensor(-1.1302), 0.6764044943820224, tensor(2.2406)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1025), 0.698876404494382, tensor(2.3919)]
[tensor(-1.0944), 0.7123595505617978, tensor(2.4674)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0944), 0.7123595505617978, tensor(2.4674)]
[tensor(-1.0944), 0.7123595505617978, tensor(2.4674)]
[tensor(-1.0944), 0.7146067415730337, tensor(2.4674)]
[tensor(-1.0944), 0.7146067415730337, tensor(2.4674)]
[2023-01-19 15:12:59,926.926 dsw44977-5b9b48888d-729dj:28171 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.9152), 0.48089887640449436, tensor(0.4893)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3179), 0.6359550561797753, tensor(1.8619)]
[tensor(-1.1904), 0.6786516853932584, tensor(2.2028)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1028), 0.6786516853932584, tensor(2.2905)]
[tensor(-1.0856), 0.7123595505617978, tensor(2.4762)]
[tensor(-1.0856), 0.7146067415730337, tensor(2.4762)]
[tensor(-1.0856), 0.7146067415730337, tensor(2.4762)]
[tensor(-1.0856), 0.7258426966292135, tensor(2.4867)]
[tensor(-1.0856), 0.7280898876404495, tensor(2.4867)]
[tensor(-1.0856), 0.7303370786516854, tensor(2.4867)]
[2023-01-19 15:19:35,798.798 dsw44977-5b9b48888d-729dj:29340 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0312), 0.4966292134831461, tensor(0.4519)]
[tensor(-1.6424), 0.5528089887640449, tensor(1.1216)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2042), 0.6561797752808989, tensor(2.0767)]
[tensor(-1.1340), 0.6786516853932584, tensor(2.2593)]
[tensor(-1.1214), 0.6786516853932584, tensor(2.2593)]
[tensor(-1.0648), 0.701123595505618, tensor(2.4408)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7123595505617978, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
[tensor(-1.0648), 0.7168539325842697, tensor(2.4492)]
early stopping at 31
[2023-01-19 15:40:05,389.389 dsw44977-5b9b48888d-729dj:32949 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0355), 0.45617977528089887, tensor(0.2454)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4391), 0.5932584269662922, tensor(1.5272)]
[tensor(-1.2247), 0.6584269662921348, tensor(2.0674)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1594), 0.6876404494382022, tensor(2.2788)]
[tensor(-1.1157), 0.6943820224719102, tensor(2.3562)]
[tensor(-1.1070), 0.701123595505618, tensor(2.3986)]
[tensor(-1.1070), 0.7101123595505618, tensor(2.4247)]
[tensor(-1.1055), 0.7235955056179775, tensor(2.5125)]
[tensor(-1.1055), 0.7348314606741573, tensor(2.5364)]
[tensor(-1.1055), 0.7348314606741573, tensor(2.5364)]
[tensor(-1.1055), 0.7348314606741573, tensor(2.5364)]
[tensor(-1.1055), 0.7348314606741573, tensor(2.5364)]
[tensor(-1.1055), 0.7348314606741573, tensor(2.5364)]
[tensor(-1.1055), 0.7348314606741573, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
[tensor(-1.1055), 0.7370786516853932, tensor(2.5364)]
early stopping at 30
[2023-01-19 15:59:45,127.127 dsw44977-5b9b48888d-729dj:36369 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8593), 0.12808988764044943, 0.0]
[tensor(-2.8560), 0.12808988764044943, 0.0]
[tensor(-2.8398), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:01:51,584.584 dsw44977-5b9b48888d-729dj:36750 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8635), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8407), 0.12808988764044943, 0.0]
[tensor(-2.8354), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:03:58,044.044 dsw44977-5b9b48888d-729dj:37131 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8369), 0.12808988764044943, 0.0]
[tensor(-2.8369), 0.12808988764044943, 0.0]
[tensor(-2.8369), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:06:04,123.123 dsw44977-5b9b48888d-729dj:37512 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8522), 0.12808988764044943, 0.0]
[tensor(-2.8342), 0.12808988764044943, 0.0]
[tensor(-2.8341), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:08:07,543.543 dsw44977-5b9b48888d-729dj:37887 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8442), 0.5078651685393258, tensor(0.6951)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6868), 0.5685393258426966, tensor(1.1559)]
[tensor(-1.3067), 0.6224719101123596, tensor(1.8056)]
[tensor(-1.2963), 0.6808988764044944, tensor(2.1082)]
[tensor(-1.2570), 0.6853932584269663, tensor(2.1700)]
[tensor(-1.2570), 0.6876404494382022, tensor(2.1700)]
[tensor(-1.2570), 0.6898876404494382, tensor(2.1700)]
[tensor(-1.2570), 0.6898876404494382, tensor(2.1700)]
[tensor(-1.2570), 0.6898876404494382, tensor(2.1700)]
[tensor(-1.2570), 0.6898876404494382, tensor(2.1700)]
[2023-01-19 16:14:50,115.115 dsw44977-5b9b48888d-729dj:39070 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.1831), 0.3146067415730337, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6051), 0.503370786516854, tensor(0.9117)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.4126), 0.5955056179775281, tensor(1.5649)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.3610), 0.6449438202247191, tensor(1.8637)]
[tensor(-1.3610), 0.6696629213483146, tensor(1.9640)]
[tensor(-1.3224), 0.6696629213483146, tensor(2.0147)]
[tensor(-1.3224), 0.6696629213483146, tensor(2.0147)]
[tensor(-1.3224), 0.6719101123595506, tensor(2.0147)]
[tensor(-1.3224), 0.6741573033707865, tensor(2.0147)]
[tensor(-1.3224), 0.6966292134831461, tensor(2.0215)]
[2023-01-19 16:21:24,444.444 dsw44977-5b9b48888d-729dj:40233 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6955), 0.5325842696629214, tensor(0.9674)]
[tensor(-1.5077), 0.5797752808988764, tensor(1.3912)]
[tensor(-1.4041), 0.6269662921348315, tensor(1.7307)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2697), 0.6651685393258427, tensor(2.0561)]
[tensor(-1.2697), 0.6674157303370787, tensor(2.0561)]
[tensor(-1.2697), 0.6674157303370787, tensor(2.0561)]
[tensor(-1.2697), 0.6719101123595506, tensor(2.0561)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2697), 0.6719101123595506, tensor(2.0561)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
[tensor(-1.2697), 0.7078651685393258, tensor(2.1032)]
early stopping at 19
[2023-01-19 16:33:53,033.033 dsw44977-5b9b48888d-729dj:42400 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8495), 0.12808988764044943, 0.0]
[tensor(-2.8335), 0.12808988764044943, 0.0]
[tensor(-2.8335), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:36:00,179.179 dsw44977-5b9b48888d-729dj:42781 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8307), 0.47191011235955055, tensor(0.5288)]
[tensor(-1.3373), 0.6292134831460674, tensor(1.8088)]
[tensor(-1.2837), 0.651685393258427, tensor(1.9748)]
[tensor(-1.2837), 0.6741573033707865, tensor(2.0759)]
[tensor(-1.2837), 0.6741573033707865, tensor(2.0759)]
[tensor(-1.2837), 0.6741573033707865, tensor(2.0759)]
[tensor(-1.2837), 0.6764044943820224, tensor(2.0759)]
[tensor(-1.2837), 0.6943820224719102, tensor(2.0772)]
[tensor(-1.2837), 0.698876404494382, tensor(2.0772)]
[tensor(-1.2837), 0.698876404494382, tensor(2.0772)]
[2023-01-19 16:42:43,021.021 dsw44977-5b9b48888d-729dj:43970 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8598), 0.12808988764044943, 0.0]
[tensor(-2.8479), 0.12808988764044943, 0.0]
[tensor(-2.8360), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:44:50,025.025 dsw44977-5b9b48888d-729dj:44363 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8430), 0.12808988764044943, 0.0]
[tensor(-2.8386), 0.12808988764044943, 0.0]
[tensor(-2.8386), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:46:58,443.443 dsw44977-5b9b48888d-729dj:44756 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8552), 0.12808988764044943, 0.0]
[tensor(-2.8315), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8315), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:49:07,380.380 dsw44977-5b9b48888d-729dj:45143 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9531), 0.5146067415730337, tensor(0.6200)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6973), 0.5528089887640449, tensor(1.0667)]
[tensor(-1.2936), 0.6426966292134831, tensor(1.9198)]
[tensor(-1.2522), 0.6831460674157304, tensor(2.1635)]
[tensor(-1.2522), 0.6831460674157304, tensor(2.1635)]
[tensor(-1.2522), 0.6831460674157304, tensor(2.1635)]
[tensor(-1.2522), 0.6943820224719102, tensor(2.1635)]
[tensor(-1.2522), 0.6943820224719102, tensor(2.1635)]
[tensor(-1.2522), 0.701123595505618, tensor(2.1635)]
[tensor(-1.2522), 0.7078651685393258, tensor(2.1635)]
[2023-01-19 16:55:42,973.973 dsw44977-5b9b48888d-729dj:46298 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8510), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8409), 0.12808988764044943, 0.0]
[tensor(-2.8389), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 16:57:47,852.852 dsw44977-5b9b48888d-729dj:46684 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9061), 0.45393258426966293, tensor(0.3636)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7432), 0.49887640449438203, tensor(0.7512)]
[tensor(-1.5061), 0.5730337078651685, tensor(1.3591)]
[tensor(-1.3665), 0.6292134831460674, tensor(1.7796)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3665), 0.6292134831460674, tensor(1.7796)]
[tensor(-1.2841), 0.6741573033707865, tensor(2.0866)]
[tensor(-1.2841), 0.6741573033707865, tensor(2.0866)]
[tensor(-1.2841), 0.6741573033707865, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
[tensor(-1.2841), 0.6808988764044944, tensor(2.0866)]
early stopping at 19
[2023-01-19 17:10:17,956.956 dsw44977-5b9b48888d-729dj:48858 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8482), 0.12808988764044943, 0.0]
[tensor(-2.8340), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8340), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 17:12:28,494.494 dsw44977-5b9b48888d-729dj:49251 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.3867), 0.31235955056179776, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.8159), 0.501123595505618, tensor(0.6897)]
[tensor(-1.4984), 0.5550561797752809, tensor(1.2768)]
[tensor(-1.4984), 0.5865168539325842, tensor(1.2980)]
[tensor(-1.4984), 0.6269662921348315, tensor(1.6186)]
[tensor(-1.4984), 0.6269662921348315, tensor(1.6186)]
[tensor(-1.4605), 0.6629213483146067, tensor(1.8541)]
[tensor(-1.4605), 0.6696629213483146, tensor(1.8726)]
[tensor(-1.4605), 0.6943820224719102, tensor(2.0039)]
[tensor(-1.4605), 0.6943820224719102, tensor(2.0039)]
[2023-01-19 17:19:06,020.020 dsw44977-5b9b48888d-729dj:50410 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8701), 0.12808988764044943, 0.0]
[tensor(-2.8441), 0.12808988764044943, 0.0]
[tensor(-2.8353), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 17:21:16,362.362 dsw44977-5b9b48888d-729dj:50803 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.6039), 0.17303370786516853, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.2793), 0.31910112359550563, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.8655), 0.45617977528089887, tensor(0.4154)]
[tensor(-1.5551), 0.5415730337078651, tensor(1.1527)]
[tensor(-1.5551), 0.5415730337078651, tensor(1.1527)]
[tensor(-1.4062), 0.6112359550561798, tensor(1.6499)]
[tensor(-1.4062), 0.6112359550561798, tensor(1.6499)]
[tensor(-1.3935), 0.6426966292134831, tensor(1.8199)]
[tensor(-1.3858), 0.6449438202247191, tensor(1.8390)]
[tensor(-1.3858), 0.6449438202247191, tensor(1.8390)]
[tensor(-1.3858), 0.6584269662921348, tensor(1.8854)]
[tensor(-1.3858), 0.6584269662921348, tensor(1.8854)]
[tensor(-1.3858), 0.6584269662921348, tensor(1.8854)]
[tensor(-1.3858), 0.6584269662921348, tensor(1.8854)]
[tensor(-1.3858), 0.6651685393258427, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6853932584269663, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
[tensor(-1.3858), 0.6898876404494382, tensor(1.8854)]
early stopping at 38
[2023-01-19 17:46:19,637.637 dsw44977-5b9b48888d-729dj:55136 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8545), 0.12808988764044943, 0.0]
[tensor(-2.8338), 0.12808988764044943, 0.0]
[tensor(-2.8338), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 17:48:29,418.418 dsw44977-5b9b48888d-729dj:55529 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.9489), 0.13707865168539327, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0390), 0.41123595505617977, tensor(0.0172)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6421), 0.5617977528089888, tensor(1.1669)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5368), 0.6067415730337079, tensor(1.4969)]
[tensor(-1.3268), 0.6202247191011236, tensor(1.7743)]
[tensor(-1.3268), 0.6539325842696629, tensor(1.9048)]
[tensor(-1.3268), 0.6629213483146067, tensor(1.9048)]
[tensor(-1.3268), 0.6741573033707865, tensor(1.9725)]
[tensor(-1.3268), 0.6786516853932584, tensor(1.9725)]
[tensor(-1.3268), 0.6831460674157304, tensor(1.9725)]
[2023-01-19 17:55:05,819.819 dsw44977-5b9b48888d-729dj:56683 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-2.1980), 0.40224719101123596, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-2.0111), 0.40898876404494383, tensor(0.0338)]
[tensor(-1.7466), 0.46741573033707867, tensor(0.5905)]
[tensor(-1.5156), 0.5528089887640449, tensor(1.2484)]
[tensor(-1.4702), 0.6, tensor(1.5298)]
[tensor(-1.4702), 0.6, tensor(1.5298)]
[tensor(-1.4702), 0.6, tensor(1.5298)]
[tensor(-1.4702), 0.6, tensor(1.5298)]
[tensor(-1.4702), 0.647191011235955, tensor(1.6965)]
[tensor(-1.4702), 0.647191011235955, tensor(1.6965)]
[2023-01-19 18:01:44,354.354 dsw44977-5b9b48888d-729dj:57848 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8383), 0.12808988764044943, 0.0]
[tensor(-2.8383), 0.12808988764044943, 0.0]
[tensor(-2.8383), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:03:51,495.495 dsw44977-5b9b48888d-729dj:58235 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8506), 0.12808988764044943, 0.0]
[tensor(-2.8333), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8333), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:06:00,215.215 dsw44977-5b9b48888d-729dj:58622 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7563), 0.4606741573033708, tensor(0.5471)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3920), 0.5573033707865168, tensor(1.3945)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3920), 0.6269662921348315, tensor(1.7363)]
[tensor(-1.3911), 0.6561797752808989, tensor(1.8898)]
[tensor(-1.3911), 0.6786516853932584, tensor(2.0020)]
[tensor(-1.3911), 0.6786516853932584, tensor(2.0020)]
[tensor(-1.3911), 0.6853932584269663, tensor(2.0020)]
[tensor(-1.3911), 0.7101123595505618, tensor(2.0868)]
[tensor(-1.3911), 0.7101123595505618, tensor(2.0868)]
[tensor(-1.3911), 0.7101123595505618, tensor(2.0868)]
[2023-01-19 18:12:39,298.298 dsw44977-5b9b48888d-729dj:59788 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8566), 0.12808988764044943, 0.0]
[tensor(-2.8461), 0.12808988764044943, 0.0]
[tensor(-2.8397), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:14:45,180.180 dsw44977-5b9b48888d-729dj:60169 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8398), 0.12808988764044943, 0.0]
[tensor(-2.8398), 0.12808988764044943, 0.0]
[tensor(-2.8398), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:16:52,219.219 dsw44977-5b9b48888d-729dj:60550 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8619), 0.12808988764044943, 0.0]
[tensor(-2.8332), 0.12808988764044943, 0.0]
[tensor(-2.8332), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:18:59,680.680 dsw44977-5b9b48888d-729dj:60937 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8502), 0.21348314606741572, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8260), 0.449438202247191, tensor(0.4212)]
[tensor(-1.8260), 0.4696629213483146, tensor(0.5067)]
[tensor(-1.4769), 0.5640449438202247, tensor(1.3433)]
[tensor(-1.4769), 0.5707865168539326, tensor(1.3651)]
[tensor(-1.4235), 0.6269662921348315, tensor(1.7113)]
[tensor(-1.4235), 0.6269662921348315, tensor(1.7113)]
[tensor(-1.4235), 0.6494382022471911, tensor(1.7411)]
[tensor(-1.4235), 0.6606741573033708, tensor(1.7598)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4235), 0.6606741573033708, tensor(1.7598)]
[2023-01-19 18:25:39,440.440 dsw44977-5b9b48888d-729dj:62102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8539), 0.12808988764044943, 0.0]
[tensor(-2.8411), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8356), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:27:46,031.031 dsw44977-5b9b48888d-729dj:62484 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8342), 0.12808988764044943, 0.0]
[tensor(-2.8342), 0.12808988764044943, 0.0]
[tensor(-2.8342), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:29:53,599.599 dsw44977-5b9b48888d-729dj:62871 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8532), 0.12808988764044943, 0.0]
[tensor(-2.8345), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8345), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:32:02,043.043 dsw44977-5b9b48888d-729dj:63258 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0990), 0.34831460674157305, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.9838), 0.4337078651685393, tensor(0.1848)]
[tensor(-1.6093), 0.5460674157303371, tensor(1.1211)]
[tensor(-1.6093), 0.5730337078651685, tensor(1.2327)]
[tensor(-1.5084), 0.6247191011235955, tensor(1.6152)]
[tensor(-1.4524), 0.6426966292134831, tensor(1.7611)]
[tensor(-1.3757), 0.6651685393258427, tensor(1.9502)]
[tensor(-1.3757), 0.6651685393258427, tensor(1.9502)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3757), 0.6651685393258427, tensor(1.9502)]
[tensor(-1.3757), 0.6651685393258427, tensor(1.9502)]
[2023-01-19 18:38:38,995.995 dsw44977-5b9b48888d-729dj:64417 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8591), 0.12808988764044943, 0.0]
[tensor(-2.8410), 0.12808988764044943, 0.0]
[tensor(-2.8330), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:40:43,900.900 dsw44977-5b9b48888d-729dj:64792 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8359), 0.12808988764044943, 0.0]
[tensor(-2.8359), 0.12808988764044943, 0.0]
[tensor(-2.8359), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:42:50,921.921 dsw44977-5b9b48888d-729dj:65179 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8498), 0.12808988764044943, 0.0]
[tensor(-2.8317), 0.12808988764044943, 0.0]
[tensor(-2.8317), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:44:57,332.332 dsw44977-5b9b48888d-729dj:65560 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0719), 0.4314606741573034, tensor(0.0854)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4542), 0.5842696629213483, tensor(1.4672)]
[tensor(-1.2380), 0.6606741573033708, tensor(2.0654)]
[tensor(-1.2380), 0.6764044943820224, tensor(2.1162)]
[tensor(-1.2380), 0.6764044943820224, tensor(2.1162)]
[tensor(-1.2380), 0.6764044943820224, tensor(2.1162)]
[tensor(-1.2380), 0.6764044943820224, tensor(2.1162)]
[tensor(-1.2380), 0.6943820224719102, tensor(2.1257)]
[tensor(-1.2380), 0.6966292134831461, tensor(2.1257)]
[tensor(-1.2380), 0.698876404494382, tensor(2.1257)]
[2023-01-19 18:51:34,797.797 dsw44977-5b9b48888d-729dj:66720 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8557), 0.12808988764044943, 0.0]
[tensor(-2.8409), 0.12808988764044943, 0.0]
[tensor(-2.8408), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 18:53:40,232.232 dsw44977-5b9b48888d-729dj:67101 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.5517), 0.5640449438202247, tensor(1.2685)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5517), 0.5640449438202247, tensor(1.2685)]
[tensor(-1.4056), 0.6269662921348315, tensor(1.7292)]
[tensor(-1.2749), 0.6494382022471911, tensor(1.9723)]
[tensor(-1.2749), 0.6494382022471911, tensor(1.9723)]
[tensor(-1.2749), 0.6494382022471911, tensor(1.9723)]
[tensor(-1.2749), 0.6494382022471911, tensor(1.9723)]
[tensor(-1.2749), 0.6494382022471911, tensor(1.9723)]
[tensor(-1.2749), 0.6606741573033708, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
[tensor(-1.2749), 0.6741573033707865, tensor(1.9723)]
early stopping at 20
[2023-01-19 19:06:52,776.776 dsw44977-5b9b48888d-729dj:69394 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8509), 0.12808988764044943, 0.0]
[tensor(-2.8330), 0.12808988764044943, 0.0]
[tensor(-2.8330), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 19:08:59,353.353 dsw44977-5b9b48888d-729dj:69775 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0736), 0.3842696629213483, 0.0]
[tensor(-1.6461), 0.5348314606741573, tensor(1.0281)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3935), 0.6337078651685393, tensor(1.7750)]
[tensor(-1.3935), 0.6359550561797753, tensor(1.7750)]
[tensor(-1.3735), 0.6853932584269663, tensor(2.0535)]
[tensor(-1.3735), 0.6853932584269663, tensor(2.0535)]
[tensor(-1.3735), 0.6853932584269663, tensor(2.0535)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3735), 0.6853932584269663, tensor(2.0535)]
[tensor(-1.3735), 0.6876404494382022, tensor(2.0535)]
[tensor(-1.3735), 0.6876404494382022, tensor(2.0535)]
[2023-01-19 19:15:43,069.069 dsw44977-5b9b48888d-729dj:70953 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8642), 0.12808988764044943, 0.0]
[tensor(-2.8410), 0.12808988764044943, 0.0]
[tensor(-2.8335), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 19:17:51,194.194 dsw44977-5b9b48888d-729dj:71340 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8403), 0.12808988764044943, 0.0]
[tensor(-2.8399), 0.12808988764044943, 0.0]
[tensor(-2.8352), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 19:20:00,405.405 dsw44977-5b9b48888d-729dj:71730 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8456), 0.12808988764044943, 0.0]
[tensor(-2.8315), 0.12808988764044943, 0.0]
[tensor(-2.8315), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 19:22:06,527.527 dsw44977-5b9b48888d-729dj:72113 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2000), 0.38202247191011235, 0.0]
[tensor(-1.4087), 0.6022471910112359, tensor(1.6025)]
[tensor(-1.3888), 0.6337078651685393, tensor(1.7798)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2650), 0.6606741573033708, tensor(2.0384)]
[tensor(-1.2650), 0.6764044943820224, tensor(2.0672)]
[tensor(-1.2650), 0.6764044943820224, tensor(2.0672)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2650), 0.6808988764044944, tensor(2.0672)]
[tensor(-1.2650), 0.6808988764044944, tensor(2.0672)]
[tensor(-1.2650), 0.6831460674157304, tensor(2.0672)]
[tensor(-1.2650), 0.6876404494382022, tensor(2.0672)]
[2023-01-19 19:28:39,952.952 dsw44977-5b9b48888d-729dj:73262 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.0371), 0.4202247191011236, tensor(0.0640)]
[tensor(-1.6609), 0.5168539325842697, tensor(0.9234)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4039), 0.6179775280898876, tensor(1.6860)]
[tensor(-1.3485), 0.6337078651685393, tensor(1.8200)]
[tensor(-1.3485), 0.6337078651685393, tensor(1.8200)]
[tensor(-1.3485), 0.6337078651685393, tensor(1.8200)]
[tensor(-1.3485), 0.6561797752808989, tensor(1.8200)]
[tensor(-1.3485), 0.6606741573033708, tensor(1.8608)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.3485), 0.6606741573033708, tensor(1.8608)]
[tensor(-1.3485), 0.6606741573033708, tensor(1.8608)]
[2023-01-19 19:35:18,549.549 dsw44977-5b9b48888d-729dj:74422 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5276), 0.5393258426966292, tensor(1.1691)]
[tensor(-1.3697), 0.5842696629213483, tensor(1.5517)]
[tensor(-1.3622), 0.6314606741573033, tensor(1.7951)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2345), 0.6719101123595506, tensor(2.1251)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2345), 0.6719101123595506, tensor(2.1251)]
[tensor(-1.2345), 0.6719101123595506, tensor(2.1251)]
[tensor(-1.2345), 0.6719101123595506, tensor(2.1251)]
[tensor(-1.2345), 0.6853932584269663, tensor(2.1251)]
[tensor(-1.2345), 0.6853932584269663, tensor(2.1251)]
[tensor(-1.2345), 0.6853932584269663, tensor(2.1251)]
[tensor(-1.2345), 0.6853932584269663, tensor(2.1251)]
[tensor(-1.2345), 0.6853932584269663, tensor(2.1251)]
[tensor(-1.2345), 0.6853932584269663, tensor(2.1251)]
[tensor(-1.2345), 0.6853932584269663, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
[tensor(-1.2345), 0.6898876404494382, tensor(2.1251)]
early stopping at 32
[2023-01-19 19:56:18,968.968 dsw44977-5b9b48888d-729dj:78061 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8485), 0.12808988764044943, 0.0]
[tensor(-2.8337), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8337), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 19:58:25,193.193 dsw44977-5b9b48888d-729dj:78442 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0216), 0.4067415730337079, tensor(0.0121)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7315), 0.5056179775280899, tensor(0.7966)]
[tensor(-1.5662), 0.5707865168539326, tensor(1.2878)]
[tensor(-1.5033), 0.6089887640449438, tensor(1.5416)]
[tensor(-1.5033), 0.6179775280898876, tensor(1.5416)]
[tensor(-1.5033), 0.6359550561797753, tensor(1.6392)]
[tensor(-1.5033), 0.6674157303370787, tensor(1.8062)]
[tensor(-1.5033), 0.6674157303370787, tensor(1.8062)]
[tensor(-1.5033), 0.6674157303370787, tensor(1.8062)]
[tensor(-1.5033), 0.6741573033707865, tensor(1.8062)]
[2023-01-19 20:05:00,871.871 dsw44977-5b9b48888d-729dj:79602 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8366), 0.12808988764044943, 0.0]
[tensor(-2.8366), 0.12808988764044943, 0.0]
[tensor(-2.8350), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 20:07:08,279.279 dsw44977-5b9b48888d-729dj:79983 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5882), 0.21123595505617979, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4598), 0.251685393258427, 0.0]
[tensor(-2.4598), 0.251685393258427, 0.0]
early stopping at 3
[2023-01-19 20:09:13,993.993 dsw44977-5b9b48888d-729dj:80364 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8452), 0.12808988764044943, 0.0]
[tensor(-2.8335), 0.12808988764044943, 0.0]
[tensor(-2.8333), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 20:11:19,963.963 dsw44977-5b9b48888d-729dj:80745 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7505), 0.5483146067415731, tensor(0.9911)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4515), 0.6089887640449438, tensor(1.5934)]
[tensor(-1.4515), 0.6089887640449438, tensor(1.5934)]
[tensor(-1.2757), 0.647191011235955, tensor(1.9603)]
[tensor(-1.2757), 0.6719101123595506, tensor(2.0212)]
[tensor(-1.2757), 0.701123595505618, tensor(2.1596)]
[tensor(-1.2757), 0.701123595505618, tensor(2.1596)]
[tensor(-1.2757), 0.701123595505618, tensor(2.1596)]
[tensor(-1.2757), 0.701123595505618, tensor(2.1596)]
[tensor(-1.2757), 0.701123595505618, tensor(2.1596)]
[2023-01-19 20:17:54,572.572 dsw44977-5b9b48888d-729dj:81898 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7049), 0.48089887640449436, tensor(0.6996)]
[tensor(-1.4520), 0.5910112359550562, tensor(1.5031)]
[tensor(-1.2742), 0.6404494382022472, tensor(1.9281)]
[tensor(-1.2742), 0.6629213483146067, tensor(2.0068)]
[tensor(-1.2742), 0.6808988764044944, tensor(2.1206)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2742), 0.7033707865168539, tensor(2.1206)]
[tensor(-1.2742), 0.7033707865168539, tensor(2.1206)]
[tensor(-1.2742), 0.7033707865168539, tensor(2.1206)]
[tensor(-1.2742), 0.7235955056179775, tensor(2.2488)]
[tensor(-1.2742), 0.7235955056179775, tensor(2.2488)]
[2023-01-19 20:24:32,716.716 dsw44977-5b9b48888d-729dj:83058 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7396), 0.5123595505617977, tensor(0.8222)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4865), 0.5752808988764045, tensor(1.3899)]
[tensor(-1.4244), 0.6089887640449438, tensor(1.6205)]
[tensor(-1.2938), 0.647191011235955, tensor(1.9422)]
[tensor(-1.2938), 0.647191011235955, tensor(1.9422)]
[tensor(-1.2938), 0.6696629213483146, tensor(2.0099)]
[tensor(-1.2938), 0.6696629213483146, tensor(2.0099)]
[tensor(-1.2938), 0.6696629213483146, tensor(2.0099)]
[tensor(-1.2938), 0.6741573033707865, tensor(2.0099)]
[tensor(-1.2938), 0.6741573033707865, tensor(2.0099)]
[tensor(-1.2938), 0.6741573033707865, tensor(2.0099)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2938), 0.6741573033707865, tensor(2.0099)]
[tensor(-1.2938), 0.6741573033707865, tensor(2.0099)]
[tensor(-1.2938), 0.6741573033707865, tensor(2.0099)]
[tensor(-1.2938), 0.6831460674157304, tensor(2.0099)]
[tensor(-1.2938), 0.6831460674157304, tensor(2.0099)]
[tensor(-1.2938), 0.6831460674157304, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
[tensor(-1.2938), 0.6876404494382022, tensor(2.0099)]
early stopping at 28
[2023-01-19 20:42:50,360.360 dsw44977-5b9b48888d-729dj:86232 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8506), 0.12808988764044943, 0.0]
[tensor(-2.8333), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8333), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 20:44:58,248.248 dsw44977-5b9b48888d-729dj:86619 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1911), 0.3550561797752809, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8601), 0.4337078651685393, tensor(0.3085)]
[tensor(-1.6410), 0.5325842696629214, tensor(1.0220)]
[tensor(-1.6139), 0.5707865168539326, tensor(1.2401)]
[tensor(-1.6139), 0.6067415730337079, tensor(1.4151)]
[tensor(-1.6139), 0.6067415730337079, tensor(1.4151)]
[tensor(-1.6139), 0.6067415730337079, tensor(1.4151)]
[tensor(-1.4634), 0.6606741573033708, tensor(1.8400)]
[tensor(-1.4634), 0.6606741573033708, tensor(1.8400)]
[tensor(-1.4634), 0.6606741573033708, tensor(1.8400)]
[2023-01-19 20:51:37,189.189 dsw44977-5b9b48888d-729dj:87779 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8109), 0.13707865168539327, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0460), 0.4337078651685393, tensor(0.1225)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5964), 0.5460674157303371, tensor(1.1339)]
[tensor(-1.3724), 0.6112359550561798, tensor(1.6837)]
[tensor(-1.3724), 0.6224719101123596, tensor(1.6837)]
[tensor(-1.3724), 0.6269662921348315, tensor(1.7019)]
[tensor(-1.3724), 0.6269662921348315, tensor(1.7019)]
[tensor(-1.3724), 0.6606741573033708, tensor(1.8925)]
[tensor(-1.3724), 0.6606741573033708, tensor(1.8925)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.3724), 0.6719101123595506, tensor(1.8925)]
[2023-01-19 20:58:11,967.967 dsw44977-5b9b48888d-729dj:88932 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8385), 0.12808988764044943, 0.0]
[tensor(-2.8385), 0.12808988764044943, 0.0]
[tensor(-2.8370), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 21:00:20,485.485 dsw44977-5b9b48888d-729dj:89325 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8434), 0.12808988764044943, 0.0]
[tensor(-2.8335), 0.12808988764044943, 0.0]
[tensor(-2.8335), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 21:02:26,431.431 dsw44977-5b9b48888d-729dj:89706 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2460), 0.41797752808988764, 0.0]
[tensor(-1.6972), 0.5370786516853933, tensor(0.9882)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4136), 0.5842696629213483, tensor(1.5077)]
[tensor(-1.2980), 0.651685393258427, tensor(1.9604)]
[tensor(-1.2482), 0.6808988764044944, tensor(2.1563)]
[tensor(-1.2482), 0.6831460674157304, tensor(2.1563)]
[tensor(-1.2482), 0.6831460674157304, tensor(2.1563)]
[tensor(-1.2482), 0.6831460674157304, tensor(2.1563)]
[tensor(-1.2482), 0.6898876404494382, tensor(2.1563)]
[tensor(-1.2482), 0.6898876404494382, tensor(2.1563)]
[2023-01-19 21:09:04,081.081 dsw44977-5b9b48888d-729dj:90866 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.9696), 0.4449438202247191, tensor(0.2551)]
[tensor(-1.6277), 0.5483146067415731, tensor(1.1139)]
[tensor(-1.4426), 0.604494382022472, tensor(1.5799)]
[tensor(-1.4257), 0.6314606741573033, tensor(1.7316)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4038), 0.6539325842696629, tensor(1.8658)]
[tensor(-1.4038), 0.6606741573033708, tensor(1.8658)]
[tensor(-1.4038), 0.6606741573033708, tensor(1.8721)]
[tensor(-1.4038), 0.6696629213483146, tensor(1.9188)]
[tensor(-1.4038), 0.6853932584269663, tensor(1.9303)]
[tensor(-1.4038), 0.6853932584269663, tensor(1.9303)]
[2023-01-19 21:15:47,563.563 dsw44977-5b9b48888d-729dj:92045 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6596), 0.5168539325842697, tensor(0.9247)]
[tensor(-1.6159), 0.5370786516853933, tensor(1.0695)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3363), 0.6404494382022472, tensor(1.8660)]
[tensor(-1.2385), 0.6674157303370787, tensor(2.0986)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.2385), 0.6674157303370787, tensor(2.0986)]
[tensor(-1.2385), 0.6674157303370787, tensor(2.0986)]
[tensor(-1.2385), 0.6674157303370787, tensor(2.0986)]
[tensor(-1.2385), 0.6741573033707865, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.6876404494382022, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.701123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7056179775280899, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
[tensor(-1.2385), 0.7101123595505618, tensor(2.0986)]
early stopping at 43
[2023-01-19 21:43:49,048.048 dsw44977-5b9b48888d-729dj:96892 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.2-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8516), 0.12808988764044943, 0.0]
[tensor(-2.8342), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8342), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 21:45:55,009.009 dsw44977-5b9b48888d-729dj:97273 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.7841), 0.44269662921348313, tensor(0.4294)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4639), 0.6179775280898876, tensor(1.6260)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3345), 0.6494382022471911, tensor(1.9127)]
[tensor(-1.3238), 0.6494382022471911, tensor(1.9234)]
[tensor(-1.2922), 0.6808988764044944, tensor(2.1123)]
[tensor(-1.2922), 0.6808988764044944, tensor(2.1123)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2922), 0.6876404494382022, tensor(2.1123)]
[tensor(-1.2922), 0.6876404494382022, tensor(2.1123)]
[tensor(-1.2922), 0.6876404494382022, tensor(2.1123)]
[tensor(-1.2922), 0.7078651685393258, tensor(2.1123)]
[2023-01-19 21:52:36,810.810 dsw44977-5b9b48888d-729dj:98445 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8613), 0.12808988764044943, 0.0]
[tensor(-2.8424), 0.12808988764044943, 0.0]
[tensor(-2.8351), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 21:54:45,518.518 dsw44977-5b9b48888d-729dj:98832 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8393), 0.12808988764044943, 0.0]
[tensor(-2.8389), 0.12808988764044943, 0.0]
[tensor(-2.8389), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 21:56:53,036.036 dsw44977-5b9b48888d-729dj:99219 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8468), 0.12808988764044943, 0.0]
[tensor(-2.8333), 0.12808988764044943, 0.0]
[tensor(-2.8333), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 21:58:57,081.081 dsw44977-5b9b48888d-729dj:99594 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0242), 0.4314606741573034, tensor(0.1331)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3941), 0.6, tensor(1.6059)]
[tensor(-1.3425), 0.6426966292134831, tensor(1.8710)]
[tensor(-1.2993), 0.6426966292134831, tensor(1.9141)]
[tensor(-1.2120), 0.7056179775280899, tensor(2.3161)]
[tensor(-1.2120), 0.7056179775280899, tensor(2.3161)]
[tensor(-1.2120), 0.7056179775280899, tensor(2.3161)]
[tensor(-1.2120), 0.7056179775280899, tensor(2.3161)]
[tensor(-1.2120), 0.7056179775280899, tensor(2.3161)]
[tensor(-1.2120), 0.7056179775280899, tensor(2.3161)]
early stopping at 10
[2023-01-19 22:05:35,507.507 dsw44977-5b9b48888d-729dj:100754 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8300), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.8300), 0.12808988764044943, 0.0]
[tensor(-2.8300), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 22:07:42,450.450 dsw44977-5b9b48888d-729dj:101140 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6492), 0.5168539325842697, tensor(0.9351)]
[tensor(-1.4179), 0.604494382022472, tensor(1.6045)]
[tensor(-1.3876), 0.604494382022472, tensor(1.6124)]
[tensor(-1.2233), 0.6786516853932584, tensor(2.1700)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2233), 0.6786516853932584, tensor(2.1700)]
[tensor(-1.2233), 0.6808988764044944, tensor(2.1700)]
[tensor(-1.2233), 0.6808988764044944, tensor(2.1700)]
[tensor(-1.2233), 0.6808988764044944, tensor(2.1700)]
[tensor(-1.2233), 0.6876404494382022, tensor(2.1700)]
[tensor(-1.2233), 0.6876404494382022, tensor(2.1700)]
[tensor(-1.2233), 0.6943820224719102, tensor(2.1700)]
[tensor(-1.2233), 0.6943820224719102, tensor(2.1700)]
[tensor(-1.2233), 0.6966292134831461, tensor(2.1700)]
[tensor(-1.2233), 0.701123595505618, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
[tensor(-1.2233), 0.7123595505617978, tensor(2.1700)]
early stopping at 25
[2023-01-19 22:24:02,640.640 dsw44977-5b9b48888d-729dj:103971 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-25 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8499), 0.12808988764044943, 0.0]
[tensor(-2.8330), 0.12808988764044943, 0.0]
[tensor(-2.8330), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 22:26:09,771.771 dsw44977-5b9b48888d-729dj:104358 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.7853), 0.45393258426966293, tensor(0.4844)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4722), 0.5685393258426966, tensor(1.3705)]
[tensor(-1.4722), 0.6, tensor(1.3705)]
[tensor(-1.4044), 0.6449438202247191, tensor(1.8204)]
[tensor(-1.4044), 0.6629213483146067, tensor(1.8343)]
[tensor(-1.3294), 0.6853932584269663, tensor(2.0976)]
[tensor(-1.3294), 0.6853932584269663, tensor(2.0976)]
[tensor(-1.3294), 0.701123595505618, tensor(2.0976)]
[tensor(-1.3294), 0.7056179775280899, tensor(2.0985)]
[tensor(-1.3294), 0.7056179775280899, tensor(2.0985)]
[2023-01-19 22:32:48,101.101 dsw44977-5b9b48888d-729dj:105518 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8585), 0.12808988764044943, 0.0]
[tensor(-2.8428), 0.12808988764044943, 0.0]
[tensor(-2.8352), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 22:34:58,230.230 dsw44977-5b9b48888d-729dj:105911 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.3228), 0.3595505617977528, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.3228), 0.3595505617977528, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-2.3228), 0.3595505617977528, 0.0]
early stopping at 3
[2023-01-19 22:37:04,042.042 dsw44977-5b9b48888d-729dj:106292 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8485), 0.12808988764044943, 0.0]
[tensor(-2.8340), 0.12808988764044943, 0.0]
[tensor(-2.8338), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 22:39:12,166.166 dsw44977-5b9b48888d-729dj:106679 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9893), 0.3842696629213483, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3079), 0.6179775280898876, tensor(1.7820)]
[tensor(-1.3079), 0.6314606741573033, tensor(1.8057)]
[tensor(-1.2344), 0.6921348314606741, tensor(2.2263)]
[tensor(-1.2344), 0.701123595505618, tensor(2.2290)]
[tensor(-1.2344), 0.7078651685393258, tensor(2.2290)]
[tensor(-1.2344), 0.7078651685393258, tensor(2.2290)]
[tensor(-1.2344), 0.7078651685393258, tensor(2.2290)]
[tensor(-1.2344), 0.7078651685393258, tensor(2.2290)]
[tensor(-1.2344), 0.7078651685393258, tensor(2.2290)]
[2023-01-19 22:45:51,635.635 dsw44977-5b9b48888d-729dj:107844 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0031), 0.4, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5644), 0.5415730337078651, tensor(1.1434)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.2999), 0.6269662921348315, tensor(1.8349)]
[tensor(-1.2410), 0.6561797752808989, tensor(2.0399)]
[tensor(-1.2410), 0.6764044943820224, tensor(2.1285)]
[tensor(-1.2410), 0.6764044943820224, tensor(2.1285)]
[tensor(-1.2410), 0.6898876404494382, tensor(2.1285)]
[tensor(-1.2410), 0.6898876404494382, tensor(2.1285)]
[tensor(-1.2410), 0.6898876404494382, tensor(2.1285)]
[tensor(-1.2410), 0.6898876404494382, tensor(2.1285)]
[2023-01-19 22:52:27,726.726 dsw44977-5b9b48888d-729dj:109004 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5454), 0.5730337078651685, tensor(1.3198)]
[tensor(-1.3849), 0.6426966292134831, tensor(1.8286)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3849), 0.6426966292134831, tensor(1.8286)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6719101123595506, tensor(2.0446)]
[tensor(-1.3149), 0.6966292134831461, tensor(2.0446)]
[tensor(-1.3149), 0.698876404494382, tensor(2.0446)]
[tensor(-1.3149), 0.698876404494382, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
[tensor(-1.3149), 0.7056179775280899, tensor(2.0446)]
early stopping at 29
[2023-01-19 23:11:27,547.547 dsw44977-5b9b48888d-729dj:112293 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-50 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8484), 0.12808988764044943, 0.0]
[tensor(-2.8328), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8328), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 23:13:33,803.803 dsw44977-5b9b48888d-729dj:112674 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.7600), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.7600), 0.12808988764044943, 0.0]
[tensor(-2.7600), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 23:15:42,313.313 dsw44977-5b9b48888d-729dj:113061 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8671), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-2.7293), 0.15730337078651685, 0.0]
[tensor(-2.7293), 0.15730337078651685, 0.0]
early stopping at 3
[2023-01-19 23:17:50,948.948 dsw44977-5b9b48888d-729dj:113448 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8271), 0.12808988764044943, 0.0]
[tensor(-2.8271), 0.12808988764044943, 0.0]
[tensor(-2.8271), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 23:19:58,800.800 dsw44977-5b9b48888d-729dj:113836 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8619), 0.12808988764044943, 0.0]
[tensor(-2.8341), 0.12808988764044943, 0.0]
[tensor(-2.8341), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 23:22:03,671.671 dsw44977-5b9b48888d-729dj:114217 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0227), 0.45617977528089887, tensor(0.2582)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3812), 0.6, tensor(1.6188)]
[tensor(-1.3044), 0.6584269662921348, tensor(1.9878)]
[tensor(-1.3044), 0.6606741573033708, tensor(1.9965)]
[tensor(-1.2557), 0.6719101123595506, tensor(2.1039)]
[tensor(-1.2557), 0.6898876404494382, tensor(2.1039)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2557), 0.6898876404494382, tensor(2.1039)]
[tensor(-1.2557), 0.6898876404494382, tensor(2.1039)]
[tensor(-1.2557), 0.6898876404494382, tensor(2.1039)]
[tensor(-1.2557), 0.6898876404494382, tensor(2.1039)]
[2023-01-19 23:28:43,104.104 dsw44977-5b9b48888d-729dj:115382 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.9674), 0.46292134831460674, tensor(0.3472)]
[tensor(-1.5581), 0.5213483146067416, tensor(1.0486)]
[tensor(-1.3480), 0.6067415730337079, tensor(1.6857)]
[tensor(-1.2553), 0.6606741573033708, tensor(2.0480)]
[tensor(-1.2553), 0.6966292134831461, tensor(2.2204)]
[tensor(-1.2553), 0.6966292134831461, tensor(2.2204)]
[tensor(-1.2553), 0.6966292134831461, tensor(2.2204)]
[tensor(-1.2553), 0.6966292134831461, tensor(2.2204)]
[tensor(-1.2553), 0.6966292134831461, tensor(2.2204)]
[tensor(-1.2553), 0.6966292134831461, tensor(2.2204)]
early stopping at 10
[2023-01-19 23:35:20,564.564 dsw44977-5b9b48888d-729dj:116541 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0911), 0.3797752808988764, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9105), 0.4404494382022472, tensor(0.2918)]
[tensor(-1.6956), 0.5280898876404494, tensor(0.9448)]
[tensor(-1.5331), 0.5573033707865168, tensor(1.2534)]
[tensor(-1.5331), 0.5573033707865168, tensor(1.2534)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5331), 0.5752808988764045, tensor(1.3225)]
[tensor(-1.5331), 0.5752808988764045, tensor(1.3225)]
[tensor(-1.5331), 0.6134831460674157, tensor(1.5169)]
[tensor(-1.5331), 0.6202247191011236, tensor(1.5169)]
[tensor(-1.5331), 0.6202247191011236, tensor(1.5169)]
[tensor(-1.5331), 0.6404494382022472, tensor(1.5169)]
[tensor(-1.5331), 0.6404494382022472, tensor(1.5169)]
[tensor(-1.5331), 0.6404494382022472, tensor(1.5169)]
[tensor(-1.5331), 0.6404494382022472, tensor(1.5169)]
[tensor(-1.5331), 0.6404494382022472, tensor(1.5169)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5331), 0.6404494382022472, tensor(1.5169)]
[tensor(-1.5331), 0.6404494382022472, tensor(1.5169)]
[tensor(-1.5331), 0.6539325842696629, tensor(1.5169)]
[tensor(-1.5331), 0.6539325842696629, tensor(1.5169)]
[tensor(-1.5331), 0.6539325842696629, tensor(1.5169)]
[tensor(-1.5331), 0.6539325842696629, tensor(1.5169)]
[tensor(-1.5331), 0.6539325842696629, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
[tensor(-1.5331), 0.6629213483146067, tensor(1.5169)]
early stopping at 33
[2023-01-19 23:56:56,544.544 dsw44977-5b9b48888d-729dj:120283 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-75 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8495), 0.12808988764044943, 0.0]
[tensor(-2.8333), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8333), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-19 23:59:03,251.251 dsw44977-5b9b48888d-729dj:120664 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1833), 0.3842696629213483, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7594), 0.4449438202247191, tensor(0.4654)]
[tensor(-1.7097), 0.5573033707865168, tensor(1.0768)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5559), 0.5775280898876405, tensor(1.3318)]
[tensor(-1.5098), 0.6022471910112359, tensor(1.5015)]
[tensor(-1.5098), 0.604494382022472, tensor(1.5015)]
[tensor(-1.4557), 0.6359550561797753, tensor(1.7241)]
[tensor(-1.4557), 0.647191011235955, tensor(1.7241)]
[tensor(-1.4557), 0.651685393258427, tensor(1.7241)]
[tensor(-1.4557), 0.6606741573033708, tensor(1.7241)]
[2023-01-20 00:05:46,728.728 dsw44977-5b9b48888d-729dj:121842 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8593), 0.12808988764044943, 0.0]
[tensor(-2.8423), 0.12808988764044943, 0.0]
[tensor(-2.8344), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-20 00:07:54,642.642 dsw44977-5b9b48888d-729dj:122229 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8346), 0.12808988764044943, 0.0]
[tensor(-2.8346), 0.12808988764044943, 0.0]
[tensor(-2.8346), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-20 00:10:03,738.738 dsw44977-5b9b48888d-729dj:122616 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8544), 0.12808988764044943, 0.0]
[tensor(-2.8336), 0.12808988764044943, 0.0]
[tensor(-2.8336), 0.12808988764044943, 0.0]
early stopping at 3
[2023-01-20 00:12:08,513.513 dsw44977-5b9b48888d-729dj:122997 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0029), 0.42696629213483145, tensor(0.1320)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5164), 0.5887640449438202, tensor(1.4275)]
[tensor(-1.3051), 0.6494382022471911, tensor(1.9421)]
[tensor(-1.3051), 0.651685393258427, tensor(1.9421)]
[tensor(-1.3051), 0.6674157303370787, tensor(2.0098)]
[tensor(-1.3051), 0.6876404494382022, tensor(2.0653)]
[tensor(-1.3051), 0.6876404494382022, tensor(2.0653)]
[tensor(-1.3051), 0.7078651685393258, tensor(2.1267)]
[tensor(-1.3051), 0.7078651685393258, tensor(2.1267)]
[tensor(-1.3051), 0.7078651685393258, tensor(2.1267)]
[2023-01-20 00:18:49,077.077 dsw44977-5b9b48888d-729dj:124163 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 10 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.4767), 0.31235955056179776, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.9426), 0.4314606741573034, tensor(0.2147)]
[tensor(-1.7272), 0.48764044943820223, tensor(0.7110)]
[tensor(-1.5230), 0.5842696629213483, tensor(1.3984)]
[tensor(-1.3607), 0.6292134831460674, tensor(1.7853)]
[tensor(-1.3607), 0.6292134831460674, tensor(1.7853)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.3607), 0.6292134831460674, tensor(1.7853)]
[tensor(-1.3607), 0.6314606741573033, tensor(1.7853)]
[tensor(-1.3607), 0.6314606741573033, tensor(1.7853)]
[tensor(-1.3607), 0.6314606741573033, tensor(1.7853)]
[2023-01-20 00:25:28,524.524 dsw44977-5b9b48888d-729dj:125328 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9905), 0.45393258426966293, tensor(0.2791)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7701), 0.48314606741573035, tensor(0.6456)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.6264), 0.550561797752809, tensor(1.1264)]
[tensor(-1.4419), 0.6, tensor(1.5581)]
[tensor(-1.4419), 0.6, tensor(1.5581)]
[tensor(-1.3925), 0.6179775280898876, tensor(1.6974)]
[tensor(-1.3925), 0.6382022471910113, tensor(1.7303)]
[tensor(-1.3925), 0.647191011235955, tensor(1.7495)]
[tensor(-1.3925), 0.6494382022471911, tensor(1.7685)]
[tensor(-1.3925), 0.651685393258427, tensor(1.7685)]
[tensor(-1.3925), 0.6629213483146067, tensor(1.7685)]
[tensor(-1.3925), 0.6629213483146067, tensor(1.7685)]
[tensor(-1.3925), 0.6629213483146067, tensor(1.7685)]
[tensor(-1.3925), 0.6629213483146067, tensor(1.7685)]
[tensor(-1.3925), 0.6629213483146067, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
[tensor(-1.3925), 0.6674157303370787, tensor(1.7685)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
[tensor(-1.3925), 0.6786516853932584, tensor(1.7685)]
early stopping at 37
[2023-01-20 00:49:43,346.346 dsw44977-5b9b48888d-729dj:129523 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model v4.3.4-100 datasize 960 batchsize 16 epochs 50 lr 5.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8502), 0.12808988764044943, 0.0]
[tensor(-2.8327), 0.12808988764044943, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.8321), 0.12808988764044943, 0.0]
early stopping at 3
