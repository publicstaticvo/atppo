[2023-01-18 10:43:55,153.153 dsw44922-6f76bf568-tbjcv:74240 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4147), 0.27640449438202247, 0.0]
[tensor(-1.5465), 0.5797752808988764, tensor(1.3524)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2158), 0.6606741573033708, tensor(2.0876)]
[tensor(-1.2158), 0.6606741573033708, tensor(2.0876)]
[tensor(-1.1379), 0.6921348314606741, tensor(2.3228)]
[tensor(-1.0701), 0.6921348314606741, tensor(2.3681)]
[tensor(-1.0701), 0.7056179775280899, tensor(2.4179)]
[tensor(-1.0701), 0.7056179775280899, tensor(2.4183)]
[tensor(-1.0701), 0.7056179775280899, tensor(2.4183)]
[tensor(-1.0701), 0.7078651685393258, tensor(2.4183)]
[2023-01-18 10:48:58,807.807 dsw44922-6f76bf568-tbjcv:74274 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4738), 0.3101123595505618, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5538), 0.5168539325842697, tensor(1.0305)]
[tensor(-1.2170), 0.651685393258427, tensor(2.0415)]
[tensor(-1.2059), 0.6584269662921348, tensor(2.0862)]
[tensor(-1.1816), 0.6876404494382022, tensor(2.2566)]
[tensor(-1.1650), 0.6876404494382022, tensor(2.2732)]
[tensor(-1.1650), 0.6898876404494382, tensor(2.2732)]
[tensor(-1.1650), 0.6898876404494382, tensor(2.2732)]
[tensor(-1.1650), 0.6898876404494382, tensor(2.2732)]
[tensor(-1.1650), 0.6898876404494382, tensor(2.2732)]
[2023-01-18 10:53:59,201.201 dsw44922-6f76bf568-tbjcv:74307 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0670), 0.44719101123595506, tensor(0.1689)]
[tensor(-1.4766), 0.6089887640449438, tensor(1.5684)]
[tensor(-1.4145), 0.6292134831460674, tensor(1.7315)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2763), 0.6494382022471911, tensor(1.9709)]
[tensor(-1.1340), 0.6764044943820224, tensor(2.2480)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1340), 0.6921348314606741, tensor(2.2480)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7078651685393258, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
[tensor(-1.1340), 0.7101123595505618, tensor(2.3638)]
early stopping at 39
[2023-01-18 11:14:09,933.933 dsw44922-6f76bf568-tbjcv:74363 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.4316), 0.30786516853932583, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.9669), 0.4157303370786517, tensor(0.1117)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1837), 0.6561797752808989, tensor(2.0972)]
[tensor(-1.0800), 0.6853932584269663, tensor(2.3469)]
[tensor(-1.0800), 0.6853932584269663, tensor(2.3469)]
[tensor(-1.0800), 0.6876404494382022, tensor(2.3469)]
[tensor(-1.0800), 0.6876404494382022, tensor(2.3469)]
[tensor(-1.0800), 0.6943820224719102, tensor(2.3469)]
[tensor(-1.0800), 0.6943820224719102, tensor(2.3469)]
[tensor(-1.0800), 0.6966292134831461, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
[tensor(-1.0800), 0.701123595505618, tensor(2.3469)]
early stopping at 21
[2023-01-18 11:24:36,132.132 dsw44922-6f76bf568-tbjcv:74402 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1395), 0.4134831460674157, 0.0]
[tensor(-1.4142), 0.6067415730337079, tensor(1.6195)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2419), 0.6449438202247191, tensor(1.9829)]
[tensor(-1.0734), 0.6876404494382022, tensor(2.3648)]
[tensor(-1.0734), 0.6898876404494382, tensor(2.3648)]
[tensor(-1.0734), 0.698876404494382, tensor(2.3648)]
[tensor(-1.0734), 0.7078651685393258, tensor(2.3648)]
[tensor(-1.0734), 0.7078651685393258, tensor(2.3648)]
[tensor(-1.0734), 0.7078651685393258, tensor(2.3648)]
[tensor(-1.0734), 0.7078651685393258, tensor(2.3648)]
[2023-01-18 11:29:39,559.559 dsw44922-6f76bf568-tbjcv:74436 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8418), 0.4606741573033708, tensor(0.4615)]
[tensor(-1.3274), 0.6337078651685393, tensor(1.8412)]
[tensor(-1.1717), 0.6719101123595506, tensor(2.1879)]
[tensor(-1.1194), 0.6898876404494382, tensor(2.3301)]
[tensor(-1.1194), 0.7033707865168539, tensor(2.3441)]
[tensor(-1.1194), 0.7078651685393258, tensor(2.3448)]
[tensor(-1.1194), 0.7078651685393258, tensor(2.3448)]
[tensor(-1.1194), 0.7078651685393258, tensor(2.3448)]
[tensor(-1.1194), 0.7123595505617978, tensor(2.3448)]
[tensor(-1.1194), 0.7123595505617978, tensor(2.3448)]
[2023-01-18 11:34:40,328.328 dsw44922-6f76bf568-tbjcv:74473 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.7036), 0.503370786516854, tensor(0.8133)]
[tensor(-1.2704), 0.6674157303370787, tensor(2.0667)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2313), 0.6674157303370787, tensor(2.0945)]
[tensor(-1.1266), 0.6921348314606741, tensor(2.3341)]
[tensor(-1.1266), 0.6921348314606741, tensor(2.3341)]
[tensor(-1.1266), 0.698876404494382, tensor(2.3341)]
[tensor(-1.1266), 0.7078651685393258, tensor(2.3970)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
[tensor(-1.1266), 0.7123595505617978, tensor(2.4014)]
early stopping at 19
[2023-01-18 11:44:08,058.058 dsw44922-6f76bf568-tbjcv:74514 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6848), 0.5078651685393258, tensor(0.8546)]
[tensor(-1.3869), 0.6359550561797753, tensor(1.7928)]
[tensor(-1.1051), 0.6674157303370787, tensor(2.2319)]
[tensor(-1.0331), 0.6921348314606741, tensor(2.4276)]
[tensor(-1.0331), 0.698876404494382, tensor(2.4276)]
[tensor(-1.0331), 0.698876404494382, tensor(2.4276)]
[tensor(-1.0331), 0.698876404494382, tensor(2.4276)]
[tensor(-1.0331), 0.698876404494382, tensor(2.4276)]
[tensor(-1.0331), 0.7033707865168539, tensor(2.4276)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0331), 0.7056179775280899, tensor(2.4276)]
[tensor(-1.0331), 0.7056179775280899, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
[tensor(-1.0331), 0.7078651685393258, tensor(2.4276)]
early stopping at 22
[2023-01-18 11:55:00,121.121 dsw44922-6f76bf568-tbjcv:74557 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.7247), 0.14606741573033707, 0.0]
[tensor(-2.1392), 0.47191011235955055, tensor(0.2204)]
[tensor(-1.5685), 0.597752808988764, tensor(1.4202)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3070), 0.6404494382022472, tensor(1.8953)]
[tensor(-1.2125), 0.6561797752808989, tensor(2.0684)]
[tensor(-1.1696), 0.6808988764044944, tensor(2.2349)]
[tensor(-1.0692), 0.6876404494382022, tensor(2.3690)]
[tensor(-1.0692), 0.6943820224719102, tensor(2.4006)]
[tensor(-1.0692), 0.6943820224719102, tensor(2.4006)]
[tensor(-1.0692), 0.698876404494382, tensor(2.4239)]
[2023-01-18 12:00:32,803.803 dsw44922-6f76bf568-tbjcv:74591 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.7768), 0.12808988764044943, 0.0]
[tensor(-2.0689), 0.46292134831460674, tensor(0.2457)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4730), 0.6089887640449438, tensor(1.5720)]
[tensor(-1.2192), 0.6764044943820224, tensor(2.1629)]
[tensor(-1.1300), 0.6764044943820224, tensor(2.2183)]
[tensor(-1.0915), 0.698876404494382, tensor(2.4029)]
[tensor(-1.0393), 0.698876404494382, tensor(2.4551)]
[tensor(-1.0393), 0.7078651685393258, tensor(2.4621)]
[tensor(-1.0393), 0.7078651685393258, tensor(2.4621)]
[tensor(-1.0393), 0.7078651685393258, tensor(2.4621)]
[2023-01-18 12:05:37,140.140 dsw44922-6f76bf568-tbjcv:74625 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5018), 0.27415730337078653, 0.0]
[tensor(-2.0030), 0.4853932584269663, tensor(0.4240)]
[tensor(-1.9503), 0.501123595505618, tensor(0.5553)]
[tensor(-1.5418), 0.5865168539325842, tensor(1.3907)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1973), 0.6719101123595506, tensor(2.1622)]
[tensor(-1.1354), 0.6719101123595506, tensor(2.2242)]
[tensor(-1.0868), 0.6786516853932584, tensor(2.3065)]
[tensor(-1.0868), 0.6853932584269663, tensor(2.3376)]
[tensor(-1.0655), 0.698876404494382, tensor(2.4288)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0655), 0.698876404494382, tensor(2.4288)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0655), 0.698876404494382, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7033707865168539, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
[tensor(-1.0655), 0.7078651685393258, tensor(2.4288)]
early stopping at 32
[2023-01-18 12:21:24,727.727 dsw44922-6f76bf568-tbjcv:74753 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.4079), 0.3438202247191011, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.9317), 0.4943820224719101, tensor(0.5402)]
[tensor(-1.2207), 0.6719101123595506, tensor(2.1388)]
[tensor(-1.1759), 0.6764044943820224, tensor(2.2061)]
[tensor(-1.1548), 0.6921348314606741, tensor(2.3059)]
[tensor(-1.1254), 0.6921348314606741, tensor(2.3059)]
[tensor(-1.1254), 0.6921348314606741, tensor(2.3059)]
[tensor(-1.1254), 0.6966292134831461, tensor(2.3059)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
[tensor(-1.1254), 0.698876404494382, tensor(2.3210)]
early stopping at 19
[2023-01-18 12:30:49,436.436 dsw44922-6f76bf568-tbjcv:74793 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6119), 0.24044943820224718, 0.0]
[tensor(-1.8633), 0.47191011235955055, tensor(0.4963)]
[tensor(-1.4462), 0.5797752808988764, tensor(1.4527)]
[tensor(-1.2196), 0.6696629213483146, tensor(2.1287)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1264), 0.6943820224719102, tensor(2.3455)]
[tensor(-1.0919), 0.6943820224719102, tensor(2.3575)]
[tensor(-1.0676), 0.6966292134831461, tensor(2.4155)]
[tensor(-1.0667), 0.701123595505618, tensor(2.4389)]
[tensor(-1.0667), 0.701123595505618, tensor(2.4389)]
[tensor(-1.0667), 0.7078651685393258, tensor(2.4437)]
[2023-01-18 12:35:51,169.169 dsw44922-6f76bf568-tbjcv:74826 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2667), 0.3842696629213483, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4610), 0.6089887640449438, tensor(1.5839)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2100), 0.6629213483146067, tensor(2.1046)]
[tensor(-1.1306), 0.6921348314606741, tensor(2.3301)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0666), 0.6943820224719102, tensor(2.4053)]
[tensor(-1.0666), 0.6943820224719102, tensor(2.4053)]
[tensor(-1.0666), 0.701123595505618, tensor(2.4294)]
[tensor(-1.0616), 0.7101123595505618, tensor(2.4889)]
[tensor(-1.0616), 0.7101123595505618, tensor(2.4889)]
[tensor(-1.0616), 0.7123595505617978, tensor(2.4889)]
[2023-01-18 12:40:53,311.311 dsw44922-6f76bf568-tbjcv:74860 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1924), 0.42247191011235957, 0.0]
[tensor(-1.6934), 0.5123595505617977, tensor(0.8684)]
[tensor(-1.6411), 0.5370786516853933, tensor(1.0443)]
[tensor(-1.3487), 0.6359550561797753, tensor(1.8311)]
[tensor(-1.1619), 0.6853932584269663, tensor(2.2651)]
[tensor(-1.1069), 0.701123595505618, tensor(2.3987)]
[tensor(-1.0892), 0.701123595505618, tensor(2.4051)]
[tensor(-1.0851), 0.7033707865168539, tensor(2.4317)]
[tensor(-1.0851), 0.7033707865168539, tensor(2.4317)]
[tensor(-1.0851), 0.7078651685393258, tensor(2.4317)]
[tensor(-1.0851), 0.7078651685393258, tensor(2.4317)]
[tensor(-1.0851), 0.7123595505617978, tensor(2.4317)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
[tensor(-1.0851), 0.7258426966292135, tensor(2.4654)]
early stopping at 23
[2023-01-18 12:52:19,977.977 dsw44922-6f76bf568-tbjcv:74903 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1182), 0.4404494382022472, tensor(0.0840)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6957), 0.5573033707865168, tensor(1.0909)]
[tensor(-1.1526), 0.6831460674157304, tensor(2.2631)]
[tensor(-1.0854), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0665), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.698876404494382, tensor(2.4090)]
[tensor(-1.0491), 0.7056179775280899, tensor(2.4090)]
[tensor(-1.0491), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.0491), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.0491), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.0491), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.0491), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.0491), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.0491), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
[tensor(-1.0491), 0.7101123595505618, tensor(2.4090)]
early stopping at 33
[2023-01-18 13:08:55,814.814 dsw44922-6f76bf568-tbjcv:74954 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.5569), 0.24719101123595505, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.9884), 0.5146067415730337, tensor(0.5846)]
[tensor(-1.3799), 0.6426966292134831, tensor(1.8336)]
[tensor(-1.2205), 0.6561797752808989, tensor(2.0604)]
[tensor(-1.1582), 0.6786516853932584, tensor(2.2351)]
[tensor(-1.1553), 0.6898876404494382, tensor(2.2941)]
[tensor(-1.1222), 0.6898876404494382, tensor(2.3160)]
[tensor(-1.1222), 0.6921348314606741, tensor(2.3160)]
[tensor(-1.1222), 0.6943820224719102, tensor(2.3160)]
[tensor(-1.1222), 0.6943820224719102, tensor(2.3160)]
[2023-01-18 13:14:21,728.728 dsw44922-6f76bf568-tbjcv:74988 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4145), 0.4044943820224719, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4902), 0.6, tensor(1.5098)]
[tensor(-1.1884), 0.6741573033707865, tensor(2.1824)]
[tensor(-1.1231), 0.6943820224719102, tensor(2.3488)]
[tensor(-1.1031), 0.698876404494382, tensor(2.3913)]
[tensor(-1.1031), 0.698876404494382, tensor(2.3913)]
[tensor(-1.1031), 0.698876404494382, tensor(2.3913)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1031), 0.698876404494382, tensor(2.3913)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1031), 0.7033707865168539, tensor(2.3913)]
[tensor(-1.1031), 0.7033707865168539, tensor(2.3913)]
[2023-01-18 13:19:48,814.814 dsw44922-6f76bf568-tbjcv:75023 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2820), 0.4157303370786517, 0.0]
[tensor(-2.0276), 0.4853932584269663, tensor(0.3993)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5949), 0.6022471910112359, tensor(1.4163)]
[tensor(-1.1858), 0.6719101123595506, tensor(2.1738)]
[tensor(-1.0735), 0.6786516853932584, tensor(2.3198)]
[tensor(-1.0698), 0.6831460674157304, tensor(2.3459)]
[tensor(-1.0698), 0.6831460674157304, tensor(2.3459)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0698), 0.698876404494382, tensor(2.4005)]
[tensor(-1.0698), 0.698876404494382, tensor(2.4005)]
[tensor(-1.0698), 0.698876404494382, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
[tensor(-1.0698), 0.7033707865168539, tensor(2.4005)]
early stopping at 21
[2023-01-18 13:31:30,275.275 dsw44922-6f76bf568-tbjcv:75065 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4442), 0.3752808988764045, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4110), 0.6134831460674157, tensor(1.6564)]
[tensor(-1.2288), 0.6629213483146067, tensor(2.0858)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1120), 0.6921348314606741, tensor(2.3486)]
[tensor(-1.0665), 0.6966292134831461, tensor(2.4166)]
[tensor(-1.0665), 0.6966292134831461, tensor(2.4166)]
[tensor(-1.0665), 0.6966292134831461, tensor(2.4166)]
[tensor(-1.0665), 0.7078651685393258, tensor(2.4277)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
[tensor(-1.0665), 0.7101123595505618, tensor(2.4435)]
early stopping at 19
[2023-01-18 13:41:52,629.629 dsw44922-6f76bf568-tbjcv:75106 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4820), 0.3595505617977528, 0.0]
[tensor(-1.7552), 0.4898876404494382, tensor(0.6942)]
[tensor(-1.3164), 0.6202247191011236, tensor(1.7848)]
[tensor(-1.1466), 0.6651685393258427, tensor(2.1792)]
[tensor(-1.0875), 0.6808988764044944, tensor(2.3170)]
[tensor(-1.0875), 0.6831460674157304, tensor(2.3170)]
[tensor(-1.0782), 0.6966292134831461, tensor(2.4049)]
[tensor(-1.0782), 0.6966292134831461, tensor(2.4049)]
[tensor(-1.0782), 0.701123595505618, tensor(2.4049)]
[tensor(-1.0782), 0.701123595505618, tensor(2.4049)]
[2023-01-18 13:47:19,940.940 dsw44922-6f76bf568-tbjcv:75139 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2581), 0.4247191011235955, 0.0]
[tensor(-1.4349), 0.6112359550561798, tensor(1.6213)]
[tensor(-1.1348), 0.6696629213483146, tensor(2.2135)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1348), 0.6898876404494382, tensor(2.3052)]
[tensor(-1.0693), 0.6898876404494382, tensor(2.3689)]
[tensor(-1.0451), 0.6943820224719102, tensor(2.4268)]
[tensor(-1.0451), 0.6943820224719102, tensor(2.4268)]
[tensor(-1.0451), 0.6943820224719102, tensor(2.4268)]
[tensor(-1.0451), 0.6966292134831461, tensor(2.4268)]
[tensor(-1.0451), 0.698876404494382, tensor(2.4268)]
[2023-01-18 13:52:46,488.488 dsw44922-6f76bf568-tbjcv:75174 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2993), 0.3955056179775281, 0.0]
[tensor(-2.0381), 0.4449438202247191, tensor(0.1866)]
[tensor(-1.6025), 0.5393258426966292, tensor(1.0942)]
[tensor(-1.1860), 0.6876404494382022, tensor(2.2522)]
[tensor(-1.0838), 0.7033707865168539, tensor(2.4330)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4734)]
early stopping at 16
[2023-01-18 14:01:22,328.328 dsw44922-6f76bf568-tbjcv:75213 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-25 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.3618), 0.3842696629213483, 0.0]
[tensor(-1.5076), 0.5932584269662922, tensor(1.4587)]
[tensor(-1.2571), 0.6382022471910113, tensor(1.9339)]
[tensor(-1.1425), 0.6764044943820224, tensor(2.2395)]
[tensor(-1.0964), 0.6876404494382022, tensor(2.3418)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0964), 0.6943820224719102, tensor(2.3579)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3579)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3579)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3579)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3579)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3579)]
[tensor(-1.0964), 0.6966292134831461, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
[tensor(-1.0964), 0.7056179775280899, tensor(2.3579)]
early stopping at 23
[2023-01-18 14:13:42,387.387 dsw44922-6f76bf568-tbjcv:75257 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.5679), 0.36179775280898874, 0.0]
[tensor(-1.6048), 0.5348314606741573, tensor(1.0694)]
[tensor(-1.2689), 0.6606741573033708, tensor(2.0345)]
[tensor(-1.1389), 0.6741573033707865, tensor(2.2319)]
[tensor(-1.0779), 0.6898876404494382, tensor(2.3715)]
[tensor(-1.0779), 0.6898876404494382, tensor(2.3715)]
[tensor(-1.0779), 0.6898876404494382, tensor(2.3715)]
[tensor(-1.0779), 0.6921348314606741, tensor(2.3715)]
[tensor(-1.0779), 0.6943820224719102, tensor(2.3715)]
[tensor(-1.0779), 0.6943820224719102, tensor(2.3715)]
[2023-01-18 14:18:46,876.876 dsw44922-6f76bf568-tbjcv:75291 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0290), 0.4202247191011236, tensor(0.0721)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2463), 0.6426966292134831, tensor(1.9672)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1288), 0.6921348314606741, tensor(2.3319)]
[tensor(-1.1288), 0.6921348314606741, tensor(2.3319)]
[tensor(-1.1288), 0.6921348314606741, tensor(2.3319)]
[tensor(-1.1288), 0.7078651685393258, tensor(2.3435)]
[tensor(-1.1288), 0.7078651685393258, tensor(2.3435)]
[tensor(-1.1288), 0.7078651685393258, tensor(2.3435)]
[tensor(-1.1288), 0.7078651685393258, tensor(2.3435)]
[tensor(-1.1288), 0.7078651685393258, tensor(2.3435)]
[2023-01-18 14:23:50,754.754 dsw44922-6f76bf568-tbjcv:75326 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0237), 0.48089887640449436, tensor(0.3808)]
[tensor(-1.5046), 0.5932584269662922, tensor(1.4617)]
[tensor(-1.4543), 0.6067415730337079, tensor(1.5794)]
[tensor(-1.2792), 0.6382022471910113, tensor(1.9118)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1647), 0.6674157303370787, tensor(2.1724)]
[tensor(-1.1552), 0.6808988764044944, tensor(2.2493)]
[tensor(-1.1552), 0.6876404494382022, tensor(2.2493)]
[tensor(-1.1552), 0.6921348314606741, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.698876404494382, tensor(2.2823)]
[tensor(-1.1552), 0.7033707865168539, tensor(2.2823)]
[tensor(-1.1552), 0.7033707865168539, tensor(2.2823)]
[tensor(-1.1552), 0.7033707865168539, tensor(2.2823)]
[tensor(-1.1552), 0.7033707865168539, tensor(2.2823)]
[tensor(-1.1552), 0.7033707865168539, tensor(2.2823)]
[tensor(-1.1552), 0.7033707865168539, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
[tensor(-1.1552), 0.7123595505617978, tensor(2.2823)]
early stopping at 34
[2023-01-18 14:40:50,709.709 dsw44922-6f76bf568-tbjcv:75375 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8346), 0.5056179775280899, tensor(0.6935)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4756), 0.597752808988764, tensor(1.5132)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1200), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1004), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1004), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1004), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1004), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1004), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1004), 0.701123595505618, tensor(2.3744)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1004), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1004), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1004), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1004), 0.7123595505617978, tensor(2.3744)]
[tensor(-1.1004), 0.7123595505617978, tensor(2.3744)]
[tensor(-1.1004), 0.7123595505617978, tensor(2.3744)]
[tensor(-1.1004), 0.7123595505617978, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
[tensor(-1.1004), 0.7191011235955056, tensor(2.3744)]
early stopping at 27
[2023-01-18 14:54:07,367.367 dsw44922-6f76bf568-tbjcv:75421 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0507), 0.4314606741573034, tensor(0.1066)]
[tensor(-1.3573), 0.6337078651685393, tensor(1.8113)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1730), 0.6764044943820224, tensor(2.2090)]
[tensor(-1.1542), 0.6764044943820224, tensor(2.2279)]
[tensor(-1.1542), 0.6786516853932584, tensor(2.2279)]
[tensor(-1.1542), 0.6876404494382022, tensor(2.2279)]
[tensor(-1.1542), 0.6921348314606741, tensor(2.2279)]
[tensor(-1.1542), 0.6921348314606741, tensor(2.2279)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1542), 0.6921348314606741, tensor(2.2279)]
[tensor(-1.1542), 0.698876404494382, tensor(2.2279)]
[2023-01-18 14:59:13,959.959 dsw44922-6f76bf568-tbjcv:75482 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8427), 0.47415730337078654, tensor(0.5281)]
[tensor(-1.2263), 0.6674157303370787, tensor(2.1108)]
[tensor(-1.0859), 0.6786516853932584, tensor(2.3073)]
[tensor(-1.0859), 0.6898876404494382, tensor(2.3459)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0859), 0.6966292134831461, tensor(2.3459)]
[tensor(-1.0859), 0.6966292134831461, tensor(2.3459)]
[tensor(-1.0859), 0.6966292134831461, tensor(2.3459)]
[tensor(-1.0859), 0.6966292134831461, tensor(2.3459)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.3459)]
[tensor(-1.0859), 0.7056179775280899, tensor(2.3459)]
[2023-01-18 15:04:17,653.653 dsw44922-6f76bf568-tbjcv:75546 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6115), 0.5528089887640449, tensor(1.1526)]
[tensor(-1.2263), 0.6651685393258427, tensor(2.0995)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1883), 0.6831460674157304, tensor(2.2274)]
[tensor(-1.1873), 0.6831460674157304, tensor(2.2274)]
[tensor(-1.1873), 0.6831460674157304, tensor(2.2274)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1873), 0.6921348314606741, tensor(2.2331)]
[tensor(-1.1873), 0.6943820224719102, tensor(2.2331)]
[tensor(-1.1873), 0.6943820224719102, tensor(2.2331)]
[tensor(-1.1873), 0.6943820224719102, tensor(2.2331)]
[tensor(-1.1873), 0.6943820224719102, tensor(2.2331)]
[tensor(-1.1873), 0.7056179775280899, tensor(2.2331)]
[tensor(-1.1873), 0.7056179775280899, tensor(2.2331)]
[tensor(-1.1873), 0.7101123595505618, tensor(2.2331)]
[tensor(-1.1873), 0.7101123595505618, tensor(2.2331)]
[tensor(-1.1873), 0.7101123595505618, tensor(2.2331)]
[tensor(-1.1873), 0.7146067415730337, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7191011235955056, tensor(2.2331)]
[tensor(-1.1873), 0.7213483146067415, tensor(2.2331)]
[tensor(-1.1873), 0.7213483146067415, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
[tensor(-1.1873), 0.7235955056179775, tensor(2.2331)]
early stopping at 39
[2023-01-18 15:23:39,017.017 dsw44922-6f76bf568-tbjcv:75717 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5734), 0.5752808988764045, tensor(1.3030)]
[tensor(-1.2377), 0.6629213483146067, tensor(2.0769)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1359), 0.6741573033707865, tensor(2.2349)]
[tensor(-1.0897), 0.6808988764044944, tensor(2.3148)]
[tensor(-1.0897), 0.6966292134831461, tensor(2.3275)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0897), 0.698876404494382, tensor(2.3374)]
[tensor(-1.0897), 0.698876404494382, tensor(2.3374)]
[tensor(-1.0897), 0.698876404494382, tensor(2.3374)]
[tensor(-1.0897), 0.698876404494382, tensor(2.3374)]
[tensor(-1.0897), 0.701123595505618, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
[tensor(-1.0897), 0.7191011235955056, tensor(2.3374)]
early stopping at 21
[2023-01-18 15:34:05,729.729 dsw44922-6f76bf568-tbjcv:75772 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.7101), 0.18876404494382024, 0.0]
[tensor(-2.0776), 0.47191011235955055, tensor(0.2819)]
[tensor(-1.5628), 0.5842696629213483, tensor(1.3585)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2893), 0.6269662921348315, tensor(1.8455)]
[tensor(-1.1865), 0.6786516853932584, tensor(2.2068)]
[tensor(-1.1577), 0.6786516853932584, tensor(2.2068)]
[tensor(-1.1117), 0.6808988764044944, tensor(2.2928)]
[tensor(-1.1117), 0.6966292134831461, tensor(2.3446)]
[tensor(-1.1117), 0.6966292134831461, tensor(2.3446)]
[tensor(-1.1117), 0.701123595505618, tensor(2.3485)]
[2023-01-18 15:39:10,541.541 dsw44922-6f76bf568-tbjcv:75806 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.5020), 0.2606741573033708, 0.0]
[tensor(-1.6469), 0.5393258426966292, tensor(1.0497)]
[tensor(-1.3418), 0.6292134831460674, tensor(1.8043)]
[tensor(-1.1904), 0.6808988764044944, tensor(2.2141)]
[tensor(-1.1533), 0.6808988764044944, tensor(2.2512)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1343), 0.6808988764044944, tensor(2.2702)]
[tensor(-1.0829), 0.6853932584269663, tensor(2.3441)]
[tensor(-1.0829), 0.6966292134831461, tensor(2.3441)]
[tensor(-1.0829), 0.6966292134831461, tensor(2.3441)]
[tensor(-1.0829), 0.6966292134831461, tensor(2.3441)]
[2023-01-18 15:44:18,988.988 dsw44922-6f76bf568-tbjcv:75840 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4901), 0.33707865168539325, 0.0]
[tensor(-1.9957), 0.46741573033707867, tensor(0.3413)]
[tensor(-1.9494), 0.4853932584269663, tensor(0.4775)]
[tensor(-1.5487), 0.597752808988764, tensor(1.4401)]
[tensor(-1.2328), 0.6719101123595506, tensor(2.1268)]
[tensor(-1.1611), 0.6719101123595506, tensor(2.1310)]
[tensor(-1.1207), 0.6764044943820224, tensor(2.2614)]
[tensor(-1.1168), 0.6764044943820224, tensor(2.2614)]
[tensor(-1.1168), 0.6898876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.6898876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.6898876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.6898876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.6898876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.6943820224719102, tensor(2.3239)]
[tensor(-1.1168), 0.6943820224719102, tensor(2.3239)]
[tensor(-1.1168), 0.6943820224719102, tensor(2.3239)]
[tensor(-1.1168), 0.6943820224719102, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.6966292134831461, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
[tensor(-1.1168), 0.698876404494382, tensor(2.3239)]
early stopping at 38
[2023-01-18 16:03:04,817.817 dsw44922-6f76bf568-tbjcv:75950 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3989), 0.42921348314606744, 0.0]
[tensor(-1.8633), 0.5078651685393258, tensor(0.6760)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2187), 0.6764044943820224, tensor(2.1633)]
[tensor(-1.1800), 0.6786516853932584, tensor(2.2133)]
[tensor(-1.1800), 0.6786516853932584, tensor(2.2133)]
[tensor(-1.1041), 0.6898876404494382, tensor(2.3454)]
[tensor(-1.1041), 0.6898876404494382, tensor(2.3454)]
[tensor(-1.1041), 0.6898876404494382, tensor(2.3454)]
[tensor(-1.1041), 0.6898876404494382, tensor(2.3454)]
[tensor(-1.1041), 0.6966292134831461, tensor(2.3454)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
[tensor(-1.1041), 0.7056179775280899, tensor(2.3856)]
early stopping at 21
[2023-01-18 16:14:05,912.912 dsw44922-6f76bf568-tbjcv:75991 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5430), 0.3056179775280899, 0.0]
[tensor(-1.7218), 0.5370786516853933, tensor(0.9636)]
[tensor(-1.3399), 0.6224719101123596, tensor(1.7725)]
[tensor(-1.1838), 0.6629213483146067, tensor(2.1308)]
[tensor(-1.1446), 0.6696629213483146, tensor(2.2037)]
[tensor(-1.1363), 0.6898876404494382, tensor(2.3131)]
[tensor(-1.1363), 0.6898876404494382, tensor(2.3131)]
[tensor(-1.1363), 0.698876404494382, tensor(2.3406)]
[tensor(-1.1363), 0.698876404494382, tensor(2.3406)]
[tensor(-1.1363), 0.698876404494382, tensor(2.3406)]
[2023-01-18 16:19:33,487.487 dsw44922-6f76bf568-tbjcv:76025 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['selection_head.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1491), 0.4202247191011236, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4058), 0.6269662921348315, tensor(1.7290)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1631), 0.6786516853932584, tensor(2.2302)]
[tensor(-1.1259), 0.6786516853932584, tensor(2.2302)]
[tensor(-1.1036), 0.6876404494382022, tensor(2.3346)]
[tensor(-1.1036), 0.6876404494382022, tensor(2.3346)]
[tensor(-1.1036), 0.6876404494382022, tensor(2.3346)]
[tensor(-1.1036), 0.6921348314606741, tensor(2.3346)]
[tensor(-1.1036), 0.6921348314606741, tensor(2.3346)]
[tensor(-1.1036), 0.6921348314606741, tensor(2.3346)]
[2023-01-18 16:24:36,965.965 dsw44922-6f76bf568-tbjcv:76058 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1705), 0.43820224719101125, tensor(0.0205)]
[tensor(-1.6514), 0.5393258426966292, tensor(1.0452)]
[tensor(-1.6055), 0.5483146067415731, tensor(1.1360)]
[tensor(-1.3342), 0.6382022471910113, tensor(1.8568)]
[tensor(-1.1314), 0.6719101123595506, tensor(2.2282)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1314), 0.6943820224719102, tensor(2.3311)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.698876404494382, tensor(2.3599)]
[tensor(-1.1314), 0.701123595505618, tensor(2.3599)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1314), 0.701123595505618, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7033707865168539, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
[tensor(-1.1314), 0.7078651685393258, tensor(2.3599)]
early stopping at 42
[2023-01-18 16:46:38,269.269 dsw44922-6f76bf568-tbjcv:76116 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9619), 0.46292134831460674, tensor(0.3527)]
[tensor(-1.5832), 0.5955056179775281, tensor(1.3944)]
[tensor(-1.1375), 0.6808988764044944, tensor(2.2670)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1046), 0.6808988764044944, tensor(2.2999)]
[tensor(-1.1046), 0.6808988764044944, tensor(2.2999)]
[tensor(-1.1046), 0.6943820224719102, tensor(2.3518)]
[tensor(-1.1046), 0.6943820224719102, tensor(2.3518)]
[tensor(-1.1046), 0.6943820224719102, tensor(2.3518)]
[tensor(-1.1046), 0.6943820224719102, tensor(2.3518)]
[tensor(-1.1046), 0.6943820224719102, tensor(2.3518)]
[tensor(-1.1046), 0.6943820224719102, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
[tensor(-1.1046), 0.7033707865168539, tensor(2.3518)]
early stopping at 22
[2023-01-18 16:57:33,355.355 dsw44922-6f76bf568-tbjcv:76158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-2.6194), 0.27415730337078653, 0.0]
[tensor(-1.9827), 0.48764044943820223, tensor(0.4555)]
[tensor(-1.4183), 0.6292134831460674, tensor(1.7278)]
[tensor(-1.2031), 0.6696629213483146, tensor(2.1452)]
[tensor(-1.1576), 0.6696629213483146, tensor(2.1795)]
[tensor(-1.1385), 0.6831460674157304, tensor(2.2772)]
[tensor(-1.1126), 0.6898876404494382, tensor(2.3369)]
[tensor(-1.1126), 0.6898876404494382, tensor(2.3369)]
[tensor(-1.1126), 0.6898876404494382, tensor(2.3369)]
[tensor(-1.1126), 0.6898876404494382, tensor(2.3369)]
[2023-01-18 17:02:56,947.947 dsw44922-6f76bf568-tbjcv:76193 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2764), 0.44719101123595506, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4306), 0.6067415730337079, tensor(1.6031)]
[tensor(-1.1635), 0.6584269662921348, tensor(2.1286)]
[tensor(-1.1400), 0.6808988764044944, tensor(2.2645)]
[tensor(-1.0744), 0.701123595505618, tensor(2.4312)]
[tensor(-1.0744), 0.701123595505618, tensor(2.4312)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0744), 0.7101123595505618, tensor(2.4530)]
[tensor(-1.0744), 0.7101123595505618, tensor(2.4530)]
[tensor(-1.0744), 0.7101123595505618, tensor(2.4530)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0744), 0.7101123595505618, tensor(2.4530)]
[2023-01-18 17:08:24,003.003 dsw44922-6f76bf568-tbjcv:76228 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2978), 0.4337078651685393, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0553), 0.48089887640449436, tensor(0.3492)]
[tensor(-1.6012), 0.5887640449438202, tensor(1.3426)]
[tensor(-1.2433), 0.6539325842696629, tensor(2.0264)]
[tensor(-1.1399), 0.6876404494382022, tensor(2.2983)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1169), 0.6876404494382022, tensor(2.3213)]
[tensor(-1.1169), 0.6876404494382022, tensor(2.3213)]
[tensor(-1.1169), 0.6921348314606741, tensor(2.3339)]
[tensor(-1.1169), 0.6921348314606741, tensor(2.3339)]
[tensor(-1.1169), 0.6943820224719102, tensor(2.3339)]
[tensor(-1.1169), 0.7078651685393258, tensor(2.3339)]
[tensor(-1.1169), 0.7078651685393258, tensor(2.3339)]
[tensor(-1.1169), 0.7078651685393258, tensor(2.3339)]
[tensor(-1.1169), 0.7078651685393258, tensor(2.3339)]
[tensor(-1.1169), 0.7078651685393258, tensor(2.3339)]
[tensor(-1.1169), 0.7078651685393258, tensor(2.3339)]
[tensor(-1.1169), 0.7101123595505618, tensor(2.3339)]
[tensor(-1.1169), 0.7101123595505618, tensor(2.3339)]
[tensor(-1.1169), 0.7101123595505618, tensor(2.3339)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1169), 0.7101123595505618, tensor(2.3339)]
[tensor(-1.1169), 0.7101123595505618, tensor(2.3339)]
[tensor(-1.1169), 0.7101123595505618, tensor(2.3339)]
[tensor(-1.1169), 0.7101123595505618, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
[tensor(-1.1169), 0.7123595505617978, tensor(2.3339)]
early stopping at 34
[2023-01-18 17:26:50,984.984 dsw44922-6f76bf568-tbjcv:76279 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2898), 0.4202247191011236, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3721), 0.6247191011235955, tensor(1.7515)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2201), 0.6719101123595506, tensor(2.1394)]
[tensor(-1.1455), 0.6898876404494382, tensor(2.3039)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0978), 0.6898876404494382, tensor(2.3516)]
[tensor(-1.0978), 0.6898876404494382, tensor(2.3516)]
[tensor(-1.0978), 0.6898876404494382, tensor(2.3516)]
[tensor(-1.0978), 0.7078651685393258, tensor(2.3712)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
[tensor(-1.0978), 0.7168539325842697, tensor(2.4099)]
early stopping at 19
[2023-01-18 17:37:11,230.230 dsw44922-6f76bf568-tbjcv:76319 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4790), 0.3685393258426966, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7273), 0.501123595505618, tensor(0.7783)]
[tensor(-1.2923), 0.6359550561797753, tensor(1.8875)]
[tensor(-1.1765), 0.6764044943820224, tensor(2.2055)]
[tensor(-1.1030), 0.6786516853932584, tensor(2.2903)]
[tensor(-1.0887), 0.6853932584269663, tensor(2.3383)]
[tensor(-1.0887), 0.6853932584269663, tensor(2.3383)]
[tensor(-1.0887), 0.7033707865168539, tensor(2.3963)]
[tensor(-1.0887), 0.7033707865168539, tensor(2.3963)]
[tensor(-1.0887), 0.7033707865168539, tensor(2.3963)]
[2023-01-18 17:42:48,734.734 dsw44922-6f76bf568-tbjcv:76359 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2398), 0.4202247191011236, 0.0]
[tensor(-1.4059), 0.6247191011235955, tensor(1.7177)]
[tensor(-1.1431), 0.6674157303370787, tensor(2.1940)]
[tensor(-1.1366), 0.6831460674157304, tensor(2.2791)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0749), 0.6943820224719102, tensor(2.3970)]
[tensor(-1.0749), 0.6943820224719102, tensor(2.3970)]
[tensor(-1.0749), 0.6943820224719102, tensor(2.3970)]
[tensor(-1.0749), 0.6943820224719102, tensor(2.3970)]
[tensor(-1.0749), 0.6943820224719102, tensor(2.3970)]
[tensor(-1.0749), 0.698876404494382, tensor(2.3970)]
[2023-01-18 17:48:36,926.926 dsw44922-6f76bf568-tbjcv:76394 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1583), 0.4337078651685393, tensor(0.0102)]
[tensor(-1.8871), 0.4764044943820225, tensor(0.4949)]
[tensor(-1.5045), 0.5730337078651685, tensor(1.3606)]
[tensor(-1.1473), 0.6651685393258427, tensor(2.1786)]
[tensor(-1.0508), 0.6831460674157304, tensor(2.3649)]
[tensor(-1.0389), 0.6831460674157304, tensor(2.3768)]
[tensor(-1.0389), 0.6853932584269663, tensor(2.3768)]
[tensor(-1.0389), 0.6966292134831461, tensor(2.3868)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7033707865168539, tensor(2.4155)]
[tensor(-1.0389), 0.7078651685393258, tensor(2.4155)]
[tensor(-1.0389), 0.7078651685393258, tensor(2.4155)]
[tensor(-1.0389), 0.7078651685393258, tensor(2.4155)]
[tensor(-1.0389), 0.7078651685393258, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
[tensor(-1.0389), 0.7146067415730337, tensor(2.4155)]
early stopping at 32
[2023-01-18 18:05:57,657.657 dsw44922-6f76bf568-tbjcv:76453 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.3842), 0.39325842696629215, 0.0]
[tensor(-1.4903), 0.5730337078651685, tensor(1.3748)]
[tensor(-1.2501), 0.6651685393258427, tensor(2.0758)]
[tensor(-1.1236), 0.6853932584269663, tensor(2.3034)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0899), 0.698876404494382, tensor(2.4045)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7033707865168539, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
[tensor(-1.0899), 0.7056179775280899, tensor(2.4079)]
early stopping at 31
[2023-01-18 18:22:40,078.078 dsw44922-6f76bf568-tbjcv:76504 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4736), 0.30112359550561796, 0.0]
[tensor(-1.4859), 0.6134831460674157, tensor(1.5815)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2170), 0.6696629213483146, tensor(2.1313)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2170), 0.6696629213483146, tensor(2.1313)]
[tensor(-1.1759), 0.6741573033707865, tensor(2.1949)]
[tensor(-1.1759), 0.6741573033707865, tensor(2.1949)]
[tensor(-1.1640), 0.6831460674157304, tensor(2.2517)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1640), 0.6831460674157304, tensor(2.2517)]
[tensor(-1.1640), 0.6921348314606741, tensor(2.2588)]
[tensor(-1.1640), 0.6921348314606741, tensor(2.2588)]
[2023-01-18 18:27:44,538.538 dsw44922-6f76bf568-tbjcv:76537 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.9180), 0.46292134831460674, tensor(0.3966)]
[tensor(-1.2180), 0.647191011235955, tensor(2.0179)]
[tensor(-1.1423), 0.6786516853932584, tensor(2.2509)]
[tensor(-1.1423), 0.6786516853932584, tensor(2.2509)]
[tensor(-1.1423), 0.6831460674157304, tensor(2.2509)]
[tensor(-1.1423), 0.6831460674157304, tensor(2.2509)]
[tensor(-1.1423), 0.6831460674157304, tensor(2.2509)]
[tensor(-1.1423), 0.6876404494382022, tensor(2.2509)]
[tensor(-1.1423), 0.6876404494382022, tensor(2.2509)]
[tensor(-1.1423), 0.6898876404494382, tensor(2.2509)]
[2023-01-18 18:33:01,863.863 dsw44922-6f76bf568-tbjcv:76572 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0503), 0.46292134831460674, tensor(0.2643)]
[tensor(-1.4537), 0.6134831460674157, tensor(1.6137)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3984), 0.6247191011235955, tensor(1.7252)]
[tensor(-1.2327), 0.6561797752808989, tensor(2.0482)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1511), 0.6966292134831461, tensor(2.3321)]
[tensor(-1.1511), 0.6966292134831461, tensor(2.3321)]
[tensor(-1.1511), 0.6966292134831461, tensor(2.3321)]
[tensor(-1.1511), 0.6966292134831461, tensor(2.3321)]
[tensor(-1.1511), 0.6966292134831461, tensor(2.3321)]
[tensor(-1.1511), 0.6966292134831461, tensor(2.3321)]
[tensor(-1.1511), 0.7056179775280899, tensor(2.3321)]
[tensor(-1.1511), 0.7078651685393258, tensor(2.3321)]
[tensor(-1.1511), 0.7078651685393258, tensor(2.3321)]
[tensor(-1.1511), 0.7078651685393258, tensor(2.3321)]
[tensor(-1.1511), 0.7078651685393258, tensor(2.3321)]
[tensor(-1.1511), 0.7078651685393258, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
[tensor(-1.1511), 0.7123595505617978, tensor(2.3321)]
early stopping at 27
[2023-01-18 18:46:55,408.408 dsw44922-6f76bf568-tbjcv:76618 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7997), 0.5078651685393258, tensor(0.7396)]
[tensor(-1.3751), 0.6134831460674157, tensor(1.6923)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1105), 0.6786516853932584, tensor(2.2827)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
[tensor(-1.0605), 0.7078651685393258, tensor(2.4788)]
early stopping at 14
[2023-01-18 18:54:20,556.556 dsw44922-6f76bf568-tbjcv:76656 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1928), 0.4202247191011236, 0.0]
[tensor(-1.4378), 0.6269662921348315, tensor(1.6970)]
[tensor(-1.1517), 0.6606741573033708, tensor(2.1517)]
[tensor(-1.1369), 0.6651685393258427, tensor(2.1889)]
[tensor(-1.1192), 0.6898876404494382, tensor(2.3303)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1192), 0.6898876404494382, tensor(2.3303)]
[tensor(-1.1192), 0.6943820224719102, tensor(2.3303)]
[tensor(-1.1192), 0.6943820224719102, tensor(2.3303)]
[tensor(-1.1192), 0.6966292134831461, tensor(2.3303)]
[tensor(-1.1192), 0.6966292134831461, tensor(2.3303)]
[2023-01-18 18:59:39,238.238 dsw44922-6f76bf568-tbjcv:76690 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8374), 0.4764044943820225, tensor(0.5446)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1871), 0.6764044943820224, tensor(2.1949)]
[tensor(-1.1740), 0.6786516853932584, tensor(2.2193)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1034), 0.7101123595505618, tensor(2.4472)]
[tensor(-1.1034), 0.7101123595505618, tensor(2.4472)]
[tensor(-1.1034), 0.7101123595505618, tensor(2.4472)]
[tensor(-1.1034), 0.7101123595505618, tensor(2.4472)]
[tensor(-1.1034), 0.7101123595505618, tensor(2.4472)]
[tensor(-1.1034), 0.7101123595505618, tensor(2.4472)]
early stopping at 9
[2023-01-18 19:04:12,396.396 dsw44922-6f76bf568-tbjcv:76723 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.7494), 0.4853932584269663, tensor(0.6775)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3204), 0.6314606741573033, tensor(1.8369)]
[tensor(-1.2496), 0.6808988764044944, tensor(2.1549)]
[tensor(-1.1431), 0.6808988764044944, tensor(2.1940)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0536), 0.6831460674157304, tensor(2.3621)]
[tensor(-1.0536), 0.6853932584269663, tensor(2.3621)]
[tensor(-1.0536), 0.7056179775280899, tensor(2.3643)]
[tensor(-1.0536), 0.7056179775280899, tensor(2.3643)]
[tensor(-1.0536), 0.7056179775280899, tensor(2.3643)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0536), 0.7056179775280899, tensor(2.3643)]
[tensor(-1.0536), 0.7056179775280899, tensor(2.3643)]
[tensor(-1.0536), 0.7056179775280899, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
[tensor(-1.0536), 0.7101123595505618, tensor(2.3643)]
early stopping at 23
[2023-01-18 19:15:43,374.374 dsw44922-6f76bf568-tbjcv:76765 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5588), 0.5707865168539326, tensor(1.2952)]
[tensor(-1.2666), 0.6449438202247191, tensor(1.9581)]
[tensor(-1.2456), 0.6539325842696629, tensor(2.0241)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
[tensor(-1.1369), 0.6966292134831461, tensor(2.3462)]
early stopping at 14
[2023-01-18 19:22:45,586.586 dsw44922-6f76bf568-tbjcv:76802 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6680), 0.15955056179775282, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0718), 0.46741573033707867, tensor(0.2653)]
[tensor(-1.5348), 0.5797752808988764, tensor(1.3641)]
[tensor(-1.3121), 0.6314606741573033, tensor(1.8452)]
[tensor(-1.1976), 0.6494382022471911, tensor(2.0496)]
[tensor(-1.1455), 0.6696629213483146, tensor(2.2028)]
[tensor(-1.1253), 0.6876404494382022, tensor(2.3129)]
[tensor(-1.1253), 0.6876404494382022, tensor(2.3129)]
[tensor(-1.1253), 0.6876404494382022, tensor(2.3129)]
[tensor(-1.1253), 0.6876404494382022, tensor(2.3129)]
[2023-01-18 19:27:46,243.243 dsw44922-6f76bf568-tbjcv:76836 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4695), 0.2853932584269663, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5420), 0.5662921348314607, tensor(1.2895)]
[tensor(-1.2733), 0.6494382022471911, tensor(1.9739)]
[tensor(-1.1765), 0.6719101123595506, tensor(2.1831)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1362), 0.6831460674157304, tensor(2.2796)]
[tensor(-1.1362), 0.6831460674157304, tensor(2.2796)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1362), 0.6831460674157304, tensor(2.2796)]
[tensor(-1.1362), 0.6831460674157304, tensor(2.2796)]
[tensor(-1.1362), 0.6831460674157304, tensor(2.2796)]
[tensor(-1.1362), 0.6853932584269663, tensor(2.2796)]
[2023-01-18 19:32:47,880.880 dsw44922-6f76bf568-tbjcv:76870 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4151), 0.38202247191011235, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8993), 0.5168539325842697, tensor(0.6850)]
[tensor(-1.8516), 0.5168539325842697, tensor(0.6877)]
[tensor(-1.4799), 0.6, tensor(1.5201)]
[tensor(-1.2022), 0.6674157303370787, tensor(2.1348)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1212), 0.6808988764044944, tensor(2.2833)]
[tensor(-1.1212), 0.6808988764044944, tensor(2.2833)]
[tensor(-1.1166), 0.6808988764044944, tensor(2.2833)]
[tensor(-1.1166), 0.6831460674157304, tensor(2.2909)]
[tensor(-1.1166), 0.6831460674157304, tensor(2.2909)]
[tensor(-1.1166), 0.6898876404494382, tensor(2.2909)]
[tensor(-1.1166), 0.6898876404494382, tensor(2.2909)]
[tensor(-1.1166), 0.6943820224719102, tensor(2.2909)]
[tensor(-1.1166), 0.6943820224719102, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
[tensor(-1.1166), 0.6966292134831461, tensor(2.2909)]
early stopping at 25
[2023-01-18 19:45:18,550.550 dsw44922-6f76bf568-tbjcv:76922 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4087), 0.36179775280898874, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9161), 0.4966292134831461, tensor(0.5671)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3205), 0.6404494382022472, tensor(1.8818)]
[tensor(-1.2114), 0.6584269662921348, tensor(2.0807)]
[tensor(-1.1745), 0.6674157303370787, tensor(2.1626)]
[tensor(-1.1162), 0.6898876404494382, tensor(2.3333)]
[tensor(-1.1162), 0.6898876404494382, tensor(2.3333)]
[tensor(-1.1162), 0.6898876404494382, tensor(2.3333)]
[tensor(-1.1162), 0.6921348314606741, tensor(2.3337)]
[tensor(-1.1162), 0.6921348314606741, tensor(2.3337)]
[tensor(-1.1162), 0.6921348314606741, tensor(2.3337)]
[tensor(-1.1162), 0.6921348314606741, tensor(2.3337)]
[tensor(-1.1162), 0.701123595505618, tensor(2.3337)]
[tensor(-1.1162), 0.701123595505618, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3337)]
early stopping at 26
[2023-01-18 19:58:13,412.412 dsw44922-6f76bf568-tbjcv:76989 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5903), 0.26741573033707866, 0.0]
[tensor(-1.8001), 0.5123595505617977, tensor(0.7617)]
[tensor(-1.3718), 0.6112359550561798, tensor(1.6843)]
[tensor(-1.1612), 0.6696629213483146, tensor(2.1871)]
[tensor(-1.1187), 0.6696629213483146, tensor(2.2184)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1158), 0.6853932584269663, tensor(2.3111)]
[tensor(-1.1158), 0.6853932584269663, tensor(2.3111)]
[tensor(-1.1158), 0.6921348314606741, tensor(2.3111)]
[tensor(-1.1158), 0.6921348314606741, tensor(2.3111)]
[tensor(-1.1158), 0.6921348314606741, tensor(2.3111)]
[2023-01-18 20:03:42,756.756 dsw44922-6f76bf568-tbjcv:77026 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1398), 0.4314606741573034, tensor(0.0175)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3761), 0.6382022471910113, tensor(1.8149)]
[tensor(-1.1722), 0.6764044943820224, tensor(2.2098)]
[tensor(-1.1012), 0.6764044943820224, tensor(2.2471)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0692), 0.6876404494382022, tensor(2.3690)]
[tensor(-1.0692), 0.6876404494382022, tensor(2.3690)]
[tensor(-1.0692), 0.6898876404494382, tensor(2.3690)]
[tensor(-1.0692), 0.7033707865168539, tensor(2.4011)]
[tensor(-1.0692), 0.7056179775280899, tensor(2.4011)]
[tensor(-1.0692), 0.7056179775280899, tensor(2.4011)]
[2023-01-18 20:08:50,279.279 dsw44922-6f76bf568-tbjcv:77063 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1116), 0.449438202247191, tensor(0.1356)]
[tensor(-1.6070), 0.5393258426966292, tensor(1.0897)]
[tensor(-1.5628), 0.5415730337078651, tensor(1.1450)]
[tensor(-1.2787), 0.6449438202247191, tensor(1.9460)]
[tensor(-1.1340), 0.6853932584269663, tensor(2.2930)]
[tensor(-1.1340), 0.6853932584269663, tensor(2.2930)]
[tensor(-1.1296), 0.6853932584269663, tensor(2.2930)]
[tensor(-1.1243), 0.6853932584269663, tensor(2.2930)]
[tensor(-1.1243), 0.6853932584269663, tensor(2.2930)]
[tensor(-1.1243), 0.6853932584269663, tensor(2.2930)]
[tensor(-1.1243), 0.6853932584269663, tensor(2.2930)]
[tensor(-1.1243), 0.6921348314606741, tensor(2.2930)]
[tensor(-1.1243), 0.6921348314606741, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
[tensor(-1.1243), 0.698876404494382, tensor(2.2930)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1243), 0.7033707865168539, tensor(2.2930)]
[tensor(-1.1243), 0.7033707865168539, tensor(2.2930)]
[tensor(-1.1243), 0.7033707865168539, tensor(2.2930)]
[tensor(-1.1243), 0.7033707865168539, tensor(2.2930)]
[tensor(-1.1243), 0.7033707865168539, tensor(2.2930)]
[tensor(-1.1243), 0.7033707865168539, tensor(2.2930)]
[tensor(-1.1243), 0.7033707865168539, tensor(2.2930)]
[tensor(-1.1243), 0.7056179775280899, tensor(2.2930)]
[tensor(-1.1243), 0.7056179775280899, tensor(2.2930)]
[2023-01-18 20:34:47,937.937 dsw44922-6f76bf568-tbjcv:77127 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0652), 0.449438202247191, tensor(0.1820)]
[tensor(-1.6417), 0.5483146067415731, tensor(1.0999)]
[tensor(-1.1505), 0.6764044943820224, tensor(2.2316)]
[tensor(-1.0988), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
[tensor(-1.0590), 0.698876404494382, tensor(2.3956)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
[tensor(-1.0590), 0.701123595505618, tensor(2.3956)]
early stopping at 25
[2023-01-18 20:47:14,161.161 dsw44922-6f76bf568-tbjcv:77172 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5366), 0.2898876404494382, 0.0]
[tensor(-1.8062), 0.4943820224719101, tensor(0.6657)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3969), 0.6134831460674157, tensor(1.6705)]
[tensor(-1.2544), 0.6539325842696629, tensor(2.0153)]
[tensor(-1.1783), 0.6606741573033708, tensor(2.1251)]
[tensor(-1.1783), 0.6853932584269663, tensor(2.2308)]
[tensor(-1.1362), 0.6853932584269663, tensor(2.2683)]
[tensor(-1.1362), 0.6898876404494382, tensor(2.2824)]
[tensor(-1.1362), 0.6898876404494382, tensor(2.2824)]
[tensor(-1.1362), 0.698876404494382, tensor(2.3042)]
[2023-01-18 20:52:42,740.740 dsw44922-6f76bf568-tbjcv:77206 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2845), 0.39775280898876403, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4367), 0.6089887640449438, tensor(1.6083)]
[tensor(-1.1667), 0.6808988764044944, tensor(2.2378)]
[tensor(-1.1377), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.0938), 0.701123595505618, tensor(2.4118)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0938), 0.701123595505618, tensor(2.4118)]
[tensor(-1.0938), 0.701123595505618, tensor(2.4118)]
[tensor(-1.0938), 0.701123595505618, tensor(2.4118)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0938), 0.7033707865168539, tensor(2.4118)]
[tensor(-1.0938), 0.7033707865168539, tensor(2.4118)]
[2023-01-18 20:58:09,981.981 dsw44922-6f76bf568-tbjcv:77240 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2025), 0.4314606741573034, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9118), 0.4966292134831461, tensor(0.5714)]
[tensor(-1.5035), 0.6134831460674157, tensor(1.5639)]
[tensor(-1.1999), 0.6674157303370787, tensor(2.1372)]
[tensor(-1.1197), 0.6943820224719102, tensor(2.3522)]
[tensor(-1.1058), 0.698876404494382, tensor(2.3885)]
[tensor(-1.1058), 0.701123595505618, tensor(2.3885)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1058), 0.701123595505618, tensor(2.3885)]
[tensor(-1.1058), 0.7033707865168539, tensor(2.3885)]
[tensor(-1.1058), 0.7033707865168539, tensor(2.3885)]
[tensor(-1.1058), 0.7056179775280899, tensor(2.3885)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.1058), 0.7168539325842697, tensor(2.4092)]
early stopping at 23
[2023-01-18 21:10:39,893.893 dsw44922-6f76bf568-tbjcv:77283 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4532), 0.2853932584269663, 0.0]
[tensor(-1.5083), 0.6067415730337079, tensor(1.5254)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3067), 0.6449438202247191, tensor(1.9180)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1386), 0.6674157303370787, tensor(2.1985)]
[tensor(-1.0835), 0.6808988764044944, tensor(2.3209)]
[tensor(-1.0835), 0.6808988764044944, tensor(2.3209)]
[tensor(-1.0835), 0.6853932584269663, tensor(2.3209)]
[tensor(-1.0835), 0.6876404494382022, tensor(2.3265)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
[tensor(-1.0835), 0.701123595505618, tensor(2.3648)]
early stopping at 19
[2023-01-18 21:20:49,831.831 dsw44922-6f76bf568-tbjcv:77346 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4561), 0.35280898876404493, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.7253), 0.49887640449438203, tensor(0.7691)]
[tensor(-1.3161), 0.6269662921348315, tensor(1.8187)]
[tensor(-1.1772), 0.6382022471910113, tensor(2.0138)]
[tensor(-1.1268), 0.6808988764044944, tensor(2.2777)]
[tensor(-1.0921), 0.6808988764044944, tensor(2.2777)]
[tensor(-1.0921), 0.6808988764044944, tensor(2.2886)]
[tensor(-1.0921), 0.6876404494382022, tensor(2.3285)]
[tensor(-1.0921), 0.7033707865168539, tensor(2.3983)]
[tensor(-1.0921), 0.7033707865168539, tensor(2.3983)]
[2023-01-18 21:26:19,431.431 dsw44922-6f76bf568-tbjcv:77380 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2668), 0.4067415730337079, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4255), 0.6134831460674157, tensor(1.6419)]
[tensor(-1.1678), 0.6651685393258427, tensor(2.1580)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1072), 0.6853932584269663, tensor(2.3198)]
[tensor(-1.1008), 0.698876404494382, tensor(2.3936)]
[tensor(-1.0941), 0.698876404494382, tensor(2.3936)]
[tensor(-1.0941), 0.698876404494382, tensor(2.3936)]
[tensor(-1.0941), 0.701123595505618, tensor(2.3936)]
[tensor(-1.0941), 0.701123595505618, tensor(2.3936)]
[tensor(-1.0941), 0.701123595505618, tensor(2.3936)]
[2023-01-18 21:31:42,718.718 dsw44922-6f76bf568-tbjcv:77414 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1587), 0.449438202247191, tensor(0.0885)]
[tensor(-1.8838), 0.48764044943820223, tensor(0.5544)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5003), 0.5617977528089888, tensor(1.3087)]
[tensor(-1.1720), 0.651685393258427, tensor(2.0864)]
[tensor(-1.0909), 0.6831460674157304, tensor(2.3249)]
[tensor(-1.0826), 0.6898876404494382, tensor(2.3669)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0826), 0.6898876404494382, tensor(2.3669)]
[tensor(-1.0826), 0.6966292134831461, tensor(2.3669)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
[tensor(-1.0826), 0.7056179775280899, tensor(2.4155)]
early stopping at 19
[2023-01-18 21:41:56,751.751 dsw44922-6f76bf568-tbjcv:77455 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.3848), 0.39325842696629215, 0.0]
[tensor(-1.5188), 0.5797752808988764, tensor(1.3801)]
[tensor(-1.2495), 0.651685393258427, tensor(2.0089)]
[tensor(-1.1342), 0.6651685393258427, tensor(2.1916)]
[tensor(-1.1083), 0.6876404494382022, tensor(2.3299)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1083), 0.6943820224719102, tensor(2.3558)]
[tensor(-1.1083), 0.6943820224719102, tensor(2.3558)]
[tensor(-1.1083), 0.6943820224719102, tensor(2.3558)]
[tensor(-1.1083), 0.6943820224719102, tensor(2.3558)]
[tensor(-1.1083), 0.6943820224719102, tensor(2.3558)]
[tensor(-1.1083), 0.6943820224719102, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
[tensor(-1.1083), 0.7033707865168539, tensor(2.3558)]
early stopping at 22
[2023-01-18 21:53:44,947.947 dsw44922-6f76bf568-tbjcv:77498 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5039), 0.30337078651685395, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6366), 0.5640449438202247, tensor(1.1836)]
[tensor(-1.2743), 0.647191011235955, tensor(1.9617)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2442), 0.647191011235955, tensor(1.9805)]
[tensor(-1.1850), 0.6561797752808989, tensor(2.0959)]
[tensor(-1.1850), 0.6876404494382022, tensor(2.2500)]
[tensor(-1.1850), 0.6876404494382022, tensor(2.2500)]
[tensor(-1.1850), 0.6876404494382022, tensor(2.2500)]
[tensor(-1.1850), 0.6921348314606741, tensor(2.2500)]
[tensor(-1.1850), 0.6921348314606741, tensor(2.2500)]
[2023-01-18 21:58:44,266.266 dsw44922-6f76bf568-tbjcv:77532 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.1607), 0.4337078651685393, tensor(0.0079)]
[tensor(-1.4269), 0.5707865168539326, tensor(1.4270)]
[tensor(-1.2293), 0.6561797752808989, tensor(2.0516)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2040), 0.6943820224719102, tensor(2.2679)]
[tensor(-1.1724), 0.6943820224719102, tensor(2.2770)]
[tensor(-1.1724), 0.6943820224719102, tensor(2.2770)]
[tensor(-1.1724), 0.7101123595505618, tensor(2.3305)]
[tensor(-1.1724), 0.7101123595505618, tensor(2.3305)]
[tensor(-1.1724), 0.7101123595505618, tensor(2.3305)]
[tensor(-1.1724), 0.7168539325842697, tensor(2.3305)]
[2023-01-18 22:03:48,367.367 dsw44922-6f76bf568-tbjcv:77565 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1004), 0.42696629213483145, tensor(0.0344)]
[tensor(-1.5390), 0.5955056179775281, tensor(1.4385)]
[tensor(-1.4850), 0.6089887640449438, tensor(1.5600)]
[tensor(-1.3356), 0.6404494382022472, tensor(1.8666)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2040), 0.651685393258427, tensor(2.0544)]
[tensor(-1.1685), 0.6943820224719102, tensor(2.3035)]
[tensor(-1.1534), 0.6943820224719102, tensor(2.3035)]
[tensor(-1.1533), 0.6943820224719102, tensor(2.3035)]
[tensor(-1.1533), 0.6943820224719102, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
[tensor(-1.1533), 0.7033707865168539, tensor(2.3035)]
early stopping at 20
[2023-01-18 22:13:48,239.239 dsw44922-6f76bf568-tbjcv:77607 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.0542), 0.4449438202247191, tensor(0.1705)]
[tensor(-1.5214), 0.5662921348314607, tensor(1.3101)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2366), 0.6584269662921348, tensor(2.0555)]
[tensor(-1.0999), 0.6831460674157304, tensor(2.3158)]
[tensor(-1.0999), 0.6831460674157304, tensor(2.3158)]
[tensor(-1.0999), 0.698876404494382, tensor(2.3397)]
[tensor(-1.0999), 0.7033707865168539, tensor(2.3604)]
[tensor(-1.0999), 0.7078651685393258, tensor(2.3604)]
[tensor(-1.0999), 0.7078651685393258, tensor(2.3604)]
[tensor(-1.0999), 0.7078651685393258, tensor(2.3604)]
[tensor(-1.0999), 0.7078651685393258, tensor(2.3604)]
[tensor(-1.0999), 0.7078651685393258, tensor(2.3604)]
[tensor(-1.0999), 0.7101123595505618, tensor(2.3604)]
[tensor(-1.0999), 0.7101123595505618, tensor(2.3604)]
[tensor(-1.0999), 0.7101123595505618, tensor(2.3604)]
[tensor(-1.0999), 0.7146067415730337, tensor(2.3604)]
[tensor(-1.0999), 0.7146067415730337, tensor(2.3604)]
[tensor(-1.0999), 0.7146067415730337, tensor(2.3604)]
[tensor(-1.0999), 0.7146067415730337, tensor(2.3604)]
[tensor(-1.0999), 0.7146067415730337, tensor(2.3604)]
[tensor(-1.0999), 0.7146067415730337, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
[tensor(-1.0999), 0.7191011235955056, tensor(2.3604)]
early stopping at 32
[2023-01-18 22:29:44,132.132 dsw44922-6f76bf568-tbjcv:77656 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1837), 0.41797752808988764, 0.0]
[tensor(-1.3330), 0.6314606741573033, tensor(1.8243)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1784), 0.6764044943820224, tensor(2.2037)]
[tensor(-1.1235), 0.6831460674157304, tensor(2.2922)]
[tensor(-1.1235), 0.6831460674157304, tensor(2.2922)]
[tensor(-1.1235), 0.6943820224719102, tensor(2.3363)]
[tensor(-1.1235), 0.7101123595505618, tensor(2.3584)]
[tensor(-1.1235), 0.7101123595505618, tensor(2.3584)]
[tensor(-1.1235), 0.7101123595505618, tensor(2.3584)]
[tensor(-1.1235), 0.7101123595505618, tensor(2.3584)]
[2023-01-18 22:34:48,255.255 dsw44922-6f76bf568-tbjcv:77691 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7022), 0.4966292134831461, tensor(0.7810)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1911), 0.6696629213483146, tensor(2.1572)]
[tensor(-1.1404), 0.6719101123595506, tensor(2.2191)]
[tensor(-1.1075), 0.6876404494382022, tensor(2.3307)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1075), 0.6943820224719102, tensor(2.3307)]
[tensor(-1.1075), 0.6943820224719102, tensor(2.3307)]
[tensor(-1.1075), 0.698876404494382, tensor(2.3307)]
[tensor(-1.1075), 0.698876404494382, tensor(2.3307)]
[tensor(-1.1075), 0.698876404494382, tensor(2.3307)]
[tensor(-1.1075), 0.698876404494382, tensor(2.3307)]
[2023-01-18 22:39:57,236.236 dsw44922-6f76bf568-tbjcv:77725 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6561), 0.5213483146067416, tensor(0.9506)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2392), 0.6584269662921348, tensor(2.0529)]
[tensor(-1.2061), 0.6764044943820224, tensor(2.1759)]
[tensor(-1.1278), 0.6764044943820224, tensor(2.1980)]
[tensor(-1.1278), 0.7033707865168539, tensor(2.3866)]
[tensor(-1.1278), 0.7078651685393258, tensor(2.3866)]
[tensor(-1.1278), 0.7078651685393258, tensor(2.3866)]
[tensor(-1.1278), 0.7101123595505618, tensor(2.3866)]
[tensor(-1.1278), 0.7101123595505618, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7146067415730337, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
[tensor(-1.1278), 0.7280898876404495, tensor(2.3866)]
early stopping at 29
[2023-01-18 22:54:18,744.744 dsw44922-6f76bf568-tbjcv:77771 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5700), 0.5617977528089888, tensor(1.2390)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2599), 0.6382022471910113, tensor(1.9311)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1814), 0.6674157303370787, tensor(2.1557)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0659), 0.6786516853932584, tensor(2.3274)]
[tensor(-1.0659), 0.6786516853932584, tensor(2.3274)]
[tensor(-1.0659), 0.698876404494382, tensor(2.3388)]
[tensor(-1.0659), 0.698876404494382, tensor(2.3388)]
[tensor(-1.0659), 0.698876404494382, tensor(2.3388)]
[tensor(-1.0659), 0.7033707865168539, tensor(2.3388)]
[tensor(-1.0659), 0.7033707865168539, tensor(2.3388)]
[tensor(-1.0659), 0.7033707865168539, tensor(2.3388)]
[tensor(-1.0659), 0.7033707865168539, tensor(2.3388)]
[tensor(-1.0659), 0.7033707865168539, tensor(2.3388)]
[tensor(-1.0659), 0.7033707865168539, tensor(2.3388)]
[tensor(-1.0659), 0.7033707865168539, tensor(2.3388)]
[tensor(-1.0659), 0.7056179775280899, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
[tensor(-1.0659), 0.7168539325842697, tensor(2.3388)]
early stopping at 27
[2023-01-18 23:08:05,324.324 dsw44922-6f76bf568-tbjcv:77817 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6761), 0.18202247191011237, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0648), 0.4764044943820225, tensor(0.3172)]
[tensor(-1.5594), 0.604494382022472, tensor(1.4631)]
[tensor(-1.3566), 0.6089887640449438, tensor(1.6884)]
[tensor(-1.2534), 0.6426966292134831, tensor(1.9600)]
[tensor(-1.1912), 0.6584269662921348, tensor(2.1010)]
[tensor(-1.1285), 0.6629213483146067, tensor(2.1861)]
[tensor(-1.1210), 0.6808988764044944, tensor(2.2835)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1210), 0.6808988764044944, tensor(2.2835)]
[tensor(-1.1210), 0.6921348314606741, tensor(2.3120)]
[2023-01-18 23:13:21,480.480 dsw44922-6f76bf568-tbjcv:77851 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4596), 0.31685393258426964, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6147), 0.5797752808988764, tensor(1.2842)]
[tensor(-1.2604), 0.6561797752808989, tensor(2.0205)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1542), 0.6741573033707865, tensor(2.2166)]
[tensor(-1.1257), 0.6741573033707865, tensor(2.2451)]
[tensor(-1.1257), 0.6741573033707865, tensor(2.2451)]
[tensor(-1.1183), 0.6764044943820224, tensor(2.2637)]
[tensor(-1.1183), 0.6853932584269663, tensor(2.2723)]
[tensor(-1.1183), 0.7056179775280899, tensor(2.3929)]
[tensor(-1.1183), 0.7168539325842697, tensor(2.4118)]
[2023-01-18 23:18:24,350.350 dsw44922-6f76bf568-tbjcv:77885 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4942), 0.3438202247191011, 0.0]
[tensor(-2.0346), 0.46741573033707867, tensor(0.3025)]
[tensor(-1.9893), 0.48314606741573035, tensor(0.4265)]
[tensor(-1.5780), 0.5617977528089888, tensor(1.2309)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2562), 0.651685393258427, tensor(2.0022)]
[tensor(-1.1804), 0.6719101123595506, tensor(2.1791)]
[tensor(-1.1594), 0.6719101123595506, tensor(2.1791)]
[tensor(-1.1504), 0.6808988764044944, tensor(2.2541)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1504), 0.6808988764044944, tensor(2.2541)]
[tensor(-1.1504), 0.6831460674157304, tensor(2.2541)]
[tensor(-1.1504), 0.6831460674157304, tensor(2.2541)]
[tensor(-1.1504), 0.6831460674157304, tensor(2.2541)]
[tensor(-1.1504), 0.6876404494382022, tensor(2.2541)]
[tensor(-1.1504), 0.6876404494382022, tensor(2.2541)]
[tensor(-1.1504), 0.6876404494382022, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
[tensor(-1.1504), 0.6898876404494382, tensor(2.2541)]
early stopping at 27
[2023-01-18 23:31:44,361.361 dsw44922-6f76bf568-tbjcv:77930 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4543), 0.3707865168539326, 0.0]
[tensor(-1.9269), 0.49213483146067416, tensor(0.5338)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2553), 0.651685393258427, tensor(2.0031)]
[tensor(-1.1968), 0.6674157303370787, tensor(2.1403)]
[tensor(-1.1694), 0.6674157303370787, tensor(2.1452)]
[tensor(-1.1021), 0.6876404494382022, tensor(2.3361)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
[tensor(-1.1021), 0.7033707865168539, tensor(2.3584)]
early stopping at 25
[2023-01-18 23:44:07,698.698 dsw44922-6f76bf568-tbjcv:77973 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6055), 0.27415730337078653, 0.0]
[tensor(-1.7870), 0.5191011235955056, tensor(0.8085)]
[tensor(-1.4073), 0.5865168539325842, tensor(1.5253)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1836), 0.6674157303370787, tensor(2.1534)]
[tensor(-1.1191), 0.6786516853932584, tensor(2.2742)]
[tensor(-1.0975), 0.6898876404494382, tensor(2.3520)]
[tensor(-1.0975), 0.6898876404494382, tensor(2.3520)]
[tensor(-1.0975), 0.6898876404494382, tensor(2.3520)]
[tensor(-1.0975), 0.6898876404494382, tensor(2.3520)]
[tensor(-1.0975), 0.6898876404494382, tensor(2.3520)]
[2023-01-18 23:49:10,313.313 dsw44922-6f76bf568-tbjcv:78007 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.3222), 0.39101123595505616, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5403), 0.5820224719101124, tensor(1.3699)]
[tensor(-1.2363), 0.647191011235955, tensor(1.9996)]
[tensor(-1.1550), 0.647191011235955, tensor(2.0697)]
[tensor(-1.0708), 0.6741573033707865, tensor(2.3000)]
[tensor(-1.0708), 0.6808988764044944, tensor(2.3185)]
[tensor(-1.0708), 0.6808988764044944, tensor(2.3185)]
[tensor(-1.0708), 0.6808988764044944, tensor(2.3185)]
[tensor(-1.0708), 0.6831460674157304, tensor(2.3185)]
[tensor(-1.0708), 0.698876404494382, tensor(2.3451)]
[2023-01-18 23:54:10,732.732 dsw44922-6f76bf568-tbjcv:78040 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1944), 0.449438202247191, tensor(0.0528)]
[tensor(-1.6851), 0.5056179775280899, tensor(0.8430)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6404), 0.5191011235955056, tensor(0.9551)]
[tensor(-1.3315), 0.6292134831460674, tensor(1.8146)]
[tensor(-1.1531), 0.6629213483146067, tensor(2.1615)]
[tensor(-1.1362), 0.6741573033707865, tensor(2.2346)]
[tensor(-1.1082), 0.6808988764044944, tensor(2.2963)]
[tensor(-1.1082), 0.6808988764044944, tensor(2.2963)]
[tensor(-1.1082), 0.6808988764044944, tensor(2.2963)]
[tensor(-1.1082), 0.6966292134831461, tensor(2.3159)]
[tensor(-1.1082), 0.698876404494382, tensor(2.3159)]
[tensor(-1.1082), 0.7078651685393258, tensor(2.3359)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
[tensor(-1.1082), 0.7168539325842697, tensor(2.3684)]
early stopping at 23
[2023-01-19 00:05:45,158.158 dsw44922-6f76bf568-tbjcv:78083 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0213), 0.46741573033707867, tensor(0.3158)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5892), 0.5707865168539326, tensor(1.2647)]
[tensor(-1.1572), 0.6629213483146067, tensor(2.1574)]
[tensor(-1.1044), 0.6764044943820224, tensor(2.2776)]
[tensor(-1.1044), 0.6764044943820224, tensor(2.2776)]
[tensor(-1.0939), 0.6764044943820224, tensor(2.2776)]
[tensor(-1.0939), 0.6764044943820224, tensor(2.2776)]
[tensor(-1.0939), 0.6831460674157304, tensor(2.2776)]
[tensor(-1.0939), 0.6853932584269663, tensor(2.2776)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0939), 0.6853932584269663, tensor(2.2776)]
[tensor(-1.0939), 0.6853932584269663, tensor(2.2776)]
[tensor(-1.0939), 0.6898876404494382, tensor(2.2776)]
[tensor(-1.0939), 0.6921348314606741, tensor(2.2776)]
[tensor(-1.0939), 0.6921348314606741, tensor(2.2776)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.2776)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.2776)]
[tensor(-1.0939), 0.6966292134831461, tensor(2.2776)]
[tensor(-1.0939), 0.7033707865168539, tensor(2.2776)]
[tensor(-1.0939), 0.7033707865168539, tensor(2.2776)]
[tensor(-1.0939), 0.7033707865168539, tensor(2.2776)]
[tensor(-1.0939), 0.7033707865168539, tensor(2.2776)]
[tensor(-1.0939), 0.7033707865168539, tensor(2.2776)]
[tensor(-1.0939), 0.7033707865168539, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
[tensor(-1.0939), 0.7056179775280899, tensor(2.2776)]
early stopping at 34
[2023-01-19 00:22:24,740.740 dsw44922-6f76bf568-tbjcv:78132 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5252), 0.298876404494382, 0.0]
[tensor(-1.7781), 0.5146067415730337, tensor(0.7949)]
[tensor(-1.3176), 0.6539325842696629, tensor(1.9520)]
[tensor(-1.1751), 0.6764044943820224, tensor(2.2069)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1432), 0.6831460674157304, tensor(2.2725)]
[tensor(-1.1432), 0.6898876404494382, tensor(2.3040)]
[tensor(-1.1133), 0.7078651685393258, tensor(2.4260)]
[tensor(-1.1133), 0.7078651685393258, tensor(2.4260)]
[tensor(-1.1133), 0.7101123595505618, tensor(2.4260)]
[tensor(-1.1133), 0.7101123595505618, tensor(2.4260)]
[2023-01-19 00:27:55,951.951 dsw44922-6f76bf568-tbjcv:78166 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4097), 0.37303370786516854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5281), 0.5910112359550562, tensor(1.4269)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2192), 0.6786516853932584, tensor(2.1740)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1476), 0.6831460674157304, tensor(2.2681)]
[tensor(-1.1064), 0.6898876404494382, tensor(2.3430)]
[tensor(-1.1064), 0.7056179775280899, tensor(2.3966)]
[tensor(-1.1064), 0.7056179775280899, tensor(2.3966)]
[tensor(-1.1064), 0.7056179775280899, tensor(2.3966)]
[tensor(-1.1064), 0.7056179775280899, tensor(2.3966)]
[tensor(-1.1064), 0.7123595505617978, tensor(2.3966)]
[2023-01-19 00:33:27,614.614 dsw44922-6f76bf568-tbjcv:78199 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2359), 0.4404494382022472, 0.0]
[tensor(-1.9506), 0.49213483146067416, tensor(0.5101)]
[tensor(-1.5117), 0.6089887640449438, tensor(1.5333)]
[tensor(-1.2211), 0.6606741573033708, tensor(2.0823)]
[tensor(-1.1355), 0.6808988764044944, tensor(2.2690)]
[tensor(-1.1262), 0.6831460674157304, tensor(2.2895)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
[tensor(-1.1039), 0.7168539325842697, tensor(2.4803)]
early stopping at 17
[2023-01-19 00:42:36,031.031 dsw44922-6f76bf568-tbjcv:78239 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.3854), 0.3752808988764045, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4909), 0.6157303370786517, tensor(1.5878)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2533), 0.6494382022471911, tensor(1.9938)]
[tensor(-1.1386), 0.6853932584269663, tensor(2.2883)]
[tensor(-1.1049), 0.6966292134831461, tensor(2.3782)]
[tensor(-1.1049), 0.7056179775280899, tensor(2.4008)]
[tensor(-1.1049), 0.7056179775280899, tensor(2.4008)]
[tensor(-1.1049), 0.7056179775280899, tensor(2.4008)]
[tensor(-1.1049), 0.7056179775280899, tensor(2.4008)]
[tensor(-1.1049), 0.7235955056179775, tensor(2.4947)]
[tensor(-1.1049), 0.7235955056179775, tensor(2.4947)]
[tensor(-1.1049), 0.7235955056179775, tensor(2.4947)]
[tensor(-1.1049), 0.7258426966292135, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
[tensor(-1.1049), 0.7325842696629213, tensor(2.4947)]
early stopping at 24
[2023-01-19 00:55:29,664.664 dsw44922-6f76bf568-tbjcv:78284 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4240), 0.3707865168539326, 0.0]
[tensor(-1.6966), 0.503370786516854, tensor(0.8203)]
[tensor(-1.2996), 0.6179775280898876, tensor(1.7903)]
[tensor(-1.1840), 0.6494382022471911, tensor(2.0632)]
[tensor(-1.1637), 0.6719101123595506, tensor(2.1959)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1196), 0.6808988764044944, tensor(2.2849)]
[tensor(-1.1196), 0.6808988764044944, tensor(2.2849)]
[tensor(-1.1196), 0.6898876404494382, tensor(2.2905)]
[tensor(-1.1196), 0.6898876404494382, tensor(2.2905)]
[tensor(-1.1196), 0.6898876404494382, tensor(2.2905)]
[2023-01-19 01:01:00,166.166 dsw44922-6f76bf568-tbjcv:78318 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2203), 0.43820224719101125, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3850), 0.6179775280898876, tensor(1.7049)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1487), 0.6606741573033708, tensor(2.1547)]
[tensor(-1.1071), 0.6741573033707865, tensor(2.2637)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0788), 0.7033707865168539, tensor(2.4381)]
[tensor(-1.0788), 0.7033707865168539, tensor(2.4381)]
[tensor(-1.0788), 0.7123595505617978, tensor(2.4523)]
[tensor(-1.0788), 0.7123595505617978, tensor(2.4523)]
[tensor(-1.0788), 0.7123595505617978, tensor(2.4523)]
[tensor(-1.0788), 0.7123595505617978, tensor(2.4523)]
[2023-01-19 01:06:32,186.186 dsw44922-6f76bf568-tbjcv:78352 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1193), 0.4606741573033708, tensor(0.1840)]
[tensor(-1.8566), 0.4966292134831461, tensor(0.6265)]
[tensor(-1.4850), 0.5797752808988764, tensor(1.4139)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1851), 0.6606741573033708, tensor(2.1182)]
[tensor(-1.0788), 0.6853932584269663, tensor(2.3482)]
[tensor(-1.0788), 0.6898876404494382, tensor(2.3706)]
[tensor(-1.0788), 0.6898876404494382, tensor(2.3706)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0788), 0.698876404494382, tensor(2.3876)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
[tensor(-1.0788), 0.7056179775280899, tensor(2.4190)]
early stopping at 19
[2023-01-19 01:16:41,454.454 dsw44922-6f76bf568-tbjcv:78393 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3-100 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.bias', 'audio_encoder.feature_extractor.conv_layers.7.layer_norm.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4987), 0.3393258426966292, 0.0]
[tensor(-1.5701), 0.5528089887640449, tensor(1.1939)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2822), 0.6224719101123596, tensor(1.8301)]
[tensor(-1.1234), 0.6808988764044944, tensor(2.2811)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0704), 0.6808988764044944, tensor(2.3004)]
[tensor(-1.0704), 0.6898876404494382, tensor(2.3660)]
[tensor(-1.0704), 0.6921348314606741, tensor(2.3716)]
[tensor(-1.0704), 0.6921348314606741, tensor(2.3716)]
[tensor(-1.0704), 0.6966292134831461, tensor(2.3716)]
[tensor(-1.0704), 0.698876404494382, tensor(2.3716)]
[tensor(-1.0704), 0.698876404494382, tensor(2.3716)]
[tensor(-1.0704), 0.698876404494382, tensor(2.3716)]
[tensor(-1.0704), 0.698876404494382, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
[tensor(-1.0704), 0.7101123595505618, tensor(2.3716)]
early stopping at 28
[2023-01-19 01:31:35,803.803 dsw44922-6f76bf568-tbjcv:78441 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.7141), 0.1842696629213483, 0.0]
[tensor(-1.6730), 0.5078651685393258, tensor(0.8663)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3279), 0.6382022471910113, tensor(1.8631)]
[tensor(-1.2515), 0.6404494382022472, tensor(1.9507)]
[tensor(-1.2119), 0.6764044943820224, tensor(2.1701)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2119), 0.6764044943820224, tensor(2.1701)]
[tensor(-1.2119), 0.6764044943820224, tensor(2.1701)]
[tensor(-1.2119), 0.6853932584269663, tensor(2.1701)]
[tensor(-1.2119), 0.6853932584269663, tensor(2.1701)]
[tensor(-1.2119), 0.6853932584269663, tensor(2.1701)]
[2023-01-19 01:37:13,154.154 dsw44922-6f76bf568-tbjcv:78476 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0677), 0.4314606741573034, tensor(0.0897)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4254), 0.5752808988764045, tensor(1.4510)]
[tensor(-1.2058), 0.6629213483146067, tensor(2.1088)]
[tensor(-1.2058), 0.6831460674157304, tensor(2.1848)]
[tensor(-1.2058), 0.6853932584269663, tensor(2.1848)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2058), 0.6853932584269663, tensor(2.1848)]
[tensor(-1.2058), 0.6853932584269663, tensor(2.1848)]
[tensor(-1.2058), 0.6853932584269663, tensor(2.1848)]
[tensor(-1.2058), 0.6853932584269663, tensor(2.1848)]
[tensor(-1.2058), 0.6853932584269663, tensor(2.1848)]
early stopping at 10
[2023-01-19 01:42:23,388.388 dsw44922-6f76bf568-tbjcv:78510 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3299), 0.36404494382022473, 0.0]
[tensor(-1.6441), 0.5528089887640449, tensor(1.1200)]
[tensor(-1.5711), 0.5528089887640449, tensor(1.1817)]
[tensor(-1.3012), 0.6314606741573033, tensor(1.8561)]
[tensor(-1.2556), 0.6629213483146067, tensor(2.0590)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2556), 0.6629213483146067, tensor(2.0590)]
[tensor(-1.2556), 0.6764044943820224, tensor(2.0975)]
[tensor(-1.2556), 0.6764044943820224, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6876404494382022, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6898876404494382, tensor(2.1082)]
[tensor(-1.2556), 0.6943820224719102, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
[tensor(-1.2556), 0.6966292134831461, tensor(2.1082)]
early stopping at 38
[2023-01-19 02:01:05,395.395 dsw44922-6f76bf568-tbjcv:78562 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3241), 0.3595505617977528, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9510), 0.4449438202247191, tensor(0.2737)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4270), 0.5752808988764045, tensor(1.4494)]
[tensor(-1.3412), 0.6089887640449438, tensor(1.7038)]
[tensor(-1.3412), 0.6179775280898876, tensor(1.7038)]
[tensor(-1.3412), 0.651685393258427, tensor(1.8584)]
[tensor(-1.3412), 0.651685393258427, tensor(1.8584)]
[tensor(-1.3412), 0.6539325842696629, tensor(1.8584)]
[tensor(-1.3412), 0.6539325842696629, tensor(1.8584)]
[tensor(-1.3412), 0.6539325842696629, tensor(1.8584)]
[tensor(-1.3412), 0.6539325842696629, tensor(1.8584)]
[tensor(-1.3412), 0.6539325842696629, tensor(1.8584)]
[tensor(-1.3412), 0.6606741573033708, tensor(1.8584)]
[tensor(-1.3412), 0.6606741573033708, tensor(1.8584)]
[tensor(-1.3412), 0.6606741573033708, tensor(1.8584)]
[tensor(-1.3412), 0.6606741573033708, tensor(1.8584)]
[tensor(-1.3412), 0.6696629213483146, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
[tensor(-1.3412), 0.6741573033707865, tensor(1.8584)]
early stopping at 28
[2023-01-19 02:15:07,054.054 dsw44922-6f76bf568-tbjcv:78609 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2250), 0.4, 0.0]
[tensor(-1.4310), 0.6157303370786517, tensor(1.6477)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2434), 0.6674157303370787, tensor(2.0937)]
[tensor(-1.1636), 0.6674157303370787, tensor(2.1735)]
[tensor(-1.1636), 0.6741573033707865, tensor(2.1735)]
[tensor(-1.1636), 0.7056179775280899, tensor(2.2986)]
[tensor(-1.1636), 0.7078651685393258, tensor(2.3226)]
[tensor(-1.1636), 0.7078651685393258, tensor(2.3226)]
[tensor(-1.1636), 0.7123595505617978, tensor(2.3226)]
[tensor(-1.1636), 0.7123595505617978, tensor(2.3226)]
[2023-01-19 02:20:09,337.337 dsw44922-6f76bf568-tbjcv:78642 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7832), 0.5258426966292135, tensor(0.8460)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2279), 0.6382022471910113, tensor(1.9631)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1551), 0.6651685393258427, tensor(2.1708)]
[tensor(-1.1551), 0.6943820224719102, tensor(2.2781)]
[tensor(-1.1551), 0.698876404494382, tensor(2.2875)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1551), 0.698876404494382, tensor(2.2875)]
[tensor(-1.1551), 0.7168539325842697, tensor(2.3453)]
[tensor(-1.1551), 0.7168539325842697, tensor(2.3453)]
[tensor(-1.1551), 0.7168539325842697, tensor(2.3453)]
[tensor(-1.1551), 0.7168539325842697, tensor(2.3453)]
[2023-01-19 02:25:13,380.380 dsw44922-6f76bf568-tbjcv:78676 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8186), 0.4943820224719101, tensor(0.6533)]
[tensor(-1.3801), 0.6247191011235955, tensor(1.7435)]
[tensor(-1.3297), 0.6269662921348315, tensor(1.8051)]
[tensor(-1.2479), 0.6741573033707865, tensor(2.1229)]
[tensor(-1.2392), 0.6741573033707865, tensor(2.1229)]
[tensor(-1.2392), 0.6741573033707865, tensor(2.1229)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
[tensor(-1.2392), 0.7056179775280899, tensor(2.2341)]
early stopping at 17
[2023-01-19 02:33:41,729.729 dsw44922-6f76bf568-tbjcv:78715 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.5996), 0.5730337078651685, tensor(1.2656)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3372), 0.6314606741573033, tensor(1.8201)]
[tensor(-1.1837), 0.6674157303370787, tensor(2.1534)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0974), 0.6876404494382022, tensor(2.3408)]
[tensor(-1.0974), 0.6876404494382022, tensor(2.3408)]
[tensor(-1.0974), 0.6876404494382022, tensor(2.3408)]
[tensor(-1.0974), 0.6876404494382022, tensor(2.3408)]
[tensor(-1.0974), 0.6876404494382022, tensor(2.3408)]
[tensor(-1.0974), 0.6876404494382022, tensor(2.3408)]
[tensor(-1.0974), 0.6921348314606741, tensor(2.3408)]
[tensor(-1.0974), 0.6921348314606741, tensor(2.3408)]
[tensor(-1.0974), 0.6921348314606741, tensor(2.3408)]
[tensor(-1.0974), 0.6921348314606741, tensor(2.3408)]
[tensor(-1.0974), 0.6921348314606741, tensor(2.3408)]
[tensor(-1.0974), 0.698876404494382, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
[tensor(-1.0974), 0.701123595505618, tensor(2.3408)]
early stopping at 26
[2023-01-19 02:46:35,523.523 dsw44922-6f76bf568-tbjcv:78759 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6887), 0.14157303370786517, 0.0]
[tensor(-2.0665), 0.4584269662921348, tensor(0.2256)]
[tensor(-1.5101), 0.5887640449438202, tensor(1.4337)]
[tensor(-1.3175), 0.6359550561797753, tensor(1.8623)]
[tensor(-1.2053), 0.651685393258427, tensor(2.0531)]
[tensor(-1.2053), 0.651685393258427, tensor(2.0531)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1833), 0.6606741573033708, tensor(2.1201)]
[tensor(-1.1833), 0.6696629213483146, tensor(2.1201)]
[tensor(-1.1833), 0.6696629213483146, tensor(2.1201)]
[tensor(-1.1833), 0.6741573033707865, tensor(2.1201)]
[2023-01-19 02:51:40,984.984 dsw44922-6f76bf568-tbjcv:78793 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5143), 0.2449438202247191, 0.0]
[tensor(-1.6539), 0.5213483146067416, tensor(0.9528)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2725), 0.6539325842696629, tensor(1.9971)]
[tensor(-1.1828), 0.6539325842696629, tensor(2.0869)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1699), 0.6539325842696629, tensor(2.0885)]
[tensor(-1.1699), 0.6764044943820224, tensor(2.1872)]
[tensor(-1.1699), 0.6764044943820224, tensor(2.1872)]
[tensor(-1.1699), 0.6831460674157304, tensor(2.2019)]
[tensor(-1.1699), 0.6831460674157304, tensor(2.2019)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1699), 0.6831460674157304, tensor(2.2019)]
[2023-01-19 02:57:11,735.735 dsw44922-6f76bf568-tbjcv:78828 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4847), 0.27191011235955054, 0.0]
[tensor(-1.9817), 0.4651685393258427, tensor(0.3442)]
[tensor(-1.9204), 0.4764044943820225, tensor(0.4616)]
[tensor(-1.5278), 0.597752808988764, tensor(1.4610)]
[tensor(-1.2628), 0.6337078651685393, tensor(1.9057)]
[tensor(-1.2080), 0.6539325842696629, tensor(2.0617)]
[tensor(-1.2010), 0.6629213483146067, tensor(2.1136)]
[tensor(-1.1912), 0.6629213483146067, tensor(2.1136)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1912), 0.6629213483146067, tensor(2.1136)]
[tensor(-1.1912), 0.6674157303370787, tensor(2.1136)]
[tensor(-1.1912), 0.6674157303370787, tensor(2.1136)]
[tensor(-1.1912), 0.6674157303370787, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6741573033707865, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
[tensor(-1.1912), 0.6786516853932584, tensor(2.1136)]
early stopping at 32
[2023-01-19 03:13:11,291.291 dsw44922-6f76bf568-tbjcv:78877 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5362), 0.27415730337078653, 0.0]
[tensor(-2.0709), 0.45617977528089887, tensor(0.2100)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3167), 0.6269662921348315, tensor(1.8181)]
[tensor(-1.2370), 0.6449438202247191, tensor(1.9878)]
[tensor(-1.1786), 0.6606741573033708, tensor(2.1247)]
[tensor(-1.1416), 0.6876404494382022, tensor(2.2966)]
[tensor(-1.1416), 0.6876404494382022, tensor(2.2966)]
[tensor(-1.1416), 0.6876404494382022, tensor(2.2966)]
[tensor(-1.1416), 0.6876404494382022, tensor(2.2966)]
[tensor(-1.1416), 0.6876404494382022, tensor(2.2966)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1416), 0.6876404494382022, tensor(2.2966)]
[tensor(-1.1416), 0.6876404494382022, tensor(2.2966)]
[tensor(-1.1416), 0.6966292134831461, tensor(2.2966)]
[tensor(-1.1416), 0.6966292134831461, tensor(2.2966)]
[tensor(-1.1416), 0.6966292134831461, tensor(2.2966)]
[tensor(-1.1416), 0.6966292134831461, tensor(2.2966)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
[tensor(-1.1416), 0.7033707865168539, tensor(2.2966)]
early stopping at 27
[2023-01-19 03:26:36,233.233 dsw44922-6f76bf568-tbjcv:78921 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.6489), 0.15955056179775282, 0.0]
[tensor(-1.8733), 0.5191011235955056, tensor(0.7222)]
[tensor(-1.4347), 0.5910112359550562, tensor(1.5204)]
[tensor(-1.2389), 0.6719101123595506, tensor(2.1207)]
[tensor(-1.2148), 0.6719101123595506, tensor(2.1336)]
[tensor(-1.1880), 0.6741573033707865, tensor(2.1828)]
[tensor(-1.1814), 0.6764044943820224, tensor(2.2006)]
[tensor(-1.1708), 0.6876404494382022, tensor(2.2674)]
[tensor(-1.1708), 0.6876404494382022, tensor(2.2674)]
[tensor(-1.1708), 0.6876404494382022, tensor(2.2674)]
[2023-01-19 03:31:38,299.299 dsw44922-6f76bf568-tbjcv:78956 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1957), 0.42696629213483145, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4265), 0.6134831460674157, tensor(1.6409)]
[tensor(-1.2497), 0.6539325842696629, tensor(2.0200)]
[tensor(-1.1960), 0.6696629213483146, tensor(2.1523)]
[tensor(-1.1353), 0.6853932584269663, tensor(2.2917)]
[tensor(-1.1353), 0.6853932584269663, tensor(2.2917)]
[tensor(-1.1353), 0.6853932584269663, tensor(2.2917)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1353), 0.6898876404494382, tensor(2.2917)]
[tensor(-1.1353), 0.6943820224719102, tensor(2.2917)]
[tensor(-1.1353), 0.698876404494382, tensor(2.2917)]
[2023-01-19 03:36:41,037.037 dsw44922-6f76bf568-tbjcv:78989 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2480), 0.4202247191011236, 0.0]
[tensor(-1.7082), 0.5483146067415731, tensor(1.0334)]
[tensor(-1.6614), 0.5483146067415731, tensor(1.0689)]
[tensor(-1.3784), 0.6202247191011236, tensor(1.7227)]
[tensor(-1.2302), 0.6651685393258427, tensor(2.0956)]
[tensor(-1.1692), 0.6853932584269663, tensor(2.2578)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
[tensor(-1.1479), 0.7056179775280899, tensor(2.3802)]
early stopping at 19
[2023-01-19 03:46:03,757.757 dsw44922-6f76bf568-tbjcv:79029 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.2138), 0.4314606741573034, 0.0]
[tensor(-1.7375), 0.5438202247191011, tensor(0.9816)]
[tensor(-1.1927), 0.6696629213483146, tensor(2.1556)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1403), 0.6808988764044944, tensor(2.2642)]
[tensor(-1.1333), 0.6898876404494382, tensor(2.3162)]
[tensor(-1.0876), 0.7056179775280899, tensor(2.4405)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
[tensor(-1.0876), 0.7146067415730337, tensor(2.4405)]
early stopping at 18
[2023-01-19 03:55:04,039.039 dsw44922-6f76bf568-tbjcv:79069 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6126), 0.18202247191011237, 0.0]
[tensor(-1.9943), 0.46741573033707867, tensor(0.3428)]
[tensor(-1.3913), 0.6224719101123596, tensor(1.7211)]
[tensor(-1.2589), 0.6426966292134831, tensor(1.9546)]
[tensor(-1.2519), 0.6426966292134831, tensor(1.9546)]
[tensor(-1.1963), 0.6539325842696629, tensor(2.0734)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1963), 0.6539325842696629, tensor(2.0734)]
[tensor(-1.1963), 0.6651685393258427, tensor(2.0770)]
[tensor(-1.1963), 0.6696629213483146, tensor(2.0770)]
[tensor(-1.1963), 0.6696629213483146, tensor(2.0770)]
[2023-01-19 04:00:31,047.047 dsw44922-6f76bf568-tbjcv:79102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4347), 0.30786516853932583, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4776), 0.5730337078651685, tensor(1.3876)]
[tensor(-1.2490), 0.647191011235955, tensor(1.9869)]
[tensor(-1.2164), 0.651685393258427, tensor(2.0420)]
[tensor(-1.1693), 0.6674157303370787, tensor(2.1678)]
[tensor(-1.1693), 0.6674157303370787, tensor(2.1678)]
[tensor(-1.1693), 0.6674157303370787, tensor(2.1678)]
[tensor(-1.1693), 0.6808988764044944, tensor(2.1678)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1693), 0.6808988764044944, tensor(2.1678)]
[tensor(-1.1693), 0.6808988764044944, tensor(2.1678)]
[2023-01-19 04:06:04,585.585 dsw44922-6f76bf568-tbjcv:79136 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4755), 0.26292134831460673, 0.0]
[tensor(-2.2120), 0.39325842696629215, 0.0]
[tensor(-1.7101), 0.5348314606741573, tensor(0.9640)]
[tensor(-1.2554), 0.6606741573033708, tensor(2.0480)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1891), 0.6606741573033708, tensor(2.0806)]
[tensor(-1.1709), 0.6651685393258427, tensor(2.1550)]
[tensor(-1.1709), 0.6651685393258427, tensor(2.1550)]
[tensor(-1.1709), 0.6696629213483146, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
[tensor(-1.1709), 0.6786516853932584, tensor(2.1550)]
early stopping at 19
[2023-01-19 04:16:22,885.885 dsw44922-6f76bf568-tbjcv:79177 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4515), 0.35730337078651686, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4412), 0.6089887640449438, tensor(1.6037)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2706), 0.6359550561797753, tensor(1.9091)]
[tensor(-1.1875), 0.6629213483146067, tensor(2.1271)]
[tensor(-1.1687), 0.6629213483146067, tensor(2.1459)]
[tensor(-1.1687), 0.6696629213483146, tensor(2.1459)]
[tensor(-1.1687), 0.6696629213483146, tensor(2.1459)]
[tensor(-1.1687), 0.6741573033707865, tensor(2.1459)]
[tensor(-1.1687), 0.6764044943820224, tensor(2.1564)]
[tensor(-1.1687), 0.6876404494382022, tensor(2.1759)]
[tensor(-1.1687), 0.6876404494382022, tensor(2.1759)]
[tensor(-1.1687), 0.6876404494382022, tensor(2.1759)]
[tensor(-1.1687), 0.6876404494382022, tensor(2.1759)]
[tensor(-1.1687), 0.6943820224719102, tensor(2.1759)]
[tensor(-1.1687), 0.6943820224719102, tensor(2.1759)]
[tensor(-1.1687), 0.6943820224719102, tensor(2.1759)]
[tensor(-1.1687), 0.6943820224719102, tensor(2.1759)]
[tensor(-1.1687), 0.6943820224719102, tensor(2.1759)]
[tensor(-1.1687), 0.6943820224719102, tensor(2.1759)]
[tensor(-1.1687), 0.6943820224719102, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
[tensor(-1.1687), 0.7033707865168539, tensor(2.1759)]
early stopping at 31
[2023-01-19 04:33:00,057.057 dsw44922-6f76bf568-tbjcv:79226 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5701), 0.20674157303370785, 0.0]
[tensor(-1.8374), 0.503370786516854, tensor(0.6795)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4005), 0.6112359550561798, tensor(1.6557)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2065), 0.647191011235955, tensor(2.0295)]
[tensor(-1.1438), 0.6764044943820224, tensor(2.2382)]
[tensor(-1.1438), 0.6764044943820224, tensor(2.2382)]
[tensor(-1.1110), 0.6921348314606741, tensor(2.3497)]
[tensor(-1.1110), 0.6921348314606741, tensor(2.3497)]
[tensor(-1.1110), 0.6921348314606741, tensor(2.3497)]
[tensor(-1.1110), 0.6921348314606741, tensor(2.3497)]
[2023-01-19 04:38:36,560.560 dsw44922-6f76bf568-tbjcv:79260 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1964), 0.4067415730337079, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4237), 0.5865168539325842, tensor(1.5089)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1849), 0.6606741573033708, tensor(2.1185)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1849), 0.6651685393258427, tensor(2.1185)]
[tensor(-1.1235), 0.6876404494382022, tensor(2.3147)]
[tensor(-1.1235), 0.6943820224719102, tensor(2.3263)]
[tensor(-1.1235), 0.701123595505618, tensor(2.3263)]
[tensor(-1.1235), 0.701123595505618, tensor(2.3263)]
[tensor(-1.1235), 0.701123595505618, tensor(2.3263)]
[tensor(-1.1235), 0.701123595505618, tensor(2.3263)]
[2023-01-19 04:44:09,419.419 dsw44922-6f76bf568-tbjcv:79294 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1864), 0.43595505617977526, 0.0]
[tensor(-1.9149), 0.49213483146067416, tensor(0.5457)]
[tensor(-1.5359), 0.5707865168539326, tensor(1.3180)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3283), 0.647191011235955, tensor(1.9076)]
[tensor(-1.1225), 0.6674157303370787, tensor(2.2146)]
[tensor(-1.1178), 0.6719101123595506, tensor(2.2417)]
[tensor(-1.1178), 0.6741573033707865, tensor(2.2417)]
[tensor(-1.1178), 0.6876404494382022, tensor(2.2424)]
[tensor(-1.1178), 0.6876404494382022, tensor(2.2424)]
[tensor(-1.1178), 0.6898876404494382, tensor(2.2584)]
[tensor(-1.1178), 0.6898876404494382, tensor(2.2584)]
[tensor(-1.1178), 0.698876404494382, tensor(2.2584)]
[tensor(-1.1178), 0.698876404494382, tensor(2.2584)]
[tensor(-1.1178), 0.698876404494382, tensor(2.2584)]
[tensor(-1.1178), 0.701123595505618, tensor(2.2584)]
[tensor(-1.1178), 0.701123595505618, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
[tensor(-1.1178), 0.7056179775280899, tensor(2.2584)]
early stopping at 27
[2023-01-19 04:59:02,429.429 dsw44922-6f76bf568-tbjcv:79340 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-25 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1776), 0.449438202247191, tensor(0.0696)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3719), 0.6269662921348315, tensor(1.7629)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2275), 0.647191011235955, tensor(2.0085)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1782), 0.6719101123595506, tensor(2.1813)]
[tensor(-1.1276), 0.6853932584269663, tensor(2.2994)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1276), 0.698876404494382, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
[tensor(-1.1276), 0.7033707865168539, tensor(2.3416)]
early stopping at 28
[2023-01-19 05:13:54,797.797 dsw44922-6f76bf568-tbjcv:79386 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4451), 0.26292134831460673, 0.0]
[tensor(-1.4593), 0.597752808988764, tensor(1.5295)]
[tensor(-1.1906), 0.6674157303370787, tensor(2.1465)]
[tensor(-1.1906), 0.6674157303370787, tensor(2.1465)]
[tensor(-1.1906), 0.6876404494382022, tensor(2.2007)]
[tensor(-1.1906), 0.6876404494382022, tensor(2.2007)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1906), 0.6898876404494382, tensor(2.2007)]
[tensor(-1.1906), 0.6921348314606741, tensor(2.2007)]
[tensor(-1.1906), 0.6921348314606741, tensor(2.2007)]
[tensor(-1.1906), 0.6921348314606741, tensor(2.2007)]
[2023-01-19 05:18:54,010.010 dsw44922-6f76bf568-tbjcv:79420 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9807), 0.43820224719101125, tensor(0.2103)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3634), 0.6, tensor(1.6366)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1948), 0.6674157303370787, tensor(2.1423)]
[tensor(-1.1948), 0.6674157303370787, tensor(2.1423)]
[tensor(-1.1948), 0.6966292134831461, tensor(2.2703)]
[tensor(-1.1948), 0.6966292134831461, tensor(2.2703)]
[tensor(-1.1948), 0.7056179775280899, tensor(2.2703)]
[tensor(-1.1948), 0.7056179775280899, tensor(2.2703)]
[tensor(-1.1948), 0.7056179775280899, tensor(2.2703)]
[tensor(-1.1948), 0.7056179775280899, tensor(2.2703)]
[2023-01-19 05:23:52,693.693 dsw44922-6f76bf568-tbjcv:79454 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0069), 0.4651685393258427, tensor(0.3190)]
[tensor(-1.4597), 0.6247191011235955, tensor(1.6639)]
[tensor(-1.4111), 0.6247191011235955, tensor(1.6675)]
[tensor(-1.3252), 0.6449438202247191, tensor(1.8995)]
[tensor(-1.3252), 0.6539325842696629, tensor(1.9067)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3252), 0.6696629213483146, tensor(1.9780)]
[tensor(-1.3235), 0.6696629213483146, tensor(1.9780)]
[tensor(-1.3190), 0.6741573033707865, tensor(2.0518)]
[tensor(-1.3190), 0.6741573033707865, tensor(2.0518)]
[tensor(-1.3190), 0.6741573033707865, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
[tensor(-1.3190), 0.6764044943820224, tensor(2.0518)]
early stopping at 23
[2023-01-19 05:35:19,558.558 dsw44922-6f76bf568-tbjcv:79498 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.8291), 0.12808988764044943, 0.0]
[tensor(-2.8291), 0.12808988764044943, 0.0]
[tensor(-2.6729), 0.1842696629213483, 0.0]
early stopping at 3
[2023-01-19 05:36:56,179.179 dsw44922-6f76bf568-tbjcv:79528 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2883), 0.3438202247191011, 0.0]
[tensor(-1.3884), 0.6359550561797753, tensor(1.7914)]
[tensor(-1.2416), 0.6561797752808989, tensor(2.0393)]
[tensor(-1.1462), 0.6943820224719102, tensor(2.3257)]
[tensor(-1.1462), 0.6943820224719102, tensor(2.3257)]
[tensor(-1.1462), 0.698876404494382, tensor(2.3270)]
[tensor(-1.1462), 0.7168539325842697, tensor(2.3755)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1462), 0.7168539325842697, tensor(2.3755)]
[tensor(-1.1462), 0.7168539325842697, tensor(2.3755)]
[tensor(-1.1462), 0.7168539325842697, tensor(2.3755)]
[2023-01-19 05:42:01,857.857 dsw44922-6f76bf568-tbjcv:79562 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.7255), 0.5393258426966292, tensor(0.9711)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2327), 0.6584269662921348, tensor(2.0595)]
[tensor(-1.1612), 0.6764044943820224, tensor(2.2209)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1348), 0.698876404494382, tensor(2.3595)]
[tensor(-1.1348), 0.7033707865168539, tensor(2.3595)]
[tensor(-1.1348), 0.7235955056179775, tensor(2.4260)]
[tensor(-1.1348), 0.7235955056179775, tensor(2.4260)]
[tensor(-1.1348), 0.7235955056179775, tensor(2.4260)]
[tensor(-1.1348), 0.7235955056179775, tensor(2.4260)]
[tensor(-1.1348), 0.7235955056179775, tensor(2.4260)]
[2023-01-19 05:47:05,353.353 dsw44922-6f76bf568-tbjcv:79596 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6096), 0.5617977528089888, tensor(1.1994)]
[tensor(-1.2776), 0.6584269662921348, tensor(2.0145)]
[tensor(-1.2469), 0.6584269662921348, tensor(2.0145)]
[tensor(-1.1598), 0.6876404494382022, tensor(2.2784)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1598), 0.6876404494382022, tensor(2.2784)]
[tensor(-1.1598), 0.6876404494382022, tensor(2.2784)]
[tensor(-1.1598), 0.7033707865168539, tensor(2.2998)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
[tensor(-1.1598), 0.7191011235955056, tensor(2.3798)]
early stopping at 18
[2023-01-19 05:56:22,050.050 dsw44922-6f76bf568-tbjcv:79635 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5381), 0.5707865168539326, tensor(1.3159)]
[tensor(-1.2747), 0.6584269662921348, tensor(2.0174)]
[tensor(-1.2367), 0.6584269662921348, tensor(2.0217)]
[tensor(-1.1521), 0.6853932584269663, tensor(2.2749)]
[tensor(-1.1521), 0.6898876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.6921348314606741, tensor(2.2749)]
[tensor(-1.1521), 0.6921348314606741, tensor(2.2749)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.698876404494382, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
[tensor(-1.1521), 0.701123595505618, tensor(2.2749)]
early stopping at 28
[2023-01-19 06:10:17,579.579 dsw44922-6f76bf568-tbjcv:79680 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6776), 0.13707865168539327, 0.0]
[tensor(-1.9407), 0.48764044943820223, tensor(0.4975)]
[tensor(-1.4658), 0.5955056179775281, tensor(1.5118)]
[tensor(-1.2706), 0.6426966292134831, tensor(1.9429)]
[tensor(-1.1503), 0.6651685393258427, tensor(2.1755)]
[tensor(-1.1503), 0.6651685393258427, tensor(2.1755)]
[tensor(-1.1503), 0.6696629213483146, tensor(2.1858)]
[tensor(-1.1503), 0.6853932584269663, tensor(2.2618)]
[tensor(-1.1503), 0.6853932584269663, tensor(2.2618)]
[tensor(-1.1503), 0.6898876404494382, tensor(2.2618)]
[2023-01-19 06:15:22,964.964 dsw44922-6f76bf568-tbjcv:79714 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4029), 0.3235955056179775, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5704), 0.5775280898876405, tensor(1.3172)]
[tensor(-1.2729), 0.6494382022471911, tensor(1.9743)]
[tensor(-1.2118), 0.6629213483146067, tensor(2.1028)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1649), 0.6629213483146067, tensor(2.1385)]
[tensor(-1.1649), 0.6719101123595506, tensor(2.1683)]
[tensor(-1.1649), 0.6786516853932584, tensor(2.1985)]
[tensor(-1.1649), 0.6786516853932584, tensor(2.1985)]
[tensor(-1.1649), 0.6786516853932584, tensor(2.1985)]
[tensor(-1.1649), 0.7033707865168539, tensor(2.2140)]
[2023-01-19 06:20:33,106.106 dsw44922-6f76bf568-tbjcv:79748 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4356), 0.30337078651685395, 0.0]
[tensor(-1.8711), 0.5213483146067416, tensor(0.7356)]
[tensor(-1.8172), 0.5213483146067416, tensor(0.7895)]
[tensor(-1.4517), 0.6134831460674157, tensor(1.6157)]
[tensor(-1.2063), 0.6606741573033708, tensor(2.0970)]
[tensor(-1.1446), 0.6786516853932584, tensor(2.2486)]
[tensor(-1.1446), 0.6786516853932584, tensor(2.2486)]
[tensor(-1.1427), 0.6786516853932584, tensor(2.2486)]
[tensor(-1.1427), 0.6853932584269663, tensor(2.2680)]
[tensor(-1.1427), 0.6943820224719102, tensor(2.2680)]
[tensor(-1.1427), 0.6943820224719102, tensor(2.2680)]
[tensor(-1.1427), 0.7056179775280899, tensor(2.3014)]
[tensor(-1.1427), 0.7056179775280899, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
[tensor(-1.1427), 0.7078651685393258, tensor(2.3014)]
early stopping at 24
[2023-01-19 06:32:27,064.064 dsw44922-6f76bf568-tbjcv:79792 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4063), 0.36629213483146067, 0.0]
[tensor(-1.8917), 0.48089887640449436, tensor(0.5128)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2759), 0.6494382022471911, tensor(1.9713)]
[tensor(-1.2073), 0.6696629213483146, tensor(2.1410)]
[tensor(-1.2073), 0.6696629213483146, tensor(2.1410)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1931), 0.6696629213483146, tensor(2.1410)]
[tensor(-1.1931), 0.6696629213483146, tensor(2.1410)]
[tensor(-1.1931), 0.6741573033707865, tensor(2.1410)]
[tensor(-1.1931), 0.6741573033707865, tensor(2.1410)]
[tensor(-1.1931), 0.6741573033707865, tensor(2.1410)]
[tensor(-1.1931), 0.6741573033707865, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
[tensor(-1.1931), 0.6876404494382022, tensor(2.1410)]
early stopping at 25
[2023-01-19 06:45:01,248.248 dsw44922-6f76bf568-tbjcv:79838 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6054), 0.20449438202247192, 0.0]
[tensor(-1.8043), 0.5213483146067416, tensor(0.8024)]
[tensor(-1.4355), 0.5887640449438202, tensor(1.5083)]
[tensor(-1.2528), 0.6629213483146067, tensor(2.0618)]
[tensor(-1.1700), 0.6786516853932584, tensor(2.2232)]
[tensor(-1.1700), 0.6786516853932584, tensor(2.2232)]
[tensor(-1.1672), 0.6853932584269663, tensor(2.2598)]
[tensor(-1.1400), 0.6966292134831461, tensor(2.3432)]
[tensor(-1.1400), 0.6966292134831461, tensor(2.3432)]
[tensor(-1.1400), 0.6966292134831461, tensor(2.3432)]
[2023-01-19 06:50:05,013.013 dsw44922-6f76bf568-tbjcv:79872 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1622), 0.41123595505617977, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4070), 0.6337078651685393, tensor(1.7616)]
[tensor(-1.2505), 0.6674157303370787, tensor(2.0866)]
[tensor(-1.1976), 0.6674157303370787, tensor(2.1282)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1053), 0.6921348314606741, tensor(2.3553)]
[tensor(-1.1053), 0.6921348314606741, tensor(2.3553)]
[tensor(-1.1053), 0.6921348314606741, tensor(2.3553)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1053), 0.6966292134831461, tensor(2.3553)]
[tensor(-1.1053), 0.698876404494382, tensor(2.3553)]
[tensor(-1.1053), 0.698876404494382, tensor(2.3553)]
[2023-01-19 06:55:05,851.851 dsw44922-6f76bf568-tbjcv:79906 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1863), 0.4157303370786517, 0.0]
[tensor(-1.6631), 0.5550561797752809, tensor(1.1121)]
[tensor(-1.6170), 0.5617977528089888, tensor(1.1920)]
[tensor(-1.3539), 0.6404494382022472, tensor(1.8483)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2333), 0.6719101123595506, tensor(2.1263)]
[tensor(-1.1809), 0.6719101123595506, tensor(2.1787)]
[tensor(-1.1674), 0.6719101123595506, tensor(2.1810)]
[tensor(-1.1674), 0.6831460674157304, tensor(2.2453)]
[tensor(-1.1674), 0.6831460674157304, tensor(2.2453)]
[tensor(-1.1674), 0.6831460674157304, tensor(2.2453)]
[tensor(-1.1674), 0.6831460674157304, tensor(2.2453)]
[tensor(-1.1674), 0.6898876404494382, tensor(2.2453)]
[tensor(-1.1674), 0.6898876404494382, tensor(2.2453)]
[tensor(-1.1674), 0.6898876404494382, tensor(2.2453)]
[tensor(-1.1674), 0.6898876404494382, tensor(2.2453)]
[tensor(-1.1674), 0.6898876404494382, tensor(2.2453)]
[tensor(-1.1674), 0.6898876404494382, tensor(2.2453)]
[tensor(-1.1674), 0.698876404494382, tensor(2.2453)]
[tensor(-1.1674), 0.7033707865168539, tensor(2.2453)]
[tensor(-1.1674), 0.7033707865168539, tensor(2.2453)]
[tensor(-1.1674), 0.7033707865168539, tensor(2.2453)]
[tensor(-1.1674), 0.7033707865168539, tensor(2.2453)]
[tensor(-1.1674), 0.7078651685393258, tensor(2.2453)]
[tensor(-1.1674), 0.7078651685393258, tensor(2.2453)]
[tensor(-1.1674), 0.7123595505617978, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
[tensor(-1.1674), 0.7235955056179775, tensor(2.2453)]
early stopping at 36
[2023-01-19 07:13:04,190.190 dsw44922-6f76bf568-tbjcv:79956 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9810), 0.4786516853932584, tensor(0.4123)]
[tensor(-1.5959), 0.5865168539325842, tensor(1.3367)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1975), 0.6764044943820224, tensor(2.1845)]
[tensor(-1.1543), 0.6786516853932584, tensor(2.2390)]
[tensor(-1.1543), 0.6786516853932584, tensor(2.2390)]
[tensor(-1.1355), 0.6853932584269663, tensor(2.2915)]
[tensor(-1.1355), 0.698876404494382, tensor(2.3337)]
[tensor(-1.1355), 0.698876404494382, tensor(2.3337)]
[tensor(-1.1355), 0.7078651685393258, tensor(2.3722)]
[tensor(-1.1355), 0.7123595505617978, tensor(2.3722)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1355), 0.7123595505617978, tensor(2.3722)]
[tensor(-1.1355), 0.7123595505617978, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
[tensor(-1.1355), 0.7168539325842697, tensor(2.3722)]
early stopping at 23
[2023-01-19 07:24:24,258.258 dsw44922-6f76bf568-tbjcv:79998 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5835), 0.20224719101123595, 0.0]
[tensor(-1.9675), 0.4606741573033708, tensor(0.3358)]
[tensor(-1.4096), 0.6022471910112359, tensor(1.6016)]
[tensor(-1.2612), 0.6719101123595506, tensor(2.0983)]
[tensor(-1.1876), 0.6719101123595506, tensor(2.0983)]
[tensor(-1.1876), 0.6719101123595506, tensor(2.1253)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1405), 0.6719101123595506, tensor(2.1853)]
[tensor(-1.1405), 0.6741573033707865, tensor(2.1853)]
[tensor(-1.1405), 0.6741573033707865, tensor(2.1853)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1405), 0.6921348314606741, tensor(2.1853)]
[2023-01-19 07:29:48,484.484 dsw44922-6f76bf568-tbjcv:80033 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4034), 0.30786516853932583, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5277), 0.5842696629213483, tensor(1.3936)]
[tensor(-1.2449), 0.647191011235955, tensor(1.9911)]
[tensor(-1.1907), 0.647191011235955, tensor(2.0228)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1270), 0.6674157303370787, tensor(2.2101)]
[tensor(-1.1270), 0.6719101123595506, tensor(2.2101)]
[tensor(-1.1270), 0.6719101123595506, tensor(2.2101)]
[tensor(-1.1270), 0.6898876404494382, tensor(2.2605)]
[tensor(-1.1270), 0.6898876404494382, tensor(2.2605)]
[tensor(-1.1270), 0.6921348314606741, tensor(2.2605)]
[2023-01-19 07:35:16,125.125 dsw44922-6f76bf568-tbjcv:80066 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4977), 0.23595505617977527, 0.0]
[tensor(-2.2464), 0.3955056179775281, 0.0]
[tensor(-1.7665), 0.503370786516854, tensor(0.7504)]
[tensor(-1.2806), 0.6224719101123596, tensor(1.8318)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1632), 0.6741573033707865, tensor(2.2076)]
[tensor(-1.1589), 0.6741573033707865, tensor(2.2076)]
[tensor(-1.1589), 0.6741573033707865, tensor(2.2076)]
[tensor(-1.1589), 0.6741573033707865, tensor(2.2076)]
[tensor(-1.1586), 0.6853932584269663, tensor(2.2684)]
[tensor(-1.1448), 0.698876404494382, tensor(2.3496)]
[tensor(-1.1448), 0.698876404494382, tensor(2.3496)]
[tensor(-1.1448), 0.698876404494382, tensor(2.3496)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1448), 0.698876404494382, tensor(2.3496)]
[tensor(-1.1448), 0.698876404494382, tensor(2.3496)]
[tensor(-1.1448), 0.698876404494382, tensor(2.3496)]
[tensor(-1.1448), 0.698876404494382, tensor(2.3496)]
[tensor(-1.1448), 0.7078651685393258, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
[tensor(-1.1448), 0.7101123595505618, tensor(2.3496)]
early stopping at 28
[2023-01-19 07:50:17,851.851 dsw44922-6f76bf568-tbjcv:80115 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4974), 0.3438202247191011, 0.0]
[tensor(-1.5415), 0.5752808988764045, tensor(1.3349)]
[tensor(-1.2991), 0.6449438202247191, tensor(1.9257)]
[tensor(-1.1887), 0.6786516853932584, tensor(2.2045)]
[tensor(-1.1499), 0.6786516853932584, tensor(2.2433)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1499), 0.6786516853932584, tensor(2.2433)]
[tensor(-1.1499), 0.6898876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.6921348314606741, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
[tensor(-1.1499), 0.698876404494382, tensor(2.2969)]
early stopping at 35
[2023-01-19 08:08:48,683.683 dsw44922-6f76bf568-tbjcv:80169 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4686), 0.3303370786516854, 0.0]
[tensor(-1.7020), 0.5438202247191011, tensor(1.0171)]
[tensor(-1.3832), 0.6224719101123596, tensor(1.7292)]
[tensor(-1.2403), 0.6494382022471911, tensor(2.0069)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1558), 0.6674157303370787, tensor(2.1813)]
[tensor(-1.1558), 0.6719101123595506, tensor(2.1813)]
[tensor(-1.1558), 0.6853932584269663, tensor(2.2531)]
[tensor(-1.1558), 0.6853932584269663, tensor(2.2531)]
[tensor(-1.1558), 0.6898876404494382, tensor(2.2531)]
[tensor(-1.1558), 0.6966292134831461, tensor(2.2691)]
[2023-01-19 08:14:13,583.583 dsw44922-6f76bf568-tbjcv:80202 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0876), 0.45393258426966293, tensor(0.1820)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4032), 0.6157303370786517, tensor(1.6755)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2233), 0.6584269662921348, tensor(2.0688)]
[tensor(-1.1675), 0.6786516853932584, tensor(2.2257)]
[tensor(-1.1242), 0.6786516853932584, tensor(2.2578)]
[tensor(-1.1242), 0.6966292134831461, tensor(2.3334)]
[tensor(-1.1242), 0.6966292134831461, tensor(2.3334)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3334)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3334)]
[tensor(-1.1242), 0.701123595505618, tensor(2.3334)]
[2023-01-19 08:19:42,720.720 dsw44922-6f76bf568-tbjcv:80235 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1357), 0.48314606741573035, tensor(0.2801)]
[tensor(-1.8503), 0.5191011235955056, tensor(0.7452)]
[tensor(-1.4935), 0.5887640449438202, tensor(1.4503)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2681), 0.647191011235955, tensor(1.9678)]
[tensor(-1.1051), 0.6921348314606741, tensor(2.3556)]
[tensor(-1.0896), 0.6921348314606741, tensor(2.3556)]
[tensor(-1.0896), 0.6921348314606741, tensor(2.3556)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0896), 0.6921348314606741, tensor(2.3556)]
[tensor(-1.0896), 0.6921348314606741, tensor(2.3556)]
[tensor(-1.0896), 0.6921348314606741, tensor(2.3556)]
[tensor(-1.0896), 0.698876404494382, tensor(2.3556)]
[tensor(-1.0896), 0.701123595505618, tensor(2.3556)]
[tensor(-1.0896), 0.701123595505618, tensor(2.3556)]
[tensor(-1.0896), 0.7033707865168539, tensor(2.3556)]
[tensor(-1.0896), 0.7033707865168539, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
[tensor(-1.0896), 0.7146067415730337, tensor(2.3556)]
early stopping at 27
[2023-01-19 08:34:10,796.796 dsw44922-6f76bf568-tbjcv:80282 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-50 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1893), 0.4449438202247191, tensor(0.0354)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3705), 0.647191011235955, tensor(1.8654)]
[tensor(-1.2069), 0.6786516853932584, tensor(2.1864)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1595), 0.6786516853932584, tensor(2.2001)]
[tensor(-1.1115), 0.6921348314606741, tensor(2.3492)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.698876404494382, tensor(2.3744)]
[tensor(-1.1115), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1115), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1115), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1115), 0.701123595505618, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
[tensor(-1.1115), 0.7078651685393258, tensor(2.3744)]
early stopping at 35
[2023-01-19 08:53:02,589.589 dsw44922-6f76bf568-tbjcv:80335 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4695), 0.24943820224719102, 0.0]
[tensor(-1.4768), 0.5955056179775281, tensor(1.5007)]
[tensor(-1.2877), 0.651685393258427, tensor(1.9707)]
[tensor(-1.2869), 0.651685393258427, tensor(1.9707)]
[tensor(-1.2211), 0.6696629213483146, tensor(2.1272)]
[tensor(-1.2211), 0.6696629213483146, tensor(2.1272)]
[tensor(-1.2211), 0.6921348314606741, tensor(2.1704)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2211), 0.6921348314606741, tensor(2.1704)]
[tensor(-1.2211), 0.6921348314606741, tensor(2.1704)]
[tensor(-1.2211), 0.6943820224719102, tensor(2.1704)]
[2023-01-19 08:58:23,985.985 dsw44922-6f76bf568-tbjcv:80369 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0121), 0.45393258426966293, tensor(0.2576)]
[tensor(-1.3800), 0.6067415730337079, tensor(1.6537)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2312), 0.6696629213483146, tensor(2.1171)]
[tensor(-1.2312), 0.6808988764044944, tensor(2.1283)]
[tensor(-1.2312), 0.6808988764044944, tensor(2.1315)]
[tensor(-1.2312), 0.6808988764044944, tensor(2.1315)]
[tensor(-1.2312), 0.6808988764044944, tensor(2.1315)]
[tensor(-1.2312), 0.6876404494382022, tensor(2.1315)]
[tensor(-1.2312), 0.6876404494382022, tensor(2.1315)]
[tensor(-1.2312), 0.6876404494382022, tensor(2.1315)]
[2023-01-19 09:03:24,110.110 dsw44922-6f76bf568-tbjcv:80403 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0556), 0.4337078651685393, tensor(0.1129)]
[tensor(-1.4724), 0.6067415730337079, tensor(1.5613)]
[tensor(-1.4188), 0.6202247191011236, tensor(1.6823)]
[tensor(-1.2782), 0.6359550561797753, tensor(1.9016)]
[tensor(-1.2782), 0.6674157303370787, tensor(1.9987)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2782), 0.6674157303370787, tensor(1.9987)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.6898876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
[tensor(-1.2782), 0.698876404494382, tensor(2.1628)]
early stopping at 28
[2023-01-19 09:17:52,698.698 dsw44922-6f76bf568-tbjcv:80449 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5805), 0.35280898876404493, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0733), 0.41797752808988764, tensor(0.0166)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5196), 0.5348314606741573, tensor(1.1545)]
[tensor(-1.3846), 0.597752808988764, tensor(1.6042)]
[tensor(-1.3846), 0.597752808988764, tensor(1.6042)]
[tensor(-1.3259), 0.6449438202247191, tensor(1.8989)]
[tensor(-1.3259), 0.6449438202247191, tensor(1.8989)]
[tensor(-1.3259), 0.6449438202247191, tensor(1.8989)]
[tensor(-1.3259), 0.6539325842696629, tensor(1.8989)]
[tensor(-1.3259), 0.6539325842696629, tensor(1.8989)]
[tensor(-1.3259), 0.6629213483146067, tensor(1.8989)]
[tensor(-1.3259), 0.6629213483146067, tensor(1.8989)]
[tensor(-1.3259), 0.6629213483146067, tensor(1.8989)]
[tensor(-1.3259), 0.6651685393258427, tensor(1.8989)]
[tensor(-1.3259), 0.6651685393258427, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
[tensor(-1.3259), 0.6674157303370787, tensor(1.8989)]
early stopping at 26
[2023-01-19 09:30:47,998.998 dsw44922-6f76bf568-tbjcv:80493 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1182), 0.40898876404494383, 0.0]
[tensor(-1.3407), 0.6359550561797753, tensor(1.8390)]
[tensor(-1.2191), 0.6584269662921348, tensor(2.0730)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2191), 0.6674157303370787, tensor(2.0766)]
[tensor(-1.2114), 0.6943820224719102, tensor(2.2605)]
[tensor(-1.2114), 0.6943820224719102, tensor(2.2605)]
[tensor(-1.2114), 0.6943820224719102, tensor(2.2605)]
[tensor(-1.2114), 0.6943820224719102, tensor(2.2605)]
[tensor(-1.2114), 0.7033707865168539, tensor(2.2605)]
[tensor(-1.2114), 0.7033707865168539, tensor(2.2605)]
[2023-01-19 09:35:49,373.373 dsw44922-6f76bf568-tbjcv:80528 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6714), 0.5528089887640449, tensor(1.0926)]
[tensor(-1.1923), 0.6651685393258427, tensor(2.1335)]
[tensor(-1.1748), 0.6651685393258427, tensor(2.1335)]
[tensor(-1.1748), 0.6696629213483146, tensor(2.1716)]
[tensor(-1.1748), 0.6966292134831461, tensor(2.3057)]
[tensor(-1.1748), 0.701123595505618, tensor(2.3057)]
[tensor(-1.1748), 0.7213483146067415, tensor(2.3876)]
[tensor(-1.1748), 0.7393258426966293, tensor(2.5059)]
[tensor(-1.1748), 0.7393258426966293, tensor(2.5059)]
[tensor(-1.1748), 0.7393258426966293, tensor(2.5059)]
[2023-01-19 09:40:50,404.404 dsw44922-6f76bf568-tbjcv:80561 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8284), 0.49887640449438203, tensor(0.6660)]
[tensor(-1.3805), 0.6202247191011236, tensor(1.7206)]
[tensor(-1.3378), 0.6292134831460674, tensor(1.8083)]
[tensor(-1.1798), 0.6606741573033708, tensor(2.1236)]
[tensor(-1.1796), 0.6764044943820224, tensor(2.2024)]
[tensor(-1.1796), 0.6764044943820224, tensor(2.2024)]
[tensor(-1.1796), 0.6786516853932584, tensor(2.2024)]
[tensor(-1.1770), 0.6966292134831461, tensor(2.3061)]
[tensor(-1.1770), 0.6966292134831461, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
[tensor(-1.1770), 0.701123595505618, tensor(2.3061)]
early stopping at 20
[2023-01-19 09:51:34,198.198 dsw44922-6f76bf568-tbjcv:80602 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5115), 0.5910112359550562, tensor(1.4436)]
[tensor(-1.2598), 0.6404494382022472, tensor(1.9425)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1685), 0.651685393258427, tensor(2.0899)]
[tensor(-1.1002), 0.6943820224719102, tensor(2.3717)]
[tensor(-1.1002), 0.6943820224719102, tensor(2.3717)]
[tensor(-1.1002), 0.6943820224719102, tensor(2.3717)]
[tensor(-1.1002), 0.6943820224719102, tensor(2.3717)]
[tensor(-1.1002), 0.6943820224719102, tensor(2.3717)]
[tensor(-1.1002), 0.6966292134831461, tensor(2.3717)]
[tensor(-1.1002), 0.6966292134831461, tensor(2.3717)]
[tensor(-1.1002), 0.6966292134831461, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.701123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
[tensor(-1.1002), 0.7101123595505618, tensor(2.3717)]
early stopping at 36
[2023-01-19 10:09:52,159.159 dsw44922-6f76bf568-tbjcv:80654 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6702), 0.1348314606741573, 0.0]
[tensor(-1.9058), 0.4898876404494382, tensor(0.5436)]
[tensor(-1.4574), 0.6, tensor(1.5426)]
[tensor(-1.2714), 0.6382022471910113, tensor(1.9196)]
[tensor(-1.1479), 0.6674157303370787, tensor(2.1892)]
[tensor(-1.1479), 0.6674157303370787, tensor(2.1892)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1460), 0.6719101123595506, tensor(2.2135)]
[tensor(-1.1460), 0.6719101123595506, tensor(2.2135)]
[tensor(-1.1460), 0.6876404494382022, tensor(2.2286)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1460), 0.6876404494382022, tensor(2.2286)]
[2023-01-19 10:14:52,271.271 dsw44922-6f76bf568-tbjcv:80687 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4677), 0.2606741573033708, 0.0]
[tensor(-1.5704), 0.5707865168539326, tensor(1.2835)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2817), 0.6404494382022472, tensor(1.9206)]
[tensor(-1.2102), 0.6539325842696629, tensor(2.0594)]
[tensor(-1.1611), 0.6651685393258427, tensor(2.1647)]
[tensor(-1.1611), 0.6696629213483146, tensor(2.1647)]
[tensor(-1.1611), 0.6696629213483146, tensor(2.1647)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1611), 0.6786516853932584, tensor(2.1704)]
[tensor(-1.1611), 0.6786516853932584, tensor(2.1704)]
[tensor(-1.1611), 0.6853932584269663, tensor(2.1704)]
[2023-01-19 10:19:51,920.920 dsw44922-6f76bf568-tbjcv:80722 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4140), 0.30786516853932583, 0.0]
[tensor(-1.8535), 0.5146067415730337, tensor(0.7196)]
[tensor(-1.7977), 0.5235955056179775, tensor(0.8202)]
[tensor(-1.4361), 0.6292134831460674, tensor(1.7099)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2258), 0.6584269662921348, tensor(2.0663)]
[tensor(-1.1610), 0.6584269662921348, tensor(2.1086)]
[tensor(-1.1410), 0.6831460674157304, tensor(2.2748)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
[tensor(-1.1378), 0.6853932584269663, tensor(2.2892)]
early stopping at 18
[2023-01-19 10:28:50,805.805 dsw44922-6f76bf568-tbjcv:80761 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3758), 0.39101123595505616, 0.0]
[tensor(-1.8697), 0.48314606741573035, tensor(0.5461)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2863), 0.6426966292134831, tensor(1.9272)]
[tensor(-1.2093), 0.647191011235955, tensor(2.0267)]
[tensor(-1.2093), 0.6674157303370787, tensor(2.1178)]
[tensor(-1.2084), 0.6719101123595506, tensor(2.1512)]
[tensor(-1.2084), 0.6719101123595506, tensor(2.1512)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2059), 0.6876404494382022, tensor(2.2323)]
[tensor(-1.2041), 0.698876404494382, tensor(2.2903)]
[tensor(-1.2041), 0.698876404494382, tensor(2.2903)]
[tensor(-1.2041), 0.698876404494382, tensor(2.2903)]
[tensor(-1.2041), 0.7056179775280899, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7101123595505618, tensor(2.2903)]
[tensor(-1.2041), 0.7123595505617978, tensor(2.2903)]
[tensor(-1.2041), 0.7123595505617978, tensor(2.2903)]
[tensor(-1.2041), 0.7123595505617978, tensor(2.2903)]
[tensor(-1.2041), 0.7123595505617978, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
[tensor(-1.2041), 0.7235955056179775, tensor(2.2903)]
early stopping at 50
[2023-01-19 10:53:33,698.698 dsw44922-6f76bf568-tbjcv:80826 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5669), 0.25617977528089886, 0.0]
[tensor(-1.7576), 0.5325842696629214, tensor(0.9054)]
[tensor(-1.3839), 0.6269662921348315, tensor(1.7510)]
[tensor(-1.2508), 0.6629213483146067, tensor(2.0638)]
[tensor(-1.1578), 0.6808988764044944, tensor(2.2467)]
[tensor(-1.1578), 0.6808988764044944, tensor(2.2467)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1373), 0.6876404494382022, tensor(2.3009)]
[tensor(-1.1219), 0.701123595505618, tensor(2.3837)]
[tensor(-1.1219), 0.701123595505618, tensor(2.3837)]
[tensor(-1.1219), 0.701123595505618, tensor(2.3837)]
[2023-01-19 10:58:51,476.476 dsw44922-6f76bf568-tbjcv:80861 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2187), 0.42247191011235957, 0.0]
[tensor(-1.4276), 0.6269662921348315, tensor(1.7072)]
[tensor(-1.2410), 0.6606741573033708, tensor(2.0624)]
[tensor(-1.1880), 0.6719101123595506, tensor(2.1715)]
[tensor(-1.1222), 0.6966292134831461, tensor(2.3609)]
[tensor(-1.1222), 0.6966292134831461, tensor(2.3609)]
[tensor(-1.1222), 0.6966292134831461, tensor(2.3609)]
[tensor(-1.1222), 0.6966292134831461, tensor(2.3609)]
[tensor(-1.1222), 0.7123595505617978, tensor(2.4144)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1222), 0.7123595505617978, tensor(2.4144)]
[2023-01-19 11:04:01,121.121 dsw44922-6f76bf568-tbjcv:80895 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2355), 0.4134831460674157, 0.0]
[tensor(-1.7135), 0.5483146067415731, tensor(1.0281)]
[tensor(-1.6623), 0.5573033707865168, tensor(1.1242)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4250), 0.6067415730337079, tensor(1.6087)]
[tensor(-1.2392), 0.6426966292134831, tensor(1.9743)]
[tensor(-1.1735), 0.6764044943820224, tensor(2.2085)]
[tensor(-1.1697), 0.6764044943820224, tensor(2.2085)]
[tensor(-1.1640), 0.6898876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.6898876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.6898876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.6966292134831461, tensor(2.2855)]
[tensor(-1.1640), 0.6966292134831461, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2855)]
[tensor(-1.1640), 0.701123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
[tensor(-1.1640), 0.7101123595505618, tensor(2.2855)]
early stopping at 34
[2023-01-19 11:20:45,990.990 dsw44922-6f76bf568-tbjcv:80944 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9926), 0.4853932584269663, tensor(0.4344)]
[tensor(-1.6042), 0.5775280898876405, tensor(1.2834)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1942), 0.6629213483146067, tensor(2.1204)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1607), 0.6719101123595506, tensor(2.1988)]
[tensor(-1.1425), 0.6831460674157304, tensor(2.2733)]
[tensor(-1.1425), 0.6831460674157304, tensor(2.2733)]
[tensor(-1.1425), 0.6921348314606741, tensor(2.2996)]
[tensor(-1.1425), 0.6921348314606741, tensor(2.2996)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.701123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7033707865168539, tensor(2.3324)]
[tensor(-1.1425), 0.7101123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.7101123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.7101123595505618, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
[tensor(-1.1425), 0.7123595505617978, tensor(2.3324)]
early stopping at 44
[2023-01-19 11:42:25,257.257 dsw44922-6f76bf568-tbjcv:81000 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5246), 0.2651685393258427, 0.0]
[tensor(-1.8637), 0.49887640449438203, tensor(0.6307)]
[tensor(-1.3750), 0.6179775280898876, tensor(1.7149)]
[tensor(-1.2328), 0.6629213483146067, tensor(2.0818)]
[tensor(-1.1805), 0.6629213483146067, tensor(2.1116)]
[tensor(-1.1805), 0.6629213483146067, tensor(2.1116)]
[tensor(-1.1640), 0.6719101123595506, tensor(2.1956)]
[tensor(-1.1640), 0.6719101123595506, tensor(2.1956)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1640), 0.6808988764044944, tensor(2.1956)]
[tensor(-1.1640), 0.6831460674157304, tensor(2.1956)]
[2023-01-19 11:47:56,648.648 dsw44922-6f76bf568-tbjcv:81036 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4029), 0.34606741573033706, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5324), 0.6089887640449438, tensor(1.5126)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2644), 0.6561797752808989, tensor(2.0165)]
[tensor(-1.1832), 0.6831460674157304, tensor(2.2325)]
[tensor(-1.1483), 0.6831460674157304, tensor(2.2325)]
[tensor(-1.1449), 0.6831460674157304, tensor(2.2483)]
[tensor(-1.1449), 0.6943820224719102, tensor(2.3016)]
[tensor(-1.1449), 0.6943820224719102, tensor(2.3016)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1449), 0.6943820224719102, tensor(2.3016)]
[tensor(-1.1449), 0.6943820224719102, tensor(2.3016)]
[2023-01-19 11:53:23,416.416 dsw44922-6f76bf568-tbjcv:81070 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4533), 0.2696629213483146, 0.0]
[tensor(-2.1923), 0.39775280898876403, 0.0]
[tensor(-1.7172), 0.49887640449438203, tensor(0.7772)]
[tensor(-1.2962), 0.6426966292134831, tensor(1.9173)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1596), 0.6606741573033708, tensor(2.1437)]
[tensor(-1.1572), 0.6696629213483146, tensor(2.1911)]
[tensor(-1.1505), 0.6719101123595506, tensor(2.2091)]
[tensor(-1.1505), 0.6786516853932584, tensor(2.2091)]
[tensor(-1.1505), 0.6786516853932584, tensor(2.2091)]
[tensor(-1.1505), 0.6786516853932584, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2091)]
early stopping at 21
[2023-01-19 12:04:44,625.625 dsw44922-6f76bf568-tbjcv:81112 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.5411), 0.34606741573033706, 0.0]
[tensor(-1.5882), 0.5662921348314607, tensor(1.2433)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3439), 0.6112359550561798, tensor(1.7122)]
[tensor(-1.2407), 0.6539325842696629, tensor(2.0289)]
[tensor(-1.1708), 0.6674157303370787, tensor(2.1663)]
[tensor(-1.1708), 0.6741573033707865, tensor(2.1696)]
[tensor(-1.1708), 0.6808988764044944, tensor(2.2247)]
[tensor(-1.1708), 0.6808988764044944, tensor(2.2247)]
[tensor(-1.1708), 0.6831460674157304, tensor(2.2247)]
[tensor(-1.1708), 0.6943820224719102, tensor(2.2247)]
[tensor(-1.1708), 0.6943820224719102, tensor(2.2247)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1708), 0.6943820224719102, tensor(2.2247)]
[tensor(-1.1708), 0.6943820224719102, tensor(2.2247)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
[tensor(-1.1708), 0.7056179775280899, tensor(2.2286)]
early stopping at 24
[2023-01-19 12:17:29,003.003 dsw44922-6f76bf568-tbjcv:81157 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4205), 0.36629213483146067, 0.0]
[tensor(-1.6627), 0.5550561797752809, tensor(1.1126)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3558), 0.6449438202247191, tensor(1.8689)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1928), 0.6584269662921348, tensor(2.0994)]
[tensor(-1.1684), 0.6674157303370787, tensor(2.1687)]
[tensor(-1.1648), 0.6808988764044944, tensor(2.2397)]
[tensor(-1.1256), 0.6808988764044944, tensor(2.2789)]
[tensor(-1.1256), 0.6808988764044944, tensor(2.2789)]
[tensor(-1.1256), 0.6921348314606741, tensor(2.2869)]
[tensor(-1.1256), 0.6921348314606741, tensor(2.2869)]
[2023-01-19 12:23:00,271.271 dsw44922-6f76bf568-tbjcv:81192 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1239), 0.449438202247191, tensor(0.1233)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4100), 0.6359550561797753, tensor(1.7698)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1744), 0.6741573033707865, tensor(2.1964)]
[tensor(-1.1732), 0.6741573033707865, tensor(2.1964)]
[tensor(-1.0979), 0.6876404494382022, tensor(2.3403)]
[tensor(-1.0979), 0.6876404494382022, tensor(2.3403)]
[tensor(-1.0979), 0.6876404494382022, tensor(2.3403)]
[tensor(-1.0979), 0.7033707865168539, tensor(2.3748)]
[tensor(-1.0979), 0.7033707865168539, tensor(2.3748)]
[tensor(-1.0979), 0.7033707865168539, tensor(2.3748)]
[2023-01-19 12:28:26,665.665 dsw44922-6f76bf568-tbjcv:81225 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9732), 0.4584269662921348, tensor(0.3189)]
[tensor(-1.7261), 0.5101123595505618, tensor(0.8245)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4302), 0.6179775280898876, tensor(1.6597)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2471), 0.6741573033707865, tensor(2.1236)]
[tensor(-1.1701), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
[tensor(-1.1623), 0.7168539325842697, tensor(2.3018)]
early stopping at 31
[2023-01-19 12:45:03,588.588 dsw44922-6f76bf568-tbjcv:81276 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1667), 0.46292134831460674, tensor(0.1479)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3380), 0.6449438202247191, tensor(1.8868)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1940), 0.6741573033707865, tensor(2.1768)]
[tensor(-1.1891), 0.6741573033707865, tensor(2.1768)]
[tensor(-1.1102), 0.6741573033707865, tensor(2.2269)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.6966292134831461, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7078651685393258, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
[tensor(-1.1102), 0.7101123595505618, tensor(2.3645)]
early stopping at 34
[2023-01-19 13:03:04,997.997 dsw44922-6f76bf568-tbjcv:81330 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4022), 0.2966292134831461, 0.0]
[tensor(-1.4530), 0.6224719101123596, tensor(1.6593)]
[tensor(-1.2708), 0.651685393258427, tensor(1.9876)]
[tensor(-1.2654), 0.6584269662921348, tensor(2.0268)]
[tensor(-1.2129), 0.6853932584269663, tensor(2.2141)]
[tensor(-1.2129), 0.6853932584269663, tensor(2.2141)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2129), 0.6853932584269663, tensor(2.2141)]
[tensor(-1.2129), 0.6853932584269663, tensor(2.2141)]
[tensor(-1.2129), 0.6853932584269663, tensor(2.2141)]
[tensor(-1.2129), 0.6853932584269663, tensor(2.2141)]
early stopping at 10
[2023-01-19 13:08:08,229.229 dsw44922-6f76bf568-tbjcv:81364 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9630), 0.43595505617977526, tensor(0.2168)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3101), 0.6449438202247191, tensor(1.9146)]
[tensor(-1.2031), 0.6494382022471911, tensor(2.0440)]
[tensor(-1.2031), 0.6674157303370787, tensor(2.0440)]
[tensor(-1.2031), 0.6943820224719102, tensor(2.1898)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2031), 0.6943820224719102, tensor(2.1898)]
[tensor(-1.2031), 0.698876404494382, tensor(2.1898)]
[tensor(-1.2031), 0.698876404494382, tensor(2.1898)]
[tensor(-1.2031), 0.7078651685393258, tensor(2.1898)]
[tensor(-1.2031), 0.7078651685393258, tensor(2.1898)]
[2023-01-19 13:13:14,821.821 dsw44922-6f76bf568-tbjcv:81398 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9864), 0.4606741573033708, tensor(0.3170)]
[tensor(-1.4632), 0.6089887640449438, tensor(1.5817)]
[tensor(-1.4110), 0.6202247191011236, tensor(1.6901)]
[tensor(-1.2880), 0.6449438202247191, tensor(1.9367)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2880), 0.6449438202247191, tensor(1.9367)]
[tensor(-1.2880), 0.6741573033707865, tensor(2.0728)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
[tensor(-1.2880), 0.6876404494382022, tensor(2.1117)]
early stopping at 17
[2023-01-19 13:21:47,639.639 dsw44922-6f76bf568-tbjcv:81437 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5901), 0.25617977528089886, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1001), 0.3955056179775281, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.5480), 0.5595505617977528, tensor(1.2498)]
[tensor(-1.4216), 0.597752808988764, tensor(1.5672)]
[tensor(-1.4216), 0.6134831460674157, tensor(1.6225)]
[tensor(-1.3528), 0.6426966292134831, tensor(1.8607)]
[tensor(-1.3528), 0.6539325842696629, tensor(1.9031)]
[tensor(-1.3528), 0.6629213483146067, tensor(1.9031)]
[tensor(-1.3528), 0.6629213483146067, tensor(1.9031)]
[tensor(-1.3528), 0.6629213483146067, tensor(1.9031)]
[tensor(-1.3528), 0.6651685393258427, tensor(1.9031)]
[tensor(-1.3528), 0.6651685393258427, tensor(1.9031)]
[tensor(-1.3528), 0.6741573033707865, tensor(1.9031)]
[tensor(-1.3528), 0.6741573033707865, tensor(1.9031)]
[tensor(-1.3528), 0.6741573033707865, tensor(1.9031)]
[tensor(-1.3528), 0.6741573033707865, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
[tensor(-1.3528), 0.6876404494382022, tensor(1.9031)]
early stopping at 27
[2023-01-19 13:36:07,304.304 dsw44922-6f76bf568-tbjcv:81483 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1327), 0.4044943820224719, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4274), 0.6337078651685393, tensor(1.7412)]
[tensor(-1.2605), 0.6337078651685393, tensor(1.9080)]
[tensor(-1.2005), 0.6696629213483146, tensor(2.1478)]
[tensor(-1.2005), 0.701123595505618, tensor(2.2383)]
[tensor(-1.2005), 0.701123595505618, tensor(2.2383)]
[tensor(-1.2005), 0.701123595505618, tensor(2.2383)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2005), 0.701123595505618, tensor(2.2383)]
[tensor(-1.2005), 0.7168539325842697, tensor(2.2609)]
[tensor(-1.2005), 0.7168539325842697, tensor(2.2609)]
[2023-01-19 13:41:16,719.719 dsw44922-6f76bf568-tbjcv:81517 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6669), 0.5258426966292135, tensor(0.9623)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2331), 0.6674157303370787, tensor(2.1040)]
[tensor(-1.1744), 0.6674157303370787, tensor(2.1515)]
[tensor(-1.1732), 0.6921348314606741, tensor(2.2875)]
[tensor(-1.1732), 0.7056179775280899, tensor(2.3271)]
[tensor(-1.1732), 0.7078651685393258, tensor(2.3271)]
[tensor(-1.1732), 0.7078651685393258, tensor(2.3271)]
[tensor(-1.1732), 0.7078651685393258, tensor(2.3271)]
[tensor(-1.1732), 0.7078651685393258, tensor(2.3271)]
[tensor(-1.1732), 0.7123595505617978, tensor(2.3271)]
[2023-01-19 13:46:16,024.024 dsw44922-6f76bf568-tbjcv:81552 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6326), 0.5325842696629214, tensor(1.0303)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2940), 0.6359550561797753, tensor(1.8858)]
[tensor(-1.2495), 0.647191011235955, tensor(1.9865)]
[tensor(-1.2495), 0.6651685393258427, tensor(2.0729)]
[tensor(-1.1686), 0.6696629213483146, tensor(2.1797)]
[tensor(-1.1686), 0.6786516853932584, tensor(2.1797)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
[tensor(-1.1686), 0.7033707865168539, tensor(2.3217)]
early stopping at 17
[2023-01-19 13:54:58,933.933 dsw44922-6f76bf568-tbjcv:81591 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5319), 0.5662921348314607, tensor(1.2995)]
[tensor(-1.2718), 0.6404494382022472, tensor(1.9304)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1758), 0.6696629213483146, tensor(2.1725)]
[tensor(-1.0745), 0.6921348314606741, tensor(2.3862)]
[tensor(-1.0745), 0.6921348314606741, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
[tensor(-1.0745), 0.701123595505618, tensor(2.3862)]
early stopping at 16
[2023-01-19 14:02:54,910.910 dsw44922-6f76bf568-tbjcv:81637 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6656), 0.13707865168539327, 0.0]
[tensor(-1.8908), 0.4966292134831461, tensor(0.5923)]
[tensor(-1.4696), 0.5955056179775281, tensor(1.5080)]
[tensor(-1.2773), 0.6292134831460674, tensor(1.8688)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1665), 0.6674157303370787, tensor(2.1706)]
[tensor(-1.1665), 0.6674157303370787, tensor(2.1706)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1298), 0.6831460674157304, tensor(2.2859)]
[tensor(-1.1298), 0.6831460674157304, tensor(2.2859)]
[tensor(-1.1298), 0.6943820224719102, tensor(2.2874)]
[tensor(-1.1298), 0.6966292134831461, tensor(2.2874)]
[2023-01-19 14:08:03,964.964 dsw44922-6f76bf568-tbjcv:81670 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3968), 0.35280898876404493, 0.0]
[tensor(-1.5522), 0.5640449438202247, tensor(1.2681)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2891), 0.6606741573033708, tensor(2.0143)]
[tensor(-1.2205), 0.6606741573033708, tensor(2.0829)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1816), 0.6629213483146067, tensor(2.1330)]
[tensor(-1.1816), 0.6651685393258427, tensor(2.1330)]
[tensor(-1.1816), 0.6764044943820224, tensor(2.1944)]
[tensor(-1.1816), 0.6764044943820224, tensor(2.1944)]
[tensor(-1.1816), 0.6764044943820224, tensor(2.1944)]
[tensor(-1.1816), 0.698876404494382, tensor(2.2090)]
[2023-01-19 14:13:02,908.908 dsw44922-6f76bf568-tbjcv:81704 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3845), 0.3258426966292135, 0.0]
[tensor(-1.8317), 0.5303370786516854, tensor(0.8200)]
[tensor(-1.7773), 0.5393258426966292, tensor(0.9193)]
[tensor(-1.4332), 0.6337078651685393, tensor(1.7354)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2204), 0.651685393258427, tensor(2.0380)]
[tensor(-1.2204), 0.6561797752808989, tensor(2.0521)]
[tensor(-1.1758), 0.6674157303370787, tensor(2.1612)]
[tensor(-1.1758), 0.6764044943820224, tensor(2.1979)]
[tensor(-1.1758), 0.6898876404494382, tensor(2.2549)]
[tensor(-1.1758), 0.6898876404494382, tensor(2.2549)]
[tensor(-1.1758), 0.6898876404494382, tensor(2.2549)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1758), 0.6921348314606741, tensor(2.2549)]
[tensor(-1.1758), 0.6921348314606741, tensor(2.2549)]
[tensor(-1.1758), 0.6921348314606741, tensor(2.2549)]
[tensor(-1.1758), 0.6921348314606741, tensor(2.2549)]
[tensor(-1.1758), 0.6921348314606741, tensor(2.2549)]
[tensor(-1.1758), 0.6921348314606741, tensor(2.2549)]
[tensor(-1.1758), 0.6943820224719102, tensor(2.2549)]
[tensor(-1.1758), 0.6943820224719102, tensor(2.2549)]
[tensor(-1.1758), 0.6943820224719102, tensor(2.2549)]
[tensor(-1.1758), 0.6943820224719102, tensor(2.2549)]
[tensor(-1.1758), 0.6943820224719102, tensor(2.2549)]
[tensor(-1.1758), 0.698876404494382, tensor(2.2549)]
[tensor(-1.1758), 0.698876404494382, tensor(2.2549)]
[tensor(-1.1758), 0.698876404494382, tensor(2.2549)]
[tensor(-1.1758), 0.7056179775280899, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
[tensor(-1.1758), 0.7078651685393258, tensor(2.2549)]
early stopping at 37
[2023-01-19 14:31:25,686.686 dsw44922-6f76bf568-tbjcv:81758 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3198), 0.39775280898876403, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8283), 0.503370786516854, tensor(0.6885)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2822), 0.6337078651685393, tensor(1.8864)]
[tensor(-1.2206), 0.6651685393258427, tensor(2.1053)]
[tensor(-1.2206), 0.6651685393258427, tensor(2.1053)]
[tensor(-1.2154), 0.6651685393258427, tensor(2.1053)]
[tensor(-1.2154), 0.6651685393258427, tensor(2.1053)]
[tensor(-1.2154), 0.6876404494382022, tensor(2.1986)]
[tensor(-1.2154), 0.6876404494382022, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.6943820224719102, tensor(2.1986)]
[tensor(-1.2154), 0.7033707865168539, tensor(2.1986)]
[tensor(-1.2154), 0.7033707865168539, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
[tensor(-1.2154), 0.7078651685393258, tensor(2.1986)]
early stopping at 39
[2023-01-19 14:51:38,928.928 dsw44922-6f76bf568-tbjcv:81820 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5972), 0.20898876404494382, 0.0]
[tensor(-1.8123), 0.5191011235955056, tensor(0.7832)]
[tensor(-1.4194), 0.5887640449438202, tensor(1.5244)]
[tensor(-1.2767), 0.647191011235955, tensor(1.9592)]
[tensor(-1.1830), 0.6808988764044944, tensor(2.2215)]
[tensor(-1.1830), 0.6808988764044944, tensor(2.2215)]
[tensor(-1.1830), 0.6921348314606741, tensor(2.2756)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1387), 0.6966292134831461, tensor(2.3445)]
[tensor(-1.1387), 0.7078651685393258, tensor(2.3623)]
[tensor(-1.1387), 0.7078651685393258, tensor(2.3623)]
[2023-01-19 14:56:54,707.707 dsw44922-6f76bf568-tbjcv:81855 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1524), 0.4449438202247191, tensor(0.0723)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3917), 0.647191011235955, tensor(1.8443)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2394), 0.6539325842696629, tensor(2.0303)]
[tensor(-1.1700), 0.6741573033707865, tensor(2.2008)]
[tensor(-1.1532), 0.6808988764044944, tensor(2.2513)]
[tensor(-1.1532), 0.6831460674157304, tensor(2.2553)]
[tensor(-1.1268), 0.6921348314606741, tensor(2.3339)]
[tensor(-1.1268), 0.7101123595505618, tensor(2.4005)]
[tensor(-1.1268), 0.7101123595505618, tensor(2.4005)]
[tensor(-1.1268), 0.7101123595505618, tensor(2.4005)]
[2023-01-19 15:02:05,528.528 dsw44922-6f76bf568-tbjcv:81889 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2838), 0.4, 0.0]
[tensor(-1.7355), 0.550561797752809, tensor(1.0173)]
[tensor(-1.6856), 0.5573033707865168, tensor(1.1009)]
[tensor(-1.4312), 0.604494382022472, tensor(1.5913)]
[tensor(-1.2838), 0.6426966292134831, tensor(1.9297)]
[tensor(-1.1763), 0.6674157303370787, tensor(2.1608)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1763), 0.6674157303370787, tensor(2.1608)]
[tensor(-1.1623), 0.6674157303370787, tensor(2.1608)]
[tensor(-1.1623), 0.6876404494382022, tensor(2.2256)]
[tensor(-1.1623), 0.6876404494382022, tensor(2.2256)]
[tensor(-1.1623), 0.6876404494382022, tensor(2.2256)]
[tensor(-1.1623), 0.6898876404494382, tensor(2.2256)]
[tensor(-1.1623), 0.6943820224719102, tensor(2.2256)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.2256)]
[tensor(-1.1623), 0.6966292134831461, tensor(2.2256)]
[tensor(-1.1623), 0.698876404494382, tensor(2.2256)]
[tensor(-1.1623), 0.698876404494382, tensor(2.2256)]
[tensor(-1.1623), 0.7056179775280899, tensor(2.2256)]
[tensor(-1.1623), 0.7056179775280899, tensor(2.2256)]
[tensor(-1.1623), 0.7056179775280899, tensor(2.2256)]
[tensor(-1.1623), 0.7056179775280899, tensor(2.2256)]
[tensor(-1.1623), 0.7056179775280899, tensor(2.2256)]
[tensor(-1.1623), 0.7056179775280899, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
[tensor(-1.1623), 0.7101123595505618, tensor(2.2256)]
early stopping at 41
[2023-01-19 15:22:27,825.825 dsw44922-6f76bf568-tbjcv:81994 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9975), 0.4898876404494382, tensor(0.4519)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6271), 0.5707865168539326, tensor(1.2268)]
[tensor(-1.2448), 0.6696629213483146, tensor(2.1035)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1723), 0.6741573033707865, tensor(2.1985)]
[tensor(-1.1432), 0.6741573033707865, tensor(2.2276)]
[tensor(-1.1432), 0.6808988764044944, tensor(2.2484)]
[tensor(-1.1432), 0.6943820224719102, tensor(2.3125)]
[tensor(-1.1432), 0.6943820224719102, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
[tensor(-1.1432), 0.6966292134831461, tensor(2.3125)]
early stopping at 19
[2023-01-19 15:31:51,311.311 dsw44922-6f76bf568-tbjcv:82032 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5235), 0.2696629213483146, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9723), 0.44719101123595506, tensor(0.2636)]
[tensor(-1.4439), 0.597752808988764, tensor(1.5449)]
[tensor(-1.2843), 0.6449438202247191, tensor(1.9404)]
[tensor(-1.2349), 0.6584269662921348, tensor(2.0572)]
[tensor(-1.2349), 0.6584269662921348, tensor(2.0572)]
[tensor(-1.2231), 0.6629213483146067, tensor(2.0916)]
[tensor(-1.2231), 0.6629213483146067, tensor(2.0916)]
[tensor(-1.2231), 0.6629213483146067, tensor(2.0916)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2231), 0.6674157303370787, tensor(2.0916)]
[2023-01-19 15:37:18,614.614 dsw44922-6f76bf568-tbjcv:82065 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4192), 0.31235955056179776, 0.0]
[tensor(-1.5152), 0.5955056179775281, tensor(1.4624)]
[tensor(-1.2908), 0.6314606741573033, tensor(1.8665)]
[tensor(-1.2027), 0.6404494382022472, tensor(1.9996)]
[tensor(-1.2027), 0.6629213483146067, tensor(2.1102)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1894), 0.6651685393258427, tensor(2.1364)]
[tensor(-1.1894), 0.6719101123595506, tensor(2.1364)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1894), 0.6853932584269663, tensor(2.1364)]
[tensor(-1.1894), 0.6853932584269663, tensor(2.1364)]
[tensor(-1.1894), 0.6853932584269663, tensor(2.1364)]
[2023-01-19 15:42:40,831.831 dsw44922-6f76bf568-tbjcv:82099 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4822), 0.26292134831460673, 0.0]
[tensor(-2.2406), 0.38202247191011235, 0.0]
[tensor(-1.7380), 0.503370786516854, tensor(0.7788)]
[tensor(-1.2934), 0.6337078651685393, tensor(1.8751)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1569), 0.6674157303370787, tensor(2.1802)]
[tensor(-1.1555), 0.6674157303370787, tensor(2.1802)]
[tensor(-1.1555), 0.6674157303370787, tensor(2.1802)]
[tensor(-1.1555), 0.6674157303370787, tensor(2.1802)]
[tensor(-1.1555), 0.6764044943820224, tensor(2.2128)]
[tensor(-1.1555), 0.6786516853932584, tensor(2.2128)]
[tensor(-1.1555), 0.6808988764044944, tensor(2.2128)]
[tensor(-1.1555), 0.6831460674157304, tensor(2.2128)]
[tensor(-1.1555), 0.6853932584269663, tensor(2.2128)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1555), 0.6853932584269663, tensor(2.2128)]
[tensor(-1.1555), 0.6853932584269663, tensor(2.2128)]
[tensor(-1.1555), 0.6853932584269663, tensor(2.2128)]
[tensor(-1.1555), 0.6853932584269663, tensor(2.2128)]
[tensor(-1.1555), 0.6853932584269663, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
[tensor(-1.1555), 0.701123595505618, tensor(2.2128)]
early stopping at 30
[2023-01-19 15:58:50,500.500 dsw44922-6f76bf568-tbjcv:82151 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.5336), 0.34606741573033706, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5254), 0.5910112359550562, tensor(1.4296)]
[tensor(-1.3084), 0.647191011235955, tensor(1.9275)]
[tensor(-1.2497), 0.6674157303370787, tensor(2.0874)]
[tensor(-1.1598), 0.6719101123595506, tensor(2.1997)]
[tensor(-1.1598), 0.6719101123595506, tensor(2.1997)]
[tensor(-1.1598), 0.6719101123595506, tensor(2.1997)]
[tensor(-1.1598), 0.6764044943820224, tensor(2.1997)]
[tensor(-1.1598), 0.6808988764044944, tensor(2.1997)]
[tensor(-1.1598), 0.6808988764044944, tensor(2.1997)]
[tensor(-1.1598), 0.6808988764044944, tensor(2.1997)]
[tensor(-1.1598), 0.6808988764044944, tensor(2.1997)]
[tensor(-1.1598), 0.6943820224719102, tensor(2.1997)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1598), 0.6943820224719102, tensor(2.1997)]
[tensor(-1.1598), 0.6943820224719102, tensor(2.1997)]
[tensor(-1.1598), 0.6943820224719102, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
[tensor(-1.1598), 0.6966292134831461, tensor(2.1997)]
early stopping at 27
[2023-01-19 16:13:15,190.190 dsw44922-6f76bf568-tbjcv:82198 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mam_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4294), 0.36404494382022473, 0.0]
[tensor(-1.6761), 0.5573033707865168, tensor(1.1104)]
[tensor(-1.3627), 0.6359550561797753, tensor(1.8171)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2166), 0.6606741573033708, tensor(2.0868)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1380), 0.6943820224719102, tensor(2.3339)]
[tensor(-1.1380), 0.6943820224719102, tensor(2.3339)]
[tensor(-1.1380), 0.698876404494382, tensor(2.3452)]
[tensor(-1.1380), 0.698876404494382, tensor(2.3452)]
[tensor(-1.1380), 0.698876404494382, tensor(2.3452)]
[tensor(-1.1380), 0.698876404494382, tensor(2.3452)]
[2023-01-19 16:18:44,587.587 dsw44922-6f76bf568-tbjcv:82235 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1057), 0.45393258426966293, tensor(0.1640)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3729), 0.6292134831460674, tensor(1.7731)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2036), 0.6629213483146067, tensor(2.1110)]
[tensor(-1.1659), 0.6764044943820224, tensor(2.2161)]
[tensor(-1.1237), 0.698876404494382, tensor(2.3707)]
[tensor(-1.1008), 0.698876404494382, tensor(2.3711)]
[tensor(-1.1008), 0.698876404494382, tensor(2.3711)]
[tensor(-1.1008), 0.701123595505618, tensor(2.3711)]
[tensor(-1.1008), 0.701123595505618, tensor(2.3711)]
[tensor(-1.1008), 0.7033707865168539, tensor(2.3711)]
[2023-01-19 16:24:18,044.044 dsw44922-6f76bf568-tbjcv:82270 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'selection_head.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9965), 0.46292134831460674, tensor(0.3181)]
[tensor(-1.7456), 0.5168539325842697, tensor(0.8387)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4615), 0.6, tensor(1.5385)]
[tensor(-1.2191), 0.6764044943820224, tensor(2.1629)]
[tensor(-1.1478), 0.6943820224719102, tensor(2.3241)]
[tensor(-1.1397), 0.6943820224719102, tensor(2.3241)]
[tensor(-1.1397), 0.6943820224719102, tensor(2.3241)]
[tensor(-1.1397), 0.6943820224719102, tensor(2.3241)]
[tensor(-1.1397), 0.6943820224719102, tensor(2.3241)]
[tensor(-1.1397), 0.6943820224719102, tensor(2.3241)]
[tensor(-1.1397), 0.6943820224719102, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
[tensor(-1.1397), 0.7033707865168539, tensor(2.3241)]
early stopping at 24
[2023-01-19 16:37:40,396.396 dsw44922-6f76bf568-tbjcv:82315 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.2-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.2-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2151), 0.4404494382022472, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3920), 0.6314606741573033, tensor(1.7653)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2663), 0.6494382022471911, tensor(1.9809)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2018), 0.6741573033707865, tensor(2.1690)]
[tensor(-1.1606), 0.6853932584269663, tensor(2.2664)]
[tensor(-1.1306), 0.6853932584269663, tensor(2.2739)]
[tensor(-1.1306), 0.6853932584269663, tensor(2.2739)]
[tensor(-1.1306), 0.6853932584269663, tensor(2.2739)]
[tensor(-1.1306), 0.6853932584269663, tensor(2.2739)]
[tensor(-1.1306), 0.7078651685393258, tensor(2.3029)]
[tensor(-1.1306), 0.7078651685393258, tensor(2.3029)]
[tensor(-1.1306), 0.7078651685393258, tensor(2.3029)]
[tensor(-1.1306), 0.7078651685393258, tensor(2.3029)]
[tensor(-1.1306), 0.7078651685393258, tensor(2.3029)]
[tensor(-1.1306), 0.7078651685393258, tensor(2.3029)]
[tensor(-1.1306), 0.7078651685393258, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
[tensor(-1.1306), 0.7101123595505618, tensor(2.3029)]
early stopping at 27
[2023-01-19 16:52:20,997.997 dsw44922-6f76bf568-tbjcv:82508 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2781), 0.3550561797752809, 0.0]
[tensor(-1.4120), 0.6134831460674157, tensor(1.6555)]
[tensor(-1.2528), 0.6651685393258427, tensor(2.0731)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1523), 0.6853932584269663, tensor(2.2746)]
[tensor(-1.1523), 0.6853932584269663, tensor(2.2746)]
[tensor(-1.1523), 0.6853932584269663, tensor(2.2746)]
[tensor(-1.1523), 0.6876404494382022, tensor(2.2746)]
[tensor(-1.1523), 0.6921348314606741, tensor(2.2746)]
[tensor(-1.1523), 0.6943820224719102, tensor(2.2746)]
[tensor(-1.1523), 0.6966292134831461, tensor(2.2746)]
[2023-01-19 16:57:20,489.489 dsw44922-6f76bf568-tbjcv:82542 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9959), 0.4764044943820225, tensor(0.3861)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3375), 0.6359550561797753, tensor(1.8423)]
[tensor(-1.2189), 0.6696629213483146, tensor(2.1294)]
[tensor(-1.2144), 0.6741573033707865, tensor(2.1564)]
[tensor(-1.2144), 0.6741573033707865, tensor(2.1564)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2144), 0.6808988764044944, tensor(2.1564)]
[tensor(-1.2144), 0.6921348314606741, tensor(2.1662)]
[tensor(-1.2144), 0.6921348314606741, tensor(2.1662)]
[tensor(-1.2144), 0.7056179775280899, tensor(2.2002)]
[tensor(-1.2144), 0.7056179775280899, tensor(2.2002)]
[2023-01-19 17:02:26,835.835 dsw44922-6f76bf568-tbjcv:82576 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.7999), 0.5078651685393258, tensor(0.7394)]
[tensor(-1.3459), 0.6314606741573033, tensor(1.8114)]
[tensor(-1.3133), 0.6314606741573033, tensor(1.8114)]
[tensor(-1.3133), 0.6539325842696629, tensor(1.9532)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
[tensor(-1.2728), 0.6876404494382022, tensor(2.1654)]
early stopping at 15
[2023-01-19 17:10:20,989.989 dsw44922-6f76bf568-tbjcv:82613 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.7826), 0.12134831460674157, 0.0]
[tensor(-2.3490), 0.298876404494382, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6114), 0.5370786516853933, tensor(1.0740)]
[tensor(-1.4526), 0.5887640449438202, tensor(1.4912)]
[tensor(-1.4526), 0.5887640449438202, tensor(1.4912)]
[tensor(-1.2628), 0.6314606741573033, tensor(1.8945)]
[tensor(-1.2627), 0.6382022471910113, tensor(1.9283)]
[tensor(-1.2627), 0.6719101123595506, tensor(2.0452)]
[tensor(-1.2627), 0.6831460674157304, tensor(2.1421)]
[tensor(-1.2627), 0.6831460674157304, tensor(2.1421)]
[tensor(-1.2627), 0.6831460674157304, tensor(2.1421)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2627), 0.6831460674157304, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
[tensor(-1.2627), 0.6943820224719102, tensor(2.1421)]
early stopping at 23
[2023-01-19 17:21:41,939.939 dsw44922-6f76bf568-tbjcv:82657 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2890), 0.38202247191011235, 0.0]
[tensor(-1.5704), 0.597752808988764, tensor(1.4184)]
[tensor(-1.2538), 0.651685393258427, tensor(2.0047)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1534), 0.6651685393258427, tensor(2.1724)]
[tensor(-1.1417), 0.6921348314606741, tensor(2.3190)]
[tensor(-1.1335), 0.7078651685393258, tensor(2.4059)]
[tensor(-1.1335), 0.7078651685393258, tensor(2.4059)]
[tensor(-1.1335), 0.7280898876404495, tensor(2.5039)]
[tensor(-1.1335), 0.7393258426966293, tensor(2.5602)]
[tensor(-1.1335), 0.7393258426966293, tensor(2.5602)]
[2023-01-19 17:26:51,232.232 dsw44922-6f76bf568-tbjcv:82692 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.6674), 0.5550561797752809, tensor(1.1079)]
[tensor(-1.1609), 0.6764044943820224, tensor(2.2211)]
[tensor(-1.0598), 0.6898876404494382, tensor(2.3897)]
[tensor(-1.0598), 0.6898876404494382, tensor(2.3897)]
[tensor(-1.0598), 0.6898876404494382, tensor(2.3897)]
[tensor(-1.0598), 0.6943820224719102, tensor(2.3897)]
[tensor(-1.0598), 0.698876404494382, tensor(2.3897)]
[tensor(-1.0598), 0.7235955056179775, tensor(2.4177)]
[tensor(-1.0598), 0.7235955056179775, tensor(2.4265)]
[tensor(-1.0598), 0.7258426966292135, tensor(2.4265)]
[2023-01-19 17:32:20,404.404 dsw44922-6f76bf568-tbjcv:82727 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7012), 0.5325842696629214, tensor(0.9617)]
[tensor(-1.2931), 0.6539325842696629, tensor(1.9766)]
[tensor(-1.2448), 0.6584269662921348, tensor(2.0473)]
[tensor(-1.0984), 0.6831460674157304, tensor(2.3173)]
[tensor(-1.0984), 0.6831460674157304, tensor(2.3173)]
[tensor(-1.0984), 0.6831460674157304, tensor(2.3173)]
[tensor(-1.0984), 0.6966292134831461, tensor(2.3368)]
[tensor(-1.0984), 0.701123595505618, tensor(2.3463)]
[tensor(-1.0984), 0.701123595505618, tensor(2.3463)]
[tensor(-1.0984), 0.7033707865168539, tensor(2.3463)]
[tensor(-1.0984), 0.7033707865168539, tensor(2.3463)]
[tensor(-1.0984), 0.7033707865168539, tensor(2.3463)]
[tensor(-1.0984), 0.7056179775280899, tensor(2.3463)]
[tensor(-1.0984), 0.7056179775280899, tensor(2.3463)]
[tensor(-1.0984), 0.7056179775280899, tensor(2.3463)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0984), 0.7123595505617978, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
[tensor(-1.0984), 0.7168539325842697, tensor(2.3463)]
early stopping at 27
[2023-01-19 17:45:53,707.707 dsw44922-6f76bf568-tbjcv:82773 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.bias', 'selection_head.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.4469), 0.5820224719101124, tensor(1.4632)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2066), 0.6426966292134831, tensor(2.0069)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1807), 0.6786516853932584, tensor(2.2126)]
[tensor(-1.1309), 0.6853932584269663, tensor(2.2960)]
[tensor(-1.1309), 0.6853932584269663, tensor(2.2960)]
[tensor(-1.1309), 0.6853932584269663, tensor(2.2960)]
[tensor(-1.1309), 0.6853932584269663, tensor(2.2960)]
[tensor(-1.1309), 0.6853932584269663, tensor(2.2960)]
[tensor(-1.1309), 0.6943820224719102, tensor(2.2960)]
[tensor(-1.1309), 0.6966292134831461, tensor(2.2960)]
[tensor(-1.1309), 0.6966292134831461, tensor(2.2960)]
[tensor(-1.1309), 0.6966292134831461, tensor(2.2960)]
[tensor(-1.1309), 0.6966292134831461, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.701123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
[tensor(-1.1309), 0.7101123595505618, tensor(2.2960)]
early stopping at 34
[2023-01-19 18:03:24,276.276 dsw44922-6f76bf568-tbjcv:82826 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5979), 0.24269662921348314, 0.0]
[tensor(-1.9566), 0.48089887640449436, tensor(0.4479)]
[tensor(-1.4603), 0.6157303370786517, tensor(1.6183)]
[tensor(-1.2538), 0.6449438202247191, tensor(1.9709)]
[tensor(-1.1404), 0.6786516853932584, tensor(2.2528)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1391), 0.6786516853932584, tensor(2.2542)]
[tensor(-1.0841), 0.7033707865168539, tensor(2.4328)]
[tensor(-1.0841), 0.7033707865168539, tensor(2.4328)]
[tensor(-1.0841), 0.7168539325842697, tensor(2.4951)]
[tensor(-1.0841), 0.7168539325842697, tensor(2.4951)]
[2023-01-19 18:08:29,599.599 dsw44922-6f76bf568-tbjcv:82860 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4259), 0.3146067415730337, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6382), 0.5303370786516854, tensor(1.0135)]
[tensor(-1.2593), 0.6674157303370787, tensor(2.0778)]
[tensor(-1.1914), 0.6808988764044944, tensor(2.2131)]
[tensor(-1.1268), 0.6808988764044944, tensor(2.2215)]
[tensor(-1.1268), 0.6808988764044944, tensor(2.2215)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1250), 0.698876404494382, tensor(2.3694)]
[tensor(-1.1250), 0.698876404494382, tensor(2.3694)]
[tensor(-1.1250), 0.701123595505618, tensor(2.3694)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1250), 0.701123595505618, tensor(2.3694)]
[2023-01-19 18:13:31,946.946 dsw44922-6f76bf568-tbjcv:82895 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3887), 0.36404494382022473, 0.0]
[tensor(-1.8869), 0.47191011235955055, tensor(0.4727)]
[tensor(-1.8258), 0.5056179775280899, tensor(0.7023)]
[tensor(-1.4725), 0.6224719101123596, tensor(1.6398)]
[tensor(-1.1908), 0.6696629213483146, tensor(2.1575)]
[tensor(-1.1418), 0.6696629213483146, tensor(2.1616)]
[tensor(-1.0978), 0.6808988764044944, tensor(2.3067)]
[tensor(-1.0971), 0.6831460674157304, tensor(2.3186)]
[tensor(-1.0971), 0.6966292134831461, tensor(2.3813)]
[tensor(-1.0971), 0.698876404494382, tensor(2.3813)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0971), 0.698876404494382, tensor(2.3813)]
[tensor(-1.0971), 0.698876404494382, tensor(2.3813)]
[tensor(-1.0971), 0.701123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.701123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.701123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.701123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.701123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7056179775280899, tensor(2.3813)]
[tensor(-1.0971), 0.7078651685393258, tensor(2.3813)]
[tensor(-1.0971), 0.7078651685393258, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7101123595505618, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[tensor(-1.0971), 0.7123595505617978, tensor(2.3813)]
[2023-01-19 18:38:21,541.541 dsw44922-6f76bf568-tbjcv:82954 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3326), 0.36629213483146067, 0.0]
[tensor(-1.8774), 0.46741573033707867, tensor(0.4597)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2640), 0.651685393258427, tensor(1.9945)]
[tensor(-1.1774), 0.6719101123595506, tensor(2.1821)]
[tensor(-1.1311), 0.6786516853932584, tensor(2.2621)]
[tensor(-1.0912), 0.6831460674157304, tensor(2.3246)]
[tensor(-1.0828), 0.698876404494382, tensor(2.4116)]
[tensor(-1.0828), 0.698876404494382, tensor(2.4116)]
[tensor(-1.0828), 0.698876404494382, tensor(2.4116)]
[tensor(-1.0828), 0.698876404494382, tensor(2.4116)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0828), 0.698876404494382, tensor(2.4116)]
[tensor(-1.0828), 0.701123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.701123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.701123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.7056179775280899, tensor(2.4116)]
[tensor(-1.0828), 0.7101123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.7101123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.7101123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.7101123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.7101123595505618, tensor(2.4116)]
[tensor(-1.0828), 0.7123595505617978, tensor(2.4116)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.4116)]
[tensor(-1.0828), 0.7146067415730337, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
[tensor(-1.0828), 0.7191011235955056, tensor(2.4116)]
early stopping at 36
[2023-01-19 18:56:19,316.316 dsw44922-6f76bf568-tbjcv:83006 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6170), 0.2202247191011236, 0.0]
[tensor(-1.8308), 0.5078651685393258, tensor(0.7086)]
[tensor(-1.4398), 0.5842696629213483, tensor(1.4815)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2649), 0.6382022471910113, tensor(1.9261)]
[tensor(-1.1649), 0.6786516853932584, tensor(2.2284)]
[tensor(-1.1549), 0.6921348314606741, tensor(2.3057)]
[tensor(-1.1549), 0.6921348314606741, tensor(2.3057)]
[tensor(-1.1549), 0.6921348314606741, tensor(2.3057)]
[tensor(-1.1343), 0.7078651685393258, tensor(2.4050)]
[tensor(-1.1343), 0.7078651685393258, tensor(2.4050)]
[2023-01-19 19:01:25,914.914 dsw44922-6f76bf568-tbjcv:83040 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1183), 0.4134831460674157, 0.0]
[tensor(-1.3861), 0.6359550561797753, tensor(1.7937)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1956), 0.651685393258427, tensor(2.0628)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1307), 0.6674157303370787, tensor(2.2064)]
[tensor(-1.1057), 0.6696629213483146, tensor(2.2427)]
[tensor(-1.1057), 0.6921348314606741, tensor(2.3503)]
[tensor(-1.1057), 0.6966292134831461, tensor(2.3753)]
[tensor(-1.1057), 0.7033707865168539, tensor(2.3860)]
[tensor(-1.1057), 0.7056179775280899, tensor(2.3860)]
[tensor(-1.1057), 0.7056179775280899, tensor(2.3860)]
[2023-01-19 19:06:27,020.020 dsw44922-6f76bf568-tbjcv:83074 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2365), 0.4044943820224719, 0.0]
[tensor(-1.7262), 0.5123595505617977, tensor(0.8356)]
[tensor(-1.6792), 0.5258426966292135, tensor(0.9501)]
[tensor(-1.3971), 0.6247191011235955, tensor(1.7265)]
[tensor(-1.2047), 0.6494382022471911, tensor(2.0425)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0977), 0.6966292134831461, tensor(2.3855)]
[tensor(-1.0659), 0.698876404494382, tensor(2.4285)]
[tensor(-1.0659), 0.7078651685393258, tensor(2.4658)]
[tensor(-1.0659), 0.7078651685393258, tensor(2.4658)]
[tensor(-1.0659), 0.7078651685393258, tensor(2.4658)]
[tensor(-1.0659), 0.7078651685393258, tensor(2.4658)]
[tensor(-1.0659), 0.7078651685393258, tensor(2.4658)]
[tensor(-1.0659), 0.7146067415730337, tensor(2.4658)]
[tensor(-1.0659), 0.7146067415730337, tensor(2.4658)]
[tensor(-1.0659), 0.7191011235955056, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
[tensor(-1.0659), 0.7235955056179775, tensor(2.4658)]
early stopping at 28
[2023-01-19 19:20:19,065.065 dsw44922-6f76bf568-tbjcv:83121 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'selection_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0516), 0.451685393258427, tensor(0.2068)]
[tensor(-1.6440), 0.5595505617977528, tensor(1.1538)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1488), 0.6808988764044944, tensor(2.2557)]
[tensor(-1.0970), 0.6808988764044944, tensor(2.3075)]
[tensor(-1.0970), 0.6808988764044944, tensor(2.3075)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0970), 0.6853932584269663, tensor(2.3093)]
[tensor(-1.0970), 0.6853932584269663, tensor(2.3093)]
[tensor(-1.0970), 0.7123595505617978, tensor(2.4568)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
[tensor(-1.0970), 0.7235955056179775, tensor(2.5076)]
early stopping at 19
[2023-01-19 19:29:47,926.926 dsw44922-6f76bf568-tbjcv:83160 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.dense.bias', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5155), 0.3101123595505618, 0.0]
[tensor(-1.8464), 0.4898876404494382, tensor(0.6031)]
[tensor(-1.3813), 0.6539325842696629, tensor(1.8883)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2253), 0.6561797752808989, tensor(2.0556)]
[tensor(-1.1415), 0.6943820224719102, tensor(2.3304)]
[tensor(-1.1237), 0.6943820224719102, tensor(2.3304)]
[tensor(-1.0941), 0.6966292134831461, tensor(2.3890)]
[tensor(-1.0941), 0.6966292134831461, tensor(2.3890)]
[tensor(-1.0941), 0.7101123595505618, tensor(2.4125)]
[tensor(-1.0941), 0.7101123595505618, tensor(2.4125)]
[2023-01-19 19:35:31,528.528 dsw44922-6f76bf568-tbjcv:83195 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.3976), 0.3595505617977528, 0.0]
[tensor(-1.4854), 0.5910112359550562, tensor(1.4696)]
[tensor(-1.1968), 0.6853932584269663, tensor(2.2302)]
[tensor(-1.1682), 0.6853932584269663, tensor(2.2302)]
[tensor(-1.0996), 0.7101123595505618, tensor(2.4510)]
[tensor(-1.0996), 0.7101123595505618, tensor(2.4510)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0996), 0.7101123595505618, tensor(2.4510)]
[tensor(-1.0996), 0.7101123595505618, tensor(2.4510)]
[tensor(-1.0996), 0.7146067415730337, tensor(2.4510)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0996), 0.7168539325842697, tensor(2.4510)]
[2023-01-19 19:40:54,170.170 dsw44922-6f76bf568-tbjcv:83230 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3296), 0.37303370786516854, 0.0]
[tensor(-2.0567), 0.44269662921348313, tensor(0.1568)]
[tensor(-1.5991), 0.5685393258426966, tensor(1.2436)]
[tensor(-1.2486), 0.6539325842696629, tensor(2.0210)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1142), 0.6921348314606741, tensor(2.3465)]
[tensor(-1.1051), 0.6921348314606741, tensor(2.3465)]
[tensor(-1.1046), 0.6921348314606741, tensor(2.3465)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1046), 0.698876404494382, tensor(2.3465)]
[tensor(-1.1046), 0.7191011235955056, tensor(2.4798)]
[tensor(-1.1046), 0.7191011235955056, tensor(2.4798)]
[tensor(-1.1046), 0.7191011235955056, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
[tensor(-1.1046), 0.7258426966292135, tensor(2.4798)]
early stopping at 22
[2023-01-19 19:52:45,152.152 dsw44922-6f76bf568-tbjcv:83274 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.4660), 0.3348314606741573, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.5733), 0.5191011235955056, tensor(1.0222)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2853), 0.6426966292134831, tensor(1.9282)]
[tensor(-1.1186), 0.6898876404494382, tensor(2.3308)]
[tensor(-1.0500), 0.698876404494382, tensor(2.4444)]
[tensor(-1.0500), 0.698876404494382, tensor(2.4444)]
[tensor(-1.0500), 0.698876404494382, tensor(2.4444)]
[tensor(-1.0500), 0.7123595505617978, tensor(2.4467)]
[tensor(-1.0500), 0.7191011235955056, tensor(2.4489)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0500), 0.7191011235955056, tensor(2.4489)]
[tensor(-1.0500), 0.7191011235955056, tensor(2.4489)]
[tensor(-1.0500), 0.7191011235955056, tensor(2.4489)]
[tensor(-1.0500), 0.7213483146067415, tensor(2.4489)]
[tensor(-1.0500), 0.7235955056179775, tensor(2.4489)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
[tensor(-1.0500), 0.7303370786516854, tensor(2.4624)]
early stopping at 33
[2023-01-19 20:10:44,469.469 dsw44922-6f76bf568-tbjcv:83323 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3728), 0.38202247191011235, 0.0]
[tensor(-1.6422), 0.5280898876404494, tensor(0.9983)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3106), 0.6337078651685393, tensor(1.8580)]
[tensor(-1.1270), 0.6876404494382022, tensor(2.3112)]
[tensor(-1.1177), 0.6921348314606741, tensor(2.3430)]
[tensor(-1.1177), 0.698876404494382, tensor(2.3566)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1177), 0.698876404494382, tensor(2.3571)]
[tensor(-1.1177), 0.7123595505617978, tensor(2.3848)]
[tensor(-1.1177), 0.7123595505617978, tensor(2.3848)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1177), 0.7123595505617978, tensor(2.3848)]
[2023-01-19 20:16:15,719.719 dsw44922-6f76bf568-tbjcv:83358 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.bias', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0362), 0.4449438202247191, tensor(0.1885)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3629), 0.6359550561797753, tensor(1.8169)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1389), 0.6764044943820224, tensor(2.2432)]
[tensor(-1.0755), 0.6921348314606741, tensor(2.3851)]
[tensor(-1.0755), 0.6921348314606741, tensor(2.3851)]
[tensor(-1.0755), 0.6921348314606741, tensor(2.3851)]
[tensor(-1.0755), 0.6921348314606741, tensor(2.3851)]
[tensor(-1.0755), 0.6943820224719102, tensor(2.3851)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0755), 0.7101123595505618, tensor(2.3851)]
[tensor(-1.0755), 0.7101123595505618, tensor(2.3851)]
[2023-01-19 20:21:44,720.720 dsw44922-6f76bf568-tbjcv:83393 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'selection_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0489), 0.4606741573033708, tensor(0.2545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8113), 0.5438202247191011, tensor(0.9078)]
[tensor(-1.5000), 0.6202247191011236, tensor(1.6011)]
[tensor(-1.2142), 0.651685393258427, tensor(2.0442)]
[tensor(-1.0977), 0.6943820224719102, tensor(2.3742)]
[tensor(-1.0977), 0.6966292134831461, tensor(2.3768)]
[tensor(-1.0977), 0.6966292134831461, tensor(2.3768)]
[tensor(-1.0977), 0.701123595505618, tensor(2.3768)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
[tensor(-1.0977), 0.7101123595505618, tensor(2.4432)]
early stopping at 19
[2023-01-19 20:32:16,955.955 dsw44922-6f76bf568-tbjcv:83434 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-25 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-25 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.0499), 0.47191011235955055, tensor(0.3097)]
[tensor(-1.3115), 0.6606741573033708, tensor(1.9919)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1256), 0.6921348314606741, tensor(2.3350)]
[tensor(-1.1082), 0.6921348314606741, tensor(2.3350)]
[tensor(-1.0652), 0.6921348314606741, tensor(2.3730)]
[tensor(-1.0641), 0.6966292134831461, tensor(2.4190)]
[tensor(-1.0641), 0.701123595505618, tensor(2.4284)]
[tensor(-1.0641), 0.7078651685393258, tensor(2.4284)]
[tensor(-1.0641), 0.7078651685393258, tensor(2.4284)]
[tensor(-1.0641), 0.7078651685393258, tensor(2.4284)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0641), 0.7078651685393258, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
[tensor(-1.0641), 0.7123595505617978, tensor(2.4284)]
early stopping at 22
[2023-01-19 20:44:17,767.767 dsw44922-6f76bf568-tbjcv:83477 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3277), 0.31685393258426964, 0.0]
[tensor(-1.4042), 0.6112359550561798, tensor(1.6520)]
[tensor(-1.2314), 0.6539325842696629, tensor(2.0382)]
[tensor(-1.1731), 0.6898876404494382, tensor(2.2763)]
[tensor(-1.1731), 0.698876404494382, tensor(2.2977)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1731), 0.7078651685393258, tensor(2.3570)]
[tensor(-1.1731), 0.7078651685393258, tensor(2.3570)]
[tensor(-1.1731), 0.7146067415730337, tensor(2.3570)]
[tensor(-1.1731), 0.7146067415730337, tensor(2.3570)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1731), 0.7146067415730337, tensor(2.3570)]
[2023-01-19 20:49:41,916.916 dsw44922-6f76bf568-tbjcv:83511 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9739), 0.4651685393258427, tensor(0.3520)]
[tensor(-1.2160), 0.6449438202247191, tensor(2.0087)]
[tensor(-1.1281), 0.6719101123595506, tensor(2.2315)]
[tensor(-1.1281), 0.6966292134831461, tensor(2.3231)]
[tensor(-1.1281), 0.6966292134831461, tensor(2.3231)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1281), 0.6966292134831461, tensor(2.3231)]
[tensor(-1.1281), 0.7123595505617978, tensor(2.3254)]
[tensor(-1.1281), 0.7123595505617978, tensor(2.3254)]
[tensor(-1.1281), 0.7123595505617978, tensor(2.3254)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1281), 0.7123595505617978, tensor(2.3254)]
[2023-01-19 20:55:09,315.315 dsw44922-6f76bf568-tbjcv:83546 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8519), 0.48764044943820223, tensor(0.5863)]
[tensor(-1.3466), 0.6247191011235955, tensor(1.7770)]
[tensor(-1.3002), 0.6269662921348315, tensor(1.8346)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2108), 0.647191011235955, tensor(2.0251)]
[tensor(-1.2014), 0.6966292134831461, tensor(2.2817)]
[tensor(-1.1681), 0.698876404494382, tensor(2.3263)]
[tensor(-1.1681), 0.701123595505618, tensor(2.3314)]
[tensor(-1.1670), 0.701123595505618, tensor(2.3314)]
[tensor(-1.1670), 0.701123595505618, tensor(2.3314)]
[tensor(-1.1670), 0.701123595505618, tensor(2.3314)]
[tensor(-1.1670), 0.701123595505618, tensor(2.3314)]
[tensor(-1.1670), 0.701123595505618, tensor(2.3314)]
[tensor(-1.1670), 0.7033707865168539, tensor(2.3314)]
[tensor(-1.1670), 0.7033707865168539, tensor(2.3314)]
[tensor(-1.1670), 0.7033707865168539, tensor(2.3314)]
[tensor(-1.1670), 0.7078651685393258, tensor(2.3314)]
[tensor(-1.1670), 0.7146067415730337, tensor(2.3314)]
[tensor(-1.1670), 0.7146067415730337, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
[tensor(-1.1670), 0.7191011235955056, tensor(2.3314)]
early stopping at 29
[2023-01-19 21:09:43,203.203 dsw44922-6f76bf568-tbjcv:83594 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7764), 0.4966292134831461, tensor(0.7068)]
[tensor(-1.3697), 0.6292134831460674, tensor(1.7764)]
[tensor(-1.1638), 0.6853932584269663, tensor(2.2631)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
[tensor(-1.0847), 0.7078651685393258, tensor(2.4546)]
early stopping at 14
[2023-01-19 21:17:34,936.936 dsw44922-6f76bf568-tbjcv:83642 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mlm_head.decoder.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1465), 0.4202247191011236, 0.0]
[tensor(-1.4641), 0.6157303370786517, tensor(1.6145)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2269), 0.647191011235955, tensor(2.0091)]
[tensor(-1.1517), 0.6674157303370787, tensor(2.1853)]
[tensor(-1.1517), 0.6943820224719102, tensor(2.3028)]
[tensor(-1.1517), 0.6943820224719102, tensor(2.3028)]
[tensor(-1.1517), 0.7056179775280899, tensor(2.3082)]
[tensor(-1.1517), 0.7056179775280899, tensor(2.3082)]
[tensor(-1.1517), 0.7056179775280899, tensor(2.3082)]
[tensor(-1.1517), 0.7101123595505618, tensor(2.3082)]
[2023-01-19 21:22:43,328.328 dsw44922-6f76bf568-tbjcv:83706 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6588), 0.5662921348314607, tensor(1.1727)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2947), 0.647191011235955, tensor(1.9412)]
[tensor(-1.1436), 0.6674157303370787, tensor(2.1935)]
[tensor(-1.0868), 0.6943820224719102, tensor(2.3851)]
[tensor(-1.0868), 0.7078651685393258, tensor(2.3851)]
[tensor(-1.0868), 0.7168539325842697, tensor(2.4465)]
[tensor(-1.0868), 0.7168539325842697, tensor(2.4465)]
[tensor(-1.0868), 0.7168539325842697, tensor(2.4465)]
[tensor(-1.0868), 0.7168539325842697, tensor(2.4465)]
[tensor(-1.0868), 0.7213483146067415, tensor(2.4465)]
[2023-01-19 21:27:52,086.086 dsw44922-6f76bf568-tbjcv:83770 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8941), 0.4764044943820225, tensor(0.4880)]
[tensor(-1.4290), 0.597752808988764, tensor(1.5598)]
[tensor(-1.3845), 0.6022471910112359, tensor(1.6267)]
[tensor(-1.2234), 0.6314606741573033, tensor(1.9339)]
[tensor(-1.1880), 0.651685393258427, tensor(2.0704)]
[tensor(-1.1821), 0.6808988764044944, tensor(2.2224)]
[tensor(-1.1431), 0.698876404494382, tensor(2.3513)]
[tensor(-1.1431), 0.7101123595505618, tensor(2.3969)]
[tensor(-1.1431), 0.7101123595505618, tensor(2.3969)]
[tensor(-1.1431), 0.7101123595505618, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
[tensor(-1.1431), 0.7213483146067415, tensor(2.3969)]
early stopping at 21
[2023-01-19 21:39:02,182.182 dsw44922-6f76bf568-tbjcv:83880 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.5157), 0.5662921348314607, tensor(1.3158)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2499), 0.6426966292134831, tensor(1.9635)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1771), 0.6719101123595506, tensor(2.1825)]
[tensor(-1.1019), 0.6943820224719102, tensor(2.3700)]
[tensor(-1.1019), 0.6943820224719102, tensor(2.3700)]
[tensor(-1.1019), 0.6943820224719102, tensor(2.3700)]
[tensor(-1.1019), 0.6966292134831461, tensor(2.3700)]
[tensor(-1.1019), 0.701123595505618, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7146067415730337, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
[tensor(-1.1019), 0.7168539325842697, tensor(2.3700)]
early stopping at 28
[2023-01-19 21:52:58,783.783 dsw44922-6f76bf568-tbjcv:83928 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'selection_head.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6477), 0.15280898876404495, 0.0]
[tensor(-1.8635), 0.48764044943820223, tensor(0.5747)]
[tensor(-1.3734), 0.6404494382022472, tensor(1.8288)]
[tensor(-1.2478), 0.6584269662921348, tensor(2.0443)]
[tensor(-1.1091), 0.6853932584269663, tensor(2.3179)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0940), 0.6876404494382022, tensor(2.3442)]
[tensor(-1.0611), 0.6966292134831461, tensor(2.4221)]
[tensor(-1.0611), 0.6966292134831461, tensor(2.4221)]
[tensor(-1.0611), 0.7033707865168539, tensor(2.4221)]
[tensor(-1.0611), 0.7033707865168539, tensor(2.4221)]
[2023-01-19 21:58:03,873.873 dsw44922-6f76bf568-tbjcv:83962 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4097), 0.2966292134831461, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5581), 0.5438202247191011, tensor(1.1610)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2399), 0.6584269662921348, tensor(2.0522)]
[tensor(-1.1954), 0.6741573033707865, tensor(2.1754)]
[tensor(-1.1353), 0.6741573033707865, tensor(2.2355)]
[tensor(-1.1353), 0.6741573033707865, tensor(2.2355)]
[tensor(-1.1054), 0.6966292134831461, tensor(2.3778)]
[tensor(-1.1054), 0.6966292134831461, tensor(2.3778)]
[tensor(-1.1054), 0.7101123595505618, tensor(2.4196)]
[tensor(-1.1054), 0.7101123595505618, tensor(2.4196)]
[2023-01-19 22:03:16,036.036 dsw44922-6f76bf568-tbjcv:83996 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3107), 0.3887640449438202, 0.0]
[tensor(-1.7730), 0.49213483146067416, tensor(0.6877)]
[tensor(-1.7164), 0.5101123595505618, tensor(0.8342)]
[tensor(-1.3877), 0.6382022471910113, tensor(1.8033)]
[tensor(-1.1623), 0.6719101123595506, tensor(2.1973)]
[tensor(-1.1623), 0.6808988764044944, tensor(2.2371)]
[tensor(-1.1267), 0.698876404494382, tensor(2.3677)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1267), 0.698876404494382, tensor(2.3677)]
[tensor(-1.1267), 0.698876404494382, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
[tensor(-1.1267), 0.7033707865168539, tensor(2.3677)]
early stopping at 22
[2023-01-19 22:14:18,102.102 dsw44922-6f76bf568-tbjcv:84039 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'start_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2610), 0.4134831460674157, 0.0]
[tensor(-1.7617), 0.5123595505617977, tensor(0.8001)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1739), 0.6741573033707865, tensor(2.1969)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1294), 0.6786516853932584, tensor(2.2638)]
[tensor(-1.1294), 0.6921348314606741, tensor(2.3294)]
[tensor(-1.1147), 0.6966292134831461, tensor(2.3685)]
[tensor(-1.0993), 0.7078651685393258, tensor(2.4400)]
[tensor(-1.0993), 0.7078651685393258, tensor(2.4400)]
[tensor(-1.0993), 0.7078651685393258, tensor(2.4400)]
[tensor(-1.0993), 0.7078651685393258, tensor(2.4400)]
[tensor(-1.0993), 0.7123595505617978, tensor(2.4400)]
[tensor(-1.0993), 0.7123595505617978, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7168539325842697, tensor(2.4400)]
[tensor(-1.0993), 0.7235955056179775, tensor(2.4400)]
[tensor(-1.0993), 0.7235955056179775, tensor(2.4400)]
[tensor(-1.0993), 0.7235955056179775, tensor(2.4400)]
[tensor(-1.0993), 0.7235955056179775, tensor(2.4400)]
[tensor(-1.0993), 0.7235955056179775, tensor(2.4400)]
[tensor(-1.0993), 0.7258426966292135, tensor(2.4400)]
[tensor(-1.0993), 0.7258426966292135, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
[tensor(-1.0993), 0.7325842696629213, tensor(2.4400)]
early stopping at 38
[2023-01-19 22:34:09,093.093 dsw44922-6f76bf568-tbjcv:84090 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5876), 0.23820224719101124, 0.0]
[tensor(-1.8017), 0.5370786516853933, tensor(0.8837)]
[tensor(-1.4468), 0.5775280898876405, tensor(1.4408)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2102), 0.6494382022471911, tensor(2.0370)]
[tensor(-1.1064), 0.6853932584269663, tensor(2.3206)]
[tensor(-1.1064), 0.6853932584269663, tensor(2.3206)]
[tensor(-1.1064), 0.6853932584269663, tensor(2.3206)]
[tensor(-1.1064), 0.7033707865168539, tensor(2.3880)]
[tensor(-1.1064), 0.7033707865168539, tensor(2.3880)]
[tensor(-1.1064), 0.7168539325842697, tensor(2.4356)]
[2023-01-19 22:39:12,798.798 dsw44922-6f76bf568-tbjcv:84124 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mlm_head.decoder.weight', 'selection_head.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1167), 0.45617977528089887, tensor(0.1642)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3936), 0.6269662921348315, tensor(1.7412)]
[tensor(-1.2036), 0.6741573033707865, tensor(2.1672)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1468), 0.6921348314606741, tensor(2.3139)]
[tensor(-1.1059), 0.6921348314606741, tensor(2.3323)]
[tensor(-1.1059), 0.6921348314606741, tensor(2.3323)]
[tensor(-1.1059), 0.701123595505618, tensor(2.3750)]
[tensor(-1.1059), 0.7078651685393258, tensor(2.4037)]
[tensor(-1.1059), 0.7078651685393258, tensor(2.4037)]
[tensor(-1.1059), 0.7168539325842697, tensor(2.4037)]
[2023-01-19 22:44:18,095.095 dsw44922-6f76bf568-tbjcv:84158 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1943), 0.43595505617977526, 0.0]
[tensor(-1.6947), 0.5528089887640449, tensor(1.0693)]
[tensor(-1.6544), 0.5573033707865168, tensor(1.1321)]
[tensor(-1.3838), 0.6247191011235955, tensor(1.7398)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1927), 0.6561797752808989, tensor(2.0882)]
[tensor(-1.1425), 0.6674157303370787, tensor(2.1946)]
[tensor(-1.0764), 0.6921348314606741, tensor(2.3843)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0764), 0.6921348314606741, tensor(2.3843)]
[tensor(-1.0764), 0.6921348314606741, tensor(2.3843)]
[tensor(-1.0764), 0.6921348314606741, tensor(2.3843)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7146067415730337, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7191011235955056, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
[tensor(-1.0764), 0.7258426966292135, tensor(2.4032)]
early stopping at 38
[2023-01-19 23:04:25,653.653 dsw44922-6f76bf568-tbjcv:84212 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0246), 0.46292134831460674, tensor(0.2900)]
[tensor(-1.6203), 0.550561797752809, tensor(1.1325)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2438), 0.6696629213483146, tensor(2.1045)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1604), 0.6764044943820224, tensor(2.2216)]
[tensor(-1.1130), 0.6853932584269663, tensor(2.3140)]
[tensor(-1.0818), 0.6876404494382022, tensor(2.3564)]
[tensor(-1.0818), 0.7078651685393258, tensor(2.4141)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0818), 0.7146067415730337, tensor(2.4469)]
[tensor(-1.0818), 0.7280898876404495, tensor(2.5214)]
[tensor(-1.0818), 0.7280898876404495, tensor(2.5214)]
[tensor(-1.0818), 0.7280898876404495, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
[tensor(-1.0818), 0.7325842696629213, tensor(2.5214)]
early stopping at 22
[2023-01-19 23:16:05,528.528 dsw44922-6f76bf568-tbjcv:84256 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4548), 0.3685393258426966, 0.0]
[tensor(-1.6961), 0.5213483146067416, tensor(0.9106)]
[tensor(-1.2716), 0.6539325842696629, tensor(1.9980)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1408), 0.6921348314606741, tensor(2.3199)]
[tensor(-1.1224), 0.6921348314606741, tensor(2.3199)]
[tensor(-1.1224), 0.6921348314606741, tensor(2.3199)]
[tensor(-1.1051), 0.6943820224719102, tensor(2.3668)]
[tensor(-1.1051), 0.6943820224719102, tensor(2.3668)]
[tensor(-1.1051), 0.6966292134831461, tensor(2.3668)]
[tensor(-1.1051), 0.7033707865168539, tensor(2.3668)]
[2023-01-19 23:21:32,691.691 dsw44922-6f76bf568-tbjcv:84291 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'mlm_head.decoder.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'selection_head.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2387), 0.40898876404494383, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4206), 0.6067415730337079, tensor(1.6132)]
[tensor(-1.1712), 0.6786516853932584, tensor(2.2220)]
[tensor(-1.1424), 0.6786516853932584, tensor(2.2509)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1214), 0.698876404494382, tensor(2.3729)]
[tensor(-1.1027), 0.698876404494382, tensor(2.3729)]
[tensor(-1.1027), 0.698876404494382, tensor(2.3729)]
[tensor(-1.1027), 0.701123595505618, tensor(2.3740)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1027), 0.7056179775280899, tensor(2.3740)]
[tensor(-1.1027), 0.7056179775280899, tensor(2.3740)]
[2023-01-19 23:27:00,895.895 dsw44922-6f76bf568-tbjcv:84325 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1823), 0.42921348314606744, 0.0]
[tensor(-1.9190), 0.46741573033707867, tensor(0.4180)]
[tensor(-1.5159), 0.5820224719101124, tensor(1.3942)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2793), 0.651685393258427, tensor(1.9791)]
[tensor(-1.1127), 0.6831460674157304, tensor(2.3031)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.6921348314606741, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.698876404494382, tensor(2.3822)]
[tensor(-1.0785), 0.7078651685393258, tensor(2.3822)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
[tensor(-1.0785), 0.7146067415730337, tensor(2.3822)]
early stopping at 37
[2023-01-19 23:46:55,683.683 dsw44922-6f76bf568-tbjcv:84381 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.weight', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3713), 0.42247191011235957, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5051), 0.5842696629213483, tensor(1.4163)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.2814), 0.6314606741573033, tensor(1.8759)]
[tensor(-1.1384), 0.6741573033707865, tensor(2.2324)]
[tensor(-1.0876), 0.6966292134831461, tensor(2.3955)]
[tensor(-1.0876), 0.6966292134831461, tensor(2.3955)]
[tensor(-1.0876), 0.6966292134831461, tensor(2.3955)]
[tensor(-1.0876), 0.6966292134831461, tensor(2.3955)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
[tensor(-1.0876), 0.7123595505617978, tensor(2.4141)]
early stopping at 19
[2023-01-19 23:57:18,027.027 dsw44922-6f76bf568-tbjcv:84423 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3673), 0.3797752808988764, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6558), 0.5370786516853933, tensor(1.0296)]
[tensor(-1.3077), 0.6314606741573033, tensor(1.8496)]
[tensor(-1.1416), 0.6651685393258427, tensor(2.1842)]
[tensor(-1.1064), 0.6741573033707865, tensor(2.2644)]
[tensor(-1.1064), 0.6876404494382022, tensor(2.3277)]
[tensor(-1.0571), 0.7033707865168539, tensor(2.4598)]
[tensor(-1.0571), 0.7033707865168539, tensor(2.4598)]
[tensor(-1.0571), 0.7033707865168539, tensor(2.4598)]
[tensor(-1.0571), 0.7078651685393258, tensor(2.4598)]
[2023-01-20 00:02:46,059.059 dsw44922-6f76bf568-tbjcv:84457 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9801), 0.4651685393258427, tensor(0.3458)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3184), 0.6674157303370787, tensor(2.0186)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1247), 0.6943820224719102, tensor(2.3472)]
[tensor(-1.0955), 0.698876404494382, tensor(2.3989)]
[tensor(-1.0766), 0.7078651685393258, tensor(2.4628)]
[tensor(-1.0766), 0.7078651685393258, tensor(2.4628)]
[tensor(-1.0766), 0.7101123595505618, tensor(2.4628)]
[tensor(-1.0766), 0.7101123595505618, tensor(2.4628)]
[tensor(-1.0766), 0.7101123595505618, tensor(2.4628)]
[tensor(-1.0766), 0.7101123595505618, tensor(2.4628)]
[2023-01-20 00:08:22,343.343 dsw44922-6f76bf568-tbjcv:84492 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9733), 0.48314606741573035, tensor(0.4425)]
[tensor(-1.7402), 0.5528089887640449, tensor(1.0239)]
[tensor(-1.4555), 0.6202247191011236, tensor(1.6456)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2323), 0.6719101123595506, tensor(2.1273)]
[tensor(-1.1082), 0.7056179775280899, tensor(2.4199)]
[tensor(-1.1075), 0.7056179775280899, tensor(2.4199)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1075), 0.7056179775280899, tensor(2.4199)]
[tensor(-1.1075), 0.7146067415730337, tensor(2.4284)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
[tensor(-1.1075), 0.7168539325842697, tensor(2.4441)]
early stopping at 19
[2023-01-20 00:18:43,861.861 dsw44922-6f76bf568-tbjcv:84534 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-50 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-50 were not used when initializing ATModel: ['selection_head.weight', 'selection_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mlm_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0468), 0.4898876404494382, tensor(0.4026)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3217), 0.6426966292134831, tensor(1.8918)]
[tensor(-1.1874), 0.6606741573033708, tensor(2.1160)]
[tensor(-1.1777), 0.6606741573033708, tensor(2.1160)]
[tensor(-1.1191), 0.6831460674157304, tensor(2.2967)]
[tensor(-1.1157), 0.6966292134831461, tensor(2.3674)]
[tensor(-1.1157), 0.6966292134831461, tensor(2.3674)]
[tensor(-1.1157), 0.6966292134831461, tensor(2.3674)]
[tensor(-1.1157), 0.6966292134831461, tensor(2.3674)]
[tensor(-1.1157), 0.6966292134831461, tensor(2.3674)]
[tensor(-1.1157), 0.6966292134831461, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
[tensor(-1.1157), 0.7123595505617978, tensor(2.3674)]
early stopping at 22
[2023-01-20 00:30:45,951.951 dsw44922-6f76bf568-tbjcv:84577 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.weight', 'audio_encoder.audio_sep', 'selection_head.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3573), 0.2943820224719101, 0.0]
[tensor(-1.5109), 0.5595505617977528, tensor(1.2869)]
[tensor(-1.2976), 0.6494382022471911, tensor(1.9496)]
[tensor(-1.2264), 0.6561797752808989, tensor(2.0545)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2264), 0.6674157303370787, tensor(2.0762)]
[tensor(-1.2264), 0.6719101123595506, tensor(2.1165)]
[tensor(-1.1888), 0.7056179775280899, tensor(2.3392)]
[tensor(-1.1888), 0.7056179775280899, tensor(2.3392)]
[tensor(-1.1888), 0.7056179775280899, tensor(2.3392)]
[tensor(-1.1888), 0.7056179775280899, tensor(2.3392)]
[2023-01-20 00:35:59,662.662 dsw44922-6f76bf568-tbjcv:84613 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-2.0167), 0.43595505617977526, tensor(0.1630)]
[tensor(-1.3626), 0.6179775280898876, tensor(1.7273)]
[tensor(-1.1640), 0.6696629213483146, tensor(2.1843)]
[tensor(-1.1640), 0.6696629213483146, tensor(2.1843)]
[tensor(-1.1640), 0.6853932584269663, tensor(2.2310)]
[tensor(-1.1640), 0.6853932584269663, tensor(2.2310)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2616)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2616)]
[tensor(-1.1640), 0.698876404494382, tensor(2.2616)]
[tensor(-1.1640), 0.701123595505618, tensor(2.2616)]
[2023-01-20 00:41:09,388.388 dsw44922-6f76bf568-tbjcv:84646 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.8769), 0.48089887640449436, tensor(0.5276)]
[tensor(-1.4363), 0.5932584269662922, tensor(1.5300)]
[tensor(-1.3897), 0.5955056179775281, tensor(1.5878)]
[tensor(-1.2099), 0.6584269662921348, tensor(2.0822)]
[tensor(-1.2099), 0.6584269662921348, tensor(2.0822)]
[tensor(-1.1614), 0.7168539325842697, tensor(2.4228)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1478), 0.7168539325842697, tensor(2.4365)]
[tensor(-1.1388), 0.7191011235955056, tensor(2.4567)]
[tensor(-1.1388), 0.7191011235955056, tensor(2.4567)]
[tensor(-1.1388), 0.7191011235955056, tensor(2.4567)]
[tensor(-1.1388), 0.7191011235955056, tensor(2.4567)]
[tensor(-1.1388), 0.7191011235955056, tensor(2.4567)]
[tensor(-1.1388), 0.7191011235955056, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1388), 0.7213483146067415, tensor(2.4567)]
early stopping at 24
[2023-01-20 00:53:10,228.228 dsw44922-6f76bf568-tbjcv:84689 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.9766), 0.4314606741573034, tensor(0.1807)]
[tensor(-1.5668), 0.5617977528089888, tensor(1.2422)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2175), 0.6539325842696629, tensor(2.0522)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1284), 0.6853932584269663, tensor(2.2986)]
[tensor(-1.1284), 0.6853932584269663, tensor(2.2986)]
[tensor(-1.1284), 0.6853932584269663, tensor(2.2986)]
[tensor(-1.1284), 0.6853932584269663, tensor(2.2986)]
[tensor(-1.1284), 0.6921348314606741, tensor(2.2986)]
[tensor(-1.1284), 0.6921348314606741, tensor(2.2986)]
[tensor(-1.1284), 0.6921348314606741, tensor(2.2986)]
[tensor(-1.1284), 0.6921348314606741, tensor(2.2986)]
[tensor(-1.1284), 0.6966292134831461, tensor(2.2986)]
[tensor(-1.1284), 0.7033707865168539, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
[tensor(-1.1284), 0.7078651685393258, tensor(2.2986)]
early stopping at 34
[2023-01-20 01:10:05,113.113 dsw44922-6f76bf568-tbjcv:84740 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0567), 0.44269662921348313, tensor(0.1567)]
[tensor(-1.3840), 0.6157303370786517, tensor(1.6946)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1946), 0.651685393258427, tensor(2.0639)]
[tensor(-1.0873), 0.6876404494382022, tensor(2.3509)]
[tensor(-1.0873), 0.701123595505618, tensor(2.4006)]
[tensor(-1.0873), 0.701123595505618, tensor(2.4006)]
[tensor(-1.0873), 0.701123595505618, tensor(2.4006)]
[tensor(-1.0873), 0.701123595505618, tensor(2.4006)]
[tensor(-1.0873), 0.7213483146067415, tensor(2.4006)]
[tensor(-1.0873), 0.7213483146067415, tensor(2.4006)]
[2023-01-20 01:15:05,816.816 dsw44922-6f76bf568-tbjcv:84774 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.7135), 0.5191011235955056, tensor(0.8820)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2074), 0.6741573033707865, tensor(2.1634)]
[tensor(-1.1580), 0.6741573033707865, tensor(2.1791)]
[tensor(-1.1141), 0.6921348314606741, tensor(2.3466)]
[tensor(-1.1141), 0.6943820224719102, tensor(2.3466)]
[tensor(-1.1141), 0.6966292134831461, tensor(2.3466)]
[tensor(-1.1141), 0.7056179775280899, tensor(2.3466)]
[tensor(-1.1141), 0.7280898876404495, tensor(2.4371)]
[tensor(-1.1141), 0.7280898876404495, tensor(2.4371)]
[tensor(-1.1141), 0.7280898876404495, tensor(2.4371)]
[2023-01-20 01:20:08,512.512 dsw44922-6f76bf568-tbjcv:84808 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6461), 0.5370786516853933, tensor(1.0393)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2427), 0.651685393258427, tensor(2.0158)]
[tensor(-1.2046), 0.6696629213483146, tensor(2.1437)]
[tensor(-1.1224), 0.6764044943820224, tensor(2.2596)]
[tensor(-1.1224), 0.6898876404494382, tensor(2.3153)]
[tensor(-1.1224), 0.698876404494382, tensor(2.3153)]
[tensor(-1.1224), 0.7033707865168539, tensor(2.3275)]
[tensor(-1.1224), 0.7101123595505618, tensor(2.3682)]
[tensor(-1.1224), 0.7101123595505618, tensor(2.3682)]
[tensor(-1.1224), 0.7101123595505618, tensor(2.3682)]
[tensor(-1.1224), 0.7123595505617978, tensor(2.3682)]
[tensor(-1.1224), 0.7123595505617978, tensor(2.3682)]
[tensor(-1.1224), 0.7123595505617978, tensor(2.3682)]
[tensor(-1.1224), 0.7123595505617978, tensor(2.3682)]
[tensor(-1.1224), 0.7146067415730337, tensor(2.3682)]
[tensor(-1.1224), 0.7146067415730337, tensor(2.3682)]
[tensor(-1.1224), 0.7146067415730337, tensor(2.3682)]
[tensor(-1.1224), 0.7146067415730337, tensor(2.3682)]
[tensor(-1.1224), 0.7146067415730337, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
[tensor(-1.1224), 0.7213483146067415, tensor(2.3682)]
early stopping at 43
[2023-01-20 01:41:24,559.559 dsw44922-6f76bf568-tbjcv:84863 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5971), 0.5325842696629214, tensor(1.0658)]
[tensor(-1.2997), 0.651685393258427, tensor(1.9587)]
[tensor(-1.1883), 0.6561797752808989, tensor(2.0926)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7078651685393258, tensor(2.4622)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4622)]
[tensor(-1.0771), 0.7101123595505618, tensor(2.4622)]
[tensor(-1.0771), 0.7146067415730337, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
[tensor(-1.0771), 0.7168539325842697, tensor(2.4622)]
early stopping at 28
[2023-01-20 01:55:54,023.023 dsw44922-6f76bf568-tbjcv:84909 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6178), 0.19775280898876405, 0.0]
[tensor(-1.8824), 0.48089887640449436, tensor(0.5221)]
[tensor(-1.4145), 0.6, tensor(1.5855)]
[tensor(-1.2149), 0.6561797752808989, tensor(2.0660)]
[tensor(-1.1919), 0.6651685393258427, tensor(2.1339)]
[tensor(-1.1597), 0.6741573033707865, tensor(2.2111)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1203), 0.698876404494382, tensor(2.3741)]
[tensor(-1.1036), 0.7146067415730337, tensor(2.4694)]
[tensor(-1.1036), 0.7146067415730337, tensor(2.4694)]
[tensor(-1.1036), 0.7146067415730337, tensor(2.4694)]
[2023-01-20 02:00:59,390.390 dsw44922-6f76bf568-tbjcv:84944 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.dense.weight', 'selection_head.weight', 'selection_head.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3930), 0.3056179775280899, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5637), 0.5415730337078651, tensor(1.1442)]
[tensor(-1.2220), 0.6696629213483146, tensor(2.1263)]
[tensor(-1.1589), 0.6764044943820224, tensor(2.2231)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1162), 0.6853932584269663, tensor(2.3108)]
[tensor(-1.1162), 0.6853932584269663, tensor(2.3108)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3959)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3959)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3959)]
[tensor(-1.1162), 0.7033707865168539, tensor(2.3959)]
[2023-01-20 02:06:05,198.198 dsw44922-6f76bf568-tbjcv:84978 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3043), 0.38202247191011235, 0.0]
[tensor(-1.7582), 0.49887640449438203, tensor(0.7362)]
[tensor(-1.7146), 0.503370786516854, tensor(0.8023)]
[tensor(-1.4338), 0.6022471910112359, tensor(1.5775)]
[tensor(-1.1978), 0.6494382022471911, tensor(2.0494)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1498), 0.6966292134831461, tensor(2.3333)]
[tensor(-1.1442), 0.6966292134831461, tensor(2.3333)]
[tensor(-1.1376), 0.701123595505618, tensor(2.3680)]
[tensor(-1.1376), 0.701123595505618, tensor(2.3680)]
[tensor(-1.1376), 0.701123595505618, tensor(2.3680)]
[tensor(-1.1376), 0.701123595505618, tensor(2.3680)]
[tensor(-1.1376), 0.7123595505617978, tensor(2.3804)]
[tensor(-1.1376), 0.7123595505617978, tensor(2.3804)]
[tensor(-1.1376), 0.7123595505617978, tensor(2.3804)]
[tensor(-1.1376), 0.7123595505617978, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
[tensor(-1.1376), 0.7258426966292135, tensor(2.3804)]
early stopping at 26
[2023-01-20 02:19:13,711.711 dsw44922-6f76bf568-tbjcv:85023 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.weight', 'mlm_head.dense.weight', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mam_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2330), 0.4, 0.0]
[tensor(-1.7534), 0.5101123595505618, tensor(0.7971)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1767), 0.6741573033707865, tensor(2.1941)]
[tensor(-1.1277), 0.6921348314606741, tensor(2.3329)]
[tensor(-1.1277), 0.6921348314606741, tensor(2.3329)]
[tensor(-1.0918), 0.6943820224719102, tensor(2.3801)]
[tensor(-1.0918), 0.701123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.701123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7033707865168539, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7101123595505618, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
[tensor(-1.0918), 0.7191011235955056, tensor(2.4012)]
early stopping at 43
[2023-01-20 02:41:26,339.339 dsw44922-6f76bf568-tbjcv:85081 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'selection_head.bias', 'mlm_head.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5717), 0.24269662921348314, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8408), 0.5191011235955056, tensor(0.7547)]
[tensor(-1.4283), 0.6067415730337079, tensor(1.6054)]
[tensor(-1.2051), 0.6606741573033708, tensor(2.0983)]
[tensor(-1.1202), 0.6876404494382022, tensor(2.3180)]
[tensor(-1.1202), 0.6876404494382022, tensor(2.3180)]
[tensor(-1.1202), 0.6966292134831461, tensor(2.3556)]
[tensor(-1.1145), 0.6966292134831461, tensor(2.3556)]
[tensor(-1.1145), 0.6966292134831461, tensor(2.3589)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1145), 0.6966292134831461, tensor(2.3589)]
[2023-01-20 02:46:55,833.833 dsw44922-6f76bf568-tbjcv:85115 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'mlm_head.bias', 'selection_head.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1305), 0.42921348314606744, tensor(0.0155)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4108), 0.6337078651685393, tensor(1.7577)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1841), 0.6674157303370787, tensor(2.1530)]
[tensor(-1.1124), 0.6943820224719102, tensor(2.3595)]
[tensor(-1.1108), 0.7056179775280899, tensor(2.4173)]
[tensor(-1.1108), 0.7056179775280899, tensor(2.4173)]
[tensor(-1.1108), 0.7056179775280899, tensor(2.4173)]
[tensor(-1.1108), 0.7078651685393258, tensor(2.4173)]
[tensor(-1.1108), 0.7123595505617978, tensor(2.4189)]
[tensor(-1.1108), 0.7123595505617978, tensor(2.4189)]
[2023-01-20 02:52:03,259.259 dsw44922-6f76bf568-tbjcv:85149 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1814), 0.4247191011235955, 0.0]
[tensor(-1.6792), 0.5393258426966292, tensor(1.0175)]
[tensor(-1.6397), 0.5550561797752809, tensor(1.1356)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.3923), 0.6179775280898876, tensor(1.6976)]
[tensor(-1.1969), 0.6696629213483146, tensor(2.1514)]
[tensor(-1.1753), 0.6741573033707865, tensor(2.1954)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1076), 0.6898876404494382, tensor(2.3419)]
[tensor(-1.1017), 0.701123595505618, tensor(2.4039)]
[tensor(-1.1017), 0.7056179775280899, tensor(2.4039)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
[tensor(-1.1017), 0.7146067415730337, tensor(2.4154)]
early stopping at 28
[2023-01-20 03:06:31,066.066 dsw44922-6f76bf568-tbjcv:85193 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0247), 0.45617977528089887, tensor(0.2562)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6428), 0.5573033707865168, tensor(1.1437)]
[tensor(-1.1749), 0.6629213483146067, tensor(2.1397)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1091), 0.6786516853932584, tensor(2.2842)]
[tensor(-1.1091), 0.6876404494382022, tensor(2.3152)]
[tensor(-1.1023), 0.6876404494382022, tensor(2.3152)]
[tensor(-1.1023), 0.6876404494382022, tensor(2.3152)]
[tensor(-1.1023), 0.6943820224719102, tensor(2.3210)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1023), 0.6943820224719102, tensor(2.3210)]
[tensor(-1.1023), 0.701123595505618, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7033707865168539, tensor(2.3334)]
[tensor(-1.1023), 0.7056179775280899, tensor(2.3334)]
[tensor(-1.1023), 0.7078651685393258, tensor(2.3334)]
[tensor(-1.1023), 0.7078651685393258, tensor(2.3334)]
[tensor(-1.1023), 0.7078651685393258, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7123595505617978, tensor(2.3334)]
[tensor(-1.1023), 0.7146067415730337, tensor(2.3334)]
[tensor(-1.1023), 0.7146067415730337, tensor(2.3334)]
[tensor(-1.1023), 0.7146067415730337, tensor(2.3334)]
[tensor(-1.1023), 0.7146067415730337, tensor(2.3334)]
[tensor(-1.1023), 0.7146067415730337, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
[tensor(-1.1023), 0.7168539325842697, tensor(2.3334)]
early stopping at 49
[2023-01-20 03:30:49,684.684 dsw44922-6f76bf568-tbjcv:85253 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'selection_head.weight', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5052), 0.31685393258426964, 0.0]
[tensor(-1.7648), 0.501123595505618, tensor(0.7408)]
[tensor(-1.2921), 0.6292134831460674, tensor(1.8539)]
[tensor(-1.1826), 0.6674157303370787, tensor(2.1545)]
[tensor(-1.1347), 0.6853932584269663, tensor(2.2923)]
[tensor(-1.1347), 0.6853932584269663, tensor(2.2923)]
[tensor(-1.1040), 0.6898876404494382, tensor(2.3455)]
[tensor(-1.1040), 0.7056179775280899, tensor(2.4133)]
[tensor(-1.1040), 0.7056179775280899, tensor(2.4133)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1040), 0.7056179775280899, tensor(2.4133)]
[2023-01-20 03:36:21,201.201 dsw44922-6f76bf568-tbjcv:85287 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3169), 0.3775280898876405, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4735), 0.5932584269662922, tensor(1.4928)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1812), 0.6674157303370787, tensor(2.1559)]
[tensor(-1.1401), 0.6831460674157304, tensor(2.2757)]
[tensor(-1.0976), 0.701123595505618, tensor(2.4080)]
[tensor(-1.0976), 0.701123595505618, tensor(2.4080)]
[tensor(-1.0976), 0.701123595505618, tensor(2.4080)]
[tensor(-1.0976), 0.701123595505618, tensor(2.4080)]
[tensor(-1.0976), 0.7123595505617978, tensor(2.4080)]
[tensor(-1.0976), 0.7123595505617978, tensor(2.4080)]
[2023-01-20 03:41:48,924.924 dsw44922-6f76bf568-tbjcv:85321 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.2491), 0.3955056179775281, 0.0]
[tensor(-1.9747), 0.4696629213483146, tensor(0.3736)]
[tensor(-1.5601), 0.5595505617977528, tensor(1.2377)]
[tensor(-1.2166), 0.6651685393258427, tensor(2.1093)]
[tensor(-1.1132), 0.6696629213483146, tensor(2.2351)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1021), 0.6876404494382022, tensor(2.3361)]
[tensor(-1.1021), 0.6876404494382022, tensor(2.3361)]
[tensor(-1.1021), 0.6943820224719102, tensor(2.3557)]
[tensor(-1.1021), 0.7078651685393258, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
[tensor(-1.1021), 0.7101123595505618, tensor(2.4278)]
early stopping at 20
[2023-01-20 03:52:31,281.281 dsw44922-6f76bf568-tbjcv:85361 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mam_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5173), 0.3303370786516854, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5631), 0.5640449438202247, tensor(1.2572)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3456), 0.6292134831460674, tensor(1.8005)]
[tensor(-1.1360), 0.6786516853932584, tensor(2.2573)]
[tensor(-1.0987), 0.6786516853932584, tensor(2.2945)]
[tensor(-1.0987), 0.6786516853932584, tensor(2.2945)]
[tensor(-1.0904), 0.6966292134831461, tensor(2.3927)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
[tensor(-1.0884), 0.698876404494382, tensor(2.4060)]
early stopping at 24
[2023-01-20 04:05:21,815.815 dsw44922-6f76bf568-tbjcv:85404 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'start_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3988), 0.3865168539325843, 0.0]
[tensor(-1.6437), 0.5348314606741573, tensor(1.0304)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2717), 0.651685393258427, tensor(1.9867)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1764), 0.6561797752808989, tensor(2.1045)]
[tensor(-1.1143), 0.6943820224719102, tensor(2.3576)]
[tensor(-1.1143), 0.701123595505618, tensor(2.3710)]
[tensor(-1.1143), 0.701123595505618, tensor(2.3710)]
[tensor(-1.1143), 0.701123595505618, tensor(2.3710)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1143), 0.7033707865168539, tensor(2.3710)]
[tensor(-1.1143), 0.7033707865168539, tensor(2.3710)]
[2023-01-20 04:10:53,044.044 dsw44922-6f76bf568-tbjcv:85439 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.dense.bias', 'mam_head.dense.weight', 'end_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1378), 0.42921348314606744, tensor(0.0083)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4619), 0.6202247191011236, tensor(1.6392)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2023), 0.6741573033707865, tensor(2.1685)]
[tensor(-1.1243), 0.6921348314606741, tensor(2.3363)]
[tensor(-1.1023), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1023), 0.6921348314606741, tensor(2.3584)]
[tensor(-1.1023), 0.7056179775280899, tensor(2.4066)]
[tensor(-1.1023), 0.7056179775280899, tensor(2.4066)]
[tensor(-1.1023), 0.7056179775280899, tensor(2.4066)]
[tensor(-1.1023), 0.7078651685393258, tensor(2.4066)]
[2023-01-20 04:16:19,400.400 dsw44922-6f76bf568-tbjcv:85472 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'selection_head.bias', 'selection_head.weight', 'start_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0551), 0.44269662921348313, tensor(0.1584)]
[tensor(-1.8008), 0.5056179775280899, tensor(0.7273)]
[tensor(-1.4800), 0.6179775280898876, tensor(1.6099)]
[tensor(-1.2700), 0.6606741573033708, tensor(2.0334)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1477), 0.6853932584269663, tensor(2.2793)]
[tensor(-1.1385), 0.6898876404494382, tensor(2.3109)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1385), 0.7033707865168539, tensor(2.3761)]
[tensor(-1.1385), 0.7033707865168539, tensor(2.3761)]
[tensor(-1.1385), 0.7033707865168539, tensor(2.3761)]
[tensor(-1.1385), 0.7033707865168539, tensor(2.3761)]
[tensor(-1.1385), 0.7033707865168539, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
[tensor(-1.1385), 0.7056179775280899, tensor(2.3761)]
early stopping at 23
[2023-01-20 04:28:56,265.265 dsw44922-6f76bf568-tbjcv:85517 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-75 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-75 were not used when initializing ATModel: ['mlm_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.1905), 0.45617977528089887, tensor(0.0904)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.4723), 0.6022471910112359, tensor(1.5389)]
[tensor(-1.2670), 0.6382022471910113, tensor(1.9240)]
[tensor(-1.2055), 0.6764044943820224, tensor(2.1765)]
[tensor(-1.1337), 0.6876404494382022, tensor(2.3045)]
[tensor(-1.1209), 0.698876404494382, tensor(2.3734)]
[tensor(-1.1209), 0.698876404494382, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
[tensor(-1.1209), 0.701123595505618, tensor(2.3734)]
early stopping at 18
[2023-01-20 04:38:33,866.866 dsw44922-6f76bf568-tbjcv:85557 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['selection_head.weight', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'mlm_head.bias', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3976), 0.25842696629213485, 0.0]
[tensor(-1.5613), 0.5483146067415731, tensor(1.1802)]
[tensor(-1.2403), 0.6494382022471911, tensor(2.0069)]
[tensor(-1.2329), 0.6539325842696629, tensor(2.0368)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2106), 0.6808988764044944, tensor(2.1939)]
[tensor(-1.2106), 0.6808988764044944, tensor(2.1939)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1953), 0.6921348314606741, tensor(2.2654)]
[tensor(-1.1953), 0.6921348314606741, tensor(2.2654)]
[tensor(-1.1953), 0.6921348314606741, tensor(2.2654)]
[tensor(-1.1953), 0.698876404494382, tensor(2.2654)]
[2023-01-20 04:43:49,334.334 dsw44922-6f76bf568-tbjcv:85592 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['end_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mlm_head.bias', 'mam_head.bias', 'mam_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'selection_head.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.bias', 'mlm_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8828), 0.48089887640449436, tensor(0.5217)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3599), 0.6359550561797753, tensor(1.8199)]
[tensor(-1.1696), 0.6808988764044944, tensor(2.2349)]
[tensor(-1.1696), 0.6808988764044944, tensor(2.2349)]
[tensor(-1.1696), 0.6876404494382022, tensor(2.2349)]
[tensor(-1.1696), 0.6876404494382022, tensor(2.2349)]
[tensor(-1.1696), 0.6876404494382022, tensor(2.2349)]
[tensor(-1.1696), 0.701123595505618, tensor(2.2349)]
[tensor(-1.1696), 0.701123595505618, tensor(2.2349)]
[tensor(-1.1696), 0.701123595505618, tensor(2.2349)]
[2023-01-20 04:49:03,830.830 dsw44922-6f76bf568-tbjcv:85627 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'selection_head.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9784), 0.45617977528089887, tensor(0.3025)]
[tensor(-1.4354), 0.6, tensor(1.5646)]
[tensor(-1.3854), 0.6067415730337079, tensor(1.6483)]
[tensor(-1.2612), 0.6494382022471911, tensor(1.9860)]
[tensor(-1.2612), 0.6696629213483146, tensor(2.0531)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2612), 0.6786516853932584, tensor(2.1284)]
[tensor(-1.2612), 0.6898876404494382, tensor(2.1791)]
[tensor(-1.2566), 0.6898876404494382, tensor(2.1817)]
[tensor(-1.2566), 0.701123595505618, tensor(2.1827)]
[tensor(-1.2566), 0.701123595505618, tensor(2.1827)]
[tensor(-1.2566), 0.701123595505618, tensor(2.1827)]
[tensor(-1.2566), 0.701123595505618, tensor(2.1827)]
[tensor(-1.2566), 0.701123595505618, tensor(2.1827)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2566), 0.701123595505618, tensor(2.1827)]
[tensor(-1.2566), 0.7033707865168539, tensor(2.1827)]
[tensor(-1.2566), 0.7033707865168539, tensor(2.1827)]
[tensor(-1.2566), 0.7033707865168539, tensor(2.1827)]
[tensor(-1.2566), 0.7033707865168539, tensor(2.1827)]
[tensor(-1.2566), 0.7033707865168539, tensor(2.1827)]
[tensor(-1.2566), 0.7078651685393258, tensor(2.1827)]
[tensor(-1.2566), 0.7101123595505618, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7123595505617978, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
[tensor(-1.2566), 0.7168539325842697, tensor(2.1827)]
early stopping at 45
[2023-01-20 05:11:20,824.824 dsw44922-6f76bf568-tbjcv:85684 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.bias', 'mam_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'audio_encoder.audio_sep', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.7566), 0.4966292134831461, tensor(0.7265)]
[tensor(-1.3828), 0.6157303370786517, tensor(1.6958)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2142), 0.6629213483146067, tensor(2.1004)]
[tensor(-1.1258), 0.6831460674157304, tensor(2.2899)]
[tensor(-1.1258), 0.6831460674157304, tensor(2.2899)]
[tensor(-1.1258), 0.6943820224719102, tensor(2.2899)]
[tensor(-1.1258), 0.6943820224719102, tensor(2.2899)]
[tensor(-1.1258), 0.6943820224719102, tensor(2.2899)]
[tensor(-1.1258), 0.7033707865168539, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
[tensor(-1.1258), 0.7101123595505618, tensor(2.2899)]
early stopping at 20
[2023-01-20 05:21:26,458.458 dsw44922-6f76bf568-tbjcv:85726 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0870), 0.451685393258427, tensor(0.1714)]
[tensor(-1.3650), 0.6449438202247191, tensor(1.8597)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2233), 0.6561797752808989, tensor(2.0576)]
[tensor(-1.1466), 0.6696629213483146, tensor(2.2018)]
[tensor(-1.1409), 0.701123595505618, tensor(2.3647)]
[tensor(-1.1409), 0.7056179775280899, tensor(2.3675)]
[tensor(-1.1409), 0.7056179775280899, tensor(2.3675)]
[tensor(-1.1409), 0.7078651685393258, tensor(2.3675)]
[tensor(-1.1409), 0.7078651685393258, tensor(2.3675)]
[tensor(-1.1409), 0.7078651685393258, tensor(2.3675)]
[2023-01-20 05:26:28,697.697 dsw44922-6f76bf568-tbjcv:85761 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'mam_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'start_prediction_head.0.bias', 'selection_head.bias', 'mlm_head.dense.bias', 'mam_head.dense.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.weight', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.8113), 0.4966292134831461, tensor(0.6719)]
[tensor(-1.2878), 0.6719101123595506, tensor(2.0718)]
[tensor(-1.1531), 0.6741573033707865, tensor(2.2177)]
[tensor(-1.1384), 0.7078651685393258, tensor(2.4010)]
[tensor(-1.1384), 0.7078651685393258, tensor(2.4010)]
[tensor(-1.1384), 0.7078651685393258, tensor(2.4010)]
[tensor(-1.1384), 0.7123595505617978, tensor(2.4010)]
[tensor(-1.1384), 0.7325842696629213, tensor(2.4307)]
[tensor(-1.1384), 0.7325842696629213, tensor(2.4307)]
[tensor(-1.1384), 0.7325842696629213, tensor(2.4307)]
[2023-01-20 05:31:43,953.953 dsw44922-6f76bf568-tbjcv:85794 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'selection_head.bias', 'start_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'mlm_head.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.6511), 0.5235955056179775, tensor(0.9669)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2394), 0.6651685393258427, tensor(2.0864)]
[tensor(-1.2068), 0.6719101123595506, tensor(2.1528)]
[tensor(-1.1300), 0.6808988764044944, tensor(2.2745)]
[tensor(-1.1200), 0.6921348314606741, tensor(2.3407)]
[tensor(-1.1200), 0.6921348314606741, tensor(2.3407)]
[tensor(-1.1200), 0.7123595505617978, tensor(2.4338)]
[tensor(-1.1200), 0.7213483146067415, tensor(2.4752)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
[tensor(-1.1200), 0.7303370786516854, tensor(2.4958)]
early stopping at 19
[2023-01-20 05:41:10,157.157 dsw44922-6f76bf568-tbjcv:85834 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 2.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.weight', 'end_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.weight', 'mlm_head.decoder.weight', 'mam_head.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mam_head.dense.bias', 'selection_head.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.6173), 0.5460674157303371, tensor(1.1130)]
[tensor(-1.3570), 0.6314606741573033, tensor(1.8003)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1781), 0.6606741573033708, tensor(2.1253)]
[tensor(-1.0739), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0739), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0739), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0739), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0739), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0739), 0.6966292134831461, tensor(2.4092)]
[tensor(-1.0739), 0.7033707865168539, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7101123595505618, tensor(2.4092)]
[tensor(-1.0739), 0.7123595505617978, tensor(2.4092)]
[tensor(-1.0739), 0.7146067415730337, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
[tensor(-1.0739), 0.7168539325842697, tensor(2.4092)]
early stopping at 31
[2023-01-20 05:56:59,947.947 dsw44922-6f76bf568-tbjcv:85884 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mlm_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'mam_head.decoder.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mam_head.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.layer_norm.bias', 'end_prediction_head.0.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.6221), 0.18202247191011237, 0.0]
[tensor(-1.8853), 0.4764044943820225, tensor(0.4967)]
[tensor(-1.4185), 0.6112359550561798, tensor(1.6377)]
[tensor(-1.2305), 0.651685393258427, tensor(2.0280)]
[tensor(-1.2013), 0.6561797752808989, tensor(2.0796)]
[tensor(-1.1806), 0.6764044943820224, tensor(2.2015)]
[tensor(-1.1553), 0.6764044943820224, tensor(2.2155)]
[tensor(-1.1553), 0.6966292134831461, tensor(2.3095)]
[tensor(-1.1553), 0.6966292134831461, tensor(2.3135)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1553), 0.7033707865168539, tensor(2.3296)]
[2023-01-20 06:02:04,336.336 dsw44922-6f76bf568-tbjcv:85918 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.bias', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'selection_head.weight', 'mlm_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3940), 0.2898876404494382, 0.0]
[tensor(-1.5232), 0.5617977528089888, tensor(1.2858)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1890), 0.6741573033707865, tensor(2.1818)]
[tensor(-1.1714), 0.6786516853932584, tensor(2.2218)]
[tensor(-1.1170), 0.6853932584269663, tensor(2.3100)]
[tensor(-1.1170), 0.6853932584269663, tensor(2.3100)]
[tensor(-1.1170), 0.6898876404494382, tensor(2.3100)]
[tensor(-1.1170), 0.7056179775280899, tensor(2.3690)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1170), 0.7056179775280899, tensor(2.3690)]
[tensor(-1.1170), 0.7056179775280899, tensor(2.3690)]
[2023-01-20 06:07:18,664.664 dsw44922-6f76bf568-tbjcv:85952 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'mlm_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'start_prediction_head.0.weight', 'selection_head.weight', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3207), 0.36629213483146067, 0.0]
[tensor(-1.7788), 0.4943820224719101, tensor(0.6931)]
[tensor(-1.7335), 0.4943820224719101, tensor(0.7384)]
[tensor(-1.4242), 0.6112359550561798, tensor(1.6320)]
[tensor(-1.2055), 0.6561797752808989, tensor(2.0754)]
[tensor(-1.1751), 0.6786516853932584, tensor(2.2181)]
[tensor(-1.1591), 0.6966292134831461, tensor(2.3241)]
[tensor(-1.1534), 0.6966292134831461, tensor(2.3241)]
[tensor(-1.1534), 0.6966292134831461, tensor(2.3241)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1534), 0.6966292134831461, tensor(2.3241)]
[tensor(-1.1534), 0.7146067415730337, tensor(2.3837)]
[tensor(-1.1534), 0.7146067415730337, tensor(2.3837)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
[tensor(-1.1534), 0.7191011235955056, tensor(2.3861)]
early stopping at 23
[2023-01-20 06:18:54,885.885 dsw44922-6f76bf568-tbjcv:85996 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.weight', 'mam_head.layer_norm.bias', 'selection_head.weight', 'mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'mlm_head.dense.bias', 'end_prediction_head.0.weight', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'start_prediction_head.0.bias', 'mlm_head.layer_norm.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'mlm_head.decoder.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1940), 0.4134831460674157, 0.0]
[tensor(-1.7154), 0.5370786516853933, tensor(0.9700)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1903), 0.6764044943820224, tensor(2.1917)]
[tensor(-1.1248), 0.6853932584269663, tensor(2.3021)]
[tensor(-1.1248), 0.6876404494382022, tensor(2.3021)]
[tensor(-1.1109), 0.6898876404494382, tensor(2.3386)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1109), 0.701123595505618, tensor(2.3735)]
[tensor(-1.1109), 0.7078651685393258, tensor(2.3735)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
[tensor(-1.1109), 0.7168539325842697, tensor(2.4164)]
early stopping at 19
[2023-01-20 06:28:25,232.232 dsw44922-6f76bf568-tbjcv:86034 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mlm_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'selection_head.weight', 'mam_head.layer_norm.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.weight', 'audio_encoder.audio_sep', 'mam_head.dense.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5769), 0.22696629213483147, 0.0]
[tensor(-1.8255), 0.550561797752809, tensor(0.9273)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.4744), 0.5752808988764045, tensor(1.4020)]
[tensor(-1.2587), 0.6494382022471911, tensor(1.9885)]
[tensor(-1.1076), 0.6831460674157304, tensor(2.3081)]
[tensor(-1.1076), 0.6876404494382022, tensor(2.3081)]
[tensor(-1.1076), 0.7033707865168539, tensor(2.4059)]
[tensor(-1.1076), 0.7033707865168539, tensor(2.4059)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1076), 0.7033707865168539, tensor(2.4059)]
[tensor(-1.1076), 0.7033707865168539, tensor(2.4059)]
[2023-01-20 06:33:46,440.440 dsw44922-6f76bf568-tbjcv:86068 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['audio_encoder.audio_sep', 'mam_head.dense.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'mam_head.bias', 'mlm_head.dense.weight', 'end_prediction_head.0.bias', 'selection_head.weight', 'mam_head.decoder.bias', 'mam_head.layer_norm.bias', 'mlm_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mlm_head.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.1244), 0.44269662921348313, tensor(0.0891)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4304), 0.6337078651685393, tensor(1.7382)]
[tensor(-1.2233), 0.6741573033707865, tensor(2.1475)]
[tensor(-1.1494), 0.6786516853932584, tensor(2.2439)]
[tensor(-1.0842), 0.698876404494382, tensor(2.4102)]
[tensor(-1.0842), 0.698876404494382, tensor(2.4102)]
[tensor(-1.0842), 0.698876404494382, tensor(2.4102)]
[tensor(-1.0842), 0.698876404494382, tensor(2.4102)]
[tensor(-1.0842), 0.7078651685393258, tensor(2.4102)]
[tensor(-1.0842), 0.7078651685393258, tensor(2.4102)]
[2023-01-20 06:38:49,622.622 dsw44922-6f76bf568-tbjcv:86102 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.dense.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mlm_head.dense.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'mam_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.decoder.bias', 'mlm_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mlm_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.bias', 'selection_head.weight', 'selection_head.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2567), 0.42921348314606744, 0.0]
[tensor(-1.7425), 0.5393258426966292, tensor(0.9541)]
[tensor(-1.7050), 0.5573033707865168, tensor(1.0815)]
[tensor(-1.4302), 0.6112359550561798, tensor(1.6260)]
[tensor(-1.2006), 0.6584269662921348, tensor(2.0915)]
[tensor(-1.1526), 0.6921348314606741, tensor(2.3080)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.0860), 0.7056179775280899, tensor(2.4421)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
[tensor(-1.0860), 0.7078651685393258, tensor(2.4429)]
early stopping at 18
[2023-01-20 06:48:15,808.808 dsw44922-6f76bf568-tbjcv:86142 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 32 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.dense.weight', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'mam_head.decoder.weight', 'mam_head.decoder.bias', 'start_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mlm_head.layer_norm.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-1.9936), 0.5146067415730337, tensor(0.5794)]
[tensor(-1.6239), 0.5662921348314607, tensor(1.2076)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.2054), 0.6696629213483146, tensor(2.1429)]
[tensor(-1.1351), 0.6921348314606741, tensor(2.3255)]
[tensor(-1.1210), 0.6921348314606741, tensor(2.3255)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1037), 0.6921348314606741, tensor(2.3255)]
[tensor(-1.1037), 0.698876404494382, tensor(2.3691)]
[tensor(-1.1037), 0.7033707865168539, tensor(2.3772)]
[tensor(-1.1037), 0.7101123595505618, tensor(2.4134)]
[tensor(-1.1037), 0.7101123595505618, tensor(2.4134)]
[tensor(-1.1037), 0.7101123595505618, tensor(2.4134)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1037), 0.7101123595505618, tensor(2.4134)]
[tensor(-1.1037), 0.7101123595505618, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
[tensor(-1.1037), 0.7146067415730337, tensor(2.4134)]
early stopping at 24
[2023-01-20 07:00:11,092.092 dsw44922-6f76bf568-tbjcv:86183 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.decoder.weight', 'audio_encoder.audio_sep', 'selection_head.weight', 'mlm_head.dense.bias', 'mam_head.layer_norm.bias', 'start_prediction_head.0.bias', 'selection_head.bias', 'mam_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.weight', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'mam_head.bias', 'mam_head.dense.weight', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4911), 0.32808988764044944, 0.0]
[tensor(-1.7776), 0.5056179775280899, tensor(0.7505)]
[tensor(-1.3396), 0.6292134831460674, tensor(1.8065)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.1816), 0.6786516853932584, tensor(2.2116)]
[tensor(-1.1779), 0.6786516853932584, tensor(2.2116)]
[tensor(-1.1427), 0.6921348314606741, tensor(2.3179)]
[tensor(-1.1248), 0.6921348314606741, tensor(2.3179)]
[tensor(-1.1248), 0.6921348314606741, tensor(2.3179)]
[tensor(-1.1248), 0.6921348314606741, tensor(2.3179)]
[tensor(-1.1248), 0.6921348314606741, tensor(2.3179)]
[2023-01-20 07:05:52,041.041 dsw44922-6f76bf568-tbjcv:86218 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.dense.weight', 'start_prediction_head.0.bias', 'selection_head.weight', 'mlm_head.bias', 'selection_head.bias', 'end_prediction_head.0.weight', 'mam_head.decoder.weight', 'mam_head.bias', 'mam_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'end_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4128), 0.33707865168539325, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5260), 0.5528089887640449, tensor(1.2381)]
[tensor(-1.2282), 0.6382022471910113, tensor(1.9628)]
[tensor(-1.1600), 0.698876404494382, tensor(2.3344)]
[tensor(-1.1377), 0.698876404494382, tensor(2.3344)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1190), 0.698876404494382, tensor(2.3529)]
[tensor(-1.1190), 0.698876404494382, tensor(2.3529)]
[tensor(-1.1190), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.1190), 0.7078651685393258, tensor(2.4090)]
[tensor(-1.1190), 0.7078651685393258, tensor(2.4090)]
[2023-01-20 07:11:19,533.533 dsw44922-6f76bf568-tbjcv:86254 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mlm_head.decoder.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mlm_head.decoder.weight', 'mlm_head.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight', 'selection_head.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'selection_head.bias', 'mam_head.dense.bias', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.3183), 0.3595505617977528, 0.0]
[tensor(-2.0455), 0.43595505617977526, tensor(0.1343)]
[tensor(-1.5736), 0.5595505617977528, tensor(1.2241)]
[tensor(-1.2392), 0.6449438202247191, tensor(1.9855)]
[tensor(-1.1083), 0.6741573033707865, tensor(2.2625)]
[tensor(-1.0995), 0.6741573033707865, tensor(2.2625)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.0995), 0.6831460674157304, tensor(2.3125)]
[tensor(-1.0995), 0.701123595505618, tensor(2.3767)]
[tensor(-1.0995), 0.701123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.701123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.701123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.701123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.701123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7033707865168539, tensor(2.3780)]
[tensor(-1.0995), 0.7078651685393258, tensor(2.3780)]
[tensor(-1.0995), 0.7078651685393258, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
[tensor(-1.0995), 0.7101123595505618, tensor(2.3780)]
early stopping at 27
[2023-01-20 07:25:46,283.283 dsw44922-6f76bf568-tbjcv:86302 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mam_head.layer_norm.bias', 'audio_encoder.audio_sep', 'selection_head.weight', 'end_prediction_head.0.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.bias', 'start_prediction_head.0.weight', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.bias', 'end_prediction_head.0.weight', 'mlm_head.layer_norm.bias', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.bias', 'mam_head.decoder.bias', 'mam_head.decoder.weight', 'mlm_head.dense.bias', 'mam_head.dense.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.5214), 0.3438202247191011, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.5443), 0.5707865168539326, tensor(1.3096)]
[tensor(-1.3400), 0.6202247191011236, tensor(1.7611)]
[tensor(-1.1505), 0.6853932584269663, tensor(2.2765)]
[tensor(-1.1114), 0.6898876404494382, tensor(2.3381)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1114), 0.6898876404494382, tensor(2.3381)]
[tensor(-1.1114), 0.6943820224719102, tensor(2.3598)]
[tensor(-1.1114), 0.6943820224719102, tensor(2.3598)]
[tensor(-1.1114), 0.6943820224719102, tensor(2.3598)]
[tensor(-1.1114), 0.6943820224719102, tensor(2.3598)]
[tensor(-1.1114), 0.6943820224719102, tensor(2.3598)]
[tensor(-1.1114), 0.7033707865168539, tensor(2.3598)]
[tensor(-1.1114), 0.7033707865168539, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7056179775280899, tensor(2.3598)]
[tensor(-1.1114), 0.7078651685393258, tensor(2.3598)]
[tensor(-1.1114), 0.7078651685393258, tensor(2.3598)]
[tensor(-1.1114), 0.7078651685393258, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
[tensor(-1.1114), 0.7101123595505618, tensor(2.3598)]
early stopping at 39
[2023-01-20 07:46:49,046.046 dsw44922-6f76bf568-tbjcv:86361 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['start_prediction_head.0.bias', 'mlm_head.layer_norm.weight', 'mam_head.layer_norm.bias', 'mam_head.dense.weight', 'mlm_head.layer_norm.bias', 'mlm_head.dense.weight', 'mlm_head.decoder.bias', 'end_prediction_head.0.weight', 'mlm_head.bias', 'mam_head.layer_norm.weight', 'audio_encoder.audio_sep', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mam_head.dense.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'selection_head.bias', 'end_prediction_head.0.bias', 'mam_head.bias', 'mam_head.decoder.bias', 'mlm_head.decoder.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.4030), 0.3842696629213483, 0.0]
[tensor(-1.6505), 0.5483146067415731, tensor(1.0911)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.3254), 0.6337078651685393, tensor(1.8431)]
[tensor(-1.1550), 0.6719101123595506, tensor(2.2045)]
[tensor(-1.1144), 0.6764044943820224, tensor(2.2676)]
[tensor(-1.0892), 0.6921348314606741, tensor(2.3714)]
[tensor(-1.0721), 0.6921348314606741, tensor(2.3714)]
[tensor(-1.0721), 0.6921348314606741, tensor(2.3714)]
[tensor(-1.0721), 0.7078651685393258, tensor(2.4155)]
[tensor(-1.0721), 0.7078651685393258, tensor(2.4155)]
[2023-01-20 07:52:20,206.206 dsw44922-6f76bf568-tbjcv:86396 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 10 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.bias', 'mam_head.layer_norm.bias', 'mam_head.decoder.bias', 'audio_encoder.audio_sep', 'start_prediction_head.0.bias', 'mlm_head.dense.bias', 'mam_head.layer_norm.weight', 'end_prediction_head.0.bias', 'selection_head.bias', 'mam_head.dense.weight', 'mam_head.dense.bias', 'end_prediction_head.0.weight', 'mlm_head.dense.weight', 'mlm_head.bias', 'mam_head.bias', 'mlm_head.decoder.weight', 'mlm_head.decoder.bias', 'mam_head.decoder.weight', 'selection_head.weight', 'mlm_head.layer_norm.weight', 'start_prediction_head.0.weight']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2329), 0.4067415730337079, 0.0]
[tensor(-1.4779), 0.6247191011235955, tensor(1.6457)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.2794), 0.6404494382022472, tensor(1.9229)]
[tensor(-1.1443), 0.6921348314606741, tensor(2.3164)]
[tensor(-1.1390), 0.701123595505618, tensor(2.3666)]
[tensor(-1.1140), 0.7033707865168539, tensor(2.4029)]
[tensor(-1.1140), 0.7033707865168539, tensor(2.4029)]
[tensor(-1.1140), 0.7033707865168539, tensor(2.4029)]
[tensor(-1.1140), 0.7191011235955056, tensor(2.4655)]
[tensor(-1.1140), 0.7191011235955056, tensor(2.4655)]
[2023-01-20 07:57:44,998.998 dsw44922-6f76bf568-tbjcv:86431 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 2 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.layer_norm.weight', 'mam_head.layer_norm.weight', 'mam_head.dense.weight', 'start_prediction_head.0.weight', 'mlm_head.dense.bias', 'mlm_head.layer_norm.bias', 'selection_head.bias', 'mlm_head.decoder.weight', 'mam_head.dense.bias', 'mam_head.decoder.weight', 'mlm_head.bias', 'mlm_head.dense.weight', 'audio_encoder.audio_sep', 'mam_head.bias', 'mam_head.decoder.bias', 'end_prediction_head.0.weight', 'start_prediction_head.0.bias', 'mlm_head.decoder.bias', 'selection_head.weight', 'mam_head.layer_norm.bias', 'end_prediction_head.0.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
[tensor(-2.0515), 0.42921348314606744, tensor(0.0946)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-1.8055), 0.5056179775280899, tensor(0.7226)]
[tensor(-1.5025), 0.6247191011235955, tensor(1.6211)]
[tensor(-1.1998), 0.6674157303370787, tensor(2.1373)]
[tensor(-1.1280), 0.6943820224719102, tensor(2.3439)]
[tensor(-1.1206), 0.6966292134831461, tensor(2.3625)]
[tensor(-1.1206), 0.6966292134831461, tensor(2.3625)]
[tensor(-1.1206), 0.6966292134831461, tensor(2.3625)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.1206), 0.6966292134831461, tensor(2.3625)]
[tensor(-1.1206), 0.6966292134831461, tensor(2.3625)]
[tensor(-1.1206), 0.6966292134831461, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
[tensor(-1.1206), 0.698876404494382, tensor(2.3625)]
early stopping at 27
[2023-01-20 08:12:26,710.710 dsw44922-6f76bf568-tbjcv:86480 INFO utils.py:30] NOTICE: PAIDEBUGGER is turned off.
/home/pai/lib/python3.6/site-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.
  from cryptography import x509
Model v4.3.4-100 datasize 960 batchsize 24 epochs 50 lr 1.0e-05 gradacc 1 task mintrec last_conv_layer no cl_mode no cl_steps 3 prompt False train_mode 
has_audio_cls True multi audio False v2 Trueprompt False bert True scheduler_type 0.0
fused layers 1
Some weights of the model checkpoint at /mnt/ewwe/yts/saved_models/v4.3.4-100 were not used when initializing ATModel: ['mlm_head.bias', 'mam_head.dense.bias', 'mlm_head.decoder.bias', 'mam_head.layer_norm.weight', 'mlm_head.decoder.weight', 'mlm_head.layer_norm.weight', 'mlm_head.layer_norm.bias', 'mam_head.bias', 'selection_head.weight', 'mam_head.decoder.weight', 'mam_head.dense.weight', 'mam_head.layer_norm.bias', 'start_prediction_head.0.weight', 'end_prediction_head.0.weight', 'selection_head.bias', 'mlm_head.dense.weight', 'mam_head.decoder.bias', 'end_prediction_head.0.bias', 'start_prediction_head.0.bias', 'audio_encoder.audio_sep', 'mlm_head.dense.bias']
- This IS expected if you are initializing ATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
downstreamv2 mintrec
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[tensor(-2.2507), 0.42247191011235957, 0.0]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
[tensor(-1.4661), 0.6112359550561798, tensor(1.5901)]
[tensor(-1.2619), 0.6494382022471911, tensor(1.9852)]
[tensor(-1.1768), 0.6943820224719102, tensor(2.2951)]
[tensor(-1.0849), 0.6943820224719102, tensor(2.3421)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
[tensor(-1.0849), 0.7101123595505618, tensor(2.4295)]
early stopping at 16
